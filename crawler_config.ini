[CRAWLER]
# Seed URLs (komma-getrennt)
SEED_URLS = https://de.wikipedia.org/wiki/Künstliche_Intelligenz, https://de.wikipedia.org/wiki/Maschinelles_Lernen

# Erlaubte Domains (komma-getrennt) - nur deutsche Wikipedia
ALLOWED_DOMAINS = de.wikipedia.org

# Ignorierte Wikipedia-Namespaces (komma-getrennt)
IGNORED_NAMESPACES = Benutzer:, Datei:, Spezial:, Kategorie:, Hilfe:, Diskussion:, Vorlage:, Portal:, MediaWiki:

# Batch-Größe für Best-N-First
BATCH_SIZE = 15

# Maximale Anzahl zu crawlender Seiten
MAX_PAGES = 500

# Maximale Anzahl relevanter Seiten (Beendigungskriterium)
MAX_RELEVANT_PAGES = 150

# Relevanz-Schwellenwert (0.0 - 1.0)
# 0.25 = mäßig relevant, 0.5 = stark relevant, 0.75 = sehr stark relevant
RELEVANCE_THRESHOLD = 0.25

# Maximale Laufzeit in Minuten
MAX_RUNTIME_MINUTES = 30

# Report-Intervall in Sekunden (veraltet, siehe REPORTING)
REPORT_INTERVAL_SECONDS = 30

# Maximale Frontier-Größe (Speicher-Optimierung)
FRONTIER_MAX_SIZE = 3000

[REPORTING]
# Reporting nach Anzahl gecrawlter Seiten
REPORT_PAGES_INTERVAL = 50

# Reporting nach Zeit in Sekunden
REPORT_TIME_INTERVAL_SECONDS = 60

[WEIGHTS]
# Gewichtung für Link-Relevanz vs. Elterndokument-Relevanz
# Höherer Link-Weight hilft beim Thema zu bleiben
LINK_WEIGHT = 0.5
PARENT_WEIGHT = 0.5

# Gewichtungen für Elterndokument-Teile
TITLE_WEIGHT = 0.4
HEADING_WEIGHT = 0.3
PARAGRAPH_WEIGHT = 0.3

[MULTIPLIERS]
# Multiplikatoren für Textteile (ganzzahlig, >= 1)
# Verwendet von Keyword und VectorSpace Spiders
TITLE_MULTIPLIER = 4
HEADING_MULTIPLIER = 2
PARAGRAPH_MULTIPLIER = 1

[KEYWORDS]
# Keywords für Keyword-basierte Strategie (komma-getrennt)
# Erweiterte Keyword-Liste für bessere Trefferquote
KEYWORDS = künstliche intelligenz, machine learning, deep learning, neuronale netze, algorithmus, datenanalyse, ki, artificial intelligence, neural network, datenwissenschaft, maschinelles lernen, algorithmen, daten, modell, training, klassifikation, regression, clustering, supervised learning, unsupervised learning, reinforcement learning, tensorflow, pytorch, scikit-learn, python, nlp, computer vision, bilderkennung, spracherkennung, robotik, automation

[VECTORSPACE]
# Themenprofil-Text für Vektorraum-Modell
TOPIC_PROFILE = Künstliche Intelligenz (KI) ist ein Teilgebiet der Informatik, das sich mit der Automatisierung intelligenten Verhaltens und dem maschinellen Lernen befasst. Machine Learning und Deep Learning sind wichtige Untergebiete der KI. Neuronale Netze, Algorithmen und Datenanalyse sind zentrale Konzepte. Die Anwendung von KI reicht von der Bilderkennung über Sprachverarbeitung bis zur Robotik. Maschinelles Lernen ermöglicht es Computern, aus Daten zu lernen ohne explizit programmiert zu werden. Wichtige Methoden sind Klassifikation, Regression und Clustering. Frameworks wie TensorFlow und PyTorch unterstützen die Entwicklung. Python ist die dominierende Programmiersprache im Bereich KI.

# Vektorisierungs-Modus: tfidf oder tf_norm (default: tf_norm)
MODE = tf_norm

[NAIVEBAYES]
# Pfade für Naive Bayes Modell
MODEL_PATH = models/naive_bayes_model.pkl
VECTORIZER_PATH = models/vectorizer.pkl
TRAINING_DATA_PATH = training_data/training_samples.txt

[SCRAPY]
# Scrapy-spezifische Einstellungen
USER_AGENT = TopicalCrawler/1.0 (+http://example.com/bot)
ROBOTSTXT_OBEY = True
CONCURRENT_REQUESTS = 2
DOWNLOAD_DELAY = 0.5
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 0.5
AUTOTHROTTLE_MAX_DELAY = 3.0
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0