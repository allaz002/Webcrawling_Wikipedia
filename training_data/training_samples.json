[
  {
    "label": 1,
    "text": "Überwachtes Lernen – Wikipedia Überwachtes Lernen Inhaltsverzeichnis Vorgehen Herausforderungen Definitionen Regressionsprobleme Klassifikationsprobleme Siehe auch Einzelnachweise Zu wenige Daten für die Komplexität der „wahren Funktion“ Nicht repräsentative Daten Fehlerhafte Zielwerte Viele nicht relevante Merkmale Auswahl eines Modells Verzerrung-Varianz-Dilemma Lineare Regression Weitere Beispiele für Modelle zur Lösung von Regressionsproblemen Logistische Regression Perzeptron-Algorithmus Weitere Beispiele für überwachte Lernalgorithmen zur Klassifikation Überwachtes Lernen(englischsupervised learning) ist eine wichtige Kategorie desMaschinellen Lernens. Dabei wird ein Lernalgorithmus mit Datensätzen trainiert und validiert, die für jede Eingabe einen passenden Ausgabewert enthalten. Man bezeichnet solche Datensätze als markiert oder gelabelt. Ein Beispiel wäre ein Datensatz mit Bildern von Katzen und Hunden, dem jemand (in der Regel ein Mensch) zu jedem Bild ein Label hinzugefügt hat, das die Information enthält, ob auf dem Bild eine Katze oder ein Hund abgebildet ist. Mit dem Datensatz wird dann einAlgorithmustrainiert, der mit Hilfe der Information der Label eine Funktion erzeugt, die idealerweise auch bei neuen Bildern korrekt erkennt, ob sie einen Hund oder eine Katze zeigen. Häufige Anwendungen für das überwachte Lernen sindKlassifikationundRegression.[1] Die Methode richtet sich also nach vorgegebenen Antworten für die Ausgabe. Die Ergebnisse des Lernprozesses können mit den bekannten, richtigen Antworten verglichen, also „überwacht“, werden.[2]Liegen die Ergebnisse der Ausgabe in einer stetigen Verteilung vor, deren Ergebnisse beliebige quantitative Werte eines vorgegebenen Wertebereiches annehmen kann, spricht man meistens von einemRegressionsproblem.[3]Ein Beispiel für ein solches Regressionsproblem ist die Vorhersage der Preisentwicklung von Häusern auf Basis von bestimmten Variablen oder das Bestimmen des Alters einer Person aus anderen Informationen über die Person. Es geht demnach meistens um Vorhersagen.[3]Liegen die Ergebnisse hingegen in diskreter Form vor bzw. sind die Werte qualitativ, spricht man von einem Klassifikationsproblem. Ein Beispiel hierfür ist, zu bestimmen, ob es sich bei einer E-Mail um Spam oder keinen Spam handelt.[4]Der folgende Artikel beschreibt das Vorgehen beim überwachten Lernen und stellt einige Methoden zur Lösung von Regressionsproblemen respektive zur Lösung von Klassifikationsproblemen vor. Um ein bestimmtes Problem mit überwachtem Lernen zu lösen, werden folgende Schritte durchgeführt:[5] Eine Herausforderung ist die Menge der verfügbaren Trainingsdaten in Relation zur Komplexität der „wahren Funktion“ (Klassifikator- oder Regressionsfunktion). Wenn die wahre Funktion einfach ist, dann kann ein „unflexibler“ Lernalgorithmus mit hoher Verzerrung und geringer Varianz aus einer kleinen Datenmenge lernen. Wenn die wahre Funktion jedoch sehr komplex ist (z. B. weil sie komplexe Interaktionen zwischen vielen verschiedenen Eingabemerkmalen beinhaltet und sich in verschiedenen Teilen des Eingaberaums unterschiedlich verhält), dann wird die Funktion nur aus einer sehr großen Menge von Trainingsdaten und unter Verwendung eines „flexiblen“ Lernalgorithmus mit geringer Vorspannung und hoher Varianz erlernbar sein.[3] Das Modell kann nur dann gut verallgemeinern, wenn die Trainingsdaten alle zu verallgemeinernden Fälle gut repräsentieren. Auch ein großer Datensatz kann für einzelne Fälle zu wenige Daten enthalten. Eine solcheStichprobenverzerrunghat zur Folge, dass das trainierte Modell für diese Fälle nicht gut verallgemeinern kann.[5]:57 Ein weiteres mögliches Problem sind sogenannte „Ausreißer“ in den Zielwerten. Wenn einige vorgegebene Zielwerte falsch sind (aufgrund von menschlichen Fehlern oder Sensorfehlern) und der Lernalgorithmus versucht, eine Funktion zu finden, die genau zu allen Zielwerten passt, dann kommt es zu einerÜberanpassung. In der Praxis gibt es mehrere Ansätze um Probleme mit den Ausgabewerten zu verhindern, wie z. B. frühzeitiges Anhalten des Algorithmus zur Vermeidung von Überanpassung sowie das Erkennen und Entfernen der Ausreißer vor dem Training des überwachten Lernalgorithmus. Es gibt mehrere Algorithmen, die Ausreißer identifizieren und deren Entfernen ermöglichen.[3] Das Modell kann nur dann gut verallgemeinern, wenn die Trainingsdaten alle relevanten Merkmale enthalten und nur wenige nicht relevante Merkmale. Deshalb sollte man analysieren, welche Merkmale wirklich ausgewertet werden müssen, um das Problem zu lösen, und das Modell nur mit diesen Merkmalen trainieren.[5]:59 Es steht eine breite Palette von Lernalgorithmen zur Verfügung, von denen jeder seine Stärken und Schwächen hat. Man wählt ein bestimmtes Modell aus, indem man implizit Annahmen über die Daten macht. Beispielsweise wählt man einlineares Modell, wenn man davon ausgeht, dass die Daten linear sind und Abweichungen der Datenpunkte von einer berechneten Geraden ignoriert werden können. Für andere Daten ist vielleicht einBinärbaumoder einkünstliches neuronales Netzbesser geeignet. DieNo-free-Lunch-Theoremebesagen, dass eine bestimmte Strategie, die in einemTeilbereichbesser ist als eine andere, in einem anderen Teilbereich schlechter sein muss. Um das beste Modell zu finden, ohne Annahmen über die Daten zu treffen, müsste man alle bekannten Modelle evaluieren. In der Praxis trifft man Annahmen über die Verteilung der Daten, wählt wenige Modelle aus, die dafür gut geeignet sind und evaluiert nur diese.[5]:66 Beim überwachten Lernen muss oft ein Kompromiss zwischen Verzerrung und Varianz gefunden werden.[6] Die Varianz bezieht sich auf den Betrag, um den sich die Vorhersage ändern würde, wenn das Modell sie auf der Basis eines anderen Trainingsdatensatzes treffen würde. Im Idealfall sollte die Vorhersage bei unterschiedlichen Trainingsdatensätzen nur wenig variieren. Dann kann das trainierte Modell gut verallgemeinern. Bei hoher Varianz führen jedoch unterschiedliche Trainingsdatensätze, also kleine Änderungen an den Trainingsdaten, zu sehr unterschiedlichen Vorhersagen. Dann kann das trainierte Modell nicht gut verallgemeinern. Grundsätzlich haben flexiblere statistische Methoden eine höhere Varianz, weil sie den Trainingsdatensatz sehr genau abbilden können und dadurch beiverrauschtenoder wenig repräsentativen Daten das Risiko dafür steigt, dass das Modell viele Fehler macht, wenn es Vorhersagen für völlig neue Daten macht.[3]Dieses Problem bezeichnet man alsÜberanpassung. Man kann eine Überanpassung dadurch reduzieren, dass man das Modell vereinfacht, indem man beispielsweise vor dem Training die erlaubten Wertebereiche für die Modellparameter einschränkt. Dieses Vorgehen nennt man Regularisierung.[5]:60 Wenn das Modell andererseits zu einfach ist, dann gibt die Verzerrung den Fehler an, der dadurch entsteht, dass man das reale Problem, das sehr kompliziert sein kann, durch ein einfacheres Modell angenähert hat. Zum Beispiel geht eine lineare Regression davon aus, dass ein lineares Problem vorliegt. In der Realität liegen jedoch selten einfache lineare Probleme vor, und so führt die Durchführung einer linearen Regression oft zu einer gewissen Verzerrung zwischen der Vorhersage und dem erwarteten Ausgabewert.[3] Um im Folgenden mathematische Zusammenhänge besser darstellen zu können, werden folgende Definitionen verwendet[7]: Das Ziel von überwachtem Lernen im Falle von Regressionsproblemen ist meist, auf Basis von bestimmten erklärenden Variablen wie Größe oder Farbe eines Hauses etwas über diesen Sachverhalt vorauszusagen. Der Sachverhalt kann dabei grundlegend verschieden sein, beispielsweise der Preis von Häusern in bestimmter Umgebung oder die Entwicklung des Preises einer Aktie am nächsten Tag. Das Ziel ist es dementsprechend den Zusammenhang zwischen der erklärenden und der erklärten Variable anhand eines Trainingsdatensatzes zu erlernen und mit Hilfe dieses Zusammenhangs zukünftige Ereignisse, die noch nicht bekannt sind, vorherzusagen. Ein Beispiel für einen solchen Lernalgorithmus, der Vorhersagen treffen kann, ist dielineare Regression. Dielineare Regressionist sehr einfaches Verfahren zur Durchführung einer Regression.[3]Das dazu verwendete Modell ist linear in den Parametern, wobei die abhängige Variable eine Funktion der unabhängigen Variablen ist.[3]Bei der Regression sind die Ausgaben der unbekannten Funktion verrauscht (fehlerbehaftet). wobeihθ(x)∈R{\\displaystyle h_{\\theta }(x)\\in \\mathbb {R} }die unbekannte Funktion darstellt undε{\\displaystyle \\varepsilon }für zufälligesRauschensteht. Die Erklärung für das Rauschen liegt darin, dass es zusätzliche verborgene Variablen gibt, die unbeobachtbar sind.[8]Hierzu wird die folgende Regressionsfunktion verwendet: bzw. in Vektorschreibweise: Dieθi{\\displaystyle \\theta _{i}}sind dabei die Parameter der Funktion undx{\\displaystyle x}ist der Vektor, welcher die erklärenden Variablen enthält. Dementsprechend gewichten die Parameter die einzelnen erklärenden Variablen und werden deshalb auch oft als Regressionsgewichte bezeichnet. Um nun aus den erklärenden Variablen eine möglichst genaue Annäherung an den Outputy{\\displaystyle y}zu erhalten, wird eineKostenfunktionaufgestellt. Diese Funktion beschreibt diemittlere quadratische Abweichung, die dadurch entsteht, dass die Hypothesenfunktion die zu erklärende Variabley{\\displaystyle y}nur approximiert und nicht genau darstellt. Die Kostenfunktion wird beispielsweise durch die folgende Gleichung beschrieben: Das Ziel ist es nun, diese Abweichung zu minimieren. Dazu müssen dieModellparameterso gewählt werden, dass sie die jeweiligen x-Werte richtig gewichten, um dem gewünschten y-Wert möglichst nahe zu kommen. Die optimalen Modellparameter können auf zwei verschiedene Weisen ermittelt werden. Eine Methode ist das sogenannteGradientenverfahren. Diese Methode umfasst folgende Schritte[9]: Dies ist in der folgenden Gleichung für ein einzelnes Beispiel dargestellt (Alpha steht hierbei für die Lernrate): Diese Gleichung wird so oft wiederholt bisy(i)−h(x)=0{\\displaystyle y(i)-h(x)=0}bzw. bis diese Differenz minimiert wurde und der Parameter somit seinen optimalen Wert gefunden hat. Eine weitere Methode, die verwendet werden kann, sind die sogenannten Normalgleichungen (sieheMultiple lineare Regression). Mit dieser kann die Minimierung der Kostenfunktion explizit und ohne Rückgriff auf einen iterativen Algorithmus durchgeführt werden, indem die folgende Formel implementiert wird: Diese Formel liefert uns die optimalen Werte der Parameter. In der folgenden Tabelle[9]sind die Vor- und Nachteile von Gradientenverfahren und der Normalgleichung dargestellt. Anders als bei Regressionsproblemen kann der Outputy{\\displaystyle y}bei Klassifikationsproblemen nur diskrete Werte annehmen, die meistens in Form vonqualitativen Datenvorliegen. Beispielsweise kann die Aufgabe darin bestehen, zu bestimmen, ob es sich bei einer neuen E-Mail um Spam handelt oder nicht. Dazu wird jedem Trainingsbeispiel eine der beiden Klassen „Spam“ oder „Kein Spam“ zugeordnet. Die Trainingsdaten dann bestehen aus den erklärenden Variablenx(i){\\displaystyle x^{(i)}}und dem Outputy{\\displaystyle y}, bei dem der Wert1{\\displaystyle 1}der Klasse „Spam“ entspricht und der Wert0{\\displaystyle 0}der Klasse „Kein Spam“. Man unterscheidet zudem zwischen Binären Klassifikationsproblemen und Klassifikationsproblemen, bei denen multiple Klassen vorliegen. Ein Beispiel hierfür wäre zu klassifizieren von welcher von drei Marken ein gekauftes Produkt ist (Die Klassen sind in diesem Fall Marke A, B oder C). Dielogistische Regressionist eine oft eingesetzte Methode zum Lösen von binären Klassifikationsproblemen. Sie schätzt zunächst, mit welcher Wahrscheinlichkeit ein gegebener Datenpunkt zu einer bestimmten Klasse gehört. Danach entscheidet sie, ob die berechnete Wahrscheinlichkeit größer ist als 50 %. In diesem Fall gibt sie diese Klasse als Ergebnis aus. Andernfalls gibt sie die andere Klasse als Ergebnis aus.[9] Die Funktion zur Schätzung der Wahrscheinlichkeiten muss Werte zwischen 0 und 1 annehmen. Die logistische Regression verwendet dazu dieSigmoidfunktion, gegeben durch folgende Gleichung: Sie lässt sich auf die Hypothesenfunktion folgendermaßen anwenden: Da g(z) immer Werte zwischen 0 und 1 liefert, liegen auch die Werte vonh(x){\\displaystyle h(x)}zwischen 0 und 1. Dies lässt sich im folgenden Graphen erkennen: Die Einteilung einer Beobachtung in eine bestimmte Klasse erfolgt folgendermaßen: Um eine möglichst akkurate Zuordnung der Datenpunkte zu den Zielklassen zu erreichen, werden die Parameter, ähnlich wie bei der linearen Regression, mit Hilfe einer geeigneten Funktion optimiert. Wir nehmen dazu den folgenden Zusammenhang an: P(y=0∣x;θ)=1−hθ(x){\\displaystyle P(y=0\\mid x;\\theta )=1-h_{\\theta }(x)} Diese Gleichungen bedeuten, dass die Wahrscheinlichkeit, dass ein gegebener Datenpunkt der Klasse 1 angehört, durch das Ergebnis der Hypothesenfunktionh(x){\\displaystyle h(x)}gegeben ist. Daraus folgt, dass die allgemeinebedingte Wahrscheinlichkeitfür einen bestimmten Output y für einen gegebenen Datenpunkt x durch die folgende Funktion gegeben ist: Multipliziert man diese Wahrscheinlichkeit nun für alle Trainingsdatenpunkte, erhält man die Formel für die sogenannte „Likelihood“ eines bestimmten Parameters. Hat man bei der linearen Regression die mittlere quadratische Abweichung minimiert, um die optimalen Werte für die Parameter zu erhalten, maximiert man bei der logistischen Regression dieLikelihood-Funktion, um die optimalen Werte der Parameter zu erhalten. Dieses Verfahren wird alsMaximum-Likelihood-Methodebezeichnet. Um die Maximierung zu erleichtern, wird oft auch die Log-Likelihood-Funktion gebildet: Von dieser Funktion muss nun die Steigung berechnet werden, wozu der sogenanntegradient ascentverwendet wird. Er funktioniert ähnlich wie das bei der linearen Regression angewendete Gradientenverfahren, außer dass er eine Addition anstatt einer Subtraktion durchführt, da die Log-Likelihood-Funktion maximiert und nicht minimiert werden soll. Durch die folgende Gleichung erhält man somit den optimierten Wert des Parameters: In den 1960er Jahren wurde der sogenanntePerzeptron-Algorithmus entwickelt. Er wurde entsprechend den Vorstellungen der damaligen Zeit über die Funktionsweise des Gehirns aufgebaut.[7] Der wesentliche Unterschied zwischen dem Perzepton Algorithmus und der logistischen Regression ist, dass die Funktionh(x){\\displaystyle h(x)}entweder den Wert 0 oder den Wert 1 annimmt, aber nicht wie bei der logistischen Regression einen beliebigen Wert zwischen 0 und 1.[7]Dies wird sichergestellt, indem die Funktiong(z){\\displaystyle g(z)}nicht wie bei der logistischen Regression mit Hilfe einer Sigmoid Funktion einen Wert zwischen 0 und 1 annimmt, sondern entsprechend der Formeln: entweder genau 0 oder genau 1 entspricht. Und die Updating Regel ist ebenfalls beschrieben durch: Diese Gleichung sieht sehr ähnlich aus zu den Lernprozessen der vorherigen Algorithmen. Es muss jedoch beachtet werden, dass durch die Definition vong(z){\\displaystyle g(z)}Perzeptron einen nicht sonderlich fließenden Lernprozess hat, da der Fehler, der entsteht, wenn ein Input durch den Algorithmus falsch klassifiziert wird, entweder wesentlich überschätzt oder unterschätzt werden kann, in demh(x){\\displaystyle h(x)}nur 1 oder 0 annehmen kann. So wird beispielsweise, wennz=−0,0001{\\displaystyle z=-0{,}0001}beträgt sowie wennz=−100{\\displaystyle z=-100}beträgt in beiden Fällen die Klasse 0 vorhergesagt. Gehören die Beobachtungen allerdings in Wahrheit Klasse 1 an, so werden die Parameter in beiden Fällen, um den gleichen Wert angepasst.[7] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Vorgehen 2Herausforderungen 2.1Zu wenige Daten für die Komplexität der „wahren Funktion“ 2.2Nicht repräsentative Daten 2.3Fehlerhafte Zielwerte 2.4Viele nicht relevante Merkmale 2.5Auswahl eines Modells 2.6Verzerrung-Varianz-Dilemma 3Definitionen 4Regressionsprobleme 4.1Lineare Regression 4.2Weitere Beispiele für Modelle zur Lösung von Regressionsproblemen 5Klassifikationsprobleme 5.1Logistische Regression 5.2Perzeptron-Algorithmus 5.3Weitere Beispiele für überwachte Lernalgorithmen zur Klassifikation 6Siehe auch 7Einzelnachweise العربية تۆرکجه বাংলা Bosanski Català کوردی Čeština Dansk Ελληνικά English Español Eesti فارسی"
  },
  {
    "label": 1,
    "text": "Alan Turing – Wikipedia Alan Turing Inhaltsverzeichnis Leben und Wirken Karriere und Forschung Verfolgung wegen Homosexualität und Turings Tod Offizielle Entschuldigung, Danksagung und Rehabilitierung Nachwirkungen der Rehabilitierung Postume Ehrungen Verschiedenes Werke Literatur (Auswahl) Filme (Auswahl) Theaterstücke Musik Comic Weblinks Einzelnachweise Kindheit und Jugend Collegezeit und theoretische Arbeiten Kryptoanalyse Arbeit an frühen Computern – Der Turing-Test Arbeit an mathematischen Problemen der theoretischen Biologie Veröffentlichungen Patente Theorie und Diskussion Geschichte und Biographie Belletristik Alan Mathison TuringOBE,[2]FRS[3][ˈælən ˈmæθɪsən ˈtjʊəɹɪŋ] (*23. Juni1912inLondon; †7. Juni1954inWilmslow,Cheshire) war einbritischerLogiker,Mathematiker,KryptoanalytikerundInformatiker. Er gilt heute als einer der einflussreichsten Theoretiker der frühen Computerentwicklung undInformatik. Turing schuf einen großen Teil der theoretischen Grundlagen für die moderneInformations-undComputertechnologie. Als richtungsweisend erwiesen sich auch seine Beiträge zurtheoretischen Biologie. Das von ihm entwickelte Berechenbarkeitsmodell derTuringmaschinebildet eines der Fundamente derTheoretischen Informatik. Während desZweiten Weltkriegeswar er maßgeblich an derEntzifferungder mit der deutschenRotor-ChiffriermaschineEnigmaverschlüsseltendeutschenFunksprüchebeteiligt. Der Großteil seiner Arbeiten blieb auch nach Kriegsende unter Verschluss. Turing entwickelte 1953 eines der erstenSchachprogramme, dessen Berechnungen er mangelsHardwareselbst vornahm. Nach ihm benannt sind derTuring Award, die bedeutendste Auszeichnung in der Informatik, sowie derTuring-Testzum Überprüfen des Vorhandenseins vonkünstlicher Intelligenz.[4] Im März 1952 wurde Turing wegen seinerHomosexualität, die damals noch als Straftat verfolgt wurde, zur chemischenKastrationverurteilt.[5][6]Turing erkrankte in Folge der Hormonbehandlung an einerDepressionund starb etwa zwei Jahre später durchSuizid. Im Jahr 2009 sprach der britische PremierministerGordon Browneine offizielle Entschuldigung im Namen der Regierung für die „entsetzliche Behandlung“ Turings aus und würdigte dessen „außerordentliche Verdienste“ während des Krieges. Noch 2011 wurde eineBegnadigungtrotz einerPetitionabgelehnt. 2013, am Weihnachtsabend, sprach KöniginElisabeth II.postum ein „Royal Pardon“ (Königliche Begnadigung) aus.[7][8][9][10] Alan Turings Vater, Julius Mathison Turing, war britischer Beamter beimIndian Civil Service. Er und seine FrauEthel Sara Turing(geborene Stoney) wünschten, dass ihre Kinder in Großbritannien aufwüchsen.[11]Deshalb kehrte die Familie vor Alans Geburt ausChhatrapur, damalsBritisch-Indien, nachLondon-Paddingtonzurück, wo Alan Turing am 23. Juni 1912 zur Welt kam. Da der Staatsdienst seines Vaters noch nicht beendet war, reiste dieser im Frühjahr 1913 erneut nach Indien, wohin ihm seine Ehefrau im Herbst folgte. Turing und sein älterer Bruder John wurden nach St Leonards-on-the-Sea,Hastings, in die Familie eines pensionierten Obersts und dessen Frau in Pflege gegeben. In der Folgezeit pendelten die Eltern zwischen England und Indien, bis sich Turings Mutter 1916 entschied, längere Zeit in England zu bleiben, und die Söhne wieder zu sich nahm. Schon in früher Kindheit zeigte sich die hohe Begabung und Intelligenz Turings. Es wird berichtet, dass er sich innerhalb von drei Wochen selbst das Lesen beibrachte und sich schon früh zu Zahlen und Rätseln hingezogen fühlte.[11] Im Alter von sechs Jahren wurde Turing auf die private Tagesschule St Michael’s in St Leonards-on-the-Sea geschickt, wo die Schulleiterin frühzeitig seine Begabung bemerkte. 1926, im Alter von vierzehn Jahren, wechselte er auf dieSherborne SchoolinDorset. Sein erster Schultag dort fiel auf einen Generalstreik in England. Turing war jedoch so motiviert, dass er die 100 Kilometer vonSouthamptonzur Schule allein auf dem Fahrrad zurücklegte und dabei nur einmal in der Nacht an einer Gaststätte Halt machte; so berichtete jedenfalls die Lokalpresse. Turings Drang zur Naturwissenschaft traf bei seinen Lehrern in Sherborne auf wenig Gegenliebe; sie setzten eher auf Geistes- als auf Naturwissenschaften. Trotzdem zeigte Turing auch weiterhin bemerkenswerte Fähigkeiten in den von ihm geliebten Bereichen. So löste er für sein Alter fortgeschrittene Aufgabenstellungen, ohne zuvor irgendwelche Kenntnisse der elementarenInfinitesimalrechnungerworben zu haben. Im Jahr 1928 stieß Turing auf die ArbeitenAlbert Einsteins. Er verstand sie nicht nur, sondern entnahm einem Text selbständig NewtonsBewegungsgesetz, obwohl dieses nicht explizit erwähnt wurde. Turings Widerstreben, für Geisteswissenschaften genauso hart wie für Naturwissenschaften zu arbeiten, hatte zur Folge, dass er einige Male durch die Prüfungen fiel. Weil dies seinen Notendurchschnitt verschlechterte, musste er 1931 auf ein College zweiter Wahl gehen, dasKing’s College,Cambridge, entgegen seinem Wunsch, amTrinity Collegezu studieren. Er studierte von 1931 bis 1934 unterGodfrey Harold Hardy(1877–1947), einem renommierten Mathematiker, der denSadleirian Chairin Cambridge innehatte, das zu der Zeit ein Zentrum der mathematischen Forschung war. In seiner für diesen Zweig der Mathematik grundlegenden ArbeitOn Computable Numbers, with an Application to the “Entscheidungsproblem”(28. Mai 1936) formulierte Turing die ErgebnisseKurt Gödelsvon 1931 neu. Er ersetzte dabei Gödels universelle, arithmetisch-basierte formale Sprache durch einen einfachen gedanklichen Mechanismus, eine abstrakt-formale Zeichenketten verarbeitende mathematische Maschine, die heute unter dem NamenTuringmaschinebekannt ist. („Entscheidungsproblem“ verweist auf eines der 23 wichtigsten offenen Probleme der Mathematik des 20. Jahrhunderts, vorgestellt vonDavid Hilbert1900 auf dem2. Internationalen Mathematiker-Kongressin Paris [„Hilbertsche Probleme“].) Turing bewies, dass solch ein Gerät in der Lage ist, „jedes vorstellbare mathematische Problem zu lösen, sofern dieses auch durch einenAlgorithmusgelöst werden kann“. Turingmaschinen sind bis zum heutigen Tag einer der Schwerpunkte derTheoretischen Informatik, nämlichder Berechenbarkeitstheorie. Mit Hilfe der Turingmaschine gelang Turing der Beweis, dass es keine Lösung für das „Entscheidungsproblem“ gibt. Er zeigte, dass die Mathematik in gewissem Sinne unvollständig ist, weil es allgemein keine Möglichkeit gibt, festzustellen, ob eine beliebige, syntaktisch korrekt gebildete mathematische Aussage beweisbar oder widerlegbar ist. Dazu bewies er, dass dasHalteproblemfür Turingmaschinen nicht lösbar ist, d. h., dass es nicht möglich ist, algorithmisch zu entscheiden, ob eine Turingmaschine, angesetzt auf eine Eingabe (initiale Bandbelegung), jemals zum Stillstand kommen wird, das heißt die Berechnung terminiert. Turings Beweis wurde erst nach dem vonAlonzo Church(1903–1995) mit Hilfe desLambda-Kalkülsgeführten Beweis veröffentlicht; unabhängig davon ist Turings Arbeit beträchtlich einfacher undintuitivzugänglich. Auch war der Begriff der „Universellen (Turing-) Maschine“ neu, einer Maschine, welche jede beliebige andere Turing-Maschine simulieren kann. Die Eingabe für diese Maschine ist also ein kodiertes Programm, das von der universellen Maschine interpretiert wird, und der Startwert, auf den es angewendet werden soll. Alle bis heute definierten Berechenbarkeitsbegriffe haben sich (bis auf die Abbildung von Worten auf Zahlen und umgekehrt) als äquivalent erwiesen. 1938 und 1939 verbrachte Turing zumeist an derPrinceton Universityund studierte dort unter Alonzo Church. 1938 erwarb er den Doktorgrad in Princeton. Seine Doktorarbeit führte den Begriff der „Hypercomputation“ ein, bei der Turingmaschinen zu sogenanntenOrakel-Maschinenerweitert werden. So wurde das Studium von nicht-deterministisch lösbaren Problemen ermöglicht. Nach seiner Rückkehr nach Cambridge im Jahr 1939 besuchte Turing Vorlesungen des österreichisch-britischen PhilosophenLudwig Wittgenstein(1889–1951) über die Grundlagen der Mathematik.[12]Die Vorlesungen wurden Wort für Wort aus den Notizen der Studenten rekonstruiert, einschließlich Zwischenrufe von Turing und anderen Studenten.[13]Die beiden diskutierten und stritten vehement: Turing verteidigte den mathematischenFormalismus, während Wittgenstein der Meinung war, dass Mathematik überbewertet sei und keine absolute Wahrheit zutage bringen könne.[14] Während desZweiten Weltkriegswar Turing einer der herausragenden Wissenschaftler bei den erfolgreichen Versuchen inBletchley Park(B.P.),[15]verschlüsselte deutsche Funksprüche zu entziffern. Er steuerte einige mathematische Modelle bei, um sowohl dieEnigma(siehe auch:Letchworth-Enigma) als auch dieLorenz-Schlüsselmaschine(siehe auch:Turingery) zubrechen. Die Einblicke, die Turing bei derKryptoanalysederFish-Verschlüsselungengewann, halfen später bei der Entwicklung des ersten digitalen, programmierbaren elektronischenRöhrencomputersENIAC. Konstruiert vonMax Newmanund seinem Team und gebaut in derPost Office Research StationinDollis Hillvon einem vonTommy Flowersangeführten Team im Jahr 1943,entzifferteColossusdieLorenz-Maschine. Auch konzipierte Turing die nach ihm benanntenBombes. Sie waren Nachfolgerinnen der von dem PolenMarian RejewskientwickeltenBombaund dienten zur Ermittlung derSchlüsselvonEnigma-Nachrichten. Dabei handelte es sich um ein elektromechanisches Gerät, das im Prinzip mehrere Enigma-Maschinen beinhaltete und so in der Lage war, viele mögliche Schlüsseleinstellungen der Enigma-Nachrichten durchzutesten und zu eliminieren, bis eine mögliche Lösung gefunden war (Reductio ad absurdum;deutschZurückführung bis zum Widerspruch). Turings Mitwirkung als einer der wichtigstenCodeknackerbei der Entzifferung der Enigma war bis in die 1970er Jahre geheim; nicht einmal seine engsten Freunde wussten davon. Die Entzifferung geheimer deutscher Funksprüche war eine kriegsentscheidende Komponente für den Sieg der Alliierten imU-Boot-Kriegund imAfrikafeldzug.[16] Von 1945 bis 1948 war Turing imNational Physical Laboratoryin Teddington tätig, wo er am Design der ACE(Automatic Computing Engine)arbeitete. Der Name der Maschine ist abgeleitet von derAnalytical Enginedes MathematikersCharles Babbage, dessen Werk Turing zeitlebens bewunderte. Ab 1948 lehrte Turing an derUniversität Manchesterund wurde im Jahr 1949 stellvertretender Direktor der Computerabteilung. Hier arbeitete er an der Software für einen der ersten echten Computer, denManchester Mark I, und gleichzeitig weiterhin verschiedenen theoretischen Arbeiten. InComputing machinery and intelligence(Mind, Oktober 1950) griff Turing die Problematik derkünstlichen Intelligenzauf und schlug denTuring-Testals Kriterium vor, ob eine Maschine dem Menschen vergleichbar denkfähig ist. Da der Denkvorgang nicht formalisierbar ist, betrachtet der Test nur die Antworten einer Maschine im Dialog mit einem Menschen, d. h. das kommunikative Verhalten der Maschine. Wenn dieses von einem menschlichen Verhalten nicht unterscheidbar erscheint, soll von maschineller Intelligenz gesprochen werden. Er beeinflusste durch die Veröffentlichung die Entwicklung der künstlichen Intelligenz maßgeblich. 1952 schrieb er dasSchachprogrammTurochamp. Da es keine Computer mit ausreichender Leistung gab, um es auszuführen, übernahm Turing dessen Funktion und berechnete jeden Zug selbst. Dies dauerte bis zu 30 Minuten pro Zug. Das einzige schriftlich dokumentierte Spiel verlor er gegen einen Kollegen.[17] Von 1952 bis zu seinem Tod 1954 arbeitete Turing an mathematischen Problemen dertheoretischen Biologie. Er veröffentlichte 1952 eineArbeitzum ThemaThe Chemical Basis ofMorphogenesis. In diesem Artikel wurde erstmals ein Mechanismus beschrieben, wieReaktions-Diffusions-Systemespontan Strukturen entwickeln können. Dieser alsTuring-Mechanismusbekannte Prozess steht noch heute im Mittelpunkt vieler chemisch-biologischerStrukturbildungstheorien. Turings weiteres Interesse galt dem Vorkommen derFibonacci-Zahlenin der Struktur von Pflanzen. Spätere Arbeiten blieben bis zur Veröffentlichung seiner gesammelten Werke 1992 unveröffentlicht. 1952 half der 19-jährige Arnold Murray, zu dem Turing eine gleichgeschlechtliche Beziehung hatte, einem Komplizen dabei, in Turings Haus einzubrechen. Turing meldete daraufhin einen Diebstahl bei der Polizei, die ihm als Folge der Ermittlungen eine sexuelle Beziehung zu Murray vorwarf. DahomosexuelleHandlungen zu dieser Zeit in England – wie in den meisten anderen Ländern – strafbar waren, wurde Turing wegen „grober Unzucht und sexueller Perversion“ angeklagt. Turing sah keinen Anlass, sich wegen dieser Vorwürfe zu rechtfertigen. Nach seiner Verurteilung zu einer Gefängnisstrafe wurde er vor die Wahl gestellt, die Haftstrafe anzutreten oder – da zu seiner Zeit Homosexualität von weiten Teilen derPsychiatrieals Krankheit angesehen wurde – sich behandeln zu lassen. Er entschied sich für die ärztliche Behandlung, zu der auch eine medikamentöse Behandlung mit dem HormonÖstrogengehörte. Diesem wurde eine triebhemmende Wirkung zugeschrieben. Die Behandlung dauerte ein Jahr und führte zu Nebenwirkungen wie derVergrößerung der Brustdrüse. Auch wenn er seine körperlichen Veränderungen mit Humor kommentierte, muss die Verweiblichung seiner Konturen den sportlichen Läufer und Tennisspieler schwer getroffen haben. Turing erkrankte an einerDepression.[18]Im Herbst 1952 begann Turing seine Therapie bei dem aus Berlin stammenden und seit 1939 in Manchester lebendenPsychoanalytikerFranz Greenbaum. Dieser war ein AnhängerC. G. Jungsund war ihm von Freunden als für seinen Fall verständnisvoll empfohlen worden. Turing entwickelte auch ein freundschaftliches Verhältnis zur Familie Greenbaum, die er auch privat besuchte.[19] 1954 starb Turing im Alter von 41 Jahren, wahrscheinlich entsprechend der offiziellen Feststellung durchSuizid, an einerCyanidvergiftung. Dem Anschein nach von einem vergifteten Apfel herrührend, den man halb aufgegessen neben ihm auffand. Die Ermittler versäumten es jedoch, den Apfel auf Gift untersuchen zu lassen. Es wird berichtet, dass Turing seit 1938, nachdem er den FilmSchneewittchen und die sieben Zwergegesehen hatte, immer wieder die VerseDip the apple in the brew / Let the sleeping death seep through(In der deutschen Version: „Tauch den Apfel tief hinein / bis das Gift wird in ihm sein.“) sang. Der These, dass Turings Tod ein Unfall im Zusammenhang mit einem chemischen Versuch war, wird von Andrew Hodges, einem seiner Biographen, entschieden widersprochen.[19]Unter seinen Biographen ist die Annahme verbreitet, die Auswirkungen der Hormonbehandlung seien die Hauptursache für den Suizid gewesen. Ab etwa den späten 2000er Jahren unternahmen britische Bürger eine Reihe von öffentlichkeitswirksamen Aktivitäten, um das von Turing erlittene Unrecht bekannt zu machen und seine formaleRehabilitierungzu erreichen, also einen Widerruf oder eine Aufhebung des damaligen Urteils. Dies führte im Jahr 2013 zum Erfolg. Im Jahr 2009 unterzeichneten rund 30.000 Briten eine bei der Regierung eingereichte Online-Petition, in der einepostumeEntschuldigung von der britischen Regierung gefordert wurde.[20]Der Initiator der Kampagne, der britische Programmierer John Graham-Cumming, regte an, Alan Turing eineRitterwürdezu verleihen.[21][22]Am 10. September 2009 veröffentlichte der damalige britische PremierministerGordon Browneine Erklärung, in der er im Namen der britischen Regierung die Verfolgung Turings bedauerte und seinen außerordentlichen Beitrag während desZweiten Weltkriegswürdigte. Dabei spielte er auch auf denstrategischen Vorteil der Alliiertendurch die Entschlüsselung der „Enigma“ an und unterstrich deren Bedeutung: „Es ist keine Übertreibung, wenn man sagt, dass ohne seinen herausragenden Beitrag die Geschichte des Zweiten Weltkriegs durchaus deutlich anders hätte verlaufen können. Er ist tatsächlich eine der wenigen hervorzuhebenden Personen, deren einzigartiger Beitrag half, den Kriegsverlauf zu wenden. Die tiefe Dankbarkeit, die wir ihm schulden, macht es daher umso grauenhafter, dass er derart inhuman behandelt wurde. Im Namen der britischen Regierung und all derer, die dank Alans Arbeiten in Freiheit leben, bin ich daher sehr stolz zu sagen: Es tut uns leid, Sie hatten so viel Besseres verdient.“ Da die Strafverfolgung seiner sexuellen Ausrichtung damals gesetzeskonform war, wurde eine nachträgliche Aufhebung der Verurteilung Turings zunächst von offizieller Seite als unmöglich dargestellt. Noch 2012 weigerte sich die Regierung von Browns NachfolgerDavid Cameron, 49.000 Homosexuelle, die nach demCriminal Law Amendment Actvon 1885 verurteilt worden waren, postum zu rehabilitieren.[23] Im Jahr 2013 wurde bekannt, dass die britische Regierung nun doch die Absicht habe, Turing zurehabilitieren. DasOberhausmitgliedJohn Sharkeybeantragte dies. Das konservative Mitglied des OberhausesTariq Ahmadkündigte die Zustimmung der Regierung an. Der Liberaldemokrat Sharkey hatte in den 1960er Jahren inManchesterMathematik bei Turings einzigem DoktorandenRobin Gandystudiert. Eine dritte Lesung des Antrags beraumte die Regierung für Ende Oktober an.[23][24] Am 24. Dezember 2013 wurde Alan Turing durch ein allein dem Monarchen zustehendes besonderes Gnadenrecht begnadigt, das sogenannteRoyal Pardon. JustizministerChris Graylinghatte diese Begnadigung beiElisabeth II.beantragt. Turing gilt damit auch als offiziell rehabilitiert.[25] Im April 2016 entschuldigte sich Robert Hannigan, der damalige Leiter des britischen GeheimdienstesGCHQ, für die Behandlung von Homosexuellen durch seine Institution und bezog dies ausdrücklich auch auf Alan Turing.[26] Anfang 2015 verlangten Mitglieder der Familie Alan Turings unter weiterer, teils prominenter Unterstützung (Stephen Fry, Turing-DarstellerBenedict Cumberbatch) in einerPetitionan das britische Parlament dieRehabilitierungauch aller anderen der in England unter den Homosexuellen-Gesetzen Verurteilten. Die Petition wurde von ca. 500.000 Personen unterschrieben und sollte von Turings Großneffen Nevil Hunt und der Großnichte Rachel Barns überreicht werden.[27] Am 21. Oktober 2016 lehnte das britische Parlament einen Gesetzesentwurf ab, der eine Rehabilitierung in Form einer generellen Rehabilitierung aller lebenden, früher für Homosexualität verurteilten Personen vorsah. Dieser Gesetzesentwurf ging einigen zu weit, anderen nicht weit genug.[28]Am 31. Januar 2017 wurde von Königin Elisabeth II. ein Gesetz in Kraft gesetzt, das aufbauend auf der Begnadigung von Turing allen Männern die Strafe aufhebt, falls zu dem Zeitpunkt beide über 16 Jahre alt waren, als sie den geahndeten Akt in gegenseitigem Einvernehmen vollzogen. Ausgenommen sind weiterhin Verurteilungen wegen homosexueller Handlungen in öffentlichen Toiletten. Das Gesetz schließt auch bereits verstorbene Personen ein. Ein noch lebender Betroffener kann beantragen, dass die Strafe aus seiner polizeilichen Führungsakte gestrichen wird, und Historiker können darauf hinweisen, dass eine Verurteilung verstorbener Personen nach geltendem Recht ungültig ist. Das Gesetz, das von JustizministerSam Gyimahals „Turings Gesetz“ bezeichnet wurde, ist eine Ergänzung zumPolicing and Crime Actund nimmt keinen Bezug auf andere Gesetze, unter denen homosexuelle Handlungen verfolgt werden konnten. VonMichael Cashman, einem der Initiatoren des Gesetzes, wurden jedoch weitere Vereinbarungen abgesichert, die einen entsprechend umfassenden Straferlass für alle homosexuellen Handlungen ermöglichen.[29] Am 23. Juni 1998, der Turings 86. Geburtstag gewesen wäre, wurde eineBlaue Plakette(English Heritage Blue Plaque)an seinem Geburtshaus in Warrington Crescent, London, enthüllt.[30] Am 2. März 1999 wurde derAsteroid(10204) Turingnach ihm benannt. Eine Turing-Statue wurde am 23. Juni 2001 in Manchester enthüllt. Sie steht imSackville Park, zwischen den wissenschaftlichen Gebäuden derUniversität Manchesterund dem bei Homosexuellen beliebten Viertel derCanal Street. An seinem 50. Todestag, dem 7. Juni 2004, wurde zum Gedenken an Turings frühzeitigen Tod eine Tafel an seinem früheren Haus „Hollymeade“ in Wilmslow enthüllt. DerTuring Awardwird jährlich von derAssociation for Computing Machineryan Personen verliehen, die bedeutende Beiträge zur Informatik geleistet haben. Er wird weithin als „Nobelpreis“ der Informatik angesehen. DerBletchley Park Trusthat am 19. Juni 2007 eine Statue Turings inBletchley Parkenthüllt. Die Skulptur wurde vonStephen Kettlegestaltet, der als Material für sein KunstwerkwalisischenSchieferverwendete.[31] Im „Turing-Jahr 2012“ fanden zu Alan Turings hundertstem Geburtstag weltweit Veranstaltungen zur Würdigung seiner Leistungen und zum Gedenken daran statt.[32] Im Jahr 2014 wurde er in dieHall of Honor(Ehrenhalle) des US-GeheimdienstesNSA(National Security Agency)aufgenommen.[33] Am 15. Juli 2019 kündigte derGouverneur der Bank of England,Mark Carney, eine 50-Pfund-Note mit Turings Bildnis für 2021 an. Am 25. März 2021 wurde sie erstmals herausgegeben. An diesem Tag hisste dieBank of EnglanddieRegenbogenfahnestatt wie üblich diebritische Nationalflagge.[34]Die Banknote ist neben Turings Bild mit einem Zitat von ihm versehen: “This is only a foretaste of what is to come, and only the shadow of what is going to be.”[35] Das 2021 etablierte Programm derbritischen Regierungzur Förderung desAuslandsstudiums, das nach demAustritt des Vereinigten Königreichs aus der EUdie Beteiligung amErasmus-Programmersetzen soll, ist nach Alan Turing benannt.[36] Am 26. Mai 2022 wurde einTriebzugvonGreat Western Railway(800 008) während einer Zeremonie imBahnhof London Paddingtonzu Ehren von Alan Turing benannt. Die Taufe des Zuges wurde von seinen Nichten Janet Robinson und Inagh Payne durchgeführt. Dieser Zug weist außerdem an beiden Enden eine Regenbogen-Lackierung zu Ehren derLGBTQ-Communityauf.[37] Wichtige Veröffentlichungen Deutsche Ausgabe und Übersetzungen Adcock|Alexander|Atkin|Babbage|Batey|Benenson|Birch|Cairncross|Cassels|Chadwick|Clarke|Cooper|Currer‑Briggs|Denniston|Foss|Gillis|Golombek|Good|de Grey|Hall|Herivel|Hilton|Hinsley|Jeffreys|Jenkins|Knox|Lever|Lewis|Michie|Milner‑Barry|Newman|Noskwith|Oswald|Palmer|Plumb|Rees|Roberts|Rock|Shackleton Bailey|Strachey|Tiltman|Travis|Turing|Tutte|Twinn|Welchman|Whitehead|Wylie|Yoxall 1999:William F. Friedman|Elizebeth S. Friedman|Herbert O. Yardley|Laurance Safford|Frank B. Rowlett|Abraham Sinkov|Solomon Kullback|Ralph J. Canine 2000:Louis W. Tordella|Joseph J. Rochefort|Agnes Meyer Driscoll 2001:Howard C. Barlow|Mahlon E. Doyle|Sydney Jaffe|John E. Morrison 2002:Thomas H. Dyer|Norman Wild|Richard A. Leibler|Mitford M. Mathews|Charles C. Tevis|Julia Ward 2003:Lambros D. Callimahos|Lowell K. Frazer|Juanita Moody|Howard E. Rosenblum 2004:Dorothy T. Blum|James R. Chiles|Meredith Gardner|John Tiltman 2005:William Blankinship|Francis Raven|Arthur Salemme|Joseph N. Wenger 2006:Bernard Ardisana|Edward A. Everett|Cecil J. Phillips|James W. Pryde|Thomas E. Tremain 2007:Jacob Gurin|Robert J. Hermann|Samuel S. Snyder|Milton Zaslow 2008:Benson K. Buffman|Chareles L. Gandy|Alfred M. Gray|Oliver R. Kirby|Donald M. Showers 2009:Richard A. Day|Minnie M. Kenny|Doyle E. Larson|Arthur J. Levenson 2010:Joseph Amato|David Boak|Genevieve Grotjan Feinstein|Leo Rosen 2011:William D. Coffee|Joseph Desch|Parker Hitt|Laura Holmes 2012:Ann Caracristi|Robert Drake|Ronald Hunt|Juliana Mickwitz 2013:Vera Ruth Filby|Richard Proto|Washington Wong|Native American Code Talkers 2014:Frank Austin|Walter Deeley|Howard Ehret|Marian Rejewski|Alan Turing 2015:Ralph W. Adams, Jr.|Charles R. Lord|William O. Marks|Robert J. McNelis|Virginia Jenkins Riley 2016:Gerald Hale|Leonard T. Jones 2017:Mary H. Budenbach|Dennis M. Chiari|Frank E. Herrelko|Bobby R. Inman|Floyd L. Weakley 2018:Hilda Faust Mathieu|Michael J. Jacobs|Richard L. Bernard|Seymour R. Cray|Whitney E. Reed|Hilda Faust Mathieu 2019:Edward M. Drake|Harry Kidder|Alva Bryan Lasswell|Kenneth A. Minihan 2020:George R. Cotter|David Kahn|Barbara A. McNamara|Whitfield Diffie|Lester K. Myers 2021:Jack C. Mortick|Joseph E. Gilligan, Jr.|Clifford Cocks,James EllisundMalcolm Williamson 2022:Eunice Russell Willson Rice|Youn P. Kim|Richard George|Robert Orestes Ferner 2023:Evelyn Akeley|James Lovell|Joseph Mauborgne|James Radford|Harry Rashbaum Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Leben und Wirken 1.1Kindheit und Jugend 1.2Collegezeit und theoretische Arbeiten 2Karriere und Forschung 2.1Kryptoanalyse 2.2Arbeit an frühen Computern – Der Turing-Test 2.3Arbeit an mathematischen Problemen der theoretischen Biologie 3Verfolgung wegen Homosexualität und Turings Tod 4Offizielle Entschuldigung, Danksagung und Rehabilitierung 5Nachwirkungen der Rehabilitierung 6Postume Ehrungen 7Verschiedenes 8Werke 8.1Veröffentlichungen 8.2Patente 9Literatur (Auswahl) 9.1Theorie und Diskussion 9.2Geschichte und Biographie 9.3Belletristik 10Filme (Auswahl) 11Theaterstücke 12Musik 13Comic 14Weblinks 15Einzelnachweise Адыгабзэ Afrikaans Alemannisch Aragonés العربية مصرى"
  },
  {
    "label": 1,
    "text": "Algorithmus – Wikipedia Algorithmus Inhaltsverzeichnis Definition Informatik und Mathematik Abgrenzung zur Heuristik Eigenschaften Algorithmenanalyse Typen und Beispiele Wortherkunft Geschichte des Algorithmus Literatur Weblinks Fußnoten Turingmaschinen und Algorithmusbegriff Church-Turing-These Abstrakte Automaten Algorithmus und Programme Erster Computeralgorithmus Heutige Situation Populärer Gebrauch des Begriffs Determiniertheit Determinismus Finitheit Effektivität Beispiele für (weitere) Eigenschaften von Algorithmen Geschichtliche Entwicklung Antikes Griechenland Mathematik im 19. und 20. Jahrhundert Formale Definition Eigenschaften des Algorithmus Statische Finitheit Dynamische Finitheit Terminiertheit EinAlgorithmus(benannt nach dem Mathematiker und Universalgelehrtenal-Chwarizmi, von arabisch:الخوارزمیal-Ḫwārizmī, deutsch‚derChoresmier‘) ist eine eindeutige Handlungsvorschrift zur Lösung einesProblemsoder einer Klasse von Problemen. Algorithmen bestehen aus endlich vielen,wohldefiniertenEinzelschritten.[1]Damit können sie zur Ausführung in einComputerprogrammimplementiert, aber auch inmenschlicher Spracheformuliert werden. Bei derProblemlösungwird eine bestimmte Eingabe in eine bestimmte Ausgabe überführt.[2] Ein Algorithmus ist eine klare, endliche Abfolge von Anweisungen, die ein Problem löst oder eine Aufgabe ausführt. Um dies jedoch mitmathematischer Strengezu definieren, benötigt man ein mathematisches Modell, welches beschreibt, wiemathematische Funktioneneine Eingabe in eine Ausgabe umwandeln, sogenannte Berechenbarkeitsmodelle. Das Referenz-Modell ist dieTuringmaschine. Der Mangel an mathematischer Genauigkeit des Begriffs Algorithmus störte viele Mathematiker und Logiker des 19. und 20. Jahrhunderts, weswegen in der ersten Hälfte des 20. Jahrhunderts eine ganze Reihe von Ansätzen entwickelt wurde, die zu einer genauen Definition führen sollten. Eine zentrale Rolle nimmt hier der Begriff derTuringmaschinevonAlan Turingein. Weitere Formalisierungen des Berechenbarkeitsbegriffs sind dieRegistermaschinen, derLambda-Kalkül(Alonzo Church),rekursive Funktionen, Chomsky-Grammatiken (sieheChomsky-Hierarchie) undMarkow-Algorithmen. Es wurde – unter maßgeblicher Beteiligung von Alan Turing selbst – gezeigt, dass all diese Methoden die gleiche Berechnungsstärke besitzen (gleichmächtigsind). Sie können durch eine Turingmaschineemuliertwerden, und sie können umgekehrt eine Turingmaschine emulieren. Mit Hilfe des Begriffs der Turingmaschine kann folgende formale Definition des Begriffs formuliert werden: „Eine Berechnungsvorschrift zur Lösung eines Problems heißt genau dann Algorithmus, wenn eine zu dieser Berechnungsvorschrift äquivalente Turingmaschine existiert, die für jede Eingabe, die eine Lösung besitzt, stoppt.“ Aus dieser Definition sind folgende Eigenschaften eines Algorithmus ableitbar: Darüber hinaus wird der Begriff Algorithmus in praktischen Bereichen oft auf die folgenden Eigenschaften eingeschränkt: DieChurch-Turing-Thesebesagt, dass jedes intuitiv berechenbare Problem durch eine Turingmaschine gelöst werden kann. Als formales Kriterium für einen Algorithmus zieht man die Implementierbarkeit in einem beliebigen, zu einer Turingmaschine äquivalenten Formalismus heran, insbesondere die Implementierbarkeit in einerProgrammiersprache– die von Church verlangteTerminiertheitist dadurch allerdings noch nicht gegeben. Der Begriff derBerechenbarkeitist dadurch dann so definiert, dass ein Problemgenau dannberechenbarist, wenn es einen (terminierenden) Algorithmus zu dem Problem gibt, das heißt, wenn eine entsprechend programmierte Turingmaschine das Problemin endlicher Zeitlösen könnte. Es sei bemerkt, dass dieAmbiguitätdes Begriffs „intuitiv berechenbares Problem“ den mathematischen Beweis dieser These unmöglich macht. Es ist also theoretisch denkbar, dass intuitiv berechenbare Probleme existieren, die nach dieser Definition nicht als „berechenbar“ gelten. Bis heute wurde jedoch noch kein solches Problem gefunden.[3] Turingmaschinen harmonieren gut mit den ebenfalls abstrakt-mathematischenberechenbaren Funktionen, reale Probleme sind jedoch ungleich komplexer, daher wurden andere Maschinen vorgeschlagen. Diese Maschinen weichen etwa in der Mächtigkeit der Befehle ab; statt der einfachen Operationen der Turingmaschine können sie teilweise mächtige Operationen, wie etwaFourier-Transformationen, in einem Rechenschritt ausführen. Oder sie beschränken sich nicht auf eine Operation pro Rechenschritt, sondern ermöglichen parallele Operationen, wie etwa die Addition zweierVektorenin einem Schritt. Ein Modell einer echten Maschine ist dieSequential Abstract State Machine(kurzseq. ASM)[4]mit folgenden Eigenschaften: Ein Algorithmus einer seq. ASM soll Algorithmen sind eines der zentralen Themen derInformatikundMathematik. Sie sind Gegenstand einiger Spezialgebiete dertheoretischen Informatik, derKomplexitätstheorieund derBerechenbarkeitstheorie, mitunter ist ihnen ein eigener FachbereichAlgorithmikoderAlgorithmentheoriegewidmet. In Form vonComputerprogrammenundelektronischen Schaltungensteuern AlgorithmenComputerund andereMaschinen. Für Algorithmen gibt es unterschiedliche formale Repräsentationen. Diese reichen vom Algorithmus als abstraktem Gegenstück zum konkret auf eine Maschine zugeschnittenen Programm (das heißt, dieAbstraktionerfolgt hier im Weglassen der Details der realen Maschine, das Programm ist eine konkrete Form des Algorithmus, angepasst an die Notwendigkeiten und Möglichkeiten der realen Maschine) bis zur Ansicht, Algorithmen seien gerade die Maschinenprogramme vonTuringmaschinen(wobei hier die Abstraktion in der Verwendung der Turingmaschine an sich erfolgt, das heißt, einer idealenmathematischen Maschine). Ein Algorithmus beschreibt eine Vorgehensweise in ihren Teilschritten, zu deren Erledigung wiederum Algorithmen benötigt werden. Beispielsweise werden für die LösungQuadratischer GleichungendieGrundrechenartenverwendet. Entsprechend wird in Programmen aufOperatorenzurückgegriffen, welche in die Programmiersprache integriert sind, oder aufProgrammbibliotheken. Guter Programmcode zeichnet sich dadurch aus, dass der Teil mit dem eigentlichen Algorithmus kompakt und nachvollziehbar bleibt, während nebensächliche Details in Unterprogramme ausgliedert sind (Modularisierung). Algorithmen können inProgrammablaufplänennach DIN 66001 oderISO 5807grafisch dargestellt werden. Der erste für einen Computer gedachte Algorithmus (zur Berechnung vonBernoullizahlen) wurde 1843 vonAda Lovelacein ihren Notizen zuCharles BabbagesAnalytical Enginefestgehalten. Sie gilt deshalb als die ersteProgrammiererin. Weil Charles Babbage seineAnalytical Enginenicht vollenden konnte, wurde Ada Lovelaces Algorithmus nie darauf implementiert. Algorithmen für Computer sind heute so vielfältig wie die Anwendungen, die sie ermöglichen sollen. Vom elektronischenSteuergerätfür den Einsatz im Kfz über die Rechtschreib- und Satzbau-Kontrolle in einerTextverarbeitungbis hin zur Analyse vonAktienmärktenfinden sich tausende von Algorithmen. Hinsichtlich der Ideen und Grundsätze, die einem Computerprogramm zugrunde liegen, wird einem Algorithmus in der Regelurheberrechtlicher Schutzversagt.[5]Je nach nationaler Ausgestaltung der Immaterialgüterrechte sind Algorithmen der Informatik jedoch demPatentschutzzugänglich, so dass urheberrechtlich freie individuelle Werke, als Ergebnis eigener geistiger Schöpfung, wirtschaftlich trotzdem nicht immer frei verwertet werden können. Dies betrifft oder betraf z. B. Algorithmen, die auf der Mathematik derHough-Transformation(Jahrzehnte alt, aber mehrfach aktualisiertes Konzept mit Neu-Anmeldung) aufbauen, Programme, die das BildformatGIFlesen und schreiben wollten, oder auch Programme im Bereich der Audio- und Video-Verarbeitung, da die zugehörigen Algorithmen, wie sie in den zugehörigenCodecsumgesetzt sind, oftmals nicht frei verfügbar sind. Die entsprechenden Einsparpotentiale für alle Anwender weltweit (für denRete-Algorithmuswurde einst eine Million USD aufDEC XCONgenannt) dürften heute problemlos die Grenze von einer Milliarde USD im Jahr um ein Zigfaches überschreiten. Der Begriff des Algorithmus hat seit etwa 2015 im Kontext des Online-MarketingEinzug in die Presse- und Alltagssprache gehalten. Algorithmen bestimmen insbesondere bei werbefinanzierten Angeboten, welche Inhalte und welche Werbeanzeigen dem Anwender gezeigt werden. Ziel dieser Algorithmen ist es, den Anwender lange auf der jeweiligen Plattform zu halten und ihm solche Anzeigen einzublenden, bei denen die Wahrscheinlichkeit eines Klicks am höchsten ist. Der Begriff „Algorithmus“ fällt auch allgemein, wenn eine Software nach unbekannten, aber offensichtlich komplexen Regeln entscheidet. Beispielsweise, welche Ergebnisse von einer Suchmaschine angezeigt werden. Dabei schwingt häufig ein gewisses Unbehagen mit, eben weil der Algorithmus nicht transparent ist. In der Diskussion nicht scharf davon abgegrenzt ist der Begriff „Künstliche Intelligenz“. Sie bedient sich ebenfalls Algorithmen zur Lösung vorgegebener Probleme. Von künstlicher Intelligenz wird aber im Allgemeinen nur gesprochen, wenn zusätzlich auf einen Vorrat zuvor erlerntenWissenszugegriffen wird, wobei in der Lernphase charakteristische Muster identifiziert und eingeordnet werden. Mit einer passenden Wissensbasis ist es geeigneten Algorithmen beispielsweise möglich, natürliche geschriebene und gesprochene Sprache zu verarbeiten, Gesichter oder beliebige Objekte zu identifizieren, oder Texte zu formulieren. Der Übergang zwischen Algorithmus und Heuristik ist fließend: Eine Heuristik ist eine Methode, aus unvollständigen Eingangsdaten zu möglichst sinnvollen Ergebnissen zu gelangen. Viele heuristische Vorgehensweisen sind selbst exakt definiert und damit Algorithmen. Bei manchen ist jedoch nicht in jedem Schritt genau festgelegt, wie vorzugehen ist – der Anwender muss „günstig raten“. Sie können nicht (vollständig) als Algorithmus formuliert werden. Ein Algorithmus istdeterminiert, wenn dieser bei jeder Ausführung mit gleichen Startbedingungen und Eingaben gleiche Ergebnisse liefert. Ein Algorithmus istdeterministisch, wenn zu jedem Zeitpunkt der Algorithmusausführung der nächste Handlungsschritt eindeutig definiert ist. Wenn an mindestens einer Stelle mehr als eine Möglichkeit besteht (ohne Vorgabe, welche zu wählen ist), dann ist der gesamte Algorithmusnichtdeterministisch. Beispiele für deterministische Algorithmen sindBubblesortund dereuklidische Algorithmus. Dabei gilt, dass jeder deterministische Algorithmus determiniert ist, während aber nicht jeder determinierte Algorithmus deterministisch ist. So istQuicksortmit zufälliger Wahl desPivotelementsein Beispiel für einen determinierten, aber nicht deterministischen Algorithmus, da sein Ergebnis bei gleicher Eingabe und eindeutiger Sortierung immer dasselbe ist, der Weg dorthin jedoch zufällig erfolgt. Nichtdeterministische Algorithmen können im Allgemeinen mit keiner realen Maschine (auch nicht mitQuantencomputern)direktumgesetzt werden. Beispiel für einen nichtdeterministischen Algorithmus wäre ein Kochrezept, das mehrere Varianten beschreibt. Es bleibt dem Koch überlassen, welche er durchführen möchte. Auch das Laufen durch einenIrrgartenlässt an jeder Verzweigung mehrere Möglichkeiten, und neben vielen Sackgassen können mehrere Wege zum Ausgang führen. Die Beschreibung des Algorithmus besitzt eine endliche Länge, der Quelltext muss also aus einer begrenzten Anzahl von Zeichen bestehen. Ein Algorithmus darf zu jedem Zeitpunkt seiner Ausführung nur begrenzt viel Speicherplatz benötigen. Ein Algorithmus ‚terminiert überall‘ oder ‚ist terminierend‘, wenn er nach endlich vielen Schritten anhält (oder kontrolliert abbricht) – für jede mögliche Eingabe. Ein nicht-terminierender Algorithmus (somit zu keinem Ergebnis kommend) gerät (für manche Eingaben) in eine so genannte Endlosschleife. Für manche Abläufe ist ein nicht-terminierendes Verhalten gewünscht, z. B. Steuerungssysteme, Betriebssysteme und Programme, die auf Interaktion mit dem Benutzer aufbauen. Solange der Benutzer keinen Befehl zum Beenden eingibt, laufen diese Programme beabsichtigt endlos weiter.Donald E. Knuthschlägt in diesem Zusammenhang vor, nicht terminierende Algorithmen als rechnergestützte Methoden(Computational Methods)zu bezeichnen. Darüber hinaus ist die Terminierung eines Algorithmus (dasHalteproblem) nichtentscheidbar. Das heißt, das Problem, festzustellen, ob ein (beliebiger) Algorithmus mit einer beliebigen Eingabe terminiert, ist nicht durch einen Algorithmus lösbar. Der Effekt jeder Anweisung eines Algorithmus muss eindeutig festgelegt sein. Die Erforschung und Analyse von Algorithmen ist eine Hauptaufgabe der Informatik und wird meist theoretisch (ohne konkrete Umsetzung in eine Programmiersprache) durchgeführt. Sie ähnelt somit dem Vorgehen in manchen mathematischen Gebieten, in denen die Analyse eher auf die zugrunde liegenden Konzepte als auf konkrete Umsetzungen ausgerichtet ist. Algorithmen werden zur Analyse in eine stark formalisierte Form gebracht und mit den Mitteln derformalen Semantikuntersucht. Die Analyse unterteilt sich in verschiedene Teilgebiete: Der älteste bekannte nicht-trivialeAlgorithmus ist dereuklidische Algorithmus. Spezielle Algorithmus-Typen sind derrandomisierte Algorithmus(mit Zufallskomponente), derApproximationsalgorithmus(als Annäherungsverfahren), dieevolutionären Algorithmen(nach biologischem Vorbild) und derGreedy-Algorithmus. Eine weitere Übersicht geben dieListe von Algorithmenund dieKategorie Algorithmus. Das WortAlgorithmusist eine Abwandlung oderVerballhornungdes Namens des persischen[6][7][8]Rechenmeisters und AstronomenAbu Dschaʿfar Muhammad ibn Musa al-Chwārizmī, dessen Namensbestandteil (Nisba)al-Chwarizmi„der Choresmier“ bedeutet und auf die Herkunft des Trägers ausChoresmienverweist. Er baute auf die Arbeit des aus dem 7. Jahrhundert stammenden indischen MathematikersBrahmagupta.[9][10]Die ursprüngliche Bedeutung war das Einhalten derarithmetischen Regelnunter Verwendung derindisch-arabischen Ziffern. Die ursprüngliche Definition entwickelte sich mit Übersetzung ins Lateinische weiter.[11]Sein LehrbuchÜber die indischen Ziffern(verfasst um 825 imHaus der WeisheitinBagdad) wurde im 12. Jahrhundert aus dem Arabischen insLateinischeübersetzt und hierdurch in der westlichen Welt nebenLeonardo PisanosLiber Abacizur wichtigsten Quelle für die Kenntnis und Verbreitung des indisch-arabischen Zahlensystems und des schriftlichen Rechnens. Mit der lateinischen Übersetzung al-Chwārizmī wurde auch der Name des Verfassers in Anlehnung an die Anfangsworte der ältesten Fassung dieser Übersetzung (Dixit Algorismi„Algorismi hat gesagt“) latinisiert.[12]Aus al-Chwārizmī wurde mittelhochdeutschalgorismus,alchorismusoderalgoarismus –ein Wort, das aus dem Lateinischen nahezu zeitgleich und gleichlautend ins Altfranzösische (algorisme,argorisme)und Mittelenglische (augrim,augrym) übersetzt wurde. Mit Algorismus bezeichnete man bis um 1600 Lehrbücher, die in den Gebrauch der Fingerzahlen, der Rechenbretter, der Null, die indisch-arabischen Zahlen und das schriftliche Rechnen einführen.[13]Das schriftliche Rechnen setzte sich dabei erst allmählich durch. So beschreibt etwa der englische DichterGeoffrey Chaucernoch Ende des 14. Jahrhunderts in seinenCanterbury Taleseinen Astrologen, der Steine zum Rechnen (augrym stones) am Kopfende seines Betts aufbewahrt: In der mittelalterlichen Überlieferung wurde das Wort bald als erklärungsbedürftig empfunden und dann seit dem 13. Jahrhundert zumeist als Zusammensetzung aus einem PersonennamenAlgusund aus einem aus demgriechischenῥυσμός(Nebenform vonῥυθμός) in der Bedeutung „Zahl“ entlehnten Wortbestandteil-rismusinterpretiert. Algus, der vermutete Erfinder dieser Rechenkunst, wurde hierbei von einigen als Araber, von anderen als Grieche oder zumindest griechisch schreibender Autor, gelegentlich auch als „König von Kastilien“ (Johannes von Norfolk) betrachtet. In der volkssprachlichen Tradition erscheint dieser „Meister Algus“ dann zuweilen in einer Reihe mit großen antiken Denkern wiePlaton,AristotelesundEuklid, so im altfranzösischenRoman de la Rose, während das altitalienische GedichtIl Fioreihn sogar mit dem Erbauer des SchiffesArgogleichsetzt, mit dem Jason sich auf die Suche nach dem Goldenen Vlies begab. Auf derpara-etymologischenGräzisierungdes zweiten Bestandteils-rismusauf griech.ῥυσμός,ῥυθμόςberuht dann auch die lateinische Wortformalgorithmus, die seit derFrühen Neuzeit, anfangs auch mit der Schreibvariantealgorythmus, größere Verbreitung erlangte und zuletzt die heute übliche Wortbedeutung als Fachterminus für geregelte Prozeduren zur Lösung definierter Probleme annahm. Schon mit der Entwicklung der Sprache ersannen die Menschen für ihr Zusammenleben in größeren Gruppen Verhaltensregeln, Gebote, Gesetze – einfachste Algorithmen. Mit der Sprache ist auch eine geeignete Möglichkeit gegeben, Verfahren und Fertigkeiten weiterzugeben – komplexere Algorithmen. Aus der Spezialisierung einzelner Gruppenmitglieder auf bestimmte Fertigkeiten entstanden die ersten Berufe. Der Algorithmusbegriff als abstrakte Sicht auf Aufgabenlösungswege trat zuerst im Rahmen der Mathematik, Logik und Philosophie ins Bewusstsein der Menschen. Ein Beispiel für einen mathematischen Algorithmus aus dem Altertum ist derEuklidische Algorithmus. Obwohl der etymologische Ursprung des Wortes arabisch ist, entstanden die ersten Algorithmen imantiken Griechenland. Zu den wichtigsten Beispielen gehören dasSieb des Eratostheneszum Auffinden vonPrimzahlen, welches im BuchEinführung in die ArithmetikvonNikomachosbeschrieben wurde,[14]und dereuklidische Algorithmuszum Berechnen desgrößten gemeinsamen Teilerszweiernatürlicher Zahlenaus dem Werk „die Elemente“.[15]Einer der ältesten Algorithmen, die sich mit einerreellen Zahlbeschäftigen, ist derAlgorithmus des Archimedeszur Approximation vonπ{\\displaystyle \\pi }, was zugleich auch eines der ältestennumerischen Verfahrenist.[16] Bedeutende Arbeit leisteten die Logiker des 19. Jahrhunderts.George Boole, der in seiner SchriftThe Mathematical Analysis of Logicden erstenalgebraischen Logikkalkülerschuf, begründete damit die moderne mathematische Logik, die sich von der traditionellen philosophischen Logik durch eine konsequente Formalisierung abhebt.[17]Gottlob Fregeentwickelte als erster eineformale Spracheund die daraus resultierendenformalen Beweise.[18]Giuseppe Peanoreduzierte die Arithmetik auf eine Sequenz von Symbolen manipuliert von Symbolen. Er beschäftigte sich mit der Axiomatik der natürlichen Zahlen. Dabei entstanden diePeano-Axiome.[19] Die Arbeit von Frege wurde stark vonAlfred North WhiteheadundBertrand Russellin ihrem WerkPrincipia Mathematicaweiter ausgearbeitet und vereinfacht.[20]Zuvor wurde von Bertrand Russell die berühmterussellsche Antinomieformuliert, was zum Einsturz dernaiven Mengenlehreführte. Das Resultat führte auch zur ArbeitKurt Gödels. David Hilberthat um 1928 dasEntscheidungsproblemin seinemForschungsprogrammpräzise formuliert.[21]Alan TuringundAlonzo Churchhaben für das Problem 1936 festgestellt, dass es unlösbar ist.[22] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Definition 1.1Turingmaschinen und Algorithmusbegriff 1.1.1Formale Definition 1.1.2Eigenschaften des Algorithmus 1.2Church-Turing-These 1.3Abstrakte Automaten 2Informatik und Mathematik 2.1Algorithmus und Programme 2.2Erster Computeralgorithmus 2.3Heutige Situation 2.4Populärer Gebrauch des Begriffs 3Abgrenzung zur Heuristik 4Eigenschaften 4.1Determiniertheit 4.2Determinismus 4.3Finitheit 4.3.1Statische Finitheit 4.3.2Dynamische Finitheit 4.3.3Terminiertheit 4.4Effektivität 4.5Beispiele für (weitere) Eigenschaften von Algorithmen 5Algorithmenanalyse 6Typen und Beispiele 7Wortherkunft 8Geschichte des Algorithmus 8.1Geschichtliche Entwicklung 8.2Antikes Griechenland 8.3Mathematik im 19. und 20. Jahrhundert 9Literatur 10Weblinks 11Fußnoten"
  },
  {
    "label": 1,
    "text": "Anwendungen künstlicher Intelligenz – Wikipedia Anwendungen künstlicher Intelligenz Inhaltsverzeichnis Künstliche Intelligenz mit Einteilung vorhandener Hardware Arbeitswelt KI in der Medizin KI im Rechtswesen KI bei autonomen Waffensystemen KI im Marketing KI in der Chemie KI in Computer- und Gesellschaftsspielen KI in der Mathematik KI in Film und Fernsehen KI zur Erzeugung von Bildern und Kunstwerken KI im Produktdesign KI in der Schulbildung KI in der Hochschulbildung KI beim Klimaschutz KI in der Materialwissenschaft KI in Logistik und Verkehr KI bei Steuerberatung KI in Unternehmen Siehe auch Weblinks Einzelnachweise Künstliche Intelligenz mit wenig spezialisierter Standard-Hardware Künstliche Intelligenz mit spezialisierter Hardware Künstliche Intelligenzhat eine Vielzahl von Verwendungen in Forschung und Wirtschaft. Einige der Anwendungen sind die Folgenden: DasInstitut für Arbeitsmarkt- und Berufsforschung (IAB)forscht zu Veränderungen der Arbeitswelt durch künstliche Intelligenz. Vorgestellt werden auf einer Infoplattform Forschungsprojekte und Erkenntnisse zu Folgen für Beschäftigung, Löhne und Qualifikationsanforderungen.[3] KI in der Medizin ist neben Vernetzung undMedizinische Robotikals dritte große Säule derDigitalen Medizinanzusehen.[4]Gerade im Krankenhauskontext bietet KI verschiedene Potenziale und beeinflusst medizinische sowie administrative Prozesse maßgeblich.[5][6]Beispiele sind Systeme zur Entscheidungsunterstützung in Echtzeit oder zumBestandsmanagement.[7]Für medizinische Einrichtungen wie Krankenhäuser wird es also immer wichtiger auch Strategien im Umgang mit KI zu entwickeln.[8] Ein großer Teil der Arbeit von Juristen besteht in derAnalyse von Akten, zum Beispiel von Präzedenzfällen, um daraus Argumente zu entwickeln. Derartige Arbeit kann mittlerweile zu einem Teil von KI-Anwendungen übernommen werden.[9]Die Beratungsfirma McKinsey schätzte 2017, dass etwa 22 Prozent der Arbeit von Anwälten und 35 Prozent der Arbeit von Rechtshelfern mit Hilfe von KI-Systemen automatisiert werden könnte. Die KI-Systeme werden anhand von Millionen von Dokumenten und Fallbeispielen und juristischen Anträgen trainiert. Danach kann eine KI diejenigen Dokumente markieren, die ein Jurist für seinen Fall braucht; oft besser, als dies ein Mensch könnte. JPMorgan gab bekannt, die KI Contract Intelligence einzusetzen, die nach Aussagen von JPMorgan eine Menge von Daten in Sekunden analysieren kann, wofür Juristen und Rechtshelfer 360.000 Stunden benötigen würden.[10] Im Zuge desrussischen Angriffskriegesauf die Ukraine seit dem Februar 2022 ist die mediale Aufmerksamkeit für den Einsatz von KI für militärische Zwecke gestiegen.[11]In Deutschland beschäftigen sich sowohl dieBundeswehrals auch die private Rüstungswirtschaft intensiv mit der Entwicklung und Implementierung von KI-unterstützten Waffensystemen.[12] Im Mai 2023 erregte der Vortag eines Colonels derU.S. Air ForceAufsehen, der bei einer Militär-Konferenz in London geschildert hatte, wie KI ihre Einsatz-Parameter verletzt und den eigenen Kontrollturm angegriffen habe, weil sie den menschlichen Operator als Hindernis bei der Erfüllung ihrer Mission betrachtet hätte. Kurz darauf ließ das U.S.-Militär klarstellen, es habe sich um keine echte Übung, sondern lediglich um ein Gedankenexperiment gehandelt.[13] Im Rahmen eines Experiments zur Simulation eines Luftkampfs setzte die US-ForschungsbehördeDARPAim Zeitraum von Dezember 2022 bis September 2023 einen modifiziertenF-16Kampfjet ein. Das Flugzeug mit der Bezeichnung X-62A VISTA (Variable In-flight Simulator Test Aircraft)[14]war ein zum Testen und Trainieren von KI-Software umgerüsteter F-16-Kampfjet, der rund 21 Testflüge absolvierte. Während eines Tests auf derEdwards Air Force Baseim September 2023 trat der KI-trainierte Jet in einem Luftkampf gegen einen menschlichen Piloten in einer anderen F-16 an. Ziel des \"Air Combat Evolution Program\" (ACE) der DARPA war, einen ersten Demonstrator für KI-gestützte, kollaborative Luftkämpfe zwischen Mensch und Maschine zu erhalten.[15] Im Marketing wird künstliche Intelligenz eingesetzt, um zum Beispiel Werbe-E-Mails zu verschicken, den Kundendienst durch Social Bots undChatbotsabzulösen, Analysen und Prognosen des Markts und des Kunden, beispielsweise auf Basis vonBig Data, durchzuführen und kundenspezifische Werbeanzeigen, Empfehlungen und Suchergebnisse, sowie programmierte Abläufe zu entwickeln. So beabsichtigte derOnline-VersandhändlerZalandoim März 2018, 250 Arbeitsplätze im Marketingbereich im Standort Berlin zu streichen, die durch künstliche Intelligenz ersetzt werden sollen.[16] Die Nutzung von KI im Marketing schreitet immer noch rasante voran. Laut einer Umfrage des Marketing AI Institute aus dem Jahr 2024 nutzen bereits 99 % der Marketer in irgendeiner Form Künstliche Intelligenz (KI), wobei 26 % aktiv mit KI experimentieren. Zum Vergleich: Im Jahr 2023 lag dieser Anteil noch bei 45 %. Dieser Rückgang deutet darauf hin, dass viele Marketer inzwischen routinierter und professioneller im Umgang mit KI-Tools geworden sind.[17] Beim Einsatz von KI kommen zum einen intelligente Roboter zum Einsatz, die den Massendurchsatz in Großlaboren erheblich erhöhen können[18]. Zum anderen werden neuronale Netze verwendet, wenn es darum geht, große Datenmengen zu durchsuchen, Muster zu erkennen und Vorhersagen zu treffen[19] Ein erhebliches Problem beim Training neuronaler Netze ist der vergleichsweise geringe Bestand an offenen und FAIRen Forschungsdaten in der Chemie.[22] In Computerspielen wird eine KI meistens dazu verwendet um NPC, sogenannteNicht-Spieler-Charaktere, die menschenähnliches Verhalten simulieren (zum Beispiel als simulierte Verbündete oderComputergegner) zu steuern oder bestimmte Dinge in derSpielweltoder bei den Funktionen desSpielecharakters(zum BeispielRoutenfindung,prozedurale Generierung,automatische Verbesserungen und Vervollständigungenbeim Streckenbau oder andere Algorithmen) zu berechnen. Bei einigen Spielen lässt sich derSchwierigkeitsgradder KI-Gegner einstellen und optional wählen ob man gegen eine KI, gegen echte Spieler oder eine Mischform spielen möchte. Bei ein paar Spielen kann sich die KI auch automatisch an das Spielverhalten anpassen oder kann aus Fehlern lernen. Da imEinzelspielermodusoft Gegner fehlen, wird auf eine KI zurückgegriffen. Zudem wird KI in Computerspielen verwendet, um viele oder sehr spezielle Charaktere zu simulieren, die nicht oder sehr schwer von echten Menschen übernommen werden könnten. Teilweise lassen sich KI-Anwendungen in Computerspielen aber auch einfach austricksen, da ein Mensch ein bestimmtes Muster einer KI umgehen kann. Der Realismus und dasGameplayeines Computerspiels wird daher auch oft an der KI gemessen.[23][24][25] Auch wird KI in Strategie-Brettspielen als Ersatz für den menschlichen Partner eingesetzt. Gegen sehr leistungsfähige Versionen dieser Programme haben auch Weltmeister kaum Gewinnchancen. Erfolge gegen menschliche Profispieler erzielte KI zum Beispiel inBackgammon,Schach,Checkers,GoundStarCraft II. Das Meistern komplexer Spiele ist oft Gegenstand der Forschung, um so neue Methoden der künstlichen Intelligenz zu entwickeln und zu demonstrieren.[26]Inzwischen tragen diese Programme Partien untereinander aus. 2016 besiegte die aufDeepMindaufbauende KIAlphaGoden 18-maligen Go-Weltmeister, den SüdkoreanerLee Sedolunter Turnierbedingungen 4:1.[27]Ende 2017 hat die NeuentwicklungAlphaZerogegen das bis dahin weltbeste SchachprogrammStockfishin 100 ausgetragenen Partien deutlich obsiegt.[28]2019 gelang es der DeepMind-Weiterentwicklung Alpha Star, menschliche Top-Spieler beim populären und als sehr schwer geltenden Strategiespiel StarCraft II 10:1 zu besiegen.[29] Darüber hinaus werden auch KI-Anwendungen entwickelt, die anstelle eines menschlichen Spielers Videospiele wieJump ’n’ Runs,RollenspieleoderRennspielesteuern.[30][31][32]Ähnlich ist die Entwicklung imE-Sport-Bereich, in dem Profigamer versuchen, die besten KI-Systeme zu schlagen, während Entwickler darauf hinarbeiten, die besten Spieler durch eine KI zu besiegen.[33] In der Mathematik werden spezielle Formen formaler, schrittweiser Argumentation verwendet. Im Gegensatz dazu arbeitenLarge Language Models(LLM) wieGPT-4 Turbo,Gemini Ultra,Claude Opus,LLaMa-2oderMistral LargemitWahrscheinlichkeitsmodellen, die falsche Antworten in Form vonHalluzinationproduzieren können. Daher benötigen sie nicht nur eine große Datenbank von mathematischen Problemen, aus der sie lernen können, sondern auch Methoden wie überwachteFeinabstimmungoder trainierte Klassifikatoren mit menschlich vorgegebenen Daten, um Antworten für neue Probleme zu verbessern und aus Korrekturen zu lernen.[34]In einer Studie wurde 2024 gezeigt, dass bestimmte KI-Sprachmodelle bei der Lösung von mathematischen Problemen mit Anforderungen an die logische Herleitung schlecht abschneiden, falls die spezifischen Probleme nicht vortrainiert wurden.[35] Alternativ wurden spezialisierte KI-Modelle zur Lösung mathematischer Probleme mit höherer Präzision für das Ergebnis, einschließlich Beweisen von Theoremen, entwickelt. Dazu gehörenAlphaTensor,AlphaGeometry,AlphaDev,AlphaProofundAlphaEvolve[36]alle vonGoogle DeepMind[37]sowie OpenAIo3,[38]Llemma von Eleuther[39]oder Julius[40]. Wenn natürliche Sprache zur Beschreibung mathematischer Probleme verwendet wird, können Konverter solche Anfragen in eine formale Sprache wieLeanumwandeln, um mathematische Aufgaben formal korrekt zu definieren. Einige KI-Modelle wurden entwickelt, um anspruchsvolle Probleme zu lösen und gute Ergebnisse in Benchmark-Tests zu erzielen, während andere als Bildungswerkzeuge in der Mathematik dienen.[41] Sunspring ist der erste Kurzfilm (2016), dessen Drehbuch von einer KI geschrieben wurde.[42][43] Im US-BundesstaatHawaiisetzte im September 2024 eines der ersten Medienunternehmen KI-generierte Nachrichtensprecher für die live-Berichterstattung ein.[44]Nach weniger als drei Monaten wurde die Sendung aufgrund zu geringer Zuschauerzahlen jedoch wieder eingestellt.[45] Forscher aus Tübingen haben neuronale Netze dazu verwendet, ein vorgegebenes Foto im Stil eines berühmten Künstlers zu malen z. B.Van GoghoderEdvard Munch.[46]Forscher bei Google haben neuronale Netze darauf trainiert, aus einer Art weißem Rauschen Bilder im Stil von Van Gogh und anderen Künstlern zu produzieren. Die Bilder wurden später auf einer Auktion versteigert.[47][48] Im Juli 2017 stellten Forscher der Rutgers-Universität eine KI vor, die künstlerische Gemälde produziert. Die KI wurde mit ca. 80.000 Bildwerken der westlichen Kunstgeschichte trainiert. Die von der KI erstellten Gemälde wurden mit Bildern, die auf der KunstmesseArt Baselausgestellt worden waren, vermischt und 18 Testpersonen (künstlerischen Laien)[49]in einem Blindtest zur Beurteilung vorgelegt. Die Testpersonen sollten einschätzen, ob die Bilder von Menschen oder einem Computer erzeugt worden waren. Bei den durch echten, auf der Art Basel ausgestellten Kunstwerken unterstellten die Testpersonen bei 52 % aller Werke, sie seien durch einen Computer erstellt. Bei den KI-basierten Bildern, nahmen die Testpersonen das nur für 25 % aller Bilder an.[50] Im März 2018 wurde einVideokunstwerkpubliziert, in dem eine durch KI erschaffeneOrnella Mutiagierte. Der Künstler Joseph Ayerle hatte mit Hilfe eines künstlichen neuronalen Netzes neue Filmsequenzen errechnet, die die echte italienische Schauspielerin nie gespielt hat.[51][52]2021 wurde der Kurzfilm „Fellini Forward“ aufgeführt. Bei der Produktion derFrederico Fellini-Hommagesetzte das Produktionsteam auf KI-Werkzeuge, um dramaturgische, visuelle und linguistische Muster in den Werken Fellinis zu erkennen und sie im neuen Film einzusetzen.[53] Im Oktober 2018 versteigerte das AuktionshausChristie’sdas durch künstliche Intelligenz erschaffene „Portrait of Edmond de Belamy“. Das ursprünglich auf einen Verkehrswert von 7.000 bis 10.000 US-Dollar geschätzte Bild erzielte in der Auktion einen Erlös von 432.500 Dollar. Hinter der Herstellung des Porträts stand die französische Künstlergruppe Obvious, die eine künstliche Intelligenz mit den Bilddaten von 15.000 echten Gemälden[54]des 14. bis 20. Jahrhunderts trainiert hatte. Besondere Beachtung in der Presse fand, dass das Bild nicht mit den Signaturen der Künstler unterzeichnet wurde, sondern mit der Formel „min G max D Ex[log(D(x))]+Ez[log(1-D(G(z)))]“, die nach Angaben des Künstlerteams bei seiner Entstehung genutzt wurde.[55] Der AutorGeorge R. R. Martinschrieb an seinem sechsten Buch der ReiheGame of Thrones, das von der Fangemeinde ungeduldig erwartet wurde. Der Programmierer Zack Thoutt trainierte eine KI (Recurrent Neural Net) mit den ersten fünf Büchern der Serie und ließ von der KI ein sechstes Buch schreiben. Das Ergebnis wurde im Sommer 2017 im Internet veröffentlicht. Dabei entwickelte die KI einzelne Charaktere genauso weiter, wie das in manchen Fan-Theorien erwartet wurde, ohne dass die KI davon wusste. Mängel gibt es bei der Grammatik, einzelne Charaktere, die bereits verstorben waren, tauchen wieder auf und die Handlungsstränge sind nicht sehr spannend.[56] Google versucht in seinem Magenta-Projekt, KI-Systeme zu erzeugen, die kreativ sind. So wurde im Sommer 2017 eine Klavier-Improvisation vorgestellt, die von einer KI komponiert wurde.[57]Bereits im Sommer 2016 veröffentlichte das Projekt Magenta einen kurzen Pop-Song, der von einer KI komponiert wurde.[58] Die Musik des Albums „I am AI“ der SängerinTaryn Southern, vorgestellt im Herbst 2017, wurde von einer KI komponiert. Um einen Song mit Hilfe einer KI zu komponieren, verwendet man eine Software wie etwa Amper Music oder Jukedeck, wählt das Genre und weitere Parameter wie Länge des Songs, Instrumentierung usw. Innerhalb von Sekunden komponiert die KI dann einen einzigartigen Song. Ein Musiker kann daraufhin Bruchstücke dieser Beispiele zu einem eigenen Song zusammenfügen. Somit kann jedermann mehr oder weniger professionelle Musik kreieren. Immer mehr Musiker geben zu, beim Komponieren KI als Werkzeug zu benutzen.[59][60]Auch das Album „Hello World“ von Skygge wurde vollständig mit einer KI (Flow-Machine) komponiert. Die KI komponiert Soundstücke, die dann von Menschen sortiert, selektiert und zusammengesetzt werden, das sog. Kuratieren.[61]Ein Team von Musikwissenschaftlern und KI-Experten unter Leitung von Matthias Röder, Direktor desSalzburgerKarajan-Instituts, vollendete 2021 mit Hilfe einer künstlichen Intelligenz die unvollendete 10. Sinfonie des KomponistenBeethoven.[62] Ab dem Jahr 2022 wurden den Nutzern innovativeText-zu-Bild-KI-Systeme zur Erzeugung von Bildern zur Verfügung gestellt, die einen deutlichen Fortschritt gegenüber früheren Technologien darstellten. Zu den namhaften Bildgeneratoren zählten beispielsweiseMidjourney,DALL-E(entwickelt vomOpenAI-Team, das auch hinterChatGPTsteht) undStable Diffusion.[63][64]Eine herausragende Eigenschaft dieser neuen Programme bestand darin, dass Bilder mithilfe von Wortanweisungen, sogenannten „Prompts“, erstellt werden konnten.[65]Zusätzlich war es möglich, der KI eigene Bilder als Beispiele vorzugeben. Ab dem Jahr 2023 erreichten die KI-generierten Bilder ein so hohes Maß an Fotorealismus, dass man sie teilweise für echte Fotos halten konnte. Zwei KI-generierte Bilder erlangten große Aufmerksamkeit in der Öffentlichkeit, da sie eine bemerkenswerte fotografische Qualität aufwiesen und von vielen Betrachtern zunächst für echte Fotos gehalten wurden: Ein KI-generiertes Bild von Papst Franziskus, der einen auffällig modischen Wintermantel trug,[66][67][68]und ein KI-generiertes Bild eines simulierten Angriffs auf das Pentagon.[69] Kontrovers ist die Sicht der am Diskurs beteiligten Künstler und Experten über die Rolle der KI alsUrhebereines Kunstwerks. Das Motto der Künstlergruppe Obvious lautet: „Kreativität ist nicht nur etwas für Menschen.“[70]Konträr dazu steht die Aussage des Künstlers Joseph Ayerle, der vomMassachusetts Institute of Technologymit den Worten zitiert wird: „KI kann erschaffen, aber sie ist nicht schöpferisch“.[51]Matthias Röder, der ein Team leitete, das den Versuch unternahm, mit KI-Hilfe Beethovens 10. Sinfonie zu vollenden, sprach von einer „Kollaboration zwischen Mensch und Maschine“.[71] In juristischer Hinsicht ist strittig, ob und wie von einer KI geschaffene Kunstwerke dem Schutz des Urheberrechtsgesetzes unterliegen. Denn gemäß § 2 II UrhG können „Werke“ im Sinne des Urheberrechts nur „persönliche geistige Schöpfungen“ sein. Ein ausschließlich von einer Maschine geschaffenes Werk fällt nicht darunter, weil es nach einheitlicher Ansicht einer menschlich-gestalterische Tätigkeit erfordert. Jedenfalls in den Fällen, in denen die KI nicht nur als Hilfsmittel, Instrument oder Werkzeug des Werkschaffenden eingesetzt wird, sondern jegliche Kontrolle über Prozess und Ergebnis durch einen menschlichen Schöpfer aufgegeben wurde, fehlt es an einer geistigen Verbindung des „Werkes“ zu einem „Schöpfer“ im Sinne des § 2 II UrhG, sodass Urheberrecht dann nicht besteht.[72] Ein Team des US-amerikanischen 3D-Software-ExpertenAutodeskund der bekannte DesignerPhilippe Starckhaben gemeinsam den – nach Angaben der Beteiligten – ersten „von künstlicher Intelligenz und Menschen gemeinsam entwickelte Stuhl“ erschaffen, den sogenanntenA. I. Chair.[73]2023 wurde bekannt, dass dieNASAeine eigene Software nutzt, um mit Hilfe von KI das Design von Bauteilen fürRaumschiffeund andere Geräte fürRaumfahrtenoptimal zu gestalten.[74]Das organisch anmutende Aussehen dieser KI-generierten Bauteile unterscheidet sich deutlich vom menschengemachten Design. Einige Autoren betrachten künstliche Intelligenz als Schlüsseltechnologie in der Schulbildung.[75] An einigen Hochschulen werden KI-Systeme zur individuellen Unterstützung von Studierenden und Lehrenden eingesetzt.[80] KI kannSatellitenbilderauswerten und so ermitteln, wo welcheTreibhausgaseemittiert werden, ob Gebäudeenergieeffizientsind sowie wo und in welchem Umfang Wälderabgeholztoder wiederaufgeforstetwerden. Beispiele aus den Bereichen Landwirtschaft und Landnutzung sind zum Beispiel NASA Harvest[81]und derCopernicus Land Monitoring Service.[82] Mit KI können Daten zuWind-undSolarenergieerzeugung,VerkehrsaufkommenundExtremwetterereignissenanalysiert werden und daraus Prognosen für zukünftige Bedarfe und Alternative entwickelt werden. Ein Beispiel aus der Praxis ist Open Climate Fix,[83]eine Organisation, welcheOpen-Source-Modelle für ein sogenanntesNowcastingentwickelt, das heißt, die Wolkenmenge auf Satellitenbildern wird analysiert und daraus, in Kombination mit anderen Daten, die Solarstromproduktion für die nächsten Stunden sehr genau vorausgesagt. Mit Hilfe von KI können Teile großerKlimamodellenachgebildet,Stromnetzeoptimiert und klimafreundlicheStadtplanungstoolsentwickelt werden. Zwar kann KI physikalische Klimamodelle nicht ersetzen, doch kann sie in einigen Fällen gute Annäherungen für besonders rechenzeitintensive Modellkomponenten liefern, etwa indem ein näherungsweises Modell der Wolkenphysik nachgebildet wird. Auf diesem Wege lassen sich Klimamodelle nicht nur schneller berechnen, KI hilft hier auch, den hohen Energieaufwand der erforderlichenSupercomputerzu minimieren.[84] Wie 2023 bekannt wurde, setzt dieDeutsche BahnKI ein, die Pünktlichkeit ihrer Züge zu verbessern. Nach einem Pilotprojekt in Stuttgart wurde das Projekt auf das Rhein-Main-Gebiet und Berlin ausgedehnt.[85] In der Steuerberatung können durch KI vielmehr Aufgaben in kürzerer Zeit sowie Recherchearbeiten erledigt werden. Hierbei können insbesondere Chatbots Fragen von Mandanten beantworten.[86] In Unternehmen wird Künstliche Intelligenz zunehmend zur Unterstützung datenintensiver Prozesse eingesetzt. Dazu zählen unter anderem die Analyse großer Datenmengen, die automatisierte Verarbeitung von Informationen sowie die Optimierung betrieblicher Abläufe. Anwendungen finden sich in Bereichen wie Produktion, Logistik, Personalmanagement und Kundenkommunikation. Dabei kommen sowohl regelbasierte Systeme als auch lernfähige Modelle zum Einsatz, etwa zur Mustererkennung, Prognose oder Entscheidungsunterstützung. Die Einführung entsprechender Technologien erfordert neben technischer Infrastruktur auch organisatorische Anpassungen, rechtliche Klärungen und die Entwicklung neuer Kompetenzen. Untersuchungen zeigen, dass insbesondere mittelständische Unternehmen verstärkt in KI-Anwendungen investieren, um Effizienzpotenziale zu nutzen und die digitale Transformation voranzutreiben.[87] Komplexitätstheorie|Kybernetik zweiter Ordnung|Radikaler Konstruktivismus|Varietät (Kybernetik) Automatentheorie|Entscheidungstheorie|Spieltheorie|Informationstheorie|Informetrie|Konnektionismus|Semiotik|Synergetik|Systemtheorie|Systemwissenschaft|Künstliche Intelligenz Anthropokybernetik|Bildungskybernetik|Biokybernetik|Medizinische Kybernetik|Biomedizinische Kybernetik|Neuroinformatik|Psychokybernetik|Soziokybernetik|Systembiologie|Technische Kybernetik Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Künstliche Intelligenz mit Einteilung vorhandener Hardware 1.1Künstliche Intelligenz mit wenig spezialisierter Standard-Hardware 1.2Künstliche Intelligenz mit spezialisierter Hardware 2Arbeitswelt 3KI in der Medizin 4KI im Rechtswesen 5KI bei autonomen Waffensystemen 6KI im Marketing 7KI in der Chemie 8KI in Computer- und Gesellschaftsspielen 9KI in der Mathematik 10KI in Film und Fernsehen 11KI zur Erzeugung von Bildern und Kunstwerken 12KI im Produktdesign 13KI in der Schulbildung 14KI in der Hochschulbildung 15KI beim Klimaschutz 16KI in der Materialwissenschaft 17KI in Logistik und Verkehr 18KI bei Steuerberatung 19KI in Unternehmen 20Siehe auch 21Weblinks 22Einzelnachweise العربية Català English Español فارسی Français हिन्दी"
  },
  {
    "label": 1,
    "text": "Bayes’sche Optimierung – Wikipedia Bayes’sche Optimierung Inhaltsverzeichnis Geschichte Ausgangslage Strategie Pseudocode Exotische Bayes’sche Optimierung Beispiele für Akquisitionsfunktionen Lösungsmethoden Anwendungsgebiete Literatur Weblinks Einzelnachweise Surrogatmodell Vorschlagen neuer Punkte Erwartete Verbesserung DieBayes’sche Optimierung(BO, engl.Bayesian Optimization) ist eine sequenzielleOptimierungsmethodefür dieOptimierungvonBlack-Box-Funktionen, deren Auswertung teuer ist, d. h. in der Regel Minuten oder Stunden dauert.[1]Dies tritt beispielsweise auf, wenn die zu optimierende Funktion nicht geschlossen darstellbar ist, sondern etwa das Ergebnis einerSimulationausgibt, die Ergebnisse des Trainings einesMachine-Learning-Modells darstellt (s.Hyperparameteroptimierung) oder das Ergebnis eines experimentellen Versuchs zurückgibt (s.Design of Experiment). Die Grundidee der Bayes’schen Optimierung besteht aus dem PrinzipExploitation and Exploration.[2]Dies bedeutet, dass beim Vorschlagen neuer Punkte einTrade-offzwischen bereits bekannten guten Punkten (Exploitation) und neuen hoffentlich noch besseren Punkten (Exploration) gefunden wird, welcher durch die gewählte Akquisitionsfunktion (acquisition function)beeinflusst wird. Sucht man in der Bayes’schen Optimierung einen Tradeoff für die Maximierung eines Erwartungswertes und die Minimierung der Streuung, kann dies alsMehrzieloptimierungbetrachtet werden. Der Begriff wird im Allgemeinen Harold J. Kushner und Jonas Mockus zugeschrieben, welche in den 1960er und 1970er Jahren begannen, dazu zu publizieren.[3][4] Die Bayes'sche Optimierung wird typischerweise beiOptimierungsproblemender Formmaxx∈Af(x){\\textstyle \\max _{x\\in A}f(x)}eingesetzt, wobeiA{\\textstyle A}die Menge aller zulässigen Punktex{\\textstyle x}ist, welche auchzulässige Mengegenannt wird. Die Bayes'sche Optimierung ist besonders vorteilhaft für Probleme, bei denen die Berechnung einesFunktionswertsf(x){\\textstyle f(x)}derZielfunktionf{\\displaystyle f}mit großem Aufwand verbunden ist. Die Zielfunktionf{\\textstyle f}ist in der Regelstetigund ist ansonsten von unbekannter Struktur, die als „Black Box“ bezeichnet wird. Bei ihrer Auswertung wird nur der Funktionswertf(x){\\textstyle f(x)}beobachtet; dieAbleitungenwerden nicht berechnet.[5] Da dieZielfunktionf{\\displaystyle f}unbekannt ist, besteht die Bayes'sche Strategie darin, sie als Zufallsfunktion zu behandeln und ihr einenPriorzuzuweisen. Der Prior gibt die Annahmen über das Verhalten der Funktion wieder. Nach dem Sammeln einer Stichprobe bestehend aus Punktenxi{\\displaystyle x_{i}}und zugehörigen „wahren“ Funktionswertenf(xi){\\displaystyle f(x_{i})}, wird der Prior aktualisiert, um die Posterior-Verteilung über die Zielfunktion zu bilden. Die Posterior-Verteilung wird wiederum verwendet, um eine Akquisitionsfunktion (oft auch als Infill-Sampling-Kriterium bezeichnet) zu konstruieren, dieoptimiertwird, um den nächsten Abfragepunkt zu bestimmen. In derOptimierungsollen neue Punktext∗{\\displaystyle x_{t}^{*}}vorgeschlagen werden, dief{\\displaystyle f}maximieren/minimieren. Die Bayes'sche Optimierung beruht darauf, dass ein Surrogatmodellf^{\\displaystyle {\\hat {f}}}an der Stellex∈A{\\displaystyle x\\in A}leichter auszuwerten ist als die echte Black-Box Funktionf{\\displaystyle f}. Typischerweise werden als SurrogatmodelleGauß-Prozesse,Parzen-Tree Estimator,Random Forestsoder anderebootstrap aggregated modelsverwendet. Gemeinsam haben diese Schätzmodelle, dass sie eine Varianzschätzung (und damit eine Schätzung der Verteilung) erlauben. Diese Varianzsschätzung wird anschließend in der Akquisitionsfunktion verwendet, um nicht nur das (mittlere, erwartete) Optimum zu finden, sondern zusätzlich einen Erwartungswert-Varianz-Trade-offeinzugehen: Punktex{\\displaystyle x}mit hoher Varianz in der Vorhersage des Surrogatmodells könnten beispielsweise einen deutlich höheren echten Wertf{\\displaystyle f}aufweisen als das Modellf^{\\displaystyle {\\hat {f}}}bisher modelliert hat. Punkte mit hoher Chance auf eine mögliche Verbesserung (gemessen durch die Akquisitionsfunktion) werden als neue Punkte zur Auswertung vonf{\\displaystyle f}vorgeschlagen. Jedes Mal, wenn eine Auswertung erfolgt, wird das neue Wertepaar(x,f(x)){\\displaystyle (x,f(x))}in das Trainingsset des Surrogatmodells aufgenommen. Es ergeben sich dann neue Punkte mit hoher Chance auf eine mögliche Verbesserung und der Vorgang wird bis zur Konvergenz wiederholt, wobei keine Konvergenz im mathematischen Sinne erwartet wird, sondern die Methode beispielsweise nach einer bestimmten Anzahl von Iterationen aus Budget-Gründen endet oder ein vorzeitiger Abbruch vorgenommen wird, falls sich in den letzten Iterationen keine Verbesserung mehr feststellen ließ. Mit Kenntnis der durch das Surrogatmodell geschätzten bedingten Wahrscheinlichkeitsdichtep^(f|xi){\\displaystyle {\\hat {p}}(f|x_{i})}kann die Akquisitionsfunktion iterativ optimiert werden. Der nächste zu überprüfenden Punkt istxt=argmaxxAcquisition function(x){\\displaystyle x_{t}={\\text{argmax}}_{x}{\\text{Acquisition function}}(x)}(wenn die Akquisitionsfunktion maximiert werden soll). Die argmax-Funktion kann näherungsweise für eineendliche Mengean zufälligen Punkten{x1,…xn}{\\displaystyle \\{x_{1},\\dots x_{n}\\}}ausgewertet werden. Die Näherung an argmax ist dann der x-Wert, welcher den größten Wert der Akquisitionsfunktion hat. Die folgende algorithmische Grundidee der Bayes'schen Optimierung wird wie folgt in Anlehnung an die Literatur dargestellt.[2] Probleme, die von der oben gemachten Annahme der leichten Auswertung abweichen, werden alsexotische Bayes’sche Optimierungsproblemebezeichnet. Optimierungsprobleme können exotisch werden, wenn bekannt ist, dass esRauschengibt, die Auswertungen parallel durchgeführt werden, die Qualität der Auswertungen von einem Kompromiss zwischen Schwierigkeit und Genauigkeit abhängt, zufällige Umgebungsbedingungen vorhanden sind oder die AuswertungAbleitungenbeinhaltet.[5] Akquisitionsfunktionen stellen einen Kompromiss (Trade-off) zwischen Erkundung und Ausnutzung dar, um die Anzahl der Funktionsabfragen zu minimieren. Die Bayes’sche Optimierung eignet sich daher gut für Funktionen, deren Auswertung teuer ist. Beispiele für Akquisitionsfunktionen (engl.acquisition function) sind: Die erwartete Verbesserung(expected improvement)istEI(x)=E[f|x]−f(x†){\\displaystyle EI(x)=E[f|x]-f(x^{\\dagger })}[8]. Hierbei istf(x†){\\displaystyle f(x^{\\dagger })}der bisher tatsächlich beobachtete Maximalwert der Blackbox-Funktionf{\\displaystyle f}. Derbedingte Erwartungswertberechnet sich durchE[f|x]=∫Rfp(f|x)df{\\displaystyle E[f|x]=\\int _{\\mathbb {R} }fp(f|x)df}. In der Literatur gibt es auch andere nicht äquivalente Definitionen der erwarteten Verbesserung: Für einen Gauss-Prozess folgt aus der Definition[10]: wobeiΦ{\\displaystyle \\Phi }dieVerteilungsfunktionderNormalverteilungist undϕ{\\displaystyle \\phi }dieWahrscheinlichkeitsdichtefunktionder Normalverteilung ist. Die erwartete Verbesserung ermöglicht somit den Vorschlag von Punktenx{\\displaystyle x}für dieμ^(x)<f(x†){\\displaystyle {\\hat {\\mu }}(x)<f(x^{\\dagger })}, bei denen aberσ^(x){\\displaystyle {\\hat {\\sigma }}(x)}groß ist. Somit ermöglicht diese Akquisitionsfunktion die Exploration. Die erwartete Verbesserung wählt im Vergleich zur Akquisionsfunktion mit Konfidenzgrenzen einen gewissen Explorations-Exploitation-Trade-offohne weiteren expliziten Hyperparameterκ{\\displaystyle \\kappa }(welcher bei LCB or UCB als Akquisitionsfunktion vorliegt). Die Maximierung der Akquisitionsfunktion erfolgt durch Verfahren der mathematischen Optimierung (Newtonverfahren, Quasi-Newton-Methoden wie demBroyden-Fletcher-Goldfarb-Shanno-Algorithmus…) oder durchRandom Sampling, das heißt das Auswerten der Akquisitionsfunktion an zufällig gezogenen Punkten der zulässigen MengeA{\\displaystyle A}. Der Ansatz wurde zur Lösung einer Vielzahl von Problemen angewandt,[11]darunterHyperparameteroptimierung, Rangordnungslernen,[12]Computergrafikund visuelles Design,[13][14][15]Robotik,[16][17][18][19]Sensornetzwerke,[20][21]automatische Algorithmenkonfiguration,[22][23]automatische Toolboxen fürmaschinelles Lernen,[24][25][26]Reinforcement Learning, Planung, visuelle Aufmerksamkeit, Architekturkonfiguration beimDeep Learning, statischeProgrammanalyse,experimentelle Teilchenphysik,[27][28]Chemie,Materialdesign[29][30][31]undArzneimittelentwicklung.[5][32][33] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Ausgangslage 3Strategie 3.1Surrogatmodell 3.2Vorschlagen neuer Punkte 4Pseudocode 5Exotische Bayes’sche Optimierung 6Beispiele für Akquisitionsfunktionen 6.1Erwartete Verbesserung 7Lösungsmethoden 8Anwendungsgebiete 9Literatur 10Weblinks 11Einzelnachweise Català English Español فارسی Suomi Français Українська Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten"
  },
  {
    "label": 1,
    "text": "Big Data – Wikipedia Big Data Inhaltsverzeichnis Begriff Datenherkunft Beispiele Verarbeitung von Big Data Anwendung (Auswahl) Kritik Siehe auch Literatur Weblinks Einzelnachweise Weitere Bedeutungen Wachstum Politische Wahlen Social Scoring Bildungswesen Forschung Microtargeting Datenschutz Unzureichende Regulierung Mangelhafte Grundlage für Auswertungen Hype, Schwammiger Begriff Sachbücher Forschungsberichte Belletristik Der aus dem englischen Sprachraum stammende BegriffBig Data[ˈbɪɡ ˈdeɪtə] (vonenglischbig‚groß‘ unddata‚Daten‘, deutsch auchMassendaten) steht in engem Zusammenhang mit dem umfassenden Prozess derDatafizierungund bezeichnetDatenmengen, welche zu groß, zu komplex, zu schnelllebig oder zu schwachstrukturiertsind, um sie mit manuellen und herkömmlichen Methoden derDatenverarbeitungauszuwerten.[1] Big Data wird häufig alsSammelbegrifffür digitale Technologien verwendet, die in technischer Hinsicht für eine neue Äradigitaler Kommunikation und Verarbeitungund in sozialer Hinsicht für einen gesellschaftlichen Umbruch verantwortlich gemacht werden.[2]Dabei unterliegt der Begriff alsSchlagworteinem kontinuierlichen Wandel; so wird damit ergänzend auch oft der Komplex derTechnologienbeschrieben, die zum Sammeln und Auswerten dieser Datenmengen verwendet werden.[3][4] In der Definition von Big Data bezieht sich das „Big“ auf die vier Dimensionen Erweitert wird diese Definition um die zwei Vsvalue(Wert) undvalidity(Richtigkeit), welche für einen unternehmerischenMehrwertund die Sicherstellung derDatenqualitätstehen.[6] Big Data bezeichnet primär die Verarbeitung von großen, komplexen und sich schnell ändernden Datenmengen. AlsBuzzwordbezeichnet der Begriff in denMassenmedienaber andere Bedeutungen: Die gesammelten Daten können dabei aus verschiedensten Quellen stammen (Auswahl): Big Data umfasst auch Bereiche, die alsintimbzw.privatgelten: Der Wunsch der Industrie und bestimmter Behörden, möglichst freien Zugriff auf diese Daten zu erhalten, sie besser analysieren zu können und die gewonnenen Erkenntnisse zu nutzen, gerät dabei unweigerlich in Konflikt mit geschütztenPersönlichkeitsrechtender Einzelnen. Ein Ausweg ist allein durch eine Anonymisierung der Daten zu erreichen. Klassische Anwender sindProvidersozialer Netzwerkeund vonSuchmaschinen. Die Analyse, Erfassung und Verarbeitung von großen Datenmengen ist heute in vielen Bereichen alltäglich. Big Data kannGeschäftsprozessverbesserungen in allenFunktionsbereichenvon Unternehmen, vor allem aber im Bereich derTechnologieentwicklungundInformationstechniksowie desMarketingsermöglichen.[13]Die Erhebung und Verwertung der Datenmengen dient dabei im Allgemeinen der Umsetzung von Unternehmenszielen oder zur staatlichen Sicherheit. Bisher haben vor allem große Branchen, Unternehmen und Anwendungsbereiche der Wirtschaft,Marktforschung, Vertriebs- und Servicesteuerung, Medizin, Verwaltung undNachrichtendienstedie entsprechenden digitalen Methoden für sich genutzt: Die erfassten Daten sollen weiterentwickelt und nutzbringend eingesetzt werden. Die Erhebung der Daten dient dabei meistens für konzernorientierte Geschäftsmodelle sowieTrendforschungin den sozialen Medien und Werbeanalysen, um zukunftsweisende und möglicherweise gewinnbringende Entwicklungen zu erkennen und inPrognosenumzumünzen.[14] Mengen von Daten wachsen typischerweiseexponentiell. Berechnungen aus dem Jahr 2011 zufolge verdoppelt sich das weltweite erzeugteDatenvolumenalle 2 Jahre.[15]Diese Entwicklung wird vor allem getrieben durch die zunehmende maschinelle Erzeugung von Daten z. B. über Protokolle von Telekommunikationsverbindungen (Call Detail Record, CDR) und Webzugriffen (Logdateien), automatische Erfassungen vonRFID-Lesern,Kameras,Mikrofonenund sonstigen Sensoren. Big Data fällt auch in der Finanzindustrie an (Finanztransaktionen, Börsendaten) sowie im Energiesektor (Verbrauchsdaten) und imGesundheitswesen(Abrechnungsdaten derKrankenkassen). In derWissenschaftfallen ebenfalls große Datenmengen an, z. B. in derGeologie,Genetik,KlimaforschungundKernphysik. Der IT-BranchenverbandBitkomhat Big Data als einen Trend im Jahr 2012 bezeichnet.[16]Bei großen Datenkomplexen verbietet sich der unwirtschaftliche Aufwand für ein Speichern auf Vorrat. Dann werden lediglichMetadatengespeichert oder das Auswerten setzt mitlaufend oder höchstens gering zeitversetzt mit dem Entstehen der Daten auf. Zugang zu einem entsprechenden Datenvolumen haben die entsprechenden Konzerne, etwa Suchmaschinen, und bestimmte staatliche Institutionen, etwa Geheimdienste.[17] In der Forschung können durch Verknüpfung großer Datenmengen undstatistischeAuswertungen neue Erkenntnisse gewonnen werden, insbesondere in Disziplinen, in denen bisher viele Daten noch von Hand ausgewertet wurden. Unternehmen erhoffen sich von der Analyse von Big Data Möglichkeiten zur Erlangung von Wettbewerbsvorteilen, zur Generierung vonEinsparungspotentialenund zur Schaffung neuer Geschäftsfelder. Staatliche Stellen erhoffen sich dagegen bessere Ergebnisse in derKriminalistikundTerrorismusbekämpfung.[18]Beispiele für erwartete Vorteile sind: Die reine Analyse von Kundendaten ist jedoch noch nicht automatisch Big Data – oft handelt es sich bei vielen Anwendungen aus demMarketingviel mehr umSmall-Data-Analytics.[9] Klassischerelationale Datenbanksystemesowie Statistik- und Visualisierungsprogramme sind oft nicht in der Lage, derart große Datenmengen zu verarbeiten. Für Big Data kommen daher neue Arten von Datenspeicher- und Analyse-Systemen zum Einsatz, dieparallelauf bis zu Hunderten oder Tausenden von Prozessoren beziehungsweise Servern arbeiten, wie zum Beispiel inkognitiven Systemen. Dabei gibt es unter anderem folgende Probleme: Die Entwicklung von Software für die Verarbeitung von Big Data befindet sich noch in einer frühen Phase. Bekannt ist derMapReduce-Ansatz, der beiOpen-Source-Software (Apache HadoopundMongoDB) sowie bei einigen kommerziellen Produkten (unter anderemAster DataoderGreenplum) zum Einsatz kommt.[28] Bei derPräsidentschaftswahl in den Vereinigten Staaten 2016sowie bei dem Volksentscheid in Großbritannien über den Austritt aus der Europäischen Union im selben Jahr (Brexit) hatten die überraschenden Gewinner jeweils das UnternehmenCambridge Analyticaengagiert, die sich mit der Erhebung, Auswertung, Anwendung und Zuordnung sowie mit dem Verkauf hauptsächlich im Internet gewonnener persönlicher Daten beschäftigt und Methoden derPsychometrieanwendet, einem Ableger der Psychologie.[20][21] Gesammelte Daten werden zur Bewertung z. B. derKreditwürdigkeit(->Kreditscoring), derGesundheit(und entsprechender Risiken, woraus z. B. auch die Gestaltung entsprechend angepassterVersicherungsprämienfolgt) oder des Konsum- und Einkaufsverhaltens von Verbrauchern herangezogen, auch zum Versuch entsprechender Voraussagen (Predicting); inChinabaut auf ihnen dasSocial Scoring-System auf, mit dem auch das soziale Verhalten der Einwohner kontrolliert und bewertet wird und verbessert werden soll.[29][30] Der Einsatz von Big Data eröffnet für das Bildungswesen neue Möglichkeiten. Die Technik kann zur Optimierung von Lernformen und Bildungsprogrammen genutzt werden.[31]Experten wieViktor Mayer-Schönbergerund Kenneth Cukier rechnen mit einem grundlegenden Umbruch des Bildungssektors durch den Einsatz von Big Data.[32] Durch die Fortschritte in der Datenverarbeitung können anhand großer Datenmenge wesentlich zuverlässigere Ergebnisse erzielt werden. Beispiele sind eine Studie mit rund 16.000 Kindern, in der Zusammenhänge zwischenÜbergewichtundDiabetesuntersucht wurden,[33]und eine Fall-Kontroll-Studie zum Einfluss vonFluglärm, bei der die Krankenkassendaten von über einer Million Patienten ausgewertet wurden.[34] Die FirmaCambridge Analyticaließ nach der US-Präsidentschaftswahl 2016 verlauten, dass der Einsatz sogenannterMicrotargeting-Techniken entscheidend zum Wahlsieg vonDonald Trumpbeigetragen haben soll.[35]So habe man mittels psychometrischer Analysen von großen Datensätzen unentschiedene beziehungsweise leichter zu beeinflussende Wähler (swing voters) identifizieren und anschließend gezielt via Facebook mit auf sie zugeschnittenen Wahlwerbungen und Inhalten konfrontieren können.[35]Dem Einsatz besagter Techniken im US-Wahlkampf vorausgegangen waren Forschungsarbeiten des Psychologen Michal Kosinski. Darin verknüpfte Kosinski Big-Data-Auswertungen mit psychologischen Verhaltensanalysen und konnte zeigen, dass sich anhand der Facebook-Likes von Nutzer deren Persönlichkeitseigenschaften, die sexuelle Orientierung, Drogenkonsum sowie die religiöse und politische Einstellung vorhersagen lassen.[36] Die US-amerikanische WirtschaftswissenschaftlerinShoshana Zuboffprägte im Zusammenhang mit der Sammlung von personenbezogenen Daten durch Internetkonzerne wieGoogleundFacebookden BegriffÜberwachungskapitalismusund sieht darin eine Mutation desIndustriekapitalismus, der die private menschliche Erfahrung für frei verfügbares Rohmaterial für die kapitalistische Produktion und den Warenaustausch hält und der die Errungenschaften derDigitalen Revolutionzur konspirativen Überwachung, Speicherung, Manipulation und Vorhersage menschlichen Verhaltens nutzt. Zuboff befürwortet die Zerschlagung der derartiger Datenmonopole bildenden Konzerne und Verbote, um die Bildung von Datenkonzentrationen zu beenden.[37] Wie Forschungsergebnisse unterschiedlicher Wissenschaftler zeigen, lassen sich aus den von Nutzern geteilten Inhalten im Internet zum Teil hoch sensible Informationen extrahieren, die nicht beabsichtigt wurden, geteilt zu werden.[36][38][39]Zum Schutz der digitalenPrivatsphäregewinnen rechtsstaatliche Reglementierungen der Informationsspeicherung und -sammlung daher immer mehr an Relevanz. Doch auch auf Staatsebene werden Big Data zum Teil genutzt, um Informationen über Individuen zusammenzutragen, wie dasSozialkredit-Systemin China zeigt.[40] Der Datenwissenschaftler Andreas Dewes hat in einer Untersuchung gezeigt, dassanonymisierteDaten von Internetnutzern, die von Firmen gesammelt und verkauft wurden, wieder entschlüsselt und Personen zugeordnet werden können. Aus den von Dewes im Rahmen seiner Untersuchung von Werbefirmen gekauften, angeblich „anonymen“ Daten von ca. drei Millionen Deutschen waren Mitglieder desDeutschen Bundestagsund vonLandesparlamentensowie weitere Personen des öffentlichen Lebens wieRichter,Polizeibeamteoder andereFunktionäre.[41] Der Europäische DatenschutzbeauftragteGiovanni Buttarellibetonte im März 2013, persönliche Informationen seien keine Ware.[42] Mit Bezug auf die Versicherungsbeitragsanpassung mittels Big Data wird unter anderem die „Gefahr einer schleichenden Entsolidarisierung in der Versicherung“ hervorgehoben.[43] Eine entscheidende Frage ist, wem die von Privatpersonen gesammelten Daten gehören, wer die Verfügungshoheit über sie behält und wer ihre Nutzung kontrolliert. Inwieweit die europäischeDatenschutz-Grundverordnung, die seit 25. Mai 2018 anzuwenden ist, ausreicht, wird in der Öffentlichkeit diskutiert. Der schleswig-holsteinischeDatenschutzbeauftragteThilo Weichertwarnte 2013: „Big Data eröffnet Möglichkeiten des informationellenMachtmissbrauchsdurchManipulation, Diskriminierung und informationelle ökonomischeAusbeutung– verbunden mit der Verletzung der Grundrechte der Menschen.“[44][45] Dirk Helbing, Professor für Computational Social Science an der ETH Zurich, warnte im Januar 2018 vor möglichen Technologien subtiler Manipulation auf Basis von Big Data.[46]Der TechnikfolgenabschätzerArmin Grunwald, Leiter des Institut für Technikfolgenabschätzung und Systemanalyse (ITAS) in Karlsruhe, warnt, es habe zu keiner Zeit in der Menschheitsgeschichte „derart gute Bedingungen für eine totalitäre Diktatur“ gegeben wie heute.[47] Der Sozialforscher Nils Zurawski plädiert für eine \"solidarische Datenspeicherung\", um die Vorteile von Big Data für dasGemeinwohlnutzen zu können.[48] Kritik gibt es vor allem daran, dass dieDatenerhebungund -auswertung praktisch ausschließlich nach technischen Aspekten erfolgt und beispielsweise der technisch einfachste Weg gewählt wird, die Daten zu erheben. Statistische Grundprinzipien wie das einerrepräsentativen Stichprobewerden oft vernachlässigt. So kritisierte die SozialforscherinDanah Boyd:[49] Ein Forscher ermittelte beispielsweise, dass Menschen nicht mehr als 150 Freundschaften pflegen (Dunbar-Zahl), was sodann als technische Begrenzung in sozialen Netzwerken eingeführt wurde – in der falschen Annahme, als „Freunde“ bezeichnete Bekanntschaften würden echte Freundschaften widerspiegeln.[49][50]Sicherlich würde nicht jeder alle seineFacebook-Freunde in einem Interview als Freunde benennen – der Begriff eines „Freundes“ signalisiert bei Facebook lediglich eine Kommunikationsbereitschaft. Ein anderer kritischer Ansatz setzt sich mit der Frage auseinander, ob Big Data das Ende aller Theorie bedeutet. Chris Anderson, Chefredakteur beim MagazinWiredbeschrieb 2008 das Glaubwürdigkeitsproblem jeder wissenschaftlichenHypotheseund jedes Modells bei gleichzeitigerEchtzeitanalyselebender und nicht lebender Systeme.Korrelationenwerden wichtiger alskausaleErklärungsansätze, die sich oft erst später bewahrheiten oderfalsifizierenlassen.[51] Der Begriff „Big Data“ wird gelegentlich auch dann verwendet, wenn Daten weder groß noch komplex sind oder sich nicht schnell ändern oder mit herkömmlichen Techniken problemlos verarbeitet werden können.[9]Die zunehmende Aufweichung des Begriffs führt nach Meinung einiger Beobachter dazu, dass er immer mehr ein aussageloser Marketingbegriff werde und vielen Prognosen zufolge innerhalb der nächsten Jahre eine starke Abwertung erfahre („Tal der Enttäuschungen“ imHype-Zyklus). Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Begriff 1.1Weitere Bedeutungen 2Datenherkunft 2.1Wachstum 3Beispiele 4Verarbeitung von Big Data 5Anwendung (Auswahl) 5.1Politische Wahlen 5.2Social Scoring 5.3Bildungswesen 5.4Forschung 5.5Microtargeting 6Kritik 6.1Datenschutz 6.2Unzureichende Regulierung 6.3Mangelhafte Grundlage für Auswertungen 6.4Hype, Schwammiger Begriff 7Siehe auch 8Literatur 8.1Sachbücher 8.2Forschungsberichte 8.3Belletristik 9Weblinks 10Einzelnachweise Afrikaans العربية অসমীয়া Azərbaycanca Беларуская Български বাংলা"
  },
  {
    "label": 1,
    "text": "Chatbot – Wikipedia Chatbot Inhaltsverzeichnis Geschichte Funktionsweise Einrichtung eines Chatbots Multimediale Chatbots Beispiele Siehe auch Literatur Weblinks Einzelnachweise Regelbasierte Chatbots Sprachassistenten Sprachmodelle EinChatbot(IPA:[ˈt͡ʃɛtˌbɔt]anhörenⓘ/?), auchChatterbotoder kurz Bot, ist ein textbasiertes Dialogsystem, dasChattenmit einem technischen System erlaubt. Er hat je einen Bereich zur Textein- und -ausgabe, über die sich in natürlicher Sprache mit dem System kommunizieren lässt. Chatbots können, müssen aber nicht in Verbindung mit einemAvatarbenutzt werden. Technisch sind Chatbots näher mit einer Volltextsuchmaschine verwandt als mitkünstlicheroder gar natürlicher Intelligenz. Mit der steigenden Computerleistung können Chatbot-Systeme allerdings immer schneller auf immer umfangreichere Datenbestände zugreifen und daher auch intelligente Dialoge für den Nutzer bieten, wie zum Beispiel das beiOpenAIentwickelteChatGPToder das vonGoogle LLCvorgestellteLanguage Model for Dialogue Applications(LaMDA). Solche Systeme werden auch alsvirtuelle persönliche Assistentenbezeichnet. Es gibt auch Chatbots, die gar nicht erst versuchen, wie ein menschlicher Chatter zu wirken (daher keine Chatterbots), sondern ähnlich wieIRC-Dienstenur auf spezielle Befehle reagieren. Sie können als Schnittstelle zu Diensten außerhalb des Chats dienen, oder auch Funktionen nur innerhalb ihres Chatraums anbieten, z. B. neu hinzugekommene Chatter mit dem Witz des Tages begrüßen. Heute wird meistens durch digitale Assistenten wieGoogle AssistantundAmazon Alexa, über Messenger-Apps wieFacebook MessengeroderWhatsAppoder aber über Organisationstools und Webseiten auf Chatbots zugegriffen.[1][2] Die Geschichte von Chatbots geht bis in die 1960er-Jahre zurück.[3]Als erster Chatbot giltEliza, eine erste Demonstration einer virtuellen Psychotherapeutin, dieJoseph Weizenbaumin den Jahren 1964 bis 1966 programmierte.[4]Das Programm lieferte eine Reihe vongeskriptetenAntworten anhand von Schlüsselwörtern in der Anfrage des Benutzers und fest codierten Regeln. Die Skripte wurden derart gestaltet, dass Fragen möglichst mit einer Gegenfrage beantwortet wurden. In den darauffolgenden Jahrzehnten haben zahlreiche Entwickler Weizenbaums Modell verwendet, um menschenähnliche Interaktionen mit Chatbots weiterzuentwickeln. Eine erweiterte Version von Eliza war PARRY, welches 1972 erschien und vom PsychiaterKenneth Colbyentwickelt wurde. Es simuliert eine Person mit paranoiderSchizophrenie. Es handelte sich um eine verbesserte Version von ELIZA und der erste Chatbot, welcher eine einfache Version desTuring-Testsbestand, was das Ziel hinter der Entwicklung früher Chatbot war.[3]Die Verbesserung bestand vor allem im Skript, welche auch emotionale Antworten beinhaltete.[4] Im Jahr 1992 wurde vonCreative LabsmitDr. Sbaitsoein zu ELIZA ähnlich arbeitender Chatbot fürMS-DOSangeboten.[4] In 1995 wurde vonRichard Wallaceder ChatbotA.L.I.C.E.(Artificial Linguistic Internet Computer Entity) veröffentlicht. Dieser Bot war die Grundlage für dieArtificial Intelligence Markup Language(AIML).[4] Der vom InformatikerRollo Carpenterentwickelte ChatbotJabberwacky, welches 1997 im Web veröffentlicht wurde, erweiterte das Konzept um eine Datenbank aus Fragen und Antworten. Die best mögliche Antwort wurde mittels einesneuronalen Netzesbestimmt. Später wurde das Konzept zuCleverbotweiterentwickelt, welches 2008 veröffentlicht wurde. Im Jahr 2001 wurde mitSmarterChildein Chatbot veröffentlicht, welcher ebenfalls neuronale Netzwerke einsetzte und imAmerican Online Instant Messenger(AIM) undWindows Live Messengerzum Einsatz kam.[5]Auch hier wurde das neuronale Netzwerk dazu eingesetzt passende Antwortschnipsel auszuwählen Von 2001 bis 2015 wurde dieChatterbox Challengeausgerichtet, ein internationaler Wettbewerb, der denChatbot des Jahreskürte.[6] Im Jahr 2011 wurde von Siri Inc. derSprachassistentSiriveröffentlicht. Dieser Chatbot kombinierte zu dieser Zeit mehrere Technologien wieSpracherkennungund Wissensgraphen sowie die Möglichkeit Interaktionen auszuführen. Die Anfrage des Benutzers wurde hierbei, ähnlich wie beiJabberwacky, von einem neuronalen Netzwerk mitSuchmaschinen-Ergebnissen,Wissensgraphen, oder Interaktionen gematched. Das Unternehmen wurde später vonAppleübernommen. Zu Siri analoge Technologien wurden im Jahr 2012 in Form vonGoogle Nowund desGoogle Assistant[4](basierend auf derGoogle-Suchmaschine, demKnowledge Graphund DialogFlow) veröffentlicht. Im Jahr 2013 folgteAmazon Alexa[4]und im Jahr 2014 folgteMicrosoftmitCortana[4](basierend aufBingund dem Bot Framework).[7] Am 23. März 2016 veröffentlichte Microsoft den ChatbotTay. Der Chatbot übernahm Konversationen überTwitterund lernte von Benutzern. Dies führte dazu, dass Tay innerhalb kurzer Zeit negative Verhaltensweisen von den Benutzern übernahm und noch am selben Tag abgeschaltet wurde. Einen weiteren Durchbruch bei Chatbots ermöglichtenSprachmodelle, welche im Jahr 2023 in Form vonChatGPT,Google Bard,Microsoft Copilot,Grok, sowieClaudeveröffentlicht wurden. Diese Modelle boten ausreichend Sprachverständnis, um erstmals komplexere Unterhaltungen zu führen und einfache Aufgaben zu lösen. Die meisten Chatbots greifen auf eine vorgefertigte Datenbank, eine sogenannteWissensdatenbankmit Antworten und Erkennungsmustern, zurück. DasProgrammzerlegt die eingegebene Frage zuerst in Einzelteile und verarbeitet diese nach vorgegebenen Regeln. Dabei können Schreibweisen harmonisiert (Groß- und Kleinschreibung, Umlaute etc.), Satzzeichen interpretiert undTippfehlerausgeglichen werden (Preprocessing). Im zweiten Schritt erfolgt dann die eigentliche Erkennung der Frage. Diese wird üblicherweise über Erkennungsmuster gelöst, manche Chatbots erlauben darüber hinaus die Verschachtelung verschiedener Mustererkennungen über sogenannte Makros. Wird eine zur Frage passende Antwort erkannt, kann diese noch angepasst werden (beispielsweise können skriptgesteuert berechnete Daten eingefügt werden – „In Ulm sind es heute 37 °C.“). Diesen Vorgang nennt man Postprocessing. Die daraus entstandene Antwort wird dann ausgegeben. Moderne kommerzielle Chatbot-Programme erlauben darüber hinaus den direkten Zugriff auf die gesamte Verarbeitung über eingebaute Skriptsprachen undProgrammierschnittstellen. Neben regelbasierten Chatbots existieren Chatbots auf Basis künstlicher Intelligenz (KI). Chatbots, die auf KI undNatural Language Processing(NLP) basieren, kommen bei komplexeren Abläufen zur Anwendung. DieEU-Kommissionbeschreibt Systeme auf KI-Basis als solche mit einem intelligenten Verhalten, die die eigene Umgebung analysieren und mit einem spezifischen Grad an Autonomie handeln, um bestimmte Ziele zu erreichen.[8]Derartige Chatbots erkennen die jeweilige Anfrage und Intention des Nutzers, ziehen aus den fortlaufenden Dialogen Schlüsse und entwickeln so ihre Datenbank ständig weiter.[9]LautBundesministerium für Bildung und Forschung(BMBF) handelt es sich bei den lernenden Systemen und den Werkzeugen der KI um „die nächste Entwicklungsstufe der Digitalisierung“.[10]Laut BMBF handele es sich also um technische Systeme, die Probleme eigenständig bearbeiten und sich dabei selbst auf veränderte Bedingungen einstellen können. Die Herausforderung bei der Programmierung eines Chatbots liegt in der sinnvollen Zusammenstellung der Erkennungen. Präzise Erkennungen für spezielle Fragen werden dabei ergänzt durch globale Erkennungen, die sich nur auf ein Wort beziehen und alsFallbackdienen können (der Bot erkennt grob das Thema, aber nicht die genaue Frage). Manche Chatbot-Programme unterstützen die Entwicklung dabei über Priorisierungsränge, die einzelnen Antworten zuzuordnen sind. Zur Programmierung eines Chatbots werden meist Entwicklungsumgebungen verwendet, die es erlauben, Fragen zu kategorisieren, Antworten zu priorisieren und Erkennungen zu verwalten.[11][12]Dabei lassen manche auch die Gestaltung eines Gesprächskontexts zu, der auf Erkennungen und möglichen Folgeerkennungen basiert („Möchten Sie mehr darüber erfahren?“). Ist die Wissensbasis aufgebaut, wird der Bot in möglichst vielen Trainingsgesprächen mit Nutzern der Zielgruppe optimiert.[13]Fehlerhafte Erkennungen, Erkennungslücken und fehlende Antworten lassen sich so erkennen.[14]Meist bietet die Entwicklungsumgebung Analysewerkzeuge, um die Gesprächsprotokolle effizient auswerten zu können.[15]Ein guter Chatbot erreicht auf diese Weise eine mittlere Erkennungsrate von mehr als 70 % der Fragen. Er wird damit von den meisten Nutzern als unterhaltsamer Gegenpart akzeptiert. Ursprünglich rein textbasiert haben sich Chatbots durch immer stärker werdendeSpracherkennungundSprachsyntheseweiterentwickelt und bieten neben reinen Textdialogen auch vollständig gesprochene Dialoge oder einen Mix aus beidem an. Zusätzlich können auch weitere Medien genutzt werden, beispielsweise Bilder und Videos. Gerade mit der starken Nutzung von mobilen Endgeräten (Smartphones,Wearables) wird diese Möglichkeit der Nutzung von Chatbots weiter zunehmen (Stand: Nov. 2016).[16]Mit fortschreitender Verbesserung sind Chatbots dabei nicht nur auf wenige eingegrenzte Themenbereiche (Wettervorhersage, Nachrichten usw.) begrenzt, sondern ermöglichen erweiterte Dialoge und Dienstleistungen für den Nutzer. Diese entwickeln sich so zuIntelligenten Persönlichen Assistenten. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 1.1Regelbasierte Chatbots 1.2Sprachassistenten 1.3Sprachmodelle 2Funktionsweise 3Einrichtung eines Chatbots 4Multimediale Chatbots 5Beispiele 6Siehe auch 7Literatur 8Weblinks 9Einzelnachweise Afrikaans العربية Azərbaycanca Boarisch Български भोजपुरी বাংলা Bosanski Català کوردی Čeština Dansk English Esperanto Español Eesti Euskara فارسی Suomi"
  },
  {
    "label": 1,
    "text": "Computer Vision – Wikipedia Computer Vision Inhaltsverzeichnis Geschichte Komplexität Überblick der Methodik Bildentstehung Kamerakalibrierung Verzeichnungskorrektur Bildverarbeitung (Filterung, Glättung, Rauschunterdrückung) Merkmalsextraktion und Mustererkennung (feature detection and pattern recognition) Objekterkennung (object detection) Grundlagen der projektiven Geometrie Standardabbildungsmodell (Zentralprojektion) Korrespondenzproblem (Bildpunktzuordnung) Stereo-Bild-Verarbeitung Bildsequenz-Verarbeitung (Struktur aus Bewegung) Shape-from-X Aktive und sonstige Sensoren Weitere Methoden Anwendungen Einzelnachweise Literatur Weblinks Lochkameramodell Reale Kamera Digitale Sensoren Optische Begriffe in Kameras Kantendetektion (edge detection) Eckendetektion (Punktdetektion, corner detection) Bildsegmentierung (image segmentation) Hough-Transformation Konzept homogener Koordinaten Projektivtransformation (Homografie) Epipolargeometrie Shape-from-Stereo Shape-from-Silhouette / Shape-from-Contour Shape-from-Shading / Photometric Stereo Shape-from-Motion / Optischer Fluss Shape-from-Texture Strukturiertes codiertes Licht Shape-from-(De-)Focus LiDAR 3D-TOF-Kamera Kinect Omnidirektionale Kameras SLAM Maschinelles Sehen Computer Vision(engl. Aussprache) ist eine Wissenschaft im Grenzbereich zwischen Informatik und den Ingenieurwissenschaften und versucht die von Kameras aufgenommenen Bilder auf unterschiedlichste Art und Weise zu verarbeiten und zu analysieren, um deren Inhalt zu verstehen oder geometrische Informationen zu extrahieren. Der BegriffComputer Visionbedeutet auf Deutsch so viel wiecomputerbasiertes Sehen(oder kurz:Computer-Sehen). Im englischen Sprachraum wird ebenfalls der BegriffMachine Vision(auf Deutsch:Maschinelles Sehen) synonym zu Computer Vision verwendet, wobei die Anwendung im industriellen Umfeld betont wird. Typische Aufgaben der Computer Vision sind die Objekterkennung und die Vermessung der geometrischen Struktur von Objekten sowie von Bewegungen (Fremdbewegung, Eigenbewegung). Dabei wird auf Algorithmen aus derBildverarbeitungzurückgegriffen, zum Beispiel dieSegmentierungund auf Verfahren derMustererkennung, beispielsweise zur Klassifizierung von Objekten. Dabei kommenstatistische(bzw.probabilistische) Methoden zum Einsatz, Methoden der Bildverarbeitung, derprojektiven Geometrie, aus derKünstlichen Intelligenzund derComputergrafik. Die Werkzeuge stammen meistens aus derMathematik, insbesondere ausGeometrie,linearer Algebra,Statistik,Operations Research(Optimierung) undFunktionalanalysis. Darüber hinaus besteht eine enge Verwandtschaft zu benachbarten Fachgebieten, wie derPhotogrammetrie, derFernerkundungund derKartografie.[1][2][3] Anwendungsgebiete sind z. B. die autonome Navigation von Robotern (Fahrerassistenzsysteme), die Filmindustrie zur Erschaffung virtueller Welten (virtual reality), die Spieleindustrie zum Eintauchen und Interagieren in virtuellen Räumen (augmented reality), dieErkennungundVerfolgungvon Objekten (z. B. Fußgänger) oder die Registrierung von medizinischen CT-Aufnahmen und die Erkennung von krankem Gewebe. Seit ungefähr den 1960er Jahren gab es erste Versuche eine Szene durch Kantenextraktion und ihrer topologischen Struktur zu verstehen. Die Extraktion verschiedener Merkmale, wie Kanten und Ecken, war in den 1970er bis 1980er Jahren ein aktives Forschungsgebiet. Anfang der 1980er Jahre wurde untersucht, wie Variationen von Schattierungen durch topografische (Höhen-)Änderungen verursacht werden und damit der Grundstein für Fotometrie und die 3D-Rekonstruktion mittels Schattenwurf gelegt. Gleichzeitig wurden erste merkmalsbasierte Stereo-Korrespondenz-Algorithmen entwickelt sowie intensitätsbasierte Algorithmen zur Berechnung des optischen Fluss. Außerdem wurden 1979 erste Arbeiten zur simultanen Wiederherstellung der 3D-Struktur und der Kamerabewegung (Structure from Motion) begonnen. Mit dem Aufkommen digitaler Kameras in den 1980er Jahren wurden mehr und mehr Anwendungen erforscht und entwickelt. So wurden Bildpyramiden erstmals 1980 von Rosenfeld eingesetzt als Grob-zu-Fein-Strategie zur Suche homologer Bildpunkte (Korrespondenz-Suche). Auch das Konzept des Maßstabsraumes (scale-space) beruht auf Bildpyramiden und wurde maßgeblich erforscht, was die Grundlage moderner Methoden wie SIFT (Scale Invariant Feature Transform) ist. Ab den 1990er Jahren begann man projektive Invarianten zu untersuchen, um Probleme zu lösen wie Struktur-aus-Bewegung (structure from motion) und projektive 3D-Rekonstruktion, die ohne Kenntnis der Kamerakalibrierung auskommt. Gleichzeitig wurden effiziente Algorithmen entwickelt wie Faktorisierungstechniken und globale Optimierungsalgorithmen.[4] Seitdem es günstige Kameras gibt und die PCs immer leistungsfähiger wurden, bekam dieses Fachgebiet einen enormen Aufschwung. Die Aufgabenstellungen sind oftmals inverse Probleme, wo versucht wird, aus zweidimensionalen Abbildungen die Komplexität der dreidimensionalen Welt wieder herzustellen. Computer Vision versucht aus Bildern Eigenschaften zu rekonstruieren, wie die farbliche Gestalt, die Beleuchtung oder deren Form, und darauf basierend versucht man z. B. Gesichter zu erkennen, landwirtschaftliche Flächen zu klassifizieren oder komplexe Objekte zu erkennen (PKW, Fahrrad, Fußgänger). All das gelingt einem Menschen scheinbar spielerisch, es ist aber extrem schwer dies einem Computer beizubringen. Der Versuch, unsere sichtbare Welt in all seiner Gesamtheit modellieren zu wollen, ist bei weitem schwerer, als beispielsweise eine Computer-generierte künstliche Stimme zu erzeugen (Szeliski 2010, S. 3).[4]Dies wird von Wissenschaftlern, die nicht in diesem Gebiet arbeiten, oft unterschätzt, wie schwierig die Probleme sind und wie fehleranfällig darum deren Lösungen teilweise sind. Das führt einerseits dazu, dass man für Problemstellungen oft maßgeschneiderte Lösungen braucht. Andererseits wird dadurch jedoch deren Vielseitigkeit stark beschränkt. Unter anderem aus diesem Grunde gibt es für keine Aufgabenstellung nur eine Lösung, sondern viele verschiedene Lösungen, je nach den Anforderungen, und erklärt damit auch, warum so viele konkurrierende Lösungswege in der Fachwelt existieren. Die eigentliche Aufgabe des Computer Vision besteht darin, einer am Computer angeschlossenen Kamera das Sehen und Verstehen beizubringen. Dafür sind verschiedene Schritte notwendig und es gibt je nach Aufgabenstellung entsprechende unterschiedliche Methoden. Diese sollen hier kurz skizziert werden. Zunächst einmal benötigt man ein aufgenommenes Bild (Abschnitt Bildentstehung) welches meist verbessert werden muss (z. B. Helligkeits- und Kontrastausgleich). Anschließend versucht man meist Merkmale zu extrahieren wie Kanten oder Eckpunkte (Abschnitt Merkmalsextraktion). Je nach Aufgabenstellung verwendet man z. B. Eckpunkte für die Korrespondenzsuche in Stereo-Bildern. Darüber hinaus können weitere geometrische Elemente wie Geraden und Kreise mittels der Hough-Transformation erkannt werden (Abschnitt Hough-Transformation). Bestimmte Anwendungen versuchen mittels Bildsegmentierung uninteressante Bildbestandteile wie den Himmel oder den unbewegten Hintergrund zu selektieren (Abschnitt Bildsegmentierung). Möchte man eine Kamera zum Messen einsetzen werden i. d. R. die Parameter des Kameramodells (innere Orientierung) durch eine Kamerakalibrierung bestimmt (Abschnitt Kamerakalibrierung). Um die gegenseitige Lage eines Stereo-Bildpaars aus dem Bildinhalt zu schätzen, kommen verschiedene Algorithmen zur Berechnung der Fundamentalmatrix zum Einsatz (siehe Epipolargeometrie Fundamentalmatrix). Bevor man eine 3D-Rekonstruktion durchführen kann, benötigt man zunächst homologe (korrespondierende) Bildpunkte (Abschnitt Korrespondenzproblem). Anschließend ist man in der Lage die 3D-Punkte durch Vorwärtsschnitt (Triangulation) zu bestimmen. Daneben gibt es verschiedene Möglichkeiten die Form eines Objektes dreidimensional zu bestimmen. Im englischen Sprachgebrauch hat sich hier der Terminus Shape-from-X eingebürgert. Das X steht hierbei für eine dieser Methoden (Abschnitt Shape-from-X). Die Bildentstehung beschreibt den komplexen Prozess der Bildaufnahme beginnend bei derelektromagnetischen Strahlung, der Interaktion mit der Oberfläche (AbsorptionundReflexion), deroptischen Abbildungund der Detektion mittelsKamerasensoren. Neben anderen Möglichkeiten eine Kamera zu modellieren ist das am häufigsten verwendete Modell die Lochkamera. Die Lochkamera ist ein idealisiertes Modell einer Kamera, welches eine Realisierung des geometrischen Modells der Zentralprojektion darstellt. Mittels Strahlensätze lassen sich damit auf einfache Art und Weise Abbildungsformeln herleiten. Eine reale Kamera weicht in vielerlei Hinsicht vom Lochkameramodell ab. Man benötigtLinsen, um mehr Licht einzufangen und einen lichtempfindlichen Sensor um das Bild zu erfassen und zu speichern. Dabei kommt es zu diversen Abweichungen, die einerseits physikalisch bedingt sind und andererseits durch unvermeidliche Fertigungsungenauigkeiten entstehen. Beides führt zu Verzerrungen im aufgenommenen Bild. Sie werden einerseits durch den Sensor und andererseits durch das Objektiv verursacht. Es kommt beim Sensor zu farblichen Abweichungen (radiometrischebzw.fotometrischeAbweichung) und geometrischen Abweichungen (Verzeichnung). Abweichungen, die durch das Objektiv, also durch die einzelnen Linsen verursacht werden, bezeichnet man alsAberrationen. Sie führt ebenfalls zu farblichen Abweichungen (z. B. Farbsäume) und geometrischen Verzerrungen (Verzeichnung). Es kommt außerdem zuatmosphärischer Refraktion(Lichtbrechung). Im Nahbereich ist der Effekt jedoch so gering, dass man ihn meist vernachlässigen kann. Zur Detektion des Lichts benötigt man lichtempfindliche Sensoren, die Licht in Strom umwandeln können. Schon 1970 wurde einCCD-Sensor(Englisch:charge coupled device, auf Deutsch: ladungsgekoppeltes Bauelement) zur Bildaufnahme entwickelt. Durch Aneinanderreihung in einer Zeile erhält man einen Zeilensensor und entsprechende Anordnung in einer Fläche erhält man einen flächenhaften Sensor. Jedes einzelne Element wird dabei als Pixel (Englisch: picture element) bezeichnet. Alternativ dazu gibt es auch einen flächenhaften SensorCMOS(Englisch:complementary metal-oxide-semiconductor, auf Deutsch: komplementärer / sich ergänzender Metall-Oxid-Halbleiter) genannt. Ein solcher Sensor ist in der Regel über das Spektrum des sichtbaren Lichtes hinaus empfindlich im ultra-violetten Bereich und weit in den infraroten Bereich des Lichts. Um ein Farbbild aufnehmen zu können, muss man für die jeweiligen Grundfarben Rot, Grün und Blau (kurz: RGB) einen eigenen Sensor haben. Dies kann man durch Aufteilung des Lichtes auf drei unterschiedliche Flächen machen (s. Abb. rechts). Eine andere Möglichkeit besteht darin, nebeneinander liegende Pixel jeweils mit unterschiedlichen Farbfiltern zu versehen. Meist wird dafür ein von Bayer entwickeltes Muster verwendet (Bayer pattern). Darüber hinaus sind auch andere – meist wissenschaftlich motivierte – Farbkanäle im Einsatz. Im engeren Sinne wird unter einer Kamerakalibrierung die Bestimmung derinneren Orientierungverstanden. Dies sind alle Modellparameter, welche die Kamerageometrie beschreiben. Dazu zählen i. d. R. dieKoordinaten des Hauptpunktes, dieKamerakonstantesowieVerzeichnungsparameter. Im weiteren Sinne wird unter einer Kamerakalibrierung aber auch die gleichzeitige Bestimmung der äußeren Orientierung verstanden. Da man sowieso meistens beides bestimmen muss, zumindest wenn man eine Kalibrierung mittels bekannten 3D-Koordinaten durchführt, wird dies im Computer Vision oft synonym verwendet. In derPhotogrammetriehingegen ist es durchaus noch üblich eine Laborkalibierung (z. B. mittelsGoniometer) auszuführen, wo die innere Orientierung direkt bestimmt werden kann. Am häufigsten wird eine Kamera mittels eines bekannten Testfeldes oder Kalibrierrahmen kalibriert. Dabei sind die 3D-Koordinaten gegeben und die abgebildeten Bildkoordinaten werden gemessen. Somit kann man mittels den bekannten Abbildungsbeziehungen ein Gleichungssystem aufstellen, um die Parameter des Abbildungsmodells zu bestimmen. Abhängig von den Genauigkeitsanforderungen verwendet man ein geeignetes Kameramodell. Ein genaues Modell ist in der Abbildung dargestellt (s. Abb. rechts). Gegenüber dem Lochkameramodell weicht eine reale Kamera in vielerlei Hinsicht ab. Es ist deswegen notwendig einige optische Begriffe zu definieren.[5] Ein Objektiv enthält meistens eine Blende (oder die Fassung der Linsen, die genauso wirkt) und es stellt sich die Frage: Wo ist das Projektionszentrum? Je nachdem, von welcher Seite man ins Objektiv guckt, sieht man ein anderes Bild derBlende. Die beiden Bilder lassen sich nach den Regeln dergeometrischen Optikkonstruieren. Das Licht tritt aus dem Objektraum (in Abb. von links) ins Objektiv ein und erzeugt als Bild der Blende dieEintrittspupille(EP). Zum Bildraum hin tritt das Licht wieder aus und erzeugt dieAustrittspupille(AP). Die jeweiligen Mittelpunkte der Eintrittspupille und der Austrittspupille liegen auf der optischen Achse und sind die Punkte, durch die der Hauptstrahl (entspricht dem Projektionsstrahl im Lochkameramodell) ungebrochen hindurchgeht. Deswegen ist derMittelpunkt der EPdasProjektionszentrumO{\\displaystyle O}und derMittelpunkt der APdasbildseitige ProjektionszentrumO′{\\displaystyle O\\ '}. Um den Bezug herzustellen zwischen einem Kamerakoordinatensystem und einem Bildkoordinatensystem, benutzt man das bildseitige ProjektionszentrumO′{\\displaystyle O\\ '}. Es wird senkrecht in die Bildebene projiziert und erzeugt denHauptpunktH{\\displaystyle H}. Der Abstand zwischenO′{\\displaystyle O\\ '}undH{\\displaystyle H}ist definiert als dieKamerakonstantec{\\displaystyle c}. Aufgrund von unvermeidbaren Fertigungsungenauigkeiten, steht die Verlängerung der optischen Achse nicht (exakt) senkrecht auf der Bildebene und erzeugt als Durchstoßpunkt denSymmetriepunkt der VerzeichnungS{\\displaystyle S}(auch Verzeichnungszentrum genannt). Es ist jedoch oft üblich für die rechnerische Bestimmung das Verzeichnungszentrum mit dem Hauptpunkt gleichzusetzen. Denn die beiden Punkte liegen meist eng beieinander, wodurch es zu einer starken Korrelation kommt. Darunter leidet die Präzision während der Kamerakalibrierung. Um die Aufnahmerichtung zu definieren, stelle man sich vor, man würde den Hauptpunkt in den Objektraum zurückprojizieren. Weil dieser Strahl durch das bildseitige ProjektionszentrumO′{\\displaystyle O\\ '}geht, muss er ebenfalls durchs ProjektionszentrumO{\\displaystyle O}gehen. Dieser eine Strahl ist also quasi ein Hauptstrahl und darüber hinaus der einzige Strahl, der senkrecht auf die Bildebene projiziert wird. Damit entspricht dieser Strahl derAufnahmeachseund ist gleichzeitig dieZ-AchsedesKamerakoordinatensystems. Der Winkelτ{\\displaystyle \\tau }zwischen Aufnahmeachse und einem ObjektpunktP{\\displaystyle P}ändert sich beim Austritt in den Bildraum und erzeugt den BildpunktP′{\\displaystyle P'}. Diese Winkeländerung ist Ausdruck vonVerzeichnung. Verzeichnungumfasst alle durch das Objektiv verursachten Abweichungen gegenüber dem idealen Modell der Lochkamera. Daher muss der Fehler so korrigiert werden, als wenn die Bilder von einer perfekten linearen Kamera (Lochkamera) aufgenommen worden wären. Da die Linsenverzeichnung bei der ursprünglichen Abbildung des Objektpunktes auf das Bild auftritt, wird der dabei entstandene Fehler modelliert mit folgender Gleichung: Die Korrektur geschieht dann mittels x{\\displaystyle x}undy{\\displaystyle y}sind die gemessenen,x^{\\displaystyle {\\hat {x}}}undy^{\\displaystyle {\\hat {y}}}die korrigierten Bildkoordinaten undxc{\\displaystyle x_{c}},yc{\\displaystyle y_{c}}das Zentrum der Verzeichnung mitr2=(x−xc)2+(y−yc)2{\\displaystyle r^{2}=(x-x_{c})^{2}+(y-y_{c})^{2}}.L{\\displaystyle L}ist nur definiert bei positivenr{\\displaystyle r}. Eine Annäherung geschieht meist mittelsTaylor-Approximation. Wegen der Symmetrie der Verzeichnungskurve bezüglich des Zentrums der Verzeichnung sind nur ungerade Potenzen notwendig (daher auchSeidel-Reihe genannt).[6]L{\\displaystyle L}ist dann Darüber hinaus besteht eine enge Korrelation zwischen dem ersten Termk1r{\\displaystyle k_{1}r}und der Kamerakonstantenc{\\displaystyle c}, wegentan⁡τ=rc{\\displaystyle \\tan \\tau ={\\frac {r}{c}}}. Deswegen wird der erste Term oft entfernt, wodurch die Präzision bei der Ausgleichung deutlich gesteigert werden kann.[7] Die Koeffizientenki{\\displaystyle k_{i}}sind Teil der inneren Kalibrierung der Kamera. Sie werden meist mittels iterativer Verfahren der Ausgleichungsrechnung bestimmt. Eine Möglichkeit ist die Verwendung von Geraden wie z. B. aufgehängte Lote. Diese müssen sich bei richtiger Korrektur in Geraden abbilden. Die Minimierung einer Kostenfunktion (zum Beispiel der Abstand der Linienenden zum Mittelpunkt) liefert dann die Lösung. Diese Methode ist auch alsPlumbline-Kalibrierungbekannt.[8] Der Hauptpunkt wird meist – im Rahmen der Genauigkeitsanforderungen – als Zentrum der Verzeichnung angenommen. Die Verzeichniskorrektur zusammen mit der Kamerakalibrierungsmatrix beschreibt damit vollständig die Abbildung des Objektpunktes auf einen Bildpunkt. Ziel: Beleuchtungskorrektur (exposure correction), Farbausgleich (color balancing), Unterdrückung von Bildrauschen, Verbesserung der Schärfe Prinzip: lineare Filter, welche ein Signal falten (z. B. Differenzbildung zw. benachbarten Punkten) Verschiedene Kernel und deren Wirkung (Differenz, Gauß) Mit Hilfe unterschiedlicher Bildverarbeitungsalgorithmen versucht man Kanten zu extrahieren, um z. B. geometrische Modelle abzuleiten. Ebenfalls mittels Methoden der Bildverarbeitung kann man Punkte extrahieren, die sich gut von der Umgebung abheben. Um solche Punkte zu finden, kommen Gradienten-Operatorenzum Einsatz, welche entlang zweier Hauptrichtungen benachbarte Pixel auf Änderung ihrer Helligkeitswerte untersuchen. Ein guter Punkt definiert sich dadurch, dass derGradiententlang beider Hauptrichtungen möglichst groß ist. Dies lässt sich mathematisch alsFehlerellipsebeschreiben, die möglichst klein sein sollte. Die Achsen der Fehlerellipse werden durch Berechnung derEigenwertederKovarianzmatrixbestimmt (s.Förstner-Operator). Solche identifizierten Punkte haben vielfältige Anwendungszwecke u. a. zur Schätzung der Fundamentalmatrix (s.Fundamentalmatrix). Bei der Bildsegmentierung versucht man zusammenhängende Bildbereiche zu identifizieren. Dabei werden Methoden der Merkmalsextraktion kombiniert mit Bildbereichen, die ungefähr die gleiche Farbe haben. Prominentes Beispiel ist dieWasserscheidentransformation, womit man z. B. einzelne Ziegelsteine einer Hauswand extrahieren kann. Die Bildsegmentierung dient u. a. zur Klassifizierung verschiedener Flächen in der Fernerkundung und ermöglicht z. B. verschiedene Stadien des Pflanzenwachstums zu unterscheiden. In der Medizin kann dies die Detektion von krankem Gewebe in Röntgen- oder CT-aufnahmen unterstützen. Mittels der Hough-Transformation ist es möglich Linien und Kreise zu detektieren. Dies wird z. B. eingesetzt um Fahrbahnmarkierungen zu identifizieren (Spurhalteassistent) oder Straßenschilder. Objekterkennung ist ein komplexes Zusammenspiel von Merkmalsextraktion, Mustererkennung und selbst lernenden Entscheidungsalgorithmen derkünstlichen Intelligenz. Z. B. möchte man fürFahrerassistenzsystemeFußgänger von anderen Verkehrsteilnehmern unterscheiden wie PKW, Fahrrad, Motorrad, LKW usw. Homogene Koordinaten werden für die mathematische Beschreibung von projektiven Vorgängen vorteilhaft eingesetzt. Durch Hinzufügen einer weiteren Komponente zu einem zweidimensionalen Punktvektor, entsteht ein dreidimensionaler Vektor, wodurch Addition und Multiplikation in einer gesamten Transformationsmatrix ausgedrückt werden können. Hintereinandergereihte Transformationen können so zu einer einzigen gesamten Transformationsmatrix zusammengefasst werden. Neben dem Vorteil der kompakten Darstellung werden so Rundungsfehler vermieden.[9] Häufig verwendet man eine projektive Transformation, um von einer Ebene in eine andere Ebene umzurechnen. Im englischen Sprachgebrauch wird dies als Homografie bezeichnet. Eine quadratische 3x3-MatrixH{\\displaystyle H}mit vollem Rang beschreibt solch eine umkehrbar eindeutige Abbildung. Hiermit wird die Abbildung eines Objektpunktes ins Bild beschrieben. Die Suche nach einander zugeordneten (homologen) Bildpunkten zwischen Stereo-Bildern wird in Computer Vision als Korrespondenzproblem bezeichnet. Im englischen Fachjargon wird dies auch alsimage matching(Bildabgleich) bezeichnet. Dies ist ein Kernproblem, welches besonders schwierig ist, weil von der zweidimensionalen Abbildung auf ihre dreidimensionale Entsprechung rückgeschlossen wird. Es gibt deswegen viele Gründe, warum die Suche korrespondierender Bildpunkte fehlschlagen kann:[7] Entsprechend gibt es eine Vielzahl an ganz unterschiedlichen Methoden. Man unterscheidet grauwertbasierte (flächenhafte) von merkmalsbasierten Verfahren. Die flächenhaften Verfahren untersuchen kleine Bildausschnitte und vergleichen die jeweiligen Grauwerte (Helligkeitswerte). Die merkmalsbasierten Verfahren extrahieren zunächst Merkmale (z. B. Eckpunkte) und gleichen darauf aufbauende Merkmalsvektoren ab. Die Epipolargeometrie beschreibt die Abbildungsgeometrie eines 3D-Objektpunktes in einem Stereobildpaar. Die Beziehung zwischen den Bildkoordinaten korrespondierender Punkte wird durch eine Fundamentalmatrix beschrieben. Mit ihr lässt sich zu einem gegebenen Punkt im ersten Bild die dazugehörige Epipolarlinie im zweiten Bild bestimmen, auf der sich der korrespondierende Bildpunkt befindet. Man kann die Fundamentalmatrix aus einer Anzahl an korrespondierenden Bildpunkten schätzen. Dazu existieren zwei weit verbreitete Berechnungsmethoden: der minimale7-Punkt-Algorithmusund der8-Punkt-Algorithmus. Aufbauend auf diese verteilten Bildpunktpaare (sparse image matching) ist es möglich die Fundamentalmatrix zu schätzen, um die gegenseitigerelative Orientierungder Bilder zu bestimmen. Dem folgt i. d. R. eine dichte Korrespondenzsuche (dense image matching). Alternativ werden auch mit Hilfe globalerOptimierungsverfahrendie korrespondierenden Punkte geschätzt. Bei der Stereo-Rekonstruktion werden zwei Bilder von jeweils unterschiedlichen Blickpunkten aus verwendet. Als Vorbild dient das menschliche räumliche Sehen (stereoskopisches Sehen). Kennt man die gegenseitige relative Orientierung eines Bildpaars, dann kann man korrespondierende Bildpunktpaare dazu verwenden, um die ursprünglichen 3D-Objektpunkte mittels Triangulation zu berechnen. Das Schwierige daran ist die Korrespondenzsuche, insbesondere für Oberflächen mit wenig Textur oder verdeckte Gebiete.[10] Bei diesem Verfahren benutzt man mehrere Bilder, welche das Objekt aus unterschiedlichen Richtungen abbilden, um aus deren äußeren Umriss (die Silhouette) seine geometrische Form abzuleiten. Bei diesem Verfahren wird die Kontour aus einem groben Volumen quasi herausgeschnitten, so ähnlich wie ein Bildhauer eine Büste aus einem groben Holzklotz herausschnitzt. Im englischen Sprachgebrauch wird hierbei auch von Shape-from-Contour oder Space-Carving gesprochen. Voraussetzung für diese Technik ist, dass man das zu bestimmende Objekt (Vordergrund) vom Hintergrund trennen kann. Dabei kommen Techniken zur Bildsegmentierung zum Einsatz. Das Ergebnis wird dann als Representation eines Volumens mittelsVoxeldargestellt und wird auch visuelle Hülle (auf Englisch: visual hull) genannt.[10] Diese Methode versucht die Form eines Objekts anhand seiner Schattierung zu bestimmen. Sie beruht auf zwei Effekten: erstens ist dieReflexionvon auf eine Oberfläche auftreffender paralleler Strahlung abhängig von der Oberflächennormalen und der Beschaffenheit (insbesondere Rauigkeit) der Oberfläche, und zweitens ist die vom Betrachter (Kamera) gesehene Helligkeit abhängig von der Perspektive, genauer gesagt vom Winkel unter dem man die Oberfläche betrachtet. Bei einer Reflexion an einer rauen Oberfläche spricht man von diffuser Reflexion, welche durch dasLambertsche Kosinusgesetzbeschrieben wird (s. Abb. Links). Die Richtung der Beleuchtungsquelle spielt dabei nur insofern eine Rolle, dass die gesamte Strahlungsenergie verringert wird, abhängig vom Einfallswinkel. Die Reflexion (der Ausfallwinkel) ist jedoch völlig unabhängig vom Einfallswinkel, sie ist lediglich abhängig vom Winkel zur Oberflächennormalen. Unter der Annahme der diffusen Reflexion ist die zum Betrachter (Kamera) reflektierte Strahlung deshalb nur abhängig vom Kosinus des Winkels zur Oberflächennormalen. Dies lässt sich vorteilhaft nutzen, wenn man die Beleuchtungsstärke kennt, um die Richtung der Oberflächennormalen zu berechnen. Beimoptischen Flusswird eine Sequenz von Bildern untersucht, ob und wie sich die Bilder (bzw. die Kamera) bewegt hat. Dazu werden lokale Helligkeitsänderungen zwischen benachbarten Bildern untersucht. Dazu kommen verschiedene Methoden zur Merkmalsextraktion zum Einsatz und Verfahren zur Korrespondenzanalyse, um korrespondierende Punkte zu identifizieren. Die Differenz zwischen diesen korrespondierenden Punkten entspricht dann der lokalen Bewegung. Gestützt auf diese Punkte ist es möglich die Objektform durch 3D-Rekonstruktion zu bestimmen (s. Abschnitt 'Struktur aus Bewegung'). Aufgrund der Verwendung nur weniger Punkte ist das Ergebnis jedoch sehr grob und eignet sich lediglich zur Erkennung von Hindernissen, um so die Navigation zu unterstützen. Für eine genaue 3D-Modellierung ist es jedoch ungeeignet. Kennt man die auf einer Oberfläche aufgetragene Textur, z. B. ein Stück Stoff mit einem sich wiederholenden Muster, dann ändert sich das Muster aufgrund lokaler Unebenheiten. Genauer gesagt der Winkel, unter dem man die Oberfläche (und damit Oberflächennormale) betrachtet, ändert sich und verzerrt somit die sichtbare geometrische Form der Textur. In dieser Hinsicht ähnelt dieses Verfahren dem Shape-from-Shading. Es sind viele Schritte notwendig um die Form ableiten zu können inklusive der Extraktion der wiederholenden Muster, die Messung lokaler Frequenzen um lokale affine Deformationen zu berechnen und schließlich die lokale Orientierung der Oberfläche abzuleiten.[1][3][11] Im Gegensatz zum Lichtstreifenverfahren (s. Abschnitt Strukturiertes codiertes Licht) ist die Textur real auf der Oberfläche vorhanden und wird nicht durch einen Projektor künstlich erzeugt. Ersetzt man bei einem Stereo-Kamerasystem eine Kamera durch einen Projektor, welcher strukturiertes (codiertes) Licht aussendet, kann man ebenfalls eine Triangulation durchführen und somit die dreidimensionale Form des Objekts rekonstruieren. Das strukturierte Licht erzeugt eine bekannte Textur, welche auf der Oberfläche durch dasReliefverzerrt abgebildet wird. Die Kamera „erkennt“ anhand dieser Textur die jeweilige lokale codierte Struktur und kann durch Strahlenschnitt die 3D-Position berechnen (s. auchStreifenlichtscanningundLichtschnittverfahren).[10]Irrtümlicherweise wird dies manchmal gleichgesetzt mit Shape-from-Textur. DieLinsengleichungbeschreibt die prinzipielle Abbildung eines Objektpunktes und seines scharf abgebildeten Bildpunktes für eine Kamera mit einem Objektiv (s.geometrische Optik). Der Durchmesser der Unschärfe verhält sich proportional zur Änderung der Fokuseinstellung (entspricht der Änderung der Bildweite). Unter der Voraussetzung, dass die Distanz zum Objekt fixiert ist, kann damit – aus einer Reihe von unscharfen Bildern und Messung des Durchmessers von unscharf abgebildeten Punkten – die Gegenstandsweite (entspricht der Distanz zum Objekt) berechnet werden.[12] LiDAR (light detection and ranging, auf Deutsch: Licht Detektion und Entfernungsmessung) ist ein aktives Verfahren zur berührungslosen Entfernungsmessung. Das Messprinzip beruht auf der Messung der Laufzeit eines ausgesendeten Lasersignal. Dieses Verfahren wird unter anderem in der Robotik zur Navigation eingesetzt. Eine 3D-ToF-Kamera (time of flight, auf Deutsch: Laufzeit) ist eine Kamera mit einem aktiven Sensor. Der Unterschied zu anderen Verfahren wie Laserscanning oder Lidar ist, dass es ein flächenhafter Sensor ist. Ähnlich wie bei einer normalen Digitalkamera enthält die Bildebene gleichmäßig angeordnete Lichtsensoren und zusätzlich winzige LEDs (oder Laserdioden), die einen infraroten Lichtpuls aussenden. Das von der Oberfläche reflektierte Licht wird von der Optik eingefangen und auf den Sensor abgebildet. Ein Filter sorgt dafür, dass nur die ausgestrahlte Farbe durchgelassen wird. Dies ermöglicht die gleichzeitige Entfernungsbestimmung eines Oberflächenstücks. Es kommt bei der autonomen Navigation zur Objekterkennung zum Einsatz. Kinect ist ein Kamerasystem mit strukturiertem Licht zur Objektrekonstruktion. Eine omnidirektionale Kamera ist in der Lage aus allen Richtungen (360°) ein Bild aufzunehmen. Dies wird meist durch eine Kamera erreicht, welche auf einen konischen Spiegel ausgerichtet ist und somit die vom Spiegel reflektierte Umgebung aufgenommen wird. Je nach Ausrichtung ist es somit möglich mit nur einer Aufnahme ein vollständiges horizontales oder vertikales Rundumbild aufzunehmen. Als SLAM (englischSimultaneous Localization and Mapping;deutschSimultane Positionsbestimmung und Kartenerstellung) wird ein Verfahren bezeichnet, welches vor allem zur autonomen Navigation eingesetzt wird. Dabei ist ein mobiler Roboter mit verschiedenen Sensoren ausgerüstet, um seine Umgebung dreidimensional zu erfassen. Das besondere an diesem Verfahren ist, dass die Positionsbestimmung und die Kartenerstellung gleichzeitig durchgeführt werden. Die Bestimmung der absoluten Position ist eigentlich nur möglich, wenn man bereits eine Karte hat und anhand von Landmarken, die der Roboter identifiziert, dessen Lage innerhalb der Karte bestimmen kann. Oftmals sind die Karten jedoch nicht detailliert genug, weswegen ein mobiler Roboter keine – in der Karte vorhandene – Landmarken finden kann. Darüber hinaus ist die Identifikation solcher Landmarken äußerst schwierig, weil die Perspektive einer Karte eine völlig andere ist, als die Perspektive des Roboters.[13]Mit SLAM versucht man solche Problemstellungen zu lösen. Inindustriellen Umgebungenwerden die Techniken des maschinellen Sehens heutzutage erfolgreich eingesetzt. Computer unterstützen beispielsweise dieQualitätskontrolleund vermessen einfache Gegenstände. Weitgehend bestimmt der Programmierer hier die Umgebungsbedingungen, die wichtig für ein fehlerfreies Ablaufen seiner Algorithmen sind (Kameraposition, Beleuchtung, Geschwindigkeit des Fließbandes, Lage der Objekte usw.). Beispiele für den Einsatz in industriellen Umgebungen sind: Innatürlichen Umgebungenwerden weit schwierigere Anforderungen an die Techniken im Computer Vision gestellt. Hier hat der Programmierer keinen Einfluss auf die Umgebungsbedingungen, was die Erstellung eines robusten, fehlerfrei ablaufenden Programms erheblich erschwert. Man kann sich dieses Problem anhand eines Beispiels zur Erkennung von Automobilen verdeutlichen: Ein schwarzes Auto hebt sich vor einer weißen Wand gut ab, der Kontrast zwischen einem grünen Auto und einer Wiese ist allerdings sehr gering und eine Unterscheidung nicht einfach. Beispiele für den Einsatz in natürlichen Umgebungen sind: Weitere Anwendungen finden sich in einer Vielzahl unterschiedlicher Bereiche: Maschinelles Sehen umfasst alle industriellen Anwendungen, bei denen, basierend auf visuellen Systemen, automatisierte Prozesse gelenkt werden. Typische Einsatzgebiete sind industrielle Herstellungsprozesse, dieAutomatisierungstechnikund dieQualitätssicherung. Weitere Einsatzgebiete finden sich z. B. in derVerkehrstechnik– von der einfachenRadarfallebis hin zum „sehenden Fahrzeug“ – und in derSicherheitstechnik(Zutrittskontrolle, automatische Erkennung von Gefahrensituationen). Dabei werden Methoden aus dem Fachgebiet Computer Vision eingesetzt. Die Technologien und Methoden die hierbei zum Einsatz kommen, müssen speziellen Anforderungen genügen, welche sich im industriellen Umfeld ergeben. Industrielle visuelle Systeme erfordern eine hohe Zuverlässigkeit, Stabilität und müssen besonders robust sein. Insofern versucht maschinelles Sehen existierende Technologien auf neue Art und Weise anzuwenden und zu integrieren. Folgende Aufgabenstellungen können derzeit wirtschaftlich sinnvoll gelöst werden: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Komplexität 3Überblick der Methodik 4Bildentstehung 4.1Lochkameramodell 4.2Reale Kamera 4.3Digitale Sensoren 5Kamerakalibrierung 5.1Optische Begriffe in Kameras 6Verzeichnungskorrektur 7Bildverarbeitung (Filterung, Glättung, Rauschunterdrückung) 8Merkmalsextraktion und Mustererkennung (feature detection and pattern recognition) 8.1Kantendetektion (edge detection) 8.2Eckendetektion (Punktdetektion, corner detection) 8.3Bildsegmentierung (image segmentation) 8.4Hough-Transformation 9Objekterkennung (object detection) 10Grundlagen der projektiven Geometrie 10.1Konzept homogener Koordinaten 10.2Projektivtransformation (Homografie) 11Standardabbildungsmodell (Zentralprojektion) 12Korrespondenzproblem (Bildpunktzuordnung) 13Stereo-Bild-Verarbeitung 13.1Epipolargeometrie 14Bildsequenz-Verarbeitung (Struktur aus Bewegung) 15Shape-from-X 15.1Shape-from-Stereo 15.2Shape-from-Silhouette / Shape-from-Contour 15.3Shape-from-Shading / Photometric Stereo 15.4Shape-from-Motion / Optischer Fluss 15.5Shape-from-Texture"
  },
  {
    "label": 1,
    "text": "Computerlinguistik – Wikipedia Computerlinguistik Inhaltsverzeichnis Geschichte Funktionsweise Das Saarbrücker Pipelinemodell Beispiele für Probleme der Sprachverarbeitung Anwendungen in der Praxis Siehe auch Literatur Weblinks Einzelnachweise Studiengänge Tagungen Organisationen DieComputerlinguistik(CL) oderlinguistische Datenverarbeitung(LDV) untersucht, wienatürliche Sprachein Form von Text- oder Sprachdaten mit Hilfe des Computersalgorithmischverarbeitet werden kann. „Sie erarbeitet die theoretischen Grundlagen der Darstellung, Erkennung und Erzeugung gesprochener und geschriebener Sprache durch Maschinen“[1]und ist Schnittstelle zwischenSprachwissenschaftundInformatik. In der englischsprachigen Literatur und Informatik ist neben dem Begriffnatural language processing (NLP)auchcomputational linguistics (CL)gebräuchlich. Computerlinguistik lässt sich als Begriff in die 1960er Jahre zurückverfolgen.[2]Mit den Anfängen derkünstlichen Intelligenzauch beiAlan Turingwar die Aufgabenstellung schon nahegelegt.Noam ChomskysSyntactic Structuresvon 1957 präsentierte eine Sprachauffassung, nach der dieSprachein einem formalen Rahmen beschreibbar wurde (Chomsky-Hierarchieder formalen Sprachen). Hinzu kamen die Sprachlogiken vonSaul KripkeundRichard Montague. Die teilweise aus dem US-Verteidigungsbudget sehr hoch geförderten Forschungen brachten jedoch nicht die erhofften Durchbrüche. Besonders Chomsky undJoseph Weizenbaumdämpften die Erwartungen an Automatisierungen von Sprachübersetzung. Der Wende von behavioristischen Wissenschaftskonzeptionen zu mentalistischen (Chomsky) folgten umfassende Konzipierungen in denKognitionswissenschaften. In den siebziger Jahren erschienen zunehmend häufiger Publikationen mit dem BegriffComputerlinguistikim Titel. In Deutschland wurde parallel der BegriffLinguistische Datenverarbeitung (LDV)verwendet.[3][4]Es gab bereits finanziell aufwändige Versuche der Anwendungen (Konkordanzen, Wort- und Formstatistik), aber auch schon größere Projekte zur maschinellen Sprachanalyse und zu Übersetzungen. Die ersten Computerlinguistik-Studiengänge in Deutschland wurden in den 1980er Jahren an derUniversität des Saarlandesund in Stuttgart eingerichtet. Die Computerlinguistik bekam mit der Verbreitung von Arbeitsplatzrechnern (Personal Computer) und mit dem Aufkommen des Internets neue Anwendungsgebiete. Im Gegensatz zu einerInternetlinguistik, die insbesondere menschliches Sprachverhalten und die Sprachformen im und mittels Internet untersucht, entstand in der Computerlinguistik eine stärker informatisch-praktische Ausrichtung. Dennoch gab das Fach die klassischen philosophisch-linguistischen Fragen nicht ganz auf und wird heute in theoretische und praktische Computerlinguistik unterschieden. Das Fach ist eng verbunden mit der Entwicklung derKünstlichen Intelligenz; insbesondere das Aufkommen vongroßen Sprachmodellen, die auf der Verarbeitung riesigerTextkorporabasieren (ab 2017 mit der Vorstellung desTransformers), haben dem Feld neue Impulse und Forschungsrichtungen gegeben.[5] Die Computerlinguistik (CL) verwendet verschiedene Techniken, um gesprochene und geschriebene Sprache zu verarbeiten. Dazu zählen Interpretationen statistischer Daten, Datenmaterial aussozialen Netzwerken, Suchergebnisse sowie Methoden desmaschinellen Lernensund von Regeln durchsetzte algorithmische Datenverarbeitung.[6]Methoden verschiedener Disziplinen wie Informatik,Künstliche Intelligenz, Linguistik undDatenwissenschaftwerden genutzt, um Computern die Verarbeitung natürlicher Sprache zu ermöglichen. Computerlinguistik gliedert sich in die Unterbereiche „Verständnis natürlicher Sprache“ (natural language understanding, NLU) und „Erzeugung natürlicher Sprache“ (natural language generation, NLG).[7]Künstliche Intelligenz wird auch in Übersetzungsprogrammen wie zum BeispielDeepLverwendet, wodurch Sprachbarrieren reduziert werden können.[8]Computerlinguistik nimmt Teil amdigitalen Wandelin Behörden, Unternehmen und Gesellschaft, da Teile von Arbeitsprozessen durchAlgorithmenausgeführt werden. So nutzt zum Beispiel das Software-UnternehmenNvidiaCL.[9]Allerdings gibt es auch Gefahren durch inhaltliche Verzerrungen, die in den verarbeiteten sprachlichen Daten enthalten sind und durch Algorithmen verstärkt werden.[10] Computer verarbeiten Sprache entweder in der Form von akustischer Information oder in der Form von Buchstabenketten (wenn die Sprache in Schriftform vorliegt). Um die Sprache zu analysieren, arbeitet man sich schrittweise von dieser Eingangsrepräsentation in Richtung Bedeutung vor und durchläuft dabei verschiedene sprachliche Repräsentationsebenen. In praktischen Systemen werden diese Schritte typischerweise sequentiell durchgeführt, daher spricht man vom Pipelinemodell,[11]mit folgenden Schritten: Es ist allerdings nicht so, dass sämtliche Verfahren der Computerlinguistik diese komplette Kette durchlaufen. Die zunehmende Verwendung vonmaschinellen Lernverfahrenhat zu der Einsicht geführt, dass auf jeder der Analyseebenen statistische Regelmäßigkeiten existieren, die zur Modellierung sprachlicher Phänomene genutzt werden können. Beispielsweise verwenden viele aktuelle Modelle dermaschinellen ÜbersetzungSyntax nur in eingeschränktem Umfang und Semantik so gut wie gar nicht; stattdessen beschränken sie sich darauf, Korrespondenzmuster auf Wortebene auszunutzen.[12] Am anderen Ende der Skala stehen Verfahren, die nach dem PrinzipSemantics first, syntax secondarbeiten. So baut die auf demMultiNet-Paradigma beruhende, kognitiv orientierte Sprachverarbeitung auf einem semantikbasierten Computerlexikon auf, das auf einem im Wesentlichen sprachunabhängigen semantischen Kern mit sprachspezifischen morphosyntaktischen Ergänzungen beruht.[13]Dieses Lexikon wird beimParsingvon einer Wortklassen-gesteuerten Analyse zur unmittelbaren Erzeugung von semantischen Strukturen eingesetzt. Praktische Computerlinguistikist ein Begriff, der sich im Lehrangebot einiger Universitäten etabliert hat. Solche Ausbildungsgänge sind nahe an konkreten Berufsbildern um die informatisch-technische Wartung und Entwicklung von sprachverarbeitenden Maschinen und ihrerProgramme. Dazu gehören zum Beispiel: Computerlinguistik wird an mehreren Hochschulen im deutschsprachigen Raum als eigenständiger Studiengang angeboten. In der deutschen Hochschulpolitik ist die Computerlinguistik alsKleines Facheingestuft.[15]Es sind Bachelor- wie auch Master-Studienabschlüsse möglich.[16]Zu den bekanntesten Angeboten zählen die Studiengänge der: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Funktionsweise 3Das Saarbrücker Pipelinemodell 4Beispiele für Probleme der Sprachverarbeitung 5Anwendungen in der Praxis 5.1Studiengänge 5.2Tagungen 5.3Organisationen 6Siehe auch 7Literatur 8Weblinks 9Einzelnachweise Afrikaans العربية Azərbaycanca Беларуская Беларуская (тарашкевіца) Български বাংলা Brezhoneg Català Čeština Dansk Ελληνικά English Esperanto Español Eesti Euskara فارسی Suomi"
  },
  {
    "label": 1,
    "text": "Data-Mining – Wikipedia Data-Mining Inhaltsverzeichnis Abgrenzung von anderen Fachbereichen Deutsche Bezeichnung Data-Mining-Prozess Aufgabenstellungen des Data-Mining Spezialisierungen Probleme des Data-Mining Anwendungsgebiete Rechtliche, moralische und psychologische Aspekte Softwarepakete für Data-Mining Literatur Weblinks Einzelnachweise Ausreißer-Erkennung Clusteranalyse Klassifikation Assoziationsanalyse Regressionsanalyse Zusammenfassung Textmining Webmining Zeitreihenanalyse Daten-Defekte Parametrisierung Evaluation Interpretation Data-Mining in der Industrie Educational Data Mining Rechtliche Aspekte Moralische Aspekte Psychologische Aspekte UnterData-Mining[ˈdeɪtə ˈmaɪnɪŋ] (vonenglischdata mining, ausenglischdata‚Daten‘ undenglischmine‚graben‘, ‚abbauen‘, ‚fördern‘)[1]versteht man die systematische Anwendung statistischer Methoden auf große Datenbestände (insbesondere „Big Data“ bzw. Massendaten) mit dem Ziel, neueQuerverbindungen und Trendszu erkennen. Solche Datenbestände werden aufgrund ihrer Größe mittels computergestützter Methoden verarbeitet. In der Praxis wurde der UnterbegriffData-Miningauf den gesamten Prozess der sogenannten „Knowledge Discovery in Databases“ (englischfür Wissensentdeckung in Datenbanken; KDD) übertragen, der auch Schritte wie die Vorverarbeitung und Auswertung beinhaltet, während Data-Mining im engeren Sinne nur den eigentlichen Verarbeitungsschritt des Prozesses bezeichnet.[2] Die BezeichnungData-Mining(eigentlich etwa „Abbau von Daten“) ist selbst irreführend, denn es geht um die Gewinnung von Wissen aus bereits vorhandenen Daten und nicht um die Generierung oder das Abgreifen von Daten selbst.[3]Die prägnante Bezeichnung hat sich dennoch durchgesetzt. Die reineErfassung, Speicherung undVerarbeitungvon großen Datenmengen wird gelegentlich ebenfalls mit demBuzzwordData-Mining bezeichnet. Im wissenschaftlichen Kontext bezeichnet es primär die Extraktion vonWissen, das „gültig(im statistischen Sinne), bisher unbekannt und potentiell nützlich“[4]ist „zur Bestimmung bestimmter Regelmäßigkeiten, Gesetzmäßigkeiten und verborgener Zusammenhänge“.[5]Fayyad definiert es als „ein[en] Schritt desKDD-Prozesses, der darin besteht, Datenanalyse- und Entdeckungsalgorithmen anzuwenden, die unter akzeptablen Effizienzbegrenzungen eine spezielle Auflistung von Mustern (oder Modellen) der Daten liefern“.[2] Das Schließen von Daten auf (hypothetische) Modelle wird alsStatistische Inferenzbezeichnet. Viele der im Data-Mining eingesetzten Verfahren stammen eigentlich aus derStatistik, insbesondere dermultivariaten Statistikund werden oft nur in ihrerKomplexitätfür die Anwendung im Data-Mining angepasst, oft dabei zu Ungunsten der Genauigkeit approximiert. Der Verlust an Genauigkeit geht oft mit einem Verlust an statistischer Gültigkeit einher, so dass die Verfahren aus einer rein statistischen Sicht mitunter sogar „falsch“ sein können. Für die Anwendung im Data-Mining sind oft jedoch der experimentell verifizierte Nutzen und die akzeptable Laufzeit entscheidender als eine statistisch bewiesene Korrektheit. Ebenfalls eng verwandt ist das Themamaschinelles Lernen, jedoch ist bei Data-Mining der Fokus auf dem FindenneuerMuster, während im maschinellen Lernen primärbekannteMuster vom Computer automatisch in neuen Daten wiedererkannt werden sollen. Eine einfache Trennung ist hier jedoch nicht immer möglich: Werden beispielsweise Assoziationsregeln aus den Daten extrahiert, so ist das ein Prozess, der den typischen Data-Mining-Aufgaben entspricht; die extrahierten Regeln erfüllen aber auch die Ziele des maschinellen Lernens. Umgekehrt ist der Teilbereich desunüberwachten Lernensaus dem maschinellen Lernen sehr eng mit Data-Mining verwandt. Verfahren aus dem maschinellen Lernen finden oft im Data-Mining Anwendung und umgekehrt. Die Forschung im Bereich derDatenbanksysteme, insbesondere vonIndexstrukturenspielt für das Data-Mining eine große Rolle, wenn es darum geht, die Komplexität zu reduzieren. Typische Aufgaben wie Nächste-Nachbarn-Suche können mit Hilfe eines geeigneten Datenbankindexes wesentlich beschleunigt werden und dieLaufzeiteines Data-Mining-Algorithmus dadurch verbessert werden. DasInformation Retrieval(IR) ist ein weiteres Fachgebiet, das von Erkenntnissen des Data-Mining profitiert. Hier geht es vereinfacht gesprochen um die computergestützte Suche nach komplexen Inhalten, aber auch um die Präsentation für den Nutzer. Data-Mining-Verfahren wie dieClusteranalysefinden hier Anwendung, um die Suchergebnisse und ihre Präsentation für den Nutzer zu verbessern, beispielsweise indem man ähnliche Suchergebnisse gruppiert.Text MiningundWeb Miningsind zwei Spezialisierungen des Data-Mining, die eng mit dem Information Retrieval verbunden sind. DieDatenerhebung, also das Erfassen von Informationen in einer systematischen Art und Weise, ist eine wichtige Voraussetzung, um mit Hilfe von Data-Mining gültige Ergebnisse bekommen zu können. Wurden die Daten statistisch unsauber erhoben, so kann einsystematischer Fehlerin den Daten vorliegen, der anschließend im Data-Mining-Schritt gefunden wird. Das Ergebnis ist dann unter Umständen keine Konsequenz der beobachteten Objekte, sondern verursacht durch die Art, in welcher die Daten erfasst wurden. Eine etablierte deutsche Übersetzung für den englischen Terminus Data-Mining existiert bislang nicht.[6] Es gibt verschiedene Versuche, eine sachlich in allen Aspekten zutreffende deutsche Bezeichnung für den ungenauen englischen Ausdruck zu finden. Der Duden[5]beschränkt sich auf den eingedeutschten Anglizismus „Data-Mining“ (engl. „data mining“). Vorschläge zur Eindeutschung sind beispielsweise „Datenmustererkennung“[7](was oft als Wiedererkennung bestehender Muster missinterpretiert wird) und „Datenschürfung“ (was der Originalbedeutung nicht vollkommen gerecht wird). Der Fremdwörter-Duden verwendet als wörtliche Übersetzung „Datenförderung“, kennzeichnet dies aber als nicht passende Übersetzung.[8]Auch der gezielte Aufruf nach Vorschlägen durch die Zeitschrift für Künstliche Intelligenz brachte keine überzeugenden Vorschläge.[6]Keiner dieser Bezeichner konnte nennenswerte Verbreitung erreichen, oft da bestimmte Aspekte des Themas wie die Wissensentdeckung verloren gehen, und falsche Assoziationen wie zur Mustererkennung im Sinne vonBilderkennungentstehen. Gelegentlich wird die deutsche Bezeichnung „Wissensentdeckung in Datenbanken“ (für das englischeKnowledge Discovery in Databases) verwendet, die den gesamten Prozess umfasst, der auch den Data-Mining-Schritt enthält. Des Weiteren betont diese Bezeichnung sowohl die wissenschaftlichen Ansprüche, als auch, dass der Prozess in der Datenbank abläuft (und sich eben nicht beispielsweise ein Mensch durch Interpretation eine Meinung aus den Daten bildet). Data-Mining ist der eigentliche Analyseschritt des Knowledge Discovery in Databases Prozesses. Die Schritte des iterativen Prozesses sind grob umrissen:[4] In weiterenIterationenkann nun bereits gefundenes Wissen verwendet („in den Prozess integriert“) werden um in einem erneuten Durchlauf zusätzliche oder genauere Ergebnisse zu erhalten. Typische Aufgabenstellungen desData-Miningsind:[2][4] Diese Aufgabenstellungen können noch grob gegliedert werden inBeobachtungsprobleme(Ausreißer-Erkennung, Clusteranalyse) undPrognoseprobleme(Klassifikation, Regressionsanalyse). In dieser Aufgabe werden Datenobjekte gesucht, die inkonsistent zu dem Rest der Daten sind, beispielsweise indem sie ungewöhnliche Attributswerte haben oder von einem generellen Trend abweichen. Das VerfahrenLocal Outlier Factorsucht beispielsweise Objekte, die eine von ihren Nachbarn deutlich abweichende Dichte aufweisen, man spricht hier von „dichtebasierter Ausreißer-Erkennung“. Identifizierte Ausreißer werden oft anschließend manuell verifiziert und aus dem Datensatz ausgeblendet, da sie die Ergebnisse anderer Verfahren verschlechtern können. In manchen Anwendungsfällen wie der Betrugserkennung sind aber gerade die Ausreißer die interessanten Objekte. Bei der Clusteranalyse geht es darum, Gruppen von Objekten zu identifizieren, die sich auf eine gewisse Art ähnlicher sind als andere Gruppen. Oft handelt es sich dabei um Häufungen im Datenraum, woher der BegriffClusterkommt. Bei einer dichteverbundenen Clusteranalyse wie beispielsweiseDBSCANoderOPTICSkönnen die Cluster aber beliebige Formen annehmen. Andere Verfahren wie derEM-Algorithmusoderk-Means-Algorithmusbevorzugen sphärische Cluster. Objekte, die keinem Cluster zugeordnet wurden, können als Ausreißer im Sinne der zuvor genannten Ausreißer-Erkennung interpretiert werden. Bei der Klassifikation geht es ähnlich der Clusteranalyse darum, Objekte Gruppen (hier als Klassen bezeichnet) zuzuordnen. Im Gegensatz zur Clusteranalyse sind hier aber in der Regel die Klassen vordefiniert (Beispielsweise: Fahrräder, Autos) und es werden Verfahren aus demmaschinellen Lerneneingesetzt um bisher nicht zugeordnete Objekte diesen Klassen zuzuordnen. In der Assoziationsanalyse werden häufige Zusammenhänge in den Datensätzen gesucht und meist als Schlussregeln formuliert. Ein beliebtes (wenn auch anscheinend fiktives) Beispiel, das unter anderem in der FernsehserieNumbers – Die Logik des Verbrechenserwähnt wurde, ist folgendes: bei derWarenkorbanalysewurde festgestellt, dass die Produktkategorien „Windeln“ und „Bier“ überdurchschnittlich oft zusammen gekauft werden, meist dargestellt in Form einer Schlussregel „Kunde kauft Windeln⇒{\\displaystyle \\Rightarrow }Kunde kauft Bier“. Die Interpretation dieses Ergebnisses war, dass Männer, wenn sie von ihren Ehefrauen Windeln kaufen geschickt werden, sich gerne noch ein Bier mitnehmen. Durch Platzierung des Bierregals auf dem Weg von den Windeln zur Kasse konnte angeblich[9]der Bierverkauf weiter gesteigert werden. Bei der Regressionsanalyse wird der statistische Zusammenhang zwischen unterschiedlichen Attributen modelliert. Dies erlaubt unter anderem die Prognose von fehlenden Attributswerten, aber auch die Analyse der Abweichung analog zur Ausreißer-Erkennung. Verwendet man Erkenntnisse aus der Clusteranalyse und berechnet separate Modelle für jeden Cluster, so können typischerweise bessere Prognosen erstellt werden. Wird ein starker Zusammenhang festgestellt, so kann dieses Wissen auch gut für die Zusammenfassung genutzt werden. Da Data-Mining oft auf große und komplexe Datenmengen angewendet wird, ist eine wichtige Aufgabe auch die Reduktion dieser Daten auf eine für den Nutzer handhabbare Menge. Insbesondere die Ausreißer-Erkennung identifiziert hierzu einzelne Objekte, die wichtig sein können; die Clusteranalyse identifiziert Gruppen von Objekten, bei denen es oft reicht, sie nur anhand einer Stichprobe zu untersuchen, was die Anzahl der zu untersuchenden Datenobjekte deutlich reduziert. Die Regressionsanalyse erlaubt es, redundante Informationen zu entfernen und reduziert so die Komplexität der Daten. Klassifikation, Assoziationsanalyse und Regressionsanalyse (zum Teil auch die Clusteranalyse) liefern zudem abstraktere Modelle der Daten. Mit Hilfe dieser Ansätze wird sowohl die Analyse der Daten als auch beispielsweise derenVisualisierung(durch Stichproben und geringere Komplexität) vereinfacht. Während die meisten Data-Mining-Verfahren versuchen, mit möglichst allgemeinen Daten umgehen zu können, gibt es auch Spezialisierungen für speziellere Datentypen. Im Textmining geht es um die Analyse von großen textuellen Datenbeständen. Dies kann beispielsweise derPlagiats-Erkennung dienen oder um den Textbestand zuklassifizieren. Beim Webmining geht es um die Analyse von verteilten Daten, wie esInternetseitendarstellen. Für die Erkennung von Clustern und Ausreißern werden hier aber nicht nur die Seiten selbst, sondern insbesondere auch die Beziehungen (Hyperlinks) der Seiten zueinander betrachtet. Durch die sich ständig ändernden Inhalte und die nicht garantierte Verfügbarkeit der Daten ergeben sich zusätzliche Herausforderungen. Dieser Themenbereich ist auch eng mit demInformation Retrievalverbunden. In der Zeitreihenanalyse spielen die temporalen Aspekte und Beziehungen eine große Rolle. Hier können mittels spezieller Distanzfunktionen wie derDynamic-Time-Warping-Distanz bestehende Data-Mining-Verfahren verwendet werden, es werden aber auch spezialisierte Verfahren entwickelt. Eine wichtige Herausforderung besteht darin, Reihen mit einem ähnlichen Verlauf zu erkennen, auch wenn dieser etwas zeitlich versetzt ist, aber dennoch ähnliche Charakteristika aufweist. Viele der Probleme bei Data-Mining stammen aus einer ungenügenden Vorverarbeitung der Daten oder aussystematischen FehlernundVerzerrungbei derenErfassung. Diese Probleme sind oft statistischer Natur und müssen bereits bei der Erfassung gelöst werden: aus nichtrepräsentativenDaten können keine repräsentativen Ergebnisse gewonnen werden. Hier sind ähnliche Aspekte zu beachten wie bei der Erstellung einer repräsentativenStichprobe. Die im Data-Mining verwendeten Algorithmen haben oft mehrere Parameter, die geeignet zu wählen sind. Mit allen Parametern liefern sie gültige Ergebnisse, und die Parameter so zu wählen, dass die Ergebnisse auch nützlich sind, ist eine Aufgabe des Benutzers. Wählt man beim Clusteranalyse-AlgorithmusDBSCANbeispielsweise die ParameterminPts{\\displaystyle minPts}undε{\\displaystyle \\varepsilon }klein, so findet der Algorithmus eine fein aufgelöste Struktur, neigt aber auch dazu, Cluster in kleine Stücke zu zerteilen. Wählt man die Parameter größer, so findet er nur noch die Hauptcluster, die jedoch schon bekannt sein können, und dadurch auch nicht hilfreich. Weiterentwickelte Methoden haben oft weniger Parameter oder diese Parameter sind leichter zu wählen. Beispielsweise istOPTICSeine Weiterentwicklung von DBSCAN, die den Parameterε{\\displaystyle \\varepsilon }weitgehend eliminiert. Die Bewertung von Data-Mining-Ergebnissen stellt den Benutzer vor das Problem, dass er einerseits neue Erkenntnisse gewinnen möchte, andererseits Verfahren dann nur schwer automatisiert bewerten kann. Bei Prognoseproblemen wie der Klassifikation, Regressionsanalyse und Assoziationsanalyse lässt sich hier die Prognose auf neuen Daten zur Bewertung verwenden. Bei Beschreibungsproblemen wie der Ausreißer-Erkennung und der Clusteranalyse ist dies schwieriger. Cluster werden meistinternoderexternbewertet, also anhand ihrer mathematischen Kompaktheit oder ihrer Übereinstimmung mit bekannten Klassen.[10]Die Ergebnisse von Ausreißer-Erkennungs-Verfahren werden mit bekannten Ausreißern verglichen. Bei beiden stellt sich jedoch die Frage, ob diese Bewertung wirklich zur Aufgabenstellung der „neuen Erkenntnisse“ passt und nicht letztlich die „Reproduktion alter Erkenntnisse“ bewertet. Als statistische Verfahren analysieren die Algorithmen die Daten ohne Hintergrundwissen über deren Bedeutung. Daher können die Verfahren meist nur einfache Modelle wie Gruppen oder Mittelwerte liefern. Oftmals sind die Ergebnisse als solche nicht mehr nachvollziehbar. Diese maschinell gewonnenen Ergebnisse müssen aber anschließend noch von dem Benutzer interpretiert werden, bevor man sie wirklich als Wissen bezeichnen kann. Neben den Anwendungen in den verwandten Bereichen der Informatik findet Data-Mining auch zunehmend Einsatz in der Industrie: Data Mining hat ebenfalls in der Lehre, vor allem der Hochschullehre Einzug erlangt. Im Bildungsbereich spricht man vonEducational Data Mining, mit dem in derPädagogikdas Ziel verfolgt wird „aus einer riesigen Datenmenge überschaubare Typen, Profile, Zusammenhänge, Cluster und darauf bezogen typische Abfolgen, Zusammenhänge und kritische Werte zu ermitteln.“ Aus den ermittelten Daten werden Handlungsempfehlungen abgeleitet, um pädagogische Prozesse planen zu können.[13] Data-Mining als wissenschaftliche Disziplin ist zunächst wertneutral. Die Verfahren erlauben die Analyse von Daten aus nahezu beliebigen Quellen, beispielsweise Messwerte von Bauteilen oder die Analyse von historischen Knochenfunden. Beziehen sich die analysierten Daten jedoch auf Personen, so entstehen wichtige rechtliche und moralische Probleme; typischerweise aber bereits bei der Erfassung und Speicherung dieser Daten, nicht erst bei der Analyse, und unabhängig von der konkret verwendeten Analysemethode (Statistik, Datenbankanfragen, Data-Mining, …). Daten, die unzulänglichanonymisiertwurden, können möglicherweise durchDatenanalysewieder konkreten Personen zugeordnet (deanonymisiert) werden. Typischerweise wird man hier jedochnichtData-Mining einsetzen, sondern einfachere und spezialisierte Analysemethoden zur Deanonymisierung. Eine derartige Anwendung – und vor allem die unzulängliche Anonymisierung zuvor – sind dann möglicherweise illegal (nach demDatenschutzrecht). So gelang es Forschern beispielsweise anhand weniger Fragen Nutzerprofile eindeutig in einem sozialen Netzwerk zu identifizieren.[14]Werden beispielsweise Bewegungsdaten nurpseudonymisiert, so kann mit einer einfachen Datenbankanfrage (technisch gesehen kein Data-Mining!) oft der Nutzer identifiziert werden, sobald man seinen Wohnort und Arbeitsplatz kennt: die meisten Personen können anhand der 2–3 Orte, an denen sie am meisten Zeit verbringen, eindeutig identifiziert werden. DasDatenschutzrechtspricht allgemein von der „Erhebung, Verarbeitung oder Nutzung“personenbezogener Daten, da diese Problematik nicht erst bei der Verwendung von Data-Mining auftritt, sondern auch bei der Verwendung anderer Analysemethoden (bspw. Statistik). Ein zuverlässiger Schutz vor einer missbräuchlichen Analyse ist nur möglich, indem die entsprechenden Daten gar nicht erst erfasst und gespeichert werden. DieAnwendungvon Data-Mining-Verfahren auf personenbeziehbare Daten wirft auch moralische Fragen auf. Beispielsweise, ob ein Computerprogramm Menschen in „Klassen“ einteilen sollte. Zudem eignen sich viele der Verfahren zurÜberwachungund für eine fortgeschritteneRasterfahndung. So stellt beispielsweise derSCHUFA-Scoreeine durch Statistik, vielleicht auch Data-Mining, gewonnene Einteilung der Menschen in die Klassen „kreditwürdig“ und „nicht kreditwürdig“ dar und wird entsprechendkritisiert. Data-Mining-Verfahren selbst arbeiten wertneutral und berechnen nur Wahrscheinlichkeiten, ohne die Bedeutung dieser Wahrscheinlichkeit zu kennen. Werden Menschen jedoch mit dem Ergebnis dieser Berechnungen konfrontiert, so kann das überraschte, beleidigte oder befremdete Reaktionen hervorrufen. Daher ist es wichtig abzuwägen, ob und wie man jemanden mit derartigen Ergebnissen konfrontiert. Googlegewährt seinen Nutzern Einblick in die für sie ermitteltenZielgruppen[15]– sofern kein Opt-out erfolgt ist – und liegt dabei oft falsch. Eine amerikanische Kaufhauskette kann aber anhand des Einkaufsverhaltens erkennen, ob eine Kundin schwanger ist.[16]Mit Hilfe dieser Information können gezielt Einkaufsgutscheine verschickt werden. Selbst eine Vorhersage des Datums der Geburt ist so möglich. Folgende Literatur liefert einen Überblick über das Gebiet Data-Mining aus Sicht der Informatik.Aufgaben- und anwendungsspezifische Literatur findet sich in den jeweiligen Artikeln. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Abgrenzung von anderen Fachbereichen 2Deutsche Bezeichnung 3Data-Mining-Prozess 4Aufgabenstellungen des Data-Mining 4.1Ausreißer-Erkennung 4.2Clusteranalyse 4.3Klassifikation 4.4Assoziationsanalyse 4.5Regressionsanalyse 4.6Zusammenfassung 5Spezialisierungen 5.1Textmining 5.2Webmining 5.3Zeitreihenanalyse 6Probleme des Data-Mining 6.1Daten-Defekte 6.2Parametrisierung 6.3Evaluation 6.4Interpretation 7Anwendungsgebiete 7.1Data-Mining in der Industrie 7.2Educational Data Mining 8Rechtliche, moralische und psychologische Aspekte 8.1Rechtliche Aspekte 8.2Moralische Aspekte 8.3Psychologische Aspekte 9Softwarepakete für Data-Mining 10Literatur 11Weblinks 12Einzelnachweise العربية"
  },
  {
    "label": 1,
    "text": "Data Science – Wikipedia Data Science Inhaltsverzeichnis Geschichte Ausbildung Berufsfeld Siehe auch Literatur Weblinks Einzelnachweise Ausbildungsmöglichkeiten Anforderungen Aufgabenbereich Wirtschaft Data Science(vonenglischdata„Daten“ undscience„Wissenschaft“, im Deutschen auchDatenwissenschaft) bezeichnet generell die Extraktion vonWissenaus Daten, um daraus zu lernen.[1][2] Data Science ist ein interdisziplinäres Wissenschaftsfeld, welches wissenschaftlich fundierte Methoden, Prozesse,Algorithmenund Systeme zur Extraktion von Erkenntnissen, Mustern und Schlüssen sowohl aus strukturierten als auch unstrukturierten Daten ermöglicht.[3][4] Erweitert um die zentrale Bedeutung der Datenerfassung unserer Welt (siehe auch DIKW-Pyramide nach Kellerhey und Tierney[5]) und die Visualisierung gewonnener Informationen[6]beschäftigt sich Data Science mit der „exakten digitalen Erfassung, Analyse und Visualisierung vergangener, aktueller sowie zukünftiger Phänomene unserer realen Welt, um datengetrieben den Prozess der Wissensgenerierung als bestmögliche Entscheidungsbasis für menschliches Handeln zu optimieren.“[7] Der Begriff „Data Science“ existiert seit über 40 Jahren und wurde ursprünglich als Ersatz für den Begriff „Informatik“ vonPeter Naurim Jahr 1960 verwendet. 1974 veröffentlichte Naur in derConcise Survey of Computer Methodseine Umfrage über die zeitgenössische Datenverarbeitung, in welcher der Begriff „Data Science“ frei verwendet wurde. 1996 trafen sich die Mitglieder derInternational Federation of Classification Societies(IFCS) inKobefür ihre zweijährliche Konferenz. Bei dieser Konferenz war zum ersten Mal der Begriff „Data Science“ im Titel der Konferenz enthalten.[8] Die moderne Definition vonData Sciencewurde erstmals im Rahmen des zweiten japanisch-französischen Statistiksymposiums an der Universität Montpellier II (Frankreich) im Jahr 1992 entworfen.[9]Die Teilnehmer würdigten die Entstehung einer neuen Disziplin mit einem besonderen Fokus auf Daten aus verschiedenen Herkünften, Dimensionen, Typen und Strukturen. Sie prägten die Kontur dieser neuen Wissenschaft, die auf etablierten Konzepten und Prinzipien der Statistik und Datenanalyse basiert, unter weitgehender Nutzung der zunehmenden Macht der Computerwerkzeuge. Im November 1997 gab C. F. Jeff Wu den Eröffnungsvortrag mit dem Titel „Statistik = Datenwissenschaft?“[10]für seine Ernennung zumH. C. Carver Professor of Statisticsan derUniversity of Michigan.[11]In diesem Vortrag charakterisierte er die statistische Arbeit als eine Trilogie von Datenerfassung,Datenmodellierungund -analyse und die Entscheidungsfindung. Abschließend rief er den Begriff „Datenwissenschaft“ ins Leben und befürwortete, dass die Statistik in „Datenwissenschaft“ und Statistiker in „Datenwissenschaftler“ umbenannt werden.[10]Später präsentierte er einen Vortrag mit dem Titel „Statistik = Datenwissenschaft?“, als ersten von seinen Mahalanobis-Memorial-Vorträgen.[12]Diese Vorträge ehrenPrasanta Chandra Mahalanobis, einen indischen Wissenschaftler, Statistiker und Gründer des „Indian Statistical Instituts“. 2001 führte William S. Cleveland die Datenwissenschaft als eigenständige Disziplin in seinem Artikel „Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics“ ein. In seinem Bericht stellte Cleveland sechs für ihn umfassende Gebiete der Datenwissenschaft vor: multidisziplinäre Untersuchungen, Modelle und Methoden für Daten, Rechnen mit Daten, Pädagogik, Werkzeug-Bewertung und Theorie. Im April 2002 veröffentlichte der internationale Rat für Wissenschaft: Ausschuss für die Daten für Wissenschaft und Theorie, das Data Science Journal,[13]welche sich auf die Problematik, wie die Beschreibung von Datensystemen, ihre Veröffentlichung im Internet, Anwendungen und gesetzlichen Problemen konzentrierte.[14] Kurz darauf begann dieColumbia University2003 die Zeitschrift „The Journal of Data Science“[15]zu veröffentlichen, welche eine Plattform für alle Datenanbieter zur Verfügung stellte, um ihre Ansichten und Ideen zum Austausch zu präsentieren. Die Zeitschrift wurde größtenteils der Anwendung von statistischen Methoden und der quantitativen Forschung gewidmet. 2005 veröffentlichte das National Science Board den Bericht „Long-lived Digital Data Collections: Enabling Research and Education in the 21st Century“, in welchem unter dem Begriff Data Scientists verschiedene Experten aufgeführt werden, die von entscheidender Bedeutung für das erfolgreiche Management digitalen Daten sind. Genannt werden unter anderem Informatiker, Datenbankexperten, Programmierer, Domänenexperten,Bibliothekare, Archivare sowie Experten im Bereich Software Engineering. Als Teil der Verantwortlichkeiten von Data Scientists wird insbesondere die Entwicklung innovativer Konzepte in den Bereichen Datenbanktechnologie und Informationswissenschaft betont. Hierunter fallen auch Methoden derInformationsvisualisierung, Datenanalyse und Wissensentdeckung in Datenbanken.[16] Der StudiengangData Scienceverwendet Techniken und Theorien aus den Fächern Mathematik,Statistikund Informationstechnologie, einschließlich derSignalverarbeitung, verwendet Wahrscheinlichkeitsmodelle desmaschinellen Lernens, des statistischen Lernens, derProgrammierung, der Datentechnik, derMustererkennung, derPrognostik, der Modellierung von Unsicherheiten und der Datenlagerung. Im deutschen Sprachraum bieten verschiedene Hochschulen auf Data Science spezialisierte Studiengänge an. Der Schwerpunkt liegt dabei auf Masterstudiengängen, inzwischen werden aber auch Bachelorstudiengänge angeboten. Darüber hinaus gibt es spezialisierte Weiterbildungsangebote sowie berufsbegleitende Studiengänge.[17] Personen, die im Bereich Data Science arbeiten, werden alsData Scientistbzw.Datenwissenschaftlerbezeichnet, wobei meist speziellere oder Spezialisierungen anderer, übergeordneter Berufsbezeichnungen üblich sind (z. B. Statistiker, Informatiker). Weltweit besteht ein Mangel an Experten in dem Bereich der Data Science.[18][19] Ein Data Scientist sollte überzeugend und kreativ sein, aber auch ein gewisses Kommunikationstalent mitbringen, um sich mit verschiedenen Ebenen einer Organisation austauschen zu können. Er ist das Bindeglied und der Vermittler zwischen allen Ebenen eines Unternehmens und nimmt somit die Rolle des „Übersetzers“ ein, indem er die Ergebnisse für die einzelnen Fachabteilungen genauso verständlich aufbereitet wie für das Top Management. Zudem sollte ein Data Scientist aufgeschlossen genug sein, um neue Analysetools und innovative Analyseverfahren zu erforschen und zu nutzen. Unvoreingenommen sollte ein Data Scientist nach anderen Ansätzen suchen wollen und immer neue Fragen stellen. Zusätzlich setzt dieser Beruf ein gewisses Koordinationstalent voraus, nicht zuletzt weil bestimmte Aufgaben, wie zum Beispiel die Beschaffung der Daten, an andere Mitarbeiter delegiert werden können. Kontrolle und Steuerung sollten jedoch immer in der Hand des Data Scientisten bleiben.[20] Die Aufgabe eines Data Scientist ist es, aus großen Datenmengen Informationen zu generieren und Handlungsempfehlungen abzuleiten, die das Unternehmen befähigen, effizienter zu arbeiten. Dazu bedient er sich innovativer Analysetools und entwickelt Abfragen, die aus unübersichtlichen Datenmengen wertvolle Informationen destillieren. Anschließend werden Hypothesen abgeleitet, welche statistisch überprüft und für das Management als Entscheidungsgrundlage aufbereitet werden. In allen Wirtschaftszweigen werden heute große Datenmengen ausgewertet. Der Mangel an Data Scientists macht es für Unternehmen schwierig, die Daten richtig zu nutzen und konkret Erkenntnisse daraus zu ziehen. Daten werden als das „neue Gold“ gehandelt. Zudem ist der Markt an Spezialisten, die mit Datenarchitekturen und Datenmodellen umgehen können, fast nicht existent.[21] Auch in derLogistikbranchewerden zukünftig immer mehr Data Scientists gesucht. Eine weitere Branche ist die Gesundheitsbranche. Durch die genaue Analyse von Daten aus einem Krankenhausaufenthalt könnten individualisierte Behandlungen (Personalisierte Medizin) durch Ähnlichkeitsanalysen von Patientendaten abgeleitet und Medikationspläne optimiert werden. In der Handelsbranche kann das Kaufverhalten der Menschen analysiert werden, um im weiteren Verlauf die Ursachen für Retouren herauszuarbeiten. So kann die Anzahl an Warenrücksendungen reduziert werden. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Ausbildung 2.1Ausbildungsmöglichkeiten 3Berufsfeld 3.1Anforderungen 3.2Aufgabenbereich 3.3Wirtschaft 4Siehe auch 5Literatur 6Weblinks 7Einzelnachweise العربية Azərbaycanca Български বাংলা Català Čeština Ελληνικά English Esperanto Español Eesti Euskara فارسی Suomi Français Galego עברית हिन्दी Հայերեն Bahasa Indonesia"
  },
  {
    "label": 1,
    "text": "DeepMind – Wikipedia DeepMind Inhaltsverzeichnis Geschichte Forschung DeepMind Ethics and Society Weblinks Einzelnachweise AlphaGo AlphaZero DeepNash AlphaStar AlphaTensor AlphaEvolve AlphaFold MuZero WaveNet Fußball Gesundheitsdaten Materialwissenschaften Google DeepMind(früherDeepMind Technologies) ist einbritischesUnternehmen, das sich auf dieProgrammierungeinerkünstlichen Intelligenz(KI) spezialisiert hat. DeepMind wurde im September 2010 gegründet und 2014 vonGoogle LLCübernommen. Im April 2023 wurde DeepMind mit der Google-KI-ForschungsabteilungGoogle Brainzu Google DeepMind zusammengeführt.[3] DeepMind Technologies war einbritischesStart-up, gegründet 2010 vonDemis Hassabis, Shane Legg und Mustafa Suleyman.[4]Zu den ersten Geldgebern gehörten dieVenture-Capital-Unternehmen Horizons Ventures und Founders Fund sowie derBusiness AngelScott Banister.[5] Am 26. Januar 2014 gab der US-KonzernGooglebekannt, DeepMind Technologies übernommen und damit das ebenfalls an DeepMind interessierteFacebookausgestochen zu haben – es war die bis dahin größte Übernahme des kalifornischen Unternehmens inEuropa. Der Preis blieb geheim, Branchenkenner gingen von einem Kaufpreis von etwa 500 Millionen Dollar (365 Millionen Euro) aus.[6][7][8]Mehrere Milliardäre hatten bereits in DeepMind investiert, darunterElon Musk, CEO des privaten RaumfahrtunternehmensSpaceXund des ElektroautoherstellersTesla, derPayPal-Gründer und ursprüngliche Facebook-GeldgeberPeter Thiel, derSkype-MitgründerJaan Tallinnsowie der Hongkonger MagnatLi Ka-shingvonHorizon Ventures, einer der mächtigsten Männer Asiens.[9] Im Jahre 2014 erhielt DeepMind die Auszeichnung „Company of the Year“ vomCambridge Computer Laboratory.[10][11] Nach Übernahme wurde das Unternehmen in Google DeepMind umfirmiert. Google setzte auch einenEthikratein, der sicherstellen soll, dass die Technologie von DeepMind nicht missbraucht wird. Die Struktur des Ethikrates blieb unklar.[12] 2015 veröffentlichte das Unternehmen die Ergebnisse eines Forschungsprojektes, bei der die Künstliche Intelligenz alte Atari-Spiele selbstständig erlernen sollte. Nach Angaben von Google DeepMind gelang es der KI, sowohl die Spielregeln zu erlernen als auch Erfolgstaktiken selbstständig zu entwickeln.[13] Im Dezember 2019 kündigte einer der Gründer, Suleyman, an, dass er DeepMind verlassen würde, damit er bei Google arbeiten kann.[14] Im April 2023 wurde bekannt gegeben, dass das bisher unabhängige Unternehmen DeepMind und die KI-Forschungsabteilung von Google, Google Brain, zusammengelegt werden. Der Leiter von Google Brain, Jeff Dean, wurde zum Chief Scientist von Google befördert. Der bisherige CEO von DeepMind, Demis Hassabis, wurde zum CEO des neu geschaffenen Google DeepMind ernannt und soll damit die konkrete Ausrichtung der KI-Forschung von Google bestimmen.[15] Das offizielle Unternehmensziel von Google DeepMind ist, Intelligenz zu verstehen („Solve Intelligence“).[4]Im Gegensatz zu anderen Künstlichen Intelligenzen wie beispielsweiseDeep BluevonIBMhat Google DeepMind kein vordefiniertes Ziel und ist somit flexibler in der Anwendung für verschiedene Probleme.[16]Google DeepMind unterscheidet sich ebenfalls in der grundsätzlichen Strukturierung der Künstlichen Intelligenz. Statt ausschließlich auf einneuronales Netzzu setzen, erweiterte man die KI mit einem Kurzzeitspeicher, um somit die Fähigkeit eines künstlichen Gedächtnisses zu simulieren.[17]Die Entwickler von Google DeepMind bezeichnen die Künstliche Intelligenz deshalb auch als „neuronaleTuringmaschine“ und nicht als neuronales Netz.[18] Zu den leitenden Entwicklern zähltDavid Silver. Bei DeepMind wurdeAlphaGoentwickelt, ein Computerprogramm, das ausschließlich das BrettspielGospielt. Im Oktober 2015 besiegte es den mehrfachen Europameister Fan Hui.[19]Es ist damit das erste Programm, das unter Turnierbedingungen einen professionellen Go-Spieler schlagen konnte. Zwischen dem 9. und 15. März 2016 trat AlphaGo gegen den südkoreanischen ProfiLee Sedol,9. Dan, an. Das Programm gewann nach fünf Runden mit 4:1.[19] Im Jahre 2017 wurde eine verbesserte Version namensAlphaGo Zeroveröffentlicht, welche AlphaGo 100 zu 0 schlug, wobei die Strategien von AlphaGo Zero autonom erstellt wurden. Die Lernphase dauerte nur drei Tage, wobei AlphaGo im Vergleich Monate dazu brauchte.[20] AlphaZero ist eine im Dezember 2017 erstmals in einer Veröffentlichung beschriebene Verallgemeinerung des oben erwähnten AlphaGo Zero. AlphaZero lernte die Beherrschung der drei BrettspieleShōgi,SchachundGoauf höchstem Niveau nur anhand der Spielregeln und durch intensives Spielen gegen sich selbst, ohne die Nutzung von Daten zum Vorgehen menschlicher Spieler. Die SoftwareDeepNashspieltStratego. Ihr gelang es im Jahr 2022, auf dem Niveau von menschlichen Spitzenspielern zu spielen. Gegen die besten menschlichen Spieler auf der SpieleplattformGravon[21]erreichteDeepNasheine Gewinnrate von 84 Prozent.DeepNashverwendet einen neuartigen Ansatz, der auf einer Kombination ausSpieltheorieund sogenanntem modellfreiem Deep Reinforcement Learning basiert. Die Software hat dafür etwa zehn Milliarden Mal gegen sich selbst gespielt und hatte das Ziel, ein so genanntesNash-Gleichgewichtzu erreichen. Ein Spiel, das sich im Nash-Gleichgewicht befindet, verläuft stabil, denn das einseitige Abweichen von der Strategie würde einen Nachteil bedeuten. Die Entwicklung einer spielstarken Stratego-Software ist eine enorme Herausforderung, weil die Anzahl der möglichen Spielzustände auch im Vergleich zuSchach,GoundTexas Hold’emaußergewöhnlich ist. Weil die Komplexität desSpielbaumsvon Stratego so groß ist, ist es nicht möglich, eine Monte-Carlo-Baumsuche (englischMonte Carlo tree search) zu verwenden.[22][23] Im Januar 2019 wurdeAlphaStarvorgestellt, einKI-Programm, das dasEchtzeit-StrategiespielStarCraft IIspielt. Wie AlphaGo handelt es sich dabei um einkünstliches neuronales Netz, das zunächst menschliche Spieler imitierte und dann mitReinforcement Learningtrainiert wurde. In zwei Sätzen zu je fünf Spielen gegen die professionellen Spieler Dario „TLO“ Wünsch und Grzegorz „MaNa“ Komincz gewann AlphaStar jedes Spiel. Die Anzahl der Aktionen pro Minute wurde auf ein für Menschen übliches Maß beschränkt. Im Gegensatz zu menschlichen Spielern hat AlphaStar jederzeit einen vollständigen Überblick über die sichtbaren Teile der Karte, fokussiert sich aber dennoch immer nur auf einzelne Bereiche. Ein bei der Vorstellung live übertragenes Spiel konnte MaNa für sich entscheiden. Besondere Stärken von AlphaStar warenMicromanagementundMultitasking. Im letzten Spiel wurde sie durch ungewöhnliche Aktionen des menschlichen Spielers aus dem Tritt gebracht.[24]Eine Weiterentwicklung trat ab Juli 2019 anonym in Ranglisten gegen echte Spieler an und erreichte in allen 3 „Rassen“ die höchste Liga.[25] Im Oktober 2022 veröffentlichte DeepMindAlphaTensor, das ähnlich wieAlphaGoTechniken desbestärkenden Lernensnutzte, um neuartige Algorithmen für die Matrixmultiplikation zu finden.[26]Im speziellen Fall der Multiplikation zweier 4×4-Matrizen mit ganzzahligen Einträgen, bei denen nur die Geradheit oder Ungeradheit der Einträge berücksichtigt wurde, fandAlphaTensoreinen Algorithmus, der nur 47 verschiedene Multiplikationen benötigte; das bisher optimale Verfahren, bekannt seit 1969, war der allgemeinereStrassen-Algorithmus, der 49 Multiplikationen verwendete.[27] Das im Mai 2025 von Google DeepMind vorgestellteAlphaEvolveist ein KI-Agent, welcher als Programmierwerkzeug optimiert wurde.[28]Aufgaben, welche als Algorithmen formuliert werden können und als Eingabe dienen, werden autonom durchAlphaEvolveanhand von vorausdefinierten Bewertungskriterien iterativ verbessert. Die bei diesem Optimierungsprozess verwendetenLarge Language Models(LLMs) wieGeminierzeugen bei jedem Optimierungsschritt Varianten des vorherigen Algorithmus, von welchen der jeweils messbar Beste auf dem Weg zu einer optimalen Lösung weiterverfolgt wird.[29]Im Gegensatz zu bestimmten anderem KI-Modellen wird keinReinforcement learning from human feedbackverwendet, sondern ein evolutionäres Bewertungsframeworkeingesetzt. Im Gegensatz zu beispielsweiseAlphaTensorist dieses generative Modell in der Lage, Aufgaben in unterschiedlichen Bereichen zu lösen.[30]So wurden Aufgaben bei mehrdimensionalen Matrixmultiplikationen, Effizienzsteigerungen bei der Planung von Rechneranlagen und beim Entwurf vonintegrierten Schaltungenerfolgreich gelöst.[31] Seit 2016 arbeitete DeepMind auch am Problem, die dreidimensionaleStruktur der Proteinenur anhand der Abfolge derAminosäurendesProteinsvorherzusagen.[32][33]2018 nahm das von DeepMind für dieses Problem entwickelte KI-ProgrammAlphaFoldam GemeinschaftsexperimentCASPteil.[32]Dies ist eine Art Wettbewerb, bei dem KI-Programme verschiedener Institutionen genutzt werden, um die Strukturen von Proteinen vorherzusagen. Diese sind den CASP-Veranstaltern bekannt, aber der Öffentlichkeit und den CASP-Teilnehmern nicht. So ist es möglich, die Qualität der Vorhersagen zu beurteilen. DeepMinds AlphaFold schnitt dabei bereits bei seiner ersten Teilnahme 2018 besser ab als die Programme der etwa 100 weiteren Teams.[32]Beim nächsten CASP-Wettbewerb im Jahr 2020 war die Vorhersagequalität des zweiten von DeepMind entwickelten Programms AlphaFold2 so gut, dass Wissenschaftler und auch DeepMind urteilten, dass das jahrzehntealte Problem derProteinfaltungerstmals als gelöst betrachtet werden könne – das sei ein Meilenstein derStrukturbiologie.[33][34]Am 15. Juli 2021 veröffentlichte DeepMind einequelloffeneVersion von AlphaFold 2 und veröffentlichte die Funktionsweise im FachjournalNature.[35][36] In den Jahren 2019 und 2020 veröffentlichte eine Forschergruppe von DeepMind den AlgorithmusMuZero, der eine Baumsuche mit einem individuell entwickelten Machine-Learning Model kombiniert.[37][38]MuZero beruht auf Deep Reinforcement Learning und stellt eine Weiterentwicklung der schon in AlphaGo, AlphaGo Zero und AlphaZero verwendeten Technologien dar. Im Gegensatz zur „Alpha“-Serie von KI-Systemen ist MuZero nicht mehr auf eine bestimmte Wissensdomäne oder Anwendungsbereich festgelegt. Es ist keine Voreinstellung von Regeln mehr nötig und auch keine durch Menschen gesteuerte Initialisierung mit Trainingsdaten. Das MuZero-Softwaresystem erlernt diese Regeln selbstständig durch Beobachtung der Umgebung, und vor allem verfeinert es sein selbst aufgestelltes Modell und bestimmte Aspekte des eigenen Entscheidungsprozesses. Ein Vergleich des fertig optimierten MuZero mit anderen KI-Systemen ergab eine mindestens gleichwertige Leistung bei Computergo und Computerschach, aber auch in „Atarigames“ wie Ms. Pac-Man. WaveNet ist ein neuronales Netzwerk, das ursprünglich ab ca. 2014 zum Zweck der verbesserten, natürlicher klingenden Text-to-Speech-Synthese (TTS) entwickelt wurde, sprich, für menschlicher klingende Vorlesesysteme, indem das Programm natürliche menschliche Stimmen analysieren sollte, um so auch natürlicher klingende Sprache beim Vorlesen von Texten erzeugen zu können.[39]Seit März 2018 bietet Google das auf WaveNet basierende VorleseprogrammCloud Text-to-Speechim Rahmen vonGoogle Assistantan.[40][41] Inzwischen forscht DeepMind aber auch aktiv daran, mit WaveNet existierende individuelle menschliche Stimmen nicht nur zum Vorlesen von Texten möglichst exakt nachzubilden, um es mit ähnlichen Funktionen wie etwa das 2016 vonAdobe Inc.vorgestellteAdobe Vocoauszustatten. Ein Forschungsbericht vom Juni 2018 mit dem TitelDisentangled Sequential Autoencoder[42](„Entflochtener sequentieller, automatischer Stimmgenerator“) stellt fest, dass es erfolgreich gelungen sei, WaveNet dafür einzusetzen, die Stimme in einer existierenden Tonaufnahme durch jegliche andere reale Stimme zu ersetzen, die denselben Text spricht („content swapping“). Um diese Fähigkeit zu ermöglichen, seien ca. 50 Stunden an Aufnahmen jeweils der Quell- wie Zielstimme nötig, mit denen WaveNet die grundlegenden Eigenschaften der beiden Stimmen erst lernen muss (wobei es selbstständig ein jeweils eigenes, von den Sprachaufnahmen abstrahiertes Modell der zu erlernenden Stimme erstellt), bevor es die Stimmkonvertierung bei jeder beliebigen neuen Tonaufnahme in annehmbarer Qualität durchführen kann. Die Autoren des Forschungsberichts betonen außerdem, dass WaveNet dazu in der Lage sei, statische und dynamische Eigenschaften voneinander zu unterscheiden (zu: „entflechten“), d. h., das Programm trenne automatisch zwischen den bei der Stimmkonvertierung beizubehaltenden Eigenschaften (Textinhalt, Modulation, Geschwindigkeit, Stimmung usw.) und den zu konvertierenden Grundeigenschaften der Quell- wie Zielstimme. In einem Nachfolgebericht mit dem TitelUnsupervised speech representation learning using WaveNet autoencoders[43](„Selbständiges Sprachmodellernen mit WaveNet“) vom Januar 2019 hieß es, dass DeepMind die Unterscheidung statischer von dynamischen Stimmeigenschaften von WaveNet weiter verbessert habe. In dem weiteren NachfolgeberichtSample Efficient Adaptive Text-to-Speech[44](„Samplingeffizienz bei adaptiver Text-to-Speech-Synthese“) vom September 2018 (letzte Revision Januar 2019) berichtet DeepMind, das Minimum an benötigten Sprachaufnahmen für das Sampeln einer Stimme inzwischen auf wenige Minuten reduziert zu haben. Im bereits 2014 veröffentlichten BerichtTowards End-to-End Speech Recognition with Recurrent Neural Networks[45](„Zur vollständigenSpracherkennungmittelsrückgekoppelter neuronaler Netze“) verwies DeepMind auf erfolgreiche Versuche, WaveNet zur automatischen Verschriftlichung von existierenden Sprachaufnahmen einzusetzen. Der ForschungsberichtLarge-Scale Visual Speech Recognition[46](„Allgemein einsetzbare optische Spracherkennung“) vom Juli 2018 (letzte Revision vom Oktober 2018) geht auf erfolgreiche Versuche ein, WaveNet zumLippenlesenbei menschlichen Lippenbewegungen auch in völlig stummen Videoaufnahmen einzusetzen, wobei das Programm professionelle menschliche Lippenleser beim Erraten der tatsächlichen Laute bereits deutlich übertreffe. Dieses Feature scheint als eigenständiges WaveNet-Plugin unter dem TitelLipNetentwickelt zu werden. Forscher von DeepMind haben Modelle desmaschinellen Lernensauf den Fußballsport angewendet. Sie modellieren das Verhalten von Fußballspielern, einschließlich Torhütern, Verteidigern und Stürmern, in verschiedenen Szenarien, z. B. beim Elfmeterschießen. Die Forscher nutztenHeatmapsundClusteranalysen, um die Spieler nach ihrer Tendenz, sich während des Spiels auf eine bestimmte Art und Weise zu verhalten, wenn sie vor der Entscheidung stehen, wie sie ein Tor erzielen oder ein Tor der anderen Mannschaft verhindern können, zu organisieren. Die Forscher erwähnen, dass Modelle des maschinellen Lernens zur Demokratisierung der Fußballindustrie eingesetzt werden könnten, indem automatisch interessante Videoclips des Spiels ausgewählt werden, die als Highlights dienen. Dies kann durch die Suche nach bestimmten Ereignissen in Videos geschehen, was möglich ist, da die Videoanalyse ein etablierter Bereich des maschinellen Lernens ist. Möglich ist dies auch aufgrund der umfangreichen Sportanalyse, die auf Daten wie kommentierten Pässen oder Schüssen, Sensoren, die Daten über die Bewegungen der Spieler im Laufe eines Spiels erfassen, und spieltheoretischen Modellen basiert.[47][48] AufYouTubewurde 2023 ein Video hochgeladen, das anscheinend kleine Roboter von Google DeepMind bei einer einfachen Variante desRoboterfußballszeigt.[49] Im Februar 2016 gab die Firma bekannt, dass es in Großbritannien mit der GesundheitsbehördeNational Health Servicezusammenarbeitet, um eine iPhone-App mit dem Namen „Streams“ zu entwickeln, allerdings sei es noch zu früh, um sagen zu können, wo KI angewandt werden könnte.[50]Diese soll dabei helfen,Patienten, die einen „Nierenschaden“ haben, zu überwachen.[51]April 2016 veröffentlichte das Fachmagazin „New Scientist“ Details der Vereinbarung, demnach erhält Google den Zugriff auf die Daten von 1,6 Millionen NHS-Versicherten, die jährlich in den drei Krankenhäusern – Barnet, Chase Farm and the Royal Free – des Royal Free NHS Trust behandelt werden.[52]Die weitreichende Vereinbarung erlaubt einen Datenaustausch weit über das bekannt gegebene hinaus.[52]Enthalten sind die Daten vonHIV-positiven Patienten,Drogenabhängigenund Frauen, dieAbtreibungenvornehmen ließen. Eine Opt-out-Möglichkeit (Möglichkeit der Verweigerung) für Patienten gab es nicht. Neben den pathologischen und radiologischen Ergebnissen werden die Daten der Intensivmedizin und der Notfallabteilungen übertragen sowie die vollständigen Tagesaktivitäten der Kliniken, der Zustand und die Unterbringung der Patienten und die Krankenhausbesuche (wer und wann) übermittelt. Darüber hinaus bedeutet der Datenzugang auf die zentralen Aufzeichnungen aller NHS-Krankenhaus-Behandlungen in Großbritannien, dass es auf historische Daten der vergangenen fünf Jahre zurückgreifen kann – zusätzlich zu den neu auflaufenden Daten. Kritiker fürchten dabei um dieIntimsphäreund denDatenschutz. Das Personal von Google soll aber nicht in der Lage sein, bestimmte Patienten zu identifizieren, und die Daten sollen nicht mit Google-Konten oder Produkten verbunden werden, erklärte Mustafa Suleyman, Leiter des Bereichs „Angewandte KI“ bei DeepMind.[53][54][55] Das KI-ToolGNoME (Graph Networks for Materials Exploration)hat 2023 2,2 Millionen neue Kristalle entdeckt, darunter 380.000 stabile Materialien, die zukünftige Technologien antreiben könnten. Im Oktober 2017 kündigte DeepMind an, das Forschungsabteil DeepMind Ethics & Society zu gründen[56], welche sich mit den folgenden Themen beschäftigen soll: Privatsphäre, Transparenz und Gerechtigkeit und wirtschaftliche Folgen davon. Am 4. Februar 2025 änderte die DeepMind-Muttergesellschaft die Prinzipien für den Einsatz von künstlicher Intelligenz und anderen fortschrittlichen Technologien.Entfernt wurden Zusagen, \"Technologien, die Schaden verursachen oder wahrscheinlich verursachen\", \"Waffen oder andere Technologien, deren Hauptzweck oder Einsatz ist, Menschen zu verletzen oder dies zu begünstigen\", \"Technologien, die Informationen zur Überwachung sammeln oder nutzen, die gegen international anerkannte Normen verstoßen\" und \"Technologien, deren Zweck gegen weithin anerkannte Prinzipien des Völkerrechts und der Menschenrechte verstößt\",nicht zu verfolgen.[57] Google LLC|Calico|Nest Labs|X|Google Fiber|CapitalG|GV|Waymo|Verily Life Sciences|Sidewalk Labs|Jigsaw|DeepMind Projekte von X:Project Loon Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Forschung 2.1AlphaGo 2.2AlphaZero 2.3DeepNash 2.4AlphaStar 2.5AlphaTensor 2.6AlphaEvolve 2.7AlphaFold 2.8MuZero 2.9WaveNet 2.10Fußball 2.11Gesundheitsdaten 2.12Materialwissenschaften 3DeepMind Ethics and Society 4Weblinks 5Einzelnachweise العربية Azərbaycanca تۆرکجه Bikol Central বাংলা Català کوردی Zazaki Ελληνικά English Esperanto Español Eesti Euskara"
  },
  {
    "label": 1,
    "text": "Deep Learning – Wikipedia Deep Learning Inhaltsverzeichnis Voraussetzungen und Grundlagen Geschichte, Entwicklung und Verwendung Komplexität und Grenzen der Erklärbarkeit Programmbibliotheken Literatur Weblinks Einzelnachweise Deep Learning(deutschmehrschichtiges Lernen,tiefes Lernenodertiefgehendes Lernen) bezeichnet eine Methode desmaschinellen Lernens, diekünstliche neuronale Netze(KNN) mit zahlreichen Zwischenschichten (englischhidden layers) zwischen Ein- und Ausgabeschicht einsetzt und dadurch eine umfangreiche innere Struktur herausbildet. Deep Learning erlaubt die Verarbeitung und Analyse komplexer Datenmuster; dazu verwendet Deep Learning tiefe hierarchische neuronale Netze, die automatisch abstrakte Merkmale aus den Daten extrahieren. Dies ermöglicht eine effiziente Verarbeitung komplexer Informationen, was wiederum zu präzisen Vorhersagen und Entscheidungen in verschiedenen Anwendungen führt. Die in der Anfangszeit derkünstlichen Intelligenzgelösten Probleme waren für den Menschen intellektuell schwierig, aber fürComputereinfach zu verarbeiten. Diese Probleme ließen sich durch formalemathematischeRegeln beschreiben. Die wahre Herausforderung an die künstliche Intelligenz bestand jedoch in der Lösung von Aufgaben, die für die Menschen leicht durchzuführen sind, deren Lösung sich aber nur schwer durch mathematische Regeln formulieren lassen. Dies sind Aufgaben, die der Menschintuitivlöst, wieSprach-oderGesichtserkennung.[2][3] Eine computerbasierte Lösung für diese Art von Aufgaben beinhaltet die Fähigkeit von Computern, aus der Erfahrung zu lernen und die Welt in Bezug auf eineHierarchievonKonzeptenzu verstehen. Hierbei ist jedes Konzept durch seine Beziehung zu einfacheren Konzepten definiert. Durch das Sammeln von Wissen aus der Erfahrung vermeidet dieser Ansatz die Notwendigkeit für die menschlichen Bediener, all das Wissen, das der Computer für seine Arbeit benötigt, formal spezifizieren zu müssen. Die Hierarchie der Konzepte erlaubt es dem Computer, komplizierte Konzepte zu erlernen, indem er sie aus einfacheren zusammensetzt. Wenn man einDiagrammzeichnet, das zeigt, wie diese Konzepte übereinander aufgebaut werden, dann ist das Diagramm tief, mit vielen Schichten. Aus diesem Grund wird dieser Ansatz in der künstlichen Intelligenz „Deep Learning“ genannt.[2][4] Es ist schwierig für einen Computer, dieBedeutungvonrohensensorischenEingangsdaten zu verstehen, wie in derHandschrifterkennung, wo ein Text zunächst nur als eine Sammlung vonBildpunktenexistiert. Die Überführung einer Menge von Bildpunkten in eine Kette vonZiffernundBuchstabenist sehr kompliziert. KomplexeMustermüssen aus Rohdaten extrahiert werden. Das Lernen oder Auswerten dieserZuordnungscheint unüberwindbar schwierig, wenn sie manuell programmiert würde.[2] Eine der häufigsten Techniken in der künstlichen Intelligenz istmaschinelles Lernen. Maschinelles Lernen ist ein selbstadaptiverAlgorithmus. Deep Learning, eine Teilmenge des maschinellen Lernens, nutzt eine Reihe hierarchischer Schichten bzw. eine Hierarchie von Konzepten, um den Prozess des maschinellen Lernens durchzuführen. Die hierbei benutzten künstlichen neuronalen Netze wurden inspiriert vonGehirnen, in denenbiologischeNeuronenmiteinander verbunden sind. Die erste Schicht des neuronalen Netzes, die sichtbare Eingangsschicht, verarbeitet eine Rohdateneingabe, wie beispielsweise die einzelnen Pixel eines Bildes. Die Dateneingabe enthältVariablen, die der Beobachtung zugänglich sind, daher „sichtbare Schicht“.[2][5][6][7]Die erste Schicht leitet ihre Ausgaben an die nächste Schicht weiter. Diese zweite Schicht verarbeitet die Informationen der vorherigen Schicht und gibt das Ergebnis ebenfalls weiter. Die nächste Schicht nimmt die Informationen der zweiten Schicht entgegen und verarbeitet sie weiter. Dies geht über alle Ebenen des künstlichen neuronalen Netzes so weiter. Diese Schichten werden als versteckte Ebenen (englischhidden layers) bezeichnet. Die in ihnen enthaltenen Merkmale werden zunehmendabstrakt. Ihre Werte sind nicht in den Ursprungsdaten angegeben. Stattdessen muss das Modell bestimmen, welche Konzepte für die Erklärung der Beziehungen in den beobachteten Daten nützlich sind. Das Ergebnis wird in der letzten, wieder sichtbaren Schicht ausgegeben. Hierdurch wird die gewünschte komplizierteDatenverarbeitungin eine Reihe von verschachtelten einfachen Zuordnungen unterteilt, die jeweils durch eine andere Schicht des Modells beschrieben werden.[8][2][6][4][9] DieGroup method of data handling-KNNs (GMDH-ANN) der 1960er-Jahre vonOleksij Iwachnenkowaren die ersten Deep-Learning-Systeme des Feedforward-Multilayer-Perzeptron-Typs.[10][11][12] Karl SteinbuchsLernmatrix[13]war eines der erstenkünstlichen neuronalen Netze, das aus mehreren Schichten von Lerneinheiten oder lernenden „Neuronen“ bestand. Damit war er einer der Wegbereiter des Deep Learning, bei dem es um tiefe neuronale Netze geht, die viele Aufgaben erlernen können, bei denen frühere einschichtige Perzeptronen scheitern. Weitere Deep-Learning-Ansätze, vor allem aus dem Bereich desmaschinellen Sehens, begannen mit demNeocognitron, einerConvolutional Neural Network(CNN)-Architektur, die vonKunihiko Fukushima1980 entwickelt wurde.[14] Alex WaibelsCNN namens TDNN (1987) wurde durchbackpropagationtrainiert und erzielteBewegungsinvarianz.[15] Im Jahr 1989 verwendetenYann LeCunund Kollegen den Backpropagation-Algorithmusfür das Training mehrschichtiger KNNs (sieheMulti-Layer-Perzeptron), mit dem Ziel,handgeschriebenePostleitzahlenzu erkennen.[16] Sven Behnke hat seit 1997 in der Neuronalen Abstraktionspyramide[17]den vorwärtsgerichteten hierarchisch-konvolutionalen Ansatz durch seitliche und rückwärtsgerichtete Verbindungen erweitert, um so flexibelKontextinEntscheidungeneinzubeziehen und iterativ lokaleMehrdeutigkeitenaufzulösen. Der Begriff „Deep Learning“ wurde im Kontext des maschinellen Lernens erstmals 1986 vonRina Dechterverwendet, wobei sie hiermit ein Verfahren bezeichnet, bei dem alle verwendeten Lösungen eines betrachtetenSuchraumsaufgezeichnet werden, die zu keiner gewünschten Lösung geführt haben. Die Analyse dieser aufgezeichneten Lösungen soll es ermöglichen, anschließende Versuche besser zu steuern und somit mögliche Sackgassen in der Lösungsfindung frühzeitig zu verhindern.[18] Heute wird der Begriff jedoch vorwiegend im Zusammenhang mit künstlichen neuronalen Netzen verwendet und tauchte in diesem Kontext erstmals im Jahr 2000 auf, in der VeröffentlichungMulti-Valued and Universal Binary Neurons: Theory, Learning and Applicationsvon Igor Aizenberg und Kollegen.[19][20][21] Zwischen 2009 und 2012 gewannen dierekurrentenbzw. tiefen vorwärtsgerichteten neuronalen Netze der Forschungsgruppe vonJürgen SchmidhuberamSchweizer KI-Labor IDSIAeine Serie von acht internationalen Wettbewerben in den BereichenMustererkennungundmaschinelles Lernen.[22]Insbesondere gewannen ihre rekurrentenLSTM-Netze[23][24]drei Wettbewerbe zur verbundenenHandschrifterkennungbei der2009 Intl. Conf. on Document Analysis and Recognition (ICDAR)ohne eingebautesA-priori-Wissen über die drei verschiedenen zu lernenden Sprachen. Die LSTM-Netze erlernten gleichzeitigeSegmentierungund Erkennung. Dies waren die ersten internationalen Wettbewerbe, die durch Deep Learning[25]oder rekurrente Netze gewonnen wurden. Die jüngsten Erfolge von Deep Learning-Methoden, wie der Gewinn einesGo-Turniers durch das ProgrammAlphaGogegen die weltbesten menschlichen Spieler, gründen sich neben der gestiegenen Verarbeitungsgeschwindigkeit derHardwareauf den Einsatz von Deep Learning zum Training des in AlphaGo verwendeten neuronalen Netzes.[26]Gleiches gilt für die seit 2020 gelungene Vorhersage vonProteinfaltungen.[27]Diese Netze nutzen künstlich erzeugte Neuronen (Perzeptronen), um Muster zu erkennen. Für Beiträge zu neuronalen Netzwerken und Deep Learning erhieltenYann LeCun,Yoshua BengioundGeoffrey Hinton2018 denTuring Award[28]und Hinton zusammen mitJohn Hopfield2024 den Physik-Nobelpreis.[29] Tiefe neuronale Netze können eine Komplexität von bis zu hundert Millionen einzelnenParameternund zehn MilliardenRechenoperationenpro Eingangsdatum aufweisen. Die Interpretierbarkeit der Parameter und Erklärbarkeit des Zustandekommens der Ergebnisse ist dabei nur noch eingeschränkt möglich und erfordert den Einsatz spezieller Techniken, die unterExplainable Artificial Intelligencezusammengefasst werden. Ein weiterer Ansatz ist die Verwendung von Methoden aus derPhysikvonVielteilchensysteme(Statistische Physik).[30] Eine weitere Begleiterscheinung des Deep Learning ist die Anfälligkeit für Falschberechnungen, die durch geringfügig veränderte Eingabesignale ausgelöst werden können. Ein Angreifer könnte z. B.Bilddatenabsichtlich somanipulieren, dass eineBilderkennungfür Bilder, die für einen Menschen normal aussehen, falsche Ergebnisse liefert. Dieses Phänomen wird unterAdversarial Exampleszusammengefasst.[31] Zu Grenzen und Erklärbarkeit von KI gibt es zwei unterschiedliche Konzepte. Bei beiden lassen sich dieLogik, dieVorhersagenund dieEntscheidungeneiner KI nicht einfach ausdrücken. Ein bekanntes Beispiel für dieRisikenopaker KI ist ein Experiment, das Microsoft im Jahr 2016 mit der Veröffentlichung einesTwitter-ChatbotsnamensTaydurchgeführt hat. In weniger als 24 Stunden musste Microsoft den Bot abschalten, weil er damit begann, anzügliche und beleidigendeTweetszu verfassen. Neben der meist inSchulungsbeispielen zum Verständnis der internen Struktur vorgestellten Möglichkeit, ein neuronales Netz komplett eigenhändig zu programmieren, gibt es eine Reihe vonSoftwarebibliotheken,[32]häufigOpen Source, lauffähig auf meist mehrerenBetriebssystemen, die in gängigenProgrammiersprachenwieC,C++,JavaoderPythongeschrieben sind. Einige dieser Programmbibliotheken unterstützenGPUsoderTPUszur Rechenbeschleunigung oder stellenTutorialszur Benutzung dieser Bibliotheken bereit. MitONNXkönnen Modelle zwischen einigen dieser Tools ausgetauscht werden. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Voraussetzungen und Grundlagen 2Geschichte, Entwicklung und Verwendung 3Komplexität und Grenzen der Erklärbarkeit 4Programmbibliotheken 5Literatur 6Weblinks 7Einzelnachweise Afrikaans العربية الدارجة Azərbaycanca Български বাংলা Bosanski Català کوردی Čeština Dansk Ελληνικά English Esperanto Español Eesti Euskara فارسی Suomi Français Gaeilge Galego עברית हिन्दी"
  },
  {
    "label": 1,
    "text": "Deepfake – Wikipedia Deepfake Inhaltsverzeichnis Entwicklung Anwendung Software Probleme Shallowfake Sonstiges Literatur Siehe auch Weblinks Einzelnachweise Politik Kunst Datenschutz Forschung Pornografie Betrug Kreditwürdigkeit und Authentizität Wahlen Deepfakes(englischKofferwortaus den Begriffen „Deep Learning“ und „Fake“)[1]sind realistisch wirkende Medieninhalte (Foto, Audio, Video usw.), die durch Techniken derkünstlichen Intelligenz(KI bzw. AI,artificial intelligence) abgeändert, erzeugt(→Prompt)bzw. verfälscht worden sind. Zwar istMedienmanipulationkein neues Phänomen, allerdings nutzen Deepfakesmaschinelles Lernen, genauerkünstliche neuronale Netzwerke, um Fälschungen weitgehend autonom und damit in bislang ungeahnter und nicht möglicher Dimension zu erzeugen. Im Laufe der Zeit veränderte sich der BegriffDeepfake. Während er 2017 und 2018 nur für genau die Bildwerke benutzt wurde, die explizit durch die Deepfake-KI erschaffen wurden, wurde er ab dem Jahr 2022 benutzt, um Bilder und Filme zu beschreiben, die offensichtlich oder mutmaßlich durch irgendeine KI verfälscht wurden.[2]So behauptetenElon MusksAnwälte 2022 in einem Gerichtsprozess in den USA, dass ein Musk belastendes Video wahrscheinlich nicht dessen Aussagen wiedergebe, sondern eine Deepfake-Produktion sei.[3] In der Verordnung (EU) 2024/1689 (KI-Verordnung) werden Deepfakes als „einen durch KI erzeugten oder manipulierten Bild-, Ton- oder Videoinhalt, der wirklichen Personen, Gegenständen, Orten, Einrichtungen oder Ereignissen ähnelt und einer Person fälschlicherweise als echt oder wahrheitsgemäß erscheinen würde“ definiert.[4] Der erste und derzeit häufigste Einsatz von Deepfakes findet im Bereich des „face swapping“ statt. Hierbei wird in visuellem Material (z. B. Videos oder Fotos) das Gesicht einer Person durch das Gesicht einer anderen Person ersetzt, um diese Person in einem anderen Kontext darzustellen. Die so entstehenden Inhalte haben großes destruktives Potential,[5][6]wie gefälschte pornografische Inhalte. Deepfakes können weit über die Anwendung des „face-swapping“ hinausgehen und die Manipulation auditorischer Inhalte (z. B. „voice swapping“) und die als „body-puppetry“ bekannte Übertragung von Körperbewegungen auf andere Personen in Videomaterial beinhalten.[5] Obwohl Deepfakes ein relativ neuartiges Phänomen sind (erste Nutzung des Begriffs 2017), haben sie eine breite Debatte um ihre Nutzung und Gefahren für Politik, Wirtschaft und Gesellschaft entfacht. Auf Seiten der Politik sowie auf Seiten der Industrie gibt es Bestrebungen, die Identifizierung von Deepfakes zu erleichtern, ihre Nutzung einzuschränken und ihre unerlaubte Erstellung unter Strafe zu stellen.[7][8] Das MIT Center for Advanced Virtuality produzierte ein komplexes Deepfake, bei dem die Technologie sowohl zum Einsatz kam, um Gesicht als auch Stimme des ehemaligen US-PräsidentenRichard Nixonzu duplizieren. Die Macher ließen Nixon dadurch eine Rede über den Tod der Apollo-11-Astronauten vorlesen.[9]Diese war einst geschrieben, aber aufgrund der erfolgreichen Mondlandung nie öffentlich gehalten worden. Deepfakes werden genutzt, umPolitikeraufVideoportalenoderImageboardsfalsch darzustellen. So wurde zum Beispiel das Gesicht des argentinischen PräsidentenMauricio Macridurch das vonAdolf Hitleroder das vonAngela Merkeldurch das vonDonald Trumpersetzt.[10][11] Im März 2022 tauchte auf einer gehacktenukrainischenNachrichten-Website ein Fake-Video des ukrainischen PräsidentenWolodymyr Selenskyjauf, in dem er ukrainischeSoldatenaufrief, sich zu ergeben. Das Video wurde schnell als Fälschung entlarvt.Facebookklassifizierte das Video als Deepfake und entfernte es aus seinem Netzwerk. Ob das Video tatsächlich mit Deepfake-Technologie entstand, ist unbekannt.[12][13] Im Juni 2022 wurde in Videokonferenzen mehreren Politikern gegenüber vorgetäuscht, dass sie ein Gespräch mit demKiewerBürgermeisterVitali Klitschkoführen würden. Die Konferenzen wurden unter anderem mit der EU-InnenkommissarinYlva Johansson, derBerlinerBürgermeisterinFranziska Giffey, dem Bürgermeister vonMadridJosé Luis Martínez-Almeida, demBudapesterBürgermeisterGergely Karácsony, demWarschauerStadtpräsidentenRafał Trzaskowskiund demWienerBürgermeisterMichael Ludwiggeführt und durch die ersten beiden Bürgermeister vorzeitig abgebrochen.[14][15]Im Nachgang bekannte sich das als Kreml-nah geltende russische DuoVovan und Lexuszu den Telefonaten.[16]Sie widersprechen der Darstellung, dass Deepfake-Technologie eingesetzt wurde oder eine politische Motivation vorlag.[17]Stattdessen wird davon ausgegangen, dass einShallowfakegenutzt wurde.[18] Im Oktober 2022 wurde berichtet, dass im Rahmen desrussischen Überfallsauf dieUkraineein mittels Deepfake gefälschter Video-Anruf abgefangen worden sei. Die Anrufer hätten sich als der ukrainische PremierministerDenys Schmyhalausgegeben.[19] In der Kunst werden Deepfakes als Mittel der Zuspitzung verwendet, etwa um sich überpopkulturelleDinge lustig zu machen. Der Künstler Joseph Ayerle nutzte Deepfake für den KurzfilmUn’emozione per sempre 2.0, um eine Zeitreise der ProtagonistinOrnella Mutiaus dem Jahr 1978 in das Jahr 2018 zu inszenieren.[20][21]Dabei wurde der Charakter der Ornella Muti des Jahres 2018 durch eine KI errechnet, die aus den Gesichtsdaten der jungen Ornella Muti und Filmszenen des ModellsKendall Jennerneue Szenen errechnete, die die wirkliche Ornella Muti nie gespielt hat.[22]Im April 2018 veröffentlichte der US-amerikanische KabarettistJordan Peelegemeinsam mitBuzzFeedein Deepfake-Video, in dem erBarack Obamavor den Gefahren manipulierter Videos warnen lässt.[23]In diesem Video bezeichnet der „synthetische“ Ex-Präsident unter anderem seinen NachfolgerDonald Trumpals „kompletten Volltrottel“.[24]Ein YouTuber, der unter dem Namen „Speaking of AI“ auftritt, ließ wiederum den alsKylo RenausStar Warsbekannten Schauspieler das Intro zurOriginal-TV-Serie Star Treksprechen.[25] Seit 2018 wird universitär und industriell daran geforscht, generierte Gesichter zum Schutz der Identität einzusetzen.[26][27]Nach der natürlichen Entfremdung des Gesichtes können im öffentlichen Bereich aufgenommene Daten für Analysezwecke oder fürmaschinelles Lernenwie Originaldaten verwendet werden, jedoch unter größerem Schutz der Privatsphäre und Ausschlusses biometrischer Erkennungen.[28][29]Einsatzorte sind beispielsweise für die Entwicklung autonomer Fahrzeuge oder für Smart City Kameras.[30][31]Künstliche Gesichter oder andere synthetische Merkmale, stellen damit eine alternative Anonymisierungsform gegenüber demBlurringvonpersonenbezogen Datendar. InInformatik,MathematikundKunstsind Algorithmen für maschinelles Lernen, die sich mit der Generierung von solchen Mediendateien beschäftigen, einForschungs- undEntwicklungsgebiet. Deepfakes sorgten für ein großes mediales und gesellschaftliches Echo. Mehrere Journalisten führten daher Selbstversuche durch, um zu untersuchen, wie einfach oder schwierig Deepfakes zu erstellen sind[32]und wie sie die Glaubwürdigkeit von Videobildern beeinflussen könnten.[33]2020 stellteMicrosofteine Software mit dem NamenVideo Authenticatorvor, deren KI in der Lage sein soll, durch Deepfake und ähnliche Tools manipulierte Videos zu erkennen.[34] Im Herbst 2017 stellte ein anonymerReddit-Nutzer unter demPseudonym„Deepfakes“ mehrerePornovideosins Internet. So waren dieWonder-Woman-DarstellerinGal Gadotbeim Sex mit ihremStiefbruderund andere Schauspielerinnen wieEmma Watson,Katy Perry,Taylor SwiftoderScarlett Johanssonzu sehen. Diese Szenen waren unter Verwendung künstlicher Intelligenz erstellt worden und wurden rasch entlarvt. Der Reddit-Nutzer hatte Szenen aus Sexfilmen und Spielfilmen mit der eingefügten Person als Trainingsdaten für ein Neuronales Netz verwendet. Nach einem Bericht im Dezember 2017 im MagazinVicewurde das Thema von weiteren Medien aufgenommen.[1][35]In den folgenden Wochen überarbeiteten und verbesserten andere Reddit-Nutzer der Entwickler-Community das Vorgehen des Erstentwicklers, womit es zunehmend schwieriger wurde, gefälschte von echten Inhalten zu unterscheiden. Auf dem dazu auf Reddit gegründeten Subreddit „Deepfakes“ wurden bis zu dessen Sperrung am 7. Februar 2018 von mehr als 50.000 Nutzern Deepfake-Videos ausgetauscht.Rachepornoswurden durch den Einsatz von Deepfakes ohne Mitwirkung der im Porno erscheinenden Person möglich.[10][35][36][37][38]Auch die PornoseitePornhubgab an, Deepfake-Pornos zukünftig zu sperren.[39] Im Juni 2019 wurde das Programm DeepNude fürWindowsundLinuxveröffentlicht, welches neuronale Netzwerke benutzt, um Personen auf Bildern zu entkleiden. Dabei gab es eine kostenlose und eine kostenpflichtige Version für 50 USD.[40][41]Die Software wurde kurz danach von den Erstellern wieder entfernt und die Käufer erhielten eine Rückerstattung.[42] Mit der Open-Source-SoftwareDeepFaceLabkönnen Gesichter von Personen ausgetauscht werden.[43]Im April 2023 veröffentlichte das US-Start-Up ElevenLabs erstmals eine Version seiner Stimmen-Klon-Software mit deutschen Stimmen: Mittels KI können somit Stimmen mit geringem Aufwand täuschend echt geklont werden[44]Hinzugekommen per September 2023: KI-Software der FirmaHey Gen Labslässt gesprochenen Text in Videos übersetzen, samt Anpassung von Lippenbewegungen und Tausch der Gesichter.[45] Deepfakes von Stimmen können Betrügern kriminelle Handlungen ermöglichen, wenn sie Opfern gegenüber durch eine künstlich imitierte Stimme die Identität einer vertrauens- oder anweisungsberechtigten Person erwecken und das Opfer Handlungen zum Vorteil des Betrügers vornehmen lassen.[46]2019 wurde einCEOeines britischen Unternehmens durch einen Telefonanruf dazu gebracht, 220.000 Euro auf ein ungarisches Bankkonto zu überweisen, wobei die Täter durch ein Deepfake die Stimme eines Vorgesetzten des Opfers imitierten.[47] Neben der Täuschung von Personen durch Deepfakes lassen sich auchbiometrischeAuthentifizierungssysteme wie Voice-ID-Systeme, bei denen die Stimme einer Person als Passwort fungiert, mittels künstlich imitierter Stimme aushebeln. So erhalten Betrüger Zugang zu persönlichen Accounts anderer.[48] Deepfakes werden laut einer Erhebung von 2023 zunehmend in größerem Umfang im Zusammenhang mitEnkeltrick-Betrügereien genutzt.[44] Bilder lassen sich seit langem einfach verfälschen. Da dies bei Videos weitaus schwieriger ist, verlangen einige Unternehmen zur Verifikation von ihren Kunden Videos.[10]Mittels Deepfakes konntenHackerdieses Authentifizierungsverfahren überwinden. Der Professor Hao Li von der Universität von Southern California warnte, dass, wenn die Technologie von Deepfakes nicht der breiten Masse aufgezeigt werde, die missbräuchliche Anwendung von Deepfakes für Fake News oder Ähnliches eine ernste Gefahr darstellen könne.[49] In der zweiten Jahreshälfte des Jahres 2020 veröffentlichteMicrosoftdie Information, dass die Firma an einer Software arbeite, die Deepfake-Manipulationen erkennen soll.[50]Im Jahr 2022 wurde in den USA vor Gericht ein mutmaßlich authentische Videoaufnahme von Anwälten angezweifelt, weil sie ein Deepfake sein könnte. Die Anwälte von Elon Musk behaupteten, dass es sich bei dem belastenden Video wahrscheinlich um ein Deepfake-Video handle. Der Richter akzeptierte die Argumentation nicht.[51] Im April 2024 gibt es im Vorfeld derEuropawahlBeispiele für den Einsatz von KI zupropagandistischenZwecken.[52]Der Einfluss von manipulierten Fotos, Audioaufnahmen und Videos stelle nach Ansicht vonExpertenbesonders in Kombination mitsozialen Medieneine zunehmende Herausforderung für die Demokratie dar.[52]SogenannteEigenfaktencheckswerden empfohlen.[52] Als Abgrenzung zu Deepfakes sindShallowfakeoderCheapfakezu sehen. Hierbei handelt es sich zwar ebenfalls um Fälschungen, die jedoch technisch weniger anspruchsvoll sind und nicht mittels künstlicher Intelligenz generiert werden. Beispielsweise, indem Videos mit falschen Informationen gekennzeichnet werden oder in einem falschen Zusammenhang gezeigt werden.[53]Auch das Zusammenschneiden von Videos, damit ein anderer Eindruck entsteht, wird als Shallowfake bezeichnet.[18] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Entwicklung 2Anwendung 2.1Politik 2.2Kunst 2.3Datenschutz 2.4Forschung 2.5Pornografie 3Software 4Probleme 4.1Betrug 4.2Kreditwürdigkeit und Authentizität 4.3Wahlen 5Shallowfake 6Sonstiges 7Literatur 8Siehe auch 9Weblinks 10Einzelnachweise Afrikaans العربية Azərbaycanca Български Català کوردی Čeština Dansk English Esperanto Español Eesti Euskara"
  },
  {
    "label": 1,
    "text": "ELIZA-Effekt – Wikipedia ELIZA-Effekt Inhaltsverzeichnis Trivia Weblinks Einzelnachweise DerELIZA-Effekt(auchEliza-Effekt) bezeichnet das Verhalten einer Person, textbasierten Dialogsystemen (z. B.Chatbots) menschliche Eigenschaften wieGefühle,VerstehenundEmpathiezuzuschreiben. Der Effekt ist nach dem ComputerprogrammELIZAbenannt. Dessen Entwickler,Joseph Weizenbaum, hatte ursprünglich die Unmöglichkeit tiefgründigerKommunikation zwischen Mensch und Maschinezeigen wollen, beobachtete allerdings das Gegenteil: “I had not realized […] that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.”[1] DasStaatsschauspiel Dresdenzeigte im Jahr 2022 das StückEliza Effekt.[2] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Trivia 2Weblinks 3Einzelnachweise العربية Català English Español Français עברית 日本語 한국어 Polski Português Українська Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink"
  },
  {
    "label": 1,
    "text": "Embodiment – Wikipedia Embodiment Inhaltsverzeichnis Konzept Sechs Auffassungen über Embodiment Embodiment in Bildwissenschaft und Kunst Siehe auch Literatur Weblinks Einzelnachweise Embodiment(deutsch: Verkörperung,Inkarnationoder Verleiblichung) ist eine These aus einem neueren Teilbereich derKognitionswissenschaft, welcher sich mit der Erklärung vonBewusstseinbeschäftigt. Demnach benötigt die Entstehung von Bewusstsein einen Körper, setzt also eine physische Interaktion voraus. Diese Auffassung ist der klassischen Interpretation des Bewusstseins (insbesondere im Sinne desKognitivismusund computationaler Theorien) entgegengesetzt. Das Kognitionsverständnis desEmbodimententspricht etwa dem, was mittlerweile über den Vorgang derWahrnehmungbekannt ist: Die Wahrnehmung ist demnach kein Prozess der Abbildung sensorischer Stimuli auf ein inneres Modell der Welt, sondern einesensomotorischeKoordination, die sich immer im Gesamtkonzept eines handelnden Wesens ereignet. Sie wird von derKI-Forschung alsComplete agentbezeichnet. Allgemeiner wird Embodiment zunehmend in derPsychologie(besonders derSozialpsychologieundKlinischen Psychologie) verwendet, um die Wechselwirkung zwischen Körper und Psyche zu betonen. Es ist nicht nur so, dass sichpsychische Zuständeim Körper ausdrücken („nonverbal“ alsGestik,Mimik,Prosodie, Körperhaltung), es zeigen sich auch Wirkungen in umgekehrter Richtung: Körperzustände beeinflussen psychische Zustände. Beispielsweise haben Körperhaltungen, die aus irgendeinem Grund eingenommen werden, Auswirkungen auf Kognition (z. B. Urteile, Einstellungen) undEmotionalität.[1] Diese Thesen werden auch in derSoziologieund Sozialpsychologie sowie von einigen theoretischen Biologen bereits seit längerem vertreten. So wurde vonJakob Johann von Uexküllseit 1909 eine „Umweltlehre“ entwickelt, nach der Wahrnehmung auf einen Funktionskreis angewiesen ist, für den sowohl „Wirkorgane“ (der Bewegungsapparat) als auch Sinnes- bzw. „Merkorgane“ konstitutiv sind (vgl. auchEigenbewegung).[2]Als Grundlage können daneben die Theorien vonGeorge Herbert Meadund aus der LeibphänomenologieMaurice Merleau-Ponty,Hermann Schmitzund SozialphänomenologieAlfred Schützgenannt werden. In neueren Diskussionen wurde diese Erkenntnis durch soziologischePraxistheorien(vgl.Pierre BourdieuundAnthony Giddens) bzw. Theorien im Zuge des sozialtheoretischenpractice turnwiederentdeckt.[3] Ein Verbindungsstück zwischen den Konzepten findet sich in derAktivitätstheoriederkulturhistorischen Schuleder sowjetischen Psychologie, die vonWygotskiinspiriert und vonLeontjewbegründet wurde und in Deutschland v. a. durchKlaus HolzkampsKritische Psychologiebekannt wurde. Auch die interpretative Videoanalyse in den Workplace Studies, die den theoretischen Ansatz derEthnomethodologievertreten, hat sich bereits in den 1980ern intensiv mit dem Konzept des Embodiment auseinandergesetzt. Im Bereich der Psychotherapie und der Körpertherapien hatHilarion G. Petzoldmit der von ihm begründeten Integrativen Bewegungs- und Leibtherapie[4]einen konsequenten Embodiment-Ansatz vertreten, der den Menschen als Leibsubjekt eingebettet in der Lebenswelt (embodied and embedded) sieht.[5]Im Hintergrund stehen Ideen vonMaurice Merleau-Ponty,Alexander LurijaundLew Wygotski. Die Aufnahme und Interiorisierung von Information aus der ökologischen und sozialen Welt durch das „totale Sinnesorgan des Leibes“ macht den Menschen zum „informierten Leib“[6], der Weltverhältnisse verkörpert. Kommt es zu negativen und belastenden Verkörperungen, die imLeibgedächtnisgespeichert werden, können psychische und psychosomatische Störungen die Folge sein. Sie erfordern in der Therapie korrektive Embodiments durch neue, heilsame Leiberfahrungen,[7]ein Ansatz, der durch moderne Interozeptionsforschung gut gestützt wird.[8][9] Margaret Wilson hat sechs Blickwinkel auf das Embodiment formuliert:[10] 1. Kognition ist situiert / verortet „Kognitive Aktivität erfolgt im Kontext einer realen Umgebung und beinhaltet Wahrnehmung und Handlung.“ Als situierte Kognition versteht man Kognitionen, die im Kontext von aufgabenrelevanten Inputs und Outputs stattfinden. So werden z. B., während ein kognitiver Prozess ausgeführt wird, weitere Wahrnehmungsinformationen aufgenommen, die wiederum die Verarbeitung beeinflussen. Außerdem werden motorische Aktivitäten ausgeführt, welche die Umgebung im Hinblick auf die relevante Aufgabe beeinflussen. Ein Beispiel für eine kognitive Aktivität, die situiert ist, ist das Autofahren, bei dem das kognitive System der wahrnehmenden Person ständig neuen Input aus der Umgebung während des Fahrens aufnimmt. 2. Kognition steht unter Zeitdruck Dadurch, dass situierte Kognitionen in Echtzeit ablaufen, stehen diese unter Zeitdruck. Eine Metapher, die diesen Umstand verdeutlicht, ist der so genannte „repräsentationale Flaschenhals“. In Situationen, in denen schnelle und sich kontinuierlich entwickelnde Antworten erforderlich sind, besteht eventuell nicht genügend Zeit, um ein vollständiges mentales Modell der Umwelt zu konstruieren, aus dem Handlungen für die Handlungsausführung abgeleitet werden können. Daher sind effiziente Mechanismen erforderlich, um auch unter Zeitdruck situationsangemessene Handlungen hervorbringen zu können. Ein Argumentationsstandpunkt ist, dass Menschen so „gebaut sind“, dass sie diesen „repräsentationalen Flaschenhals“ umgehen können und auch in Situationen unter Zeitdruck dazu fähig sind, gut zu funktionieren. 3. Wir laden kognitive Arbeit auf die Umgebung ab Aufgrund der Beschränkungen des menschlichen Informationsverarbeitungssystems (Beschränkungen der Aufmerksamkeit und des Arbeitsgedächtnisses) ist es sinnvoll, die kognitive Belastung in bestimmten Situationen durch verschiedene Strategien zu reduzieren. Bei neuen Aufgaben lässt sich die kognitive Belastung reduzieren, indem die Umgebung strategisch genutzt wird. So können Informationen in der Umgebung, z. B. in Form von Kalendern oder Computerdateien, hinterlegt werden, auf die bei Bedarf zugegriffen werden kann. Dadurch fällt die vollständige Enkodierung dieser Informationen weg. 4. Die Umgebung ist Teil des kognitiven Systems Einige Autoren vertreten auf der Basis der Erkenntnis, dass der Körper und die Umgebung eine Rolle bei kognitiven Aktivitäten spielen, eine noch stärkere Behauptung. Sie gehen davon aus, dass Kognition nicht allein eine Aktivität des Geistes ist, sondern über die gesamte Situation verteilt ist, also sowohl den Geist wie auch den Körper und die natürliche und kulturelle Umgebung sowie andere Menschen beinhaltet. Das bedeutet, dass die kognitive Aktivität eines Individuums nicht nur aus dessen Kopf kommt, sondern auch durch die soziokulturelle Umgebungssituation, in der sich die Person befindet. Somit ist die kognitive Aktivität stets davon abhängig, in welcher Situation wir uns befinden. Daraus ergibt sich die Schlussfolgerung, dass die Situation und die wahrnehmende Person zusammen als einheitliches System zu untersuchen sind (distributed cognition). 5. Kognition dient der Handlung Kognitive Mechanismen werden hinsichtlich ihrer Funktionen und ihres Zwecks betrachtet, die/den sie erfüllen. Im Falle der visuellen Wahrnehmung ist die traditionelle Annahme, dass der Zweck des visuellen Systems darin besteht, eine interne Repräsentation der wahrgenommenen Welt aufzubauen. Dabei wird zwischen dem ventralen visuellen Pfad („Was“) und dem dorsalen visuellen Pfad („Wo“) unterschieden. Diese beiden Pfade generieren die Repräsentationen der Objektstruktur und der räumlichen Beziehungen von Objekten. Die Funktion liegt in visuell gestützten Handlungen wie Erreichen und Zupacken. In Einklang mit dieser Sichtweise wurde in einer Untersuchung von Craighero u. a.(1997)[11]herausgefunden, dass bestimmte Arten von visuellem Input motorische Aktivitätprimenkann. So erleichterte das Sehen eines Rechtecks mit einer bestimmten Orientierung eine nachfolgende motorische Greifaufgabe, wenn das zu greifende Objekt dieselbe Ausrichtung des Rechtecks besaß. 6. Kognition ist körperbasiert Abstrakte kognitive Prozesse basieren häufig auf der Simulation von sensomotorischen Prozessen. Als Beispiele dafür kann das Zählen an Fingern herangezogen werden. Ein Kind lernt zu zählen, indem es eine Anzahl an Dingen mit der Bewegung von der gleichen Anzahl an Fingern repräsentiert. Mit Übung kann das Zählen auf ein Fingerzucken reduziert werden und anschließend völlig ohne Bewegung durchgeführt werden. Allerdings gehen verkörperte Theorien davon aus, dass die motorischen Programme des Fingerbewegens für die Repräsentation von Zahlen erhalten bleiben und so das menschliche Verständnis von Zahlen an Erfahrungen geknüpft ist. Andere Beispiele, die von einer solchen Simulation von Erfahrungen bei abstrakten Denkprozessen ausgehen, sind die mentale und bildliche Vorstellungskraft, das episodische Gedächtnis, die Findung von Problemlösestrategien, die Sprache und das emphatische Verstehen von dem psychischen Zustand einer anderen Person. Das ForschungsprojektBildakt und VerkörperungderHumboldt-Universität zu Berlinschlug eine Brücke zwischen philosophischer Embodiment-Theorie und derBildwissenschaft. Eine zentrale These des Projekts ist, „dass der gesamte Körper wahrnimmt“.[12]Das Forschungsprojekt untersucht u. a. Verkörperungspraktiken und -theorien in Ästhetik und Kunst; beispielsweise befasste sich der KünstlerStephan von Huenemit Embodiment-Theorien in seinenKlangkunstwerken.[13] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Konzept 2Sechs Auffassungen über Embodiment 3Embodiment in Bildwissenschaft und Kunst 4Siehe auch 5Literatur 6Weblinks 7Einzelnachweise العربية Čeština English فارسی Français Italiano 日本語 한국어 Norsk bokmål Polski Português Русский Српски / srpski Türkçe Українська Oʻzbekcha / ўзбекча Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen"
  },
  {
    "label": 1,
    "text": "Emotionserkennung – Wikipedia Emotionserkennung Inhaltsverzeichnis Wissenschaftliche Definition von „Emotion“ Emotionserkennung in der Humanethologie Cross-Race-Effekt Visuelle Mimikerkennung Emotionsinduktion Lügenerkennung Rechtliche Grenzen technischer Emotionserkennung Siehe auch Weblinks Einzelnachweise Emotionserkennungist der Prozess derAbstraktionundKlassifikationvonaudio-visuellenSignalenund ihre Entschlüsselung alsZeichenfürEmotionenandererMenschen. Sie findet vor allem in derEmotionspsychologie, derPsychotherapie, derHumanethologieund derRobotikAnwendung. In der Robotik unterscheidet man zwischenvisueller Emotionserkennungundakustischer Emotionserkennung. Im Gegensatz zur Humanethologie und der Psychologie hat die Robotik ein sehr einfaches Bild menschlicher Emotionen. So werden hier der Ausdruck und die Darstellung von Emotionen häufig mit dem Empfinden von Emotionen gleichgesetzt. Folgendes Beispiel soll dies verdeutlichen. Sobald eine Person ihre Mundwinkel nach oben zieht oder z. B. den Satz „ich bin fröhlich“ spricht, deutet das die Robotik als Freude. Die Psychologie hingegen würde hier zunächst nur den Ausdruck eines fröhlichen Satzes sehen – aber noch lange keine echteFreude. Eine Emotion muss vom Begriff des Gefühls, der Stimmung und der Persönlichkeit abgegrenzt werden. Ein Gefühl ist z. B. einSchreck, den man empfindet, wenn auf einmal hinter einer Mauer ein maskierter Mensch auftaucht. Dann empfindet manFurcht. Ein Gefühl wird erst dann zur Emotion, wenn diese körperliche Veränderung kognitiv bewertet wird. Wenn jemand z. B. sein Herzklopfen auf den maskierten Menschen zurückführt, würde man von Furcht sprechen. Führt er es jedoch auf seine heimlich Angebetete zurück, würde man von Freude sprechen. Emotionen dauern meist nur ein paar Sekunden und haben ein klar abgrenzbares Einsetzen (on-set) und Ende (off-set).Stimmungenhingegen können über Stunden, Tage bis auch Wochen anhalten. Wenn jemand sagt, er sei heute schlecht gelaunt, dann ist er in einer schlechten Stimmung. Dies hat jedoch nicht unbedingt etwas mit Emotionen zu tun. Häufig kann eine bestimmte Stimmung die Auftretenswahrscheinlichkeit einer bestimmten Emotion erhöhen oder erniedrigen, aber diese beiden Dinge muss man analytisch dennoch voneinander trennen. Als letztes muss diePersönlichkeiteiner Person von der Stimmung abgegrenzt werden. EincholerischerMensch etwa ist dauerhaft negativ übererregt. Auf diese Weise kann man sich die Begriffe Gefühl, Emotion, Stimmung und Persönlichkeit auf einer Zeitachse angeordnet vorstellen – mit Gefühl auf der einen, kurzfristigen, und Persönlichkeit auf der anderen, langfristigen Seite. DieHumanethologieals Teilgebiet der Verhaltensbiologie versucht, anhand vieler Komponenten des Menschen Emotionen erkennen zu können. Dabei achtet sie vor allem auf Komponenten des menschlichen Verhaltens, die einen hohen Informationsgehalt besitzen. Dazu zählen neben der Ganzkörperbewegung, die Summe der Bewegungsänderungen, die Gestik, Änderungen der Stimmfrequenz und bestimmte Teile der Mimik. Berühmtester Vertreter der Humanethologie in Europa ist das Institut für Humanethologie in Wien unter Leitung von Karl Grammer. Die Emotionserkennung zwischen zwei Menschen ist starken Schwankungen unterlegen. In der Psychologie wurde ein Phänomen entdeckt, welchesCross-Race-Effektgenannt wird. Dieses Phänomen besagt, dass die Emotionserkennungsrate niedriger ist, wenn die zu erkennende Emotion zu einem Gesicht gehört, was nicht zur selben Kultur oder Ethnie wie die des Beobachters gehört. Dieser Effekt kann jedoch durch eine Form des Trainings überkommen werden.[1] Dieser Teil wird üblicherweise alsMimikerkennungbezeichnet. Als Schnittstelle zwischen Mensch und Maschine wird eine digitale Videokamera oder ein äquivalentes optisches Eingabegerät verwendet. Hierbei werden die Methoden derGesichtserkennungangewandt, um die Ausprägungen derGesichtsoberflächezu analysieren. DurchAutomatische Klassifizierungist es möglich, die Mimik der seriellen Einzelbilder einem Cluster zuzuordnen, welches möglicherweise einer Emotion zugeordnet werden könnte. Untersuchungen haben jedoch ergeben, dass nur 30 % der mimisch ausgedrückten Emotionen auch den wirklich empfundenen Emotionen entsprechen. Deswegen sollte man die visuelle Mimikerkennung nicht mit visueller Emotionserkennung gleichsetzen. Biologischer Hintergrund der visuellen Emotionserkennung ist die Simulation eines menschlichenSehnervsin einemRoboter. Für experimentelle Settings im Bereich Emotionspsychologie, Verhaltensethologie, Neuropsychologie und vielen anderen Wissenschaften ist es häufig wichtig, gezielt Emotionen unterLaborbedingungenzu „erzeugen“. Die Emotionsinduktion ist eines der schwierigsten Gebiete der Emotionsforschung. Aufgrund mehrerer Meta-Analysen über dieses Thema extrahierte man mehrere Methoden, mit denen man am validesten Emotionen induzieren kann. An erster Stelle steht die Erfassung der Emotion in der Realität (Stichwort Feldforschung). Aufgrund geringer internerValiditätenwird davon aber häufig Abstand genommen. Die zweite Methode, die eine hohe interne mit einer hohen externen Validität verbindet, ist die Methode des emotional-recalls, in dem versucht wird, Erinnerungen aus dem Emotionsgedächtnis hervorzurufen. Abzuraten für Experimente außerhalb der EEG-Emotionsforschung ist von Induktionsverfahren wie dem IAPS oder Induktionsverfahren, die angeblich emotionsinduzierende Filmsequenzen oder Musikstücke anwenden. All diese Verfahren bleiben einen Nachweis der spezifischen Wirksamkeit schuldig. Die Robotik bedient sich häufig idealisierter Experimentalabläufe, z. B.: Nach dem Abschluss der Lernphase sollte die KI selbständig in der Lage sein, Emotionen zu erkennen, ohne dass sie dies zuvor von einem Menschen gelehrt wurde. Da häufig aber weder die Induktionsmethode auf ihre Wirksamkeit hin untersucht wird, noch während des Experiments selbst die induzierten Emotionen evaluiert werden, bleiben diese idealisierten Experimentalabläufe in der Robotik häufig fehlerhaft und unvollständig. Anfang 2024 haben Wissenschaftler der Nanyang Technical University in Singapur eine Gesichtsmaske entwickelt, die Emotionen ihres Trägers in Echtzeit ermitteln kann.[2]Dabei wurde für diese Technik eine derzeitige emotionale Erkennungsgenauigkeit von 93,3 Prozent angegeben. Multisensorische Emotionswahrnehmung ist hilfreich bei dem Einschätzen des Wahrheitsgehalts von Äußerungen, genauer bei dem Erkennen von Lügen, wobei Lügen als bewusst falsche, auf Täuschung angelegte Aussagen zu verstehen sind. Zwar lässt sich kein allgemeingültiger Indikator für das sichere Feststellen von Lügen ausmachen, dennoch können Mimik, Gestik, Sprache und Körperhaltung Hinweise liefern. Relativ verlässlich sind unbewusste bzw. nicht steuerbare Signale, wie zum Beispiel Pupillenweite, Blickrichtung oder Erröten. Des Weiteren sollte die Aufmerksamkeit verstärkt auf Unstimmigkeiten zwischen den verschiedenen verbalen und nonverbalen Ausdrucksformen einer Person gerichtet werden.[3][4][5] Unter rechtlichen Aspekten ist technische Emotionserkennung, ähnlich wietechnische Gesichtserkennung, alsbiometrisches Erkennungsverfahrenein heikles Gebiet. Das unterstreicht dieEuropäische KI-Verordnung (AI Act), die am 1. August 2024 in Kraft getreten ist[6]und Emotionserkennung bis auf einige Ausnahmen ganz verbietet. Es beinhaltet also u. a. ein deutliches Verbot von technischer Emotionserkennung an Arbeitsplätzen, in Bildungseinrichtungen, für Privatpersonen etc. innerhalb derEU.[7] Neben der visuellen Emotionserkennung ist es auch seit längerem möglich, Emotionen mittelsKünstlicher Intelligenzaus der Stimme herauszuhören, die Stimme kann also sehr viel Aufschluss über Emotionen und sogar Krankheiten geben. Auch diese Systeme sind nicht ohne Weiteres erlaubt, allerdings werden sie z. B. etwa inCallcenterngenutzt[8]. Auf jeden Fall muss jeder Kunde darüber dann zu Gesprächsbeginn deutlich informiert werden und gemäß dem AI Act bestehen hier zudem Transparenz- und Dokumentationspflichten. Mit diesem Hintergrund ist Googles Initiative vom Dezember 2024 des direkt auch in der EU frei im Internet verfügbaren KI-Modells \"PaliGemma 2\"[9]unerwartet und befremdlich. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Wissenschaftliche Definition von „Emotion“ 2Emotionserkennung in der Humanethologie 3Cross-Race-Effekt 4Visuelle Mimikerkennung 5Emotionsinduktion 6Lügenerkennung 7Rechtliche Grenzen technischer Emotionserkennung 8Siehe auch 9Weblinks 10Einzelnachweise العربية English فارسی Français 日本語 Українська Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen"
  },
  {
    "label": 1,
    "text": "Enquete-Kommission Künstliche Intelligenz – Wikipedia Enquete-Kommission Künstliche Intelligenz Inhaltsverzeichnis Mitglieder Arbeitsweise Zwischenergebnisse Rezeption Weblinks Einzelnachweise Bundestag Sachverständige Arbeitsgruppen Sitzungsablauf und Partizipation Öffentliche Aussprache Projektgruppe „KI und Wirtschaft“ Projektgruppe „KI und Staat“ Projektgruppe „KI und Gesundheit“ Projektgruppe „KI und Arbeit“ Projektgruppe „KI und Mobilität“ Projektgruppe „KI und Medien“ Presse und Zivilgesellschaft Beteiligte Wissenschaften CDU/CSU SPD AfD FDP Die Linke Bündnis 90/Die Grünen CDU/CSU SPD AfD FDP Die Linke Bündnis 90/Die Grünen DieEnquete-Kommission „Künstliche Intelligenz – Gesellschaftliche Verantwortung und wirtschaftliche, soziale und ökologische Potenziale“desDeutschen Bundestagsunter Vorsitz vonDaniela Kolbe(SPD) befasst sich mit den Auswirkungen des zunehmenden EinsatzesKünstlicher Intelligenz(KI). Die Einsetzung der Kommission wurde auf gemeinsamen Antrag[1]der Fraktionen von CDU/CSU, SPD, FDP und Die Linke mit Zustimmung von AfD und Bündnis 90/Die Grünen Ende Juni 2018 beschlossen. Die konstituierende Sitzung derEnquete-Kommissionfand am 27. September 2018 statt.[2]Ein Abschlussbericht wurde am 28. Oktober 2020 anWolfgang Schäubleübergeben und in einer Kurzfassung veröffentlicht.[3] - Johannes Sven Hänig (25) Nicht aufgeführt sind stellvertretende Mitglieder, die mitunter gleichwertigen Anteil an den Projektberichten trugen. Die Mitglieder arbeiten in mehreren Projektgruppen.[4]Diese umfassen unter anderem: Daneben sollte eineTask Force„Forschung“ insbesondere Ausgaben undStand der Technik und Wissenschaftzusammentragen.[5] Entgegen der Maßnahmen früherer Kommissionen wie der Enquete „Internet und digitale Gesellschaft“[6]fand eingangs keinPartizipationswerkzeugAnwendung, was zu Kritik beteiligter Oppositionspolitikerinnen führte.[7][8]Der Konflikt setzte sich öffentlich fort, nachdem die Kommission mehrheitlich für eine allenfalls auszugsweise Vor-Veröffentlichung von Teilberichten gestimmt hatte,[9][10][11][12][13]selbige erfolgte am 19. Dezember 2019. Vom 10. März bis zum 5. April 2020 konnten Einzelpersonen Anmerkungen, Vorschläge und Fragen zu zwölf vorgegebenen Leitfragen einreichen. Für die Nutzung des aufAdhocracy+basierenden PortalsSpeakUpist dieRegistrierungauf der entsprechenden Website erforderlich.[14] Die Sitzungen fanden teilöffentlich statt: So sind die meisten Vorträge der Sachverständigen und Gäste öffentlich und aufgezeichnet; Diskussionen und Arbeitsgruppe fanden unter Ausschluss der Öffentlichkeit statt. Die Diskussion der Kommissionsarbeit in densozialen Medienfand unter anderem über dieSchlagworte„#EnqueteKI“, „#ekki“ und „#KIEnquete“ statt. Der Bundestag diskutierte die „Zwischenbilanz“ der Enquete-Kommission am 20. Dezember 2019.[15][16][17]Die Abschlusspräsentation der Arbeitsergebnisse erfolgte am 28. September 2020;[18]der vollständige Bericht soll im Oktober veröffentlicht und im Plenum debattiert werden. Er soll etwa 600 Seiten umfassen. Während die Abgeordneten der CDU/CSU und FDP die Notwendig einer schnellen Umsetzung betonten, sprachen sich Die Linke und Bündnis 90/Die Grünen für mehrTransparenz, Partizipation und eine konkrete Orientierung der Bundesregierung an den Arbeitsergebnissen aus. Die SPD vermisste rückblickend ein inhaltliche Fokussierung auf weniger Themen; die AfD forderte eine Ergänzung von Kennzahlen und entsprechenden Controllings.[19] Der Schlussbericht ist am 28. Oktober 2020 alsDrucksache19/23700 erschienen[20][21]und wurde am 5. November 2020 im Bundestag debattiert.[22] Die Berichte der Projektgruppen wurden nach demMehrheitsprinzipbeschlossen.[23]In den vorläufigen Zusammenfassungen der Projektgruppen kommen die Kommissionsmitglieder zu folgenden Empfehlungen: Die Projektgruppe empfiehlt die Akzeptanzsteigerung durch eine Aufklärungskampagne zur Vermittlung von Kenntnisse und Best-Practices und die strategische Ausrichtung an Prinzipien derDeutschen Nachhaltigkeitsstrategieund dies in Fördervorhaben zu berücksichtigen. Ferner sollen mittelständische Unternehmen durch Beratungs- und Schulungsangebote bei der Qualifizierung und Anwendung unterstützt werden.Experimentierräumesollen neue Regulierungsoptionen erörtern. Der Transfer zwischenGrundlagen-undAnwendungsforschungsoll subventioniert werden. Der Staat soll die Prozesse durch eigene Vorhaben in der Verwaltung befördern; ein deutschlandweiter Standardvertrag soll bei der Rechte- und Patentverwertung unterstützen. Start-ups sollen bei der Berücksichtigung derDatenschutzgrundverordnung(DSGVO) unterstützt werden. Weitere Empfehlungen sollen im Volltext erscheinen; bestehenderDissensgeht aus der Zusammenfassung hervor.[24] Die Projektgruppe empfiehlt, die Einsatzgebiete künstlicher Intelligenz und entsprechende Kompetenzen in öffentlichen Einrichtungen durch zentrales Monitoring und Erfahrungsaustausch, sowie routinemäßigen Prüfungen von Einsatzmöglichkeiten und der Verankerung vonPartizipationsansätzenim jeweiligen Bereich zu systematisieren. Entsprechende Inhalte sollen in Verwaltungsschulung und -ausbildung berücksichtigt werden. WeiterePilotprojektesollen folgen, dies gelte vor allem Gebiete derTeilhabe; Transparenz undNachvollziehbarkeitsollen relevante Zielparameter sein. Regelmäßige Prüfungen sollen die Diskriminierungsfreiheit sicherstellen. Ein durch Widerspruch geltendes Recht auf menschliche Bearbeitung soll gelten. Erhöhte Investitionen inSicherheitstechnologienwird empfohlen; dabei soll eine Einteilung inRisikoklassen(bspw. nach Datensensibilität und Mächtigkeit der Software) erfolgen. EinMappingsoll Angriffsflächen von KI-Systemen identifizieren, um weitere Empfehlungen zurIT-Sicherheitabzuleiten. Weitere Empfehlungen sollen im Volltext erscheinen; bestehenderDissensgeht aus der Zusammenfassung hervor.[25] Die Projektgruppe empfiehlt, die Investitionsrate für Informationstechnik im Gesundheitsbereich langfristig auf vier Prozent zu erhöhen und Finanzierungslücken durch Bund und Länder kurzfristig zu schließen. Die Freigabe von Patientendaten zu Forschungszwecken soll freiwillig, individuell abstufbar und widerrufbar sein. Diese sollen dezentral anonymisiert in ein neu aufzubauende Versorgungseinrichtung oder eines entsprechenden Verbundes überführt und für Forschung zur Verfügung gestellt werden. EineInteroperabilitätsstrategiesoll erarbeitet werden, die zeitnah gültig werden soll. Eine Bund-Länder-Arbeitsgruppe soll die jeweiligen Datenschutzregelungen schnellstmöglich auf DSGVO-Basisharmonisieren. Entsprechendes soll auch für Stakeholder in der Gesundheits- und Pflegeausbildung durch die Entwicklung einer gemeinsamenRoadmapgeschehen, wofür dieKultusministerkonferenzexemplarisch als Schirmherr genannt wird. Gemeinsam sollen auch umfassende Weiterbildungskonzepte mit hoherZugänglichkeiterarbeitet werden. Beim Ausbau vonÖkosystemen, desTechnologie-und Datentransfers soll die Datenqualität berücksichtigt werden. Bei der Zulassung digitaler Medizinprodukte soll dasBundesministerium für GesundheitbeimBundesinstitut für ArzneimittelBeratungsoptionen schaffen; selbiges soll für digitale Angebote und ihre Anbieter beimGemeinsamen Bundesausschusserfolgen. Der Zugang zu Förderung fürkleine und mittelständische Unternehmenund Start-ups soll vereinfacht werden. DieBundesregierungsoll auf europäischer und nationaler Ebene für eine Weiterentwicklung des Zulassungsrechts hinwirken undHaftungsrisikendurch die Erarbeitung vonZertifizierungsvorgabenmit demDeutschen Institut für Normungund anderen Vertretungen minimieren. Die Bedarfsermittlung in derPflegerobotiksoll durchko-kreativeProzesse intensiviert werden. Anwendungsfälle sollen auf ihre Effekte auf Pflegekräfte und Behandelte sowie mögliche finanzielleExklusivitätder Leistungen überprüft werden.[26] Die Projektgruppe spricht sich für die berufsgruppenübergreifende Breitenbildung der Gesamtbevölkerung zu KI aus. Beschäftigte und ihre Interessenvertretungen sollen Mitspracherechte bei der Ausgestaltung undEvaluierungvon betrieblich eingesetzten KI-Systemen und aufgrund möglicher Beschäftigungsrisiken durchAutomatisierungInitiativrechtebei Weiterbildungsmaßnahmen erhalten. Dabei werde besonders Wert auf die Informationsqualität bestehender Organe der Bundesregierung und derTechnikfolgenabschätzunggelegt undTechnologieberatungsstellenzum Kompetenzaufbau empfohlen. Übergeordneten Prinzipien und Standards wiePrivacy-by-Designsollten in der KI-Anwendungsentwicklung gefördert werden. Soweit Arbeitsplätzen wegfallen solle die innerbetrieblicheVersetzungder Arbeitnehmenden in unterbesetzte Organisationsteile erwogen werden. Ziel sollen dieQualitätssteigerungund dieNutzensteigerungverschiedenerStakeholdersein. KI in derBildungsoll die ErreichungpädagogischerZiele, z. B.InklusionundGleichstellung, unterstützen können.InformatiksollPflichtfachan Schulen sein und dermathematische Unterrichteng an denAnforderungenderHochschulenausgerichtet sein. Querschnittsthemen zu KI wiekritisches Denken,philosophischenHintergründen und sozialen Folgen sollen sich im gesamten Fächerkanon niederschlagen. In derErwachsenenbildungsollte neben Betrieben auchVolkshochschulen,Handelskammernund private Anbieter entsprechende Angebote bereitstellen und arbeitgeberfinanzierte Weiterbildung mit festen Qualifikationszielen und für angemessene berufliche Flexibilität ermöglichen. Die Projektgruppe empfiehlt hierzu den Aufbau einesOnline-Portals, dass mehrere der Empfehlungen verbindet. IngenieurwissenschaftlicheForschungsollte durch Arbeitswissenschaften begleitet werden und auch normative Fragestellungen einbeziehen. Staatliche Förderungen sollen zu inter- und transdisziplinären Konsortien und Forschungsfragen anregen. Eine finanzielle Aufstockung entsprechender, langfristig angesetzter Förderprogramme soll angestrebt werden.[27] Die Projektgruppe spricht sich für die vermehrte Einrichtungen vonReallaborenund deren europäische Vernetzung aus.Ex-Post-Entscheidungensollen durch rechtliche Rahmensetzung vermieden werden. Durch die Förderung vonDezentralisierungsollMonopolbildungvermieden werden. Deutschland soll treibende Kraft einer Europäisierung derStandardisierungund Datennutzung sein.Rebound-Effektedes KI-Einsatzes sollen vermieden werden. Planungsziel von Pilotprojekten desöffentlichen Personennahverkehrssoll auch die Überführung in den Regelbetrieb sein. Die hochschulische Forschungsförderung soll künftig die einfachere Einbindung vonkleinen und mittleren UnternehmensowieBehördenermöglichen. DieDeutsche Bahnwird zur Erarbeitung vonMeilensteinenfür den KI-Einsatz aufgefordert. Die Vereinheitlichung deseuropäischen Luftraumssoll beschleunigt werden. Die Potentiale fürLogistikkettenderSchifffahrtsollen ausgeschöpft werden.[28] Die Projektgruppe empfiehlt die Prüfung von Speicher-, Dokumentations- und Nutzungspflichten maschinenlesbarer Daten und offenerSchnittstellenzwischenJournalismus,Wissenschaftund Marktaufsichtsbehörden, sowie eine Reform desMedienstaatsvertrageszur KI-gestützten Selektion und Präsentation von Online-Medieninhalten; letztere ist bereits erfolgt soll im Herbst 2020 in Kraft treten. DieLandesmedienanstaltensollen reformiert werden. Eine neu zu schaffende, unabhängige Einrichtung soll Medieninhalte auf Fälschungen prüfen; im Zuge dessen soll die Forschung zuDeepfakesund deren Identifikation erhöht werden. Automatisch generierte Texte sollen einheitlichgekennzeichnetwerden. Ein Verbesserung automatischer Filter zur Erkennung von sprachlich-semantischen Sonderfällen wieSatireoderHumorsoll angestrebt werden. Der Berichtteil schließt mit einerSWOT-Analysefür den Sektor.[29] Tagesspiegel Backgroundwertete die Ergebnispräsentation im September 2020 als enttäuschend, da es zu wenig konkrete Handlungsempfehlungen gäbe.[30]Heise onlinegriff die Frage der Partizipation erneut auf und bekräftigte die Kritik der grünen und linken Bundestagsfraktionen: Hier gäbe es noch „Luft nach oben“. Wie produktiv die Kommission gewesen sei, „wird sich allerdings erst zeigen müssen“, da der Konsens der Kommission zurÄchtungautonomer Waffensystemesich imPlenumdes Bundestages nicht reproduzieren ließ.[31] In einer Vorabberichterstattung über den verabschiedeten Abschlussbericht hebtDie Tageszeitung(taz) denDissenszur Einrichtung einer Prüfbehörde und einer standardisierten Einteilung inRisikoklassenvon algorithmischen Entscheidungssystemen hervor. Für derartige Ansätze hätten sich letztlich nur die links-grünen Enquete-Delegationen ausgesprochen.[32]Mehrfach im Bericht wird lautheise onlinedie Brücke zurDatenethikkommissionund verwandten Vorhaben wieGaia-Xgeschlagen.[33] FürMarkus BeckedahlvonNetzpolitik.orgliegt der Nutzen der Enquete in derWeiterbildungder beteiligten Politiker.[34] Der Sachverständige Florian Butollo desWeizenbaum-Institutsschloss sich der Enthaltung der ihn einladenden linken Delegation zum Beschluss des Abschlussberichtes an und begründete dies mit demreaktivenWesen des Berichts. In den wesentlichen BereichenKlimaschutz, Monopolbildung undsozialer Spaltungzu unverbindlich und eine „verpasste Chance“, so Butollo aufTwitter.[35]Aljoscha Burchardt und Anna Christmann kritisierten den Austausch mit derExekutivenimDeutschlandfunkals mangelhaft.[36] Christian Vater(KIT) und Eckhard Geitz (Universität Freiburg) kommen in einer Nachbetrachtung zu dem Schluss, dass die Ergebnisse „weniger konkrete Handlungsempfehlungen“ an politische Entscheidungstragende geben als frühere Enquete-Kommissionen. Die Arbeiten zu künstlicher Intelligenz seien „[...] mit dem aktuellen Bericht nicht zu einem befriedigenden Ende gekommen“.[37] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Mitglieder 1.1Bundestag 1.1.1CDU/CSU 1.1.2SPD 1.1.3AfD 1.1.4FDP 1.1.5Die Linke 1.1.6Bündnis 90/Die Grünen 1.2Sachverständige 1.2.1CDU/CSU 1.2.2SPD 1.2.3AfD 1.2.4FDP 1.2.5Die Linke 1.2.6Bündnis 90/Die Grünen 2Arbeitsweise 2.1Arbeitsgruppen 2.2Sitzungsablauf und Partizipation 2.3Öffentliche Aussprache 3Zwischenergebnisse 3.1Projektgruppe „KI und Wirtschaft“ 3.2Projektgruppe „KI und Staat“ 3.3Projektgruppe „KI und Gesundheit“ 3.4Projektgruppe „KI und Arbeit“ 3.5Projektgruppe „KI und Mobilität“ 3.6Projektgruppe „KI und Medien“ 4Rezeption 4.1Presse und Zivilgesellschaft 4.2Beteiligte 4.3Wissenschaften 5Weblinks"
  },
  {
    "label": 1,
    "text": "Erica (Androidin) – Wikipedia Erica (Androidin) Inhaltsverzeichnis Weblinks Einzelnachweise Ericaist eineAndroidin, ein menschenähnlicherRoboter. Erica wurde 2015 von Kohei Ogawa undHiroshi Ishiguroan derUniversität Osakaentwickelt, um die Interaktion zwischen Menschen und Robotern zu untersuchen.[1][2] Erica hat das menschliche Aussehen einer jungen japanischen Frau von etwa 1,66 Meter Körpergröße. Sie versteht Sprache, kann sprechen und Unterhaltungen führen. Sie blinzelt und bewegt Mund, Augen und Kopf wie ein menschlicher Gesprächspartner.[1] Erica wird in Japan als Fernsehansagerin eingesetzt. 2019 begannen in Japan die Arbeiten an einem Film, in dem Erica eine Hauptrolle spielen wird.[2][3] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Weblinks 2Einzelnachweise Links hinzufügen Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Wikidata-Datenobjekt nicht mehr aktuell recherchieren einzufügen Wikipedia:WikiProjekt Ereignisse/Vergangenheit/fehlend Androidin Roboter Hiroshi Ishiguro Universität Osaka"
  },
  {
    "label": 1,
    "text": "Ethik der künstlichen Intelligenz – Wikipedia Ethik der künstlichen Intelligenz Inhaltsverzeichnis KI-Ethik als angewandte Ethik Ethische Prinzipien der künstlichen Intelligenz Debatten der KI-Ethik Singularität durch superintelligente KI Literatur Weblinks Einzelnachweise Prinzipienethischer Ansatz Partikularistischer Ansatz Autonomes Fahren Autonome Waffensysteme Maschinenethik Künstliche moralische Agenten Datenschutz und Überwachung Ziele für den Einsatz autonomer Fahrzeuge Reaktion in Gefahrensituationen Haftung und Verantwortung Einschränkung der menschlichen Autonomie Datenhoheit Lernende Systeme Ethikmodul für autonome Waffensysteme Gründe für und gegen autonome Waffen- und Führungssysteme Initiativen für Verbot oder Regulierung von AWS Verantwortlichkeit von Maschinen Realisierung moralischer Agenten DieEthik der künstlichen Intelligenzist ein Teilbereich derangewandten Ethik, der sich mit den ethischen Fragen vonKI-Systemen[1]befasst. Themenbereiche der KI-Ethik sind:[2] Die KI-Ethik hat Berührungspunkte und Überlappungen mit anderen Teilbereichen der angewandten Ethik:Digitale Ethik,Informationsethik,Medienethik,Virtuelle Ethik,TechnikethikundRoboterethik. Die wachsende Bedeutung der KI-Ethik ergibt sich aus der Tatsache, dass „Künstliche Intelligenz (KI) und Robotik … in naher Zukunft erhebliche Auswirkungen auf die Entwicklung der Menschheit haben werden. Sie haben grundlegende Fragen darüber aufgeworfen, was wir mit diesen Systemen tun sollten, was die Systeme selbst tun sollten, welche Risiken sie bergen und wie wir diese kontrollieren können.“ Dabei sind nicht die Auswirkungen der KI-Anwendung an sich Gegenstand der KI-Ethik, sondern die Frage, wann und unter welchen Bedingungen bestimmte Auswirkungen zulässig sind.[3] Die Debatte über ethische Fragen der KI begann bereits 1960 mit einer Arbeit[4]vonNorbert Wienerund der Erwiderung vonArthur L. Samuel[5]. KI-Ethik ist ein junges Teilgebiet derangewandten Ethik. Wie in anderen Bereichen der angewandten Ethik lassen sich auch hier zwei unterschiedliche Herangehensweisen unterscheiden: einerseits ein prinzipienethischer Ansatz, der allgemeine Prinzipien für die Behandlung praktischer Probleme aufstellt, und andererseits ein partikularistischer Ansatz, der Moralurteile für konkrete Handlungssituationen entwickelt. Derprinzipienethische Ansatz(englischprinciplism) wurde in den 1970er Jahren in derMedizinethikentwickelt[6]und dient seitdem als Vorbild für die Entwicklung anderer Bereichsethiken, auch der KI-Ethik.[7] Bei diesem Ansatz wird ein Satz von Prinzipien bereitgestellt, aus denen sich moralische Urteile für konkrete Handlungssituationen ableiten lassen, daher auch die Bezeichnungdeduktivistisches Modell.[8]Die Prinzipien werden „dadurch begründet, dass sie eine Art Schnittmenge verschiedener normativer Konzeptionen darstellen“[9], so dass sie von den unterschiedlichen normativen Theorien unabhängig und mit jeder akzeptablen Ethik verträglich sind. Sie werden daher alsPrinzipien mittlerer Reichweite(englischmid-level-principles) bezeichnet und sollen außerdem an die moralischen Alltagsüberzeugungen (englischcommon morality) anknüpfen. Es gibt hier kein oberstes Prinzip, das die Anwendung der anderen Prinzipien regelt, vielmehr kann es im konkreten Anwendungsfall durchaus Kollisionen zwischen den Prinzipien geben.[10] Die Anwendung der Prinzipien auf einen konkreten Fall ist keine einfacheHerleitung, sondern sie verlangt einerseits eine genaue Beschreibung der Handlungsalternativen und andererseits eine sorgfältige Interpretation und Gewichtung der Prinzipien. Ziel ist einÜberlegungsgleichgewichtwie esJohn Rawlsin seinerTheorie der Gerechtigkeiteingeführt hat. Das Überlegungsgleichgewicht liefert kein kategorisches Urteil, sondern eine sogenanntePrima-facie- oderCeteris-paribus-Entscheidung. „Es wird in schwierigen Fällen nicht gelingen, ein für alle Mal richtige moralische Urteile zu fällen, sie sind vielmehr immer vorläufig (defeasible).“[8] „Moralischer Partikularismus ist die Auffassung, dass der moralische Status einer Handlung nicht durch moralische Prinzipien bestimmt wird, sondern von der Konfiguration der moralisch relevanten Merkmale der Handlung in einem bestimmten Kontext abhängt. Die Hauptmotivation für den moralischen Partikularismus ergibt sich aus der Beobachtung, dass Ausnahmen von Prinzipien üblich und Ausnahmen von Ausnahmen nicht ungewöhnlich sind. Moralische Grundsätze, die nur für homogene Fälle geeignet sind, scheinen zu grob zu sein, um die feinen Nuancen in heterogenen moralischen Situationen zu berücksichtigen.“[11]Ob eine Eigenschaft ethisch für oder gegen eine Handlung spricht, hängt im Allgemeinen von den anderen Eigenschaften und Randbedingungen der Handlung ab.[10]Beim partikularistischen Ansatz werden konkrete Handlungsoptionen sorgfältig beschrieben und mit ähnlich gelagerten Fallbeispielen verglichen. Auch der partikularistische Ansatz greift auf ethische Prinzipien und Werte zurück.[8]Andere Bezeichnungen für diesen Ansatz sindrekonstruktives Modell[8]oderkasuistischerAnsatz. Eine Analyse[12]von 84 Dokumenten mit ethischen Prinzipien und Richtlinien für KI, die überwiegend zwischen 2016 und 2019 veröffentlicht wurden, identifizierte 11 übergreifende ethische Prinzipien. Nach der Häufigkeit ihres Auftretens geordnet sind dies: Luciano Floridiund Josh Cowls warnen davor, dass die Vielzahl unterschiedlicher KI-Regelwerke eher zu Verwirrung führt und dass ein „Markt für Prinzipien entsteht, auf dem Interessenten versucht sein könnten, sich die attraktivsten herauszupicken.“[14]Ihre Analyse von sechs hochrangigen Initiativen für gesellschaftlich nützlichen Einsatz von KI[15][16][17][2][18][19]ergab eine weitgehende Übereinstimmung der ethischen Prinzipien, so dass es nach ihrer Ansicht möglich ist, die KI-Ethik auf die vier Prinzipien der Medizinethik und ein zusätzliches fünftes PrinzipErklärbarkeitzu gründen[14]: Die Stufen des automatisierten Fahrens werden durch dieNormSAE J3016definiert. Ethische Probleme betreffen vor allem die Automatisierungsstufen 4 (Hochautomatisierung: das System kann in spezifischen Anwendungsfällen die Verkehrssituationen bewältigen) und 5 (Vollautomatisierung: das System beherrscht alle Verkehrssituationen). Im Folgenden wird die Diskussion in Deutschland dargestellt. Wesentliche Quelle dafür ist der Bericht der vomBundesministerium für Digitales und VerkehreingesetztenEthikkommission.[20]Die Stanford Encyclopedia of Philosophy zählt ihn zu den bemerkenswerten politischen Bemühungen in diesem Bereich. „Die daraus resultierenden deutschen und EU-Gesetze über die Zulassung des automatisierten Fahrens sind wesentlich restriktiver als die entsprechenden Vorschriften in den USA, wo das ‚Testen beim Verbraucher‘ von einigen Unternehmen als Strategie eingesetzt wird.“[21] Automatisierte Verkehrssysteme sollen helfen, die Sicherheit für alle Verkehrsteilnehmer zu verbessern, die Mobilitätschancen zu erhöhen und weitere Vorteile wie Ressourcenschonung oder Umweltschutz zu unterstützen. Die Zulassung automatisierter Systeme ist nur dann vertretbar, wenn sie im Vergleich zur menschlichen Fahrleistung zumindest eine Schadensminderung verspricht. Eine Vollautomatisierung wird kritisch gesehen, da sie zu einem Verlust von Fähigkeiten führt und damit das autonome Handeln des Menschen einschränkt.[20] Eine Vielzahl fachlicher und populärer Veröffentlichungen[22][23][24]behandelt die Frage, wie ein autonomes Fahrzeug bei plötzlich auftretenden Gefahren reagieren soll. Als besondere Schwierigkeit gilt, dass diese Entscheidung bereits während der Programmierung getroffen werden muss, also zu einem Zeitpunkt, an dem die Details der konkreten Situation noch nicht bekannt sind. Als Modell für solche Entscheidungssituationen wird häufig dasmoralische DilemmadesTrolley-Problemsin seinen verschiedenen Varianten herangezogen.[25]Fachleute bezweifeln, ob ein autonomes Auto jemals das Trolley-Problem zu lösen haben wird. Denn dabei „handelt es sich um Gedankenexperimente, bei denen die Wahlmöglichkeiten künstlich auf eine kleine, endliche Anzahl verschiedener, einmaliger Optionen beschränkt sind und der Akteur über perfektes Wissen verfügt“.[21] Die Ethikkommission ist ähnlicher Auffassung: „Echte dilemmatische Entscheidungen, wie die Entscheidung Leben gegen Leben … sind nicht eindeutig normierbar und auch nicht ethisch zweifelsfrei programmierbar.“ In Gefahrensituationen besitzt der Schutz menschlichen Lebens höchste Priorität. „Bei unausweichlichen Unfallsituationen ist jede Qualifizierung nach persönlichen Merkmalen (Alter, Geschlecht, körperliche oder geistige Konstitution) strikt untersagt. Eine Aufrechnung von Opfern ist untersagt. Eine allgemeine Programmierung auf eine Minderung der Zahl von Personenschäden kann vertretbar sein. Die an der Erzeugung von Mobilitätsrisiken Beteiligten dürfen Unbeteiligte nicht opfern.“[20] Eine Studie über die Akzeptanz von Entscheidungsstrategien hat gezeigt, dass die Mehrheit der Befragten eineutilitaristischeStrategie, die auf eine Minimierung der Opferzahl setzt, befürwortet und wünscht, dass andere solche Fahrzeuge kaufen, aber selbst ein Fahrzeug vorziehen würde, das seine Insassen um jeden Preis schützt.[26] „Die dem Menschen vorbehalteneVerantwortungverschiebt sich bei automatisierten und vernetzten Fahrsystemen vom Autofahrer auf die Hersteller und Betreiber der technischen Systeme und die infrastrukturellen, politischen und rechtlichen Entscheidungsinstanzen.“[20]Damit lehnt die Ethikkommission auch Überlegungen ab, dieHaftungder Automobilhersteller zu begrenzen, um dadurch die Entwicklung und Verbesserung autonomer Fahrzeuge zu fördern und die Zahl schwerer Unfälle erwartungsgemäß zu senken.[27] Soll der Nutzer verpflichtet werden, auf die Straße und den Verkehr zu achten und gegebenenfalls einzugreifen?Nida-Rümelinlehnt das ab, weil es Blinde oder ältere Menschen von der Benutzung autonomer Fahrzeuge ausschlösse und weil die Entwicklung bald einen Stand erreichen wird, an dem der Mensch nicht mehr wirksam eingreifen kann.[27]Die Ethikkommission verlangt darüber hinaus, „Software und Technik hochautomatisierter Fahrzeuge müssen so ausgelegt werden, dass die Notwendigkeit einer abrupten Übergabe der Kontrolle an den Fahrer (‚Notstand‘) praktisch ausgeschlossen ist.“[20] Dieverschuldensunabhängige Haftungfür das allgemeine Risiko der Fahrzeugnutzung soll auch für autonome Fahrzeuge über eine allgemeine Haftpflichtversicherung abgesichert werden.[27] Wenn autonome Fahrzeuge zur Erhöhung der Sicherheit führen, muss dann die (freiwillige) Übernahme der Steuerung durch den Menschen (Overruling), ausgeschlossen werden? Die Ethikkommission hält es für einen Ausdruck der Autonomie des Menschen, dass er auch objektiv unvernünftige Entscheidungen wie aggressives Fahren oder überhöhte Geschwindigkeit treffen kann. Es widerspräche dem Leitbild des mündigen Bürgers, wenn der Staat ein solches Verhalten bereits im Ansatz unterbinden wollte. „Er könnte damit die Grundlage einer humanen, freiheitlichen Gesellschaft untergraben. … Die Verminderung von Sicherheitsrisiken und die Begrenzung der Freiheit muss im demokratischen und grundrechtlichen Abwägungsprozess entschieden werden: Es besteht keine ethische Regel, die Sicherheit immer vor Freiheit setzt.“ Auch eine verpflichtende Einführung autonomer Systeme lässt sich nicht mit einer Erhöhung der Sicherheit begründen.[20] Grundsätzlich entscheiden Fahrzeughalter oder Fahrzeugnutzer über Weitergabe und Verwendung ihrer anfallenden Fahrzeugdaten. Die Entwicklung des automatisierten Fahrens könnte aber auf eine zentrale Verkehrssteuerung mit der Erfassung aller Kraftfahrzeuge hinauslaufen, die einen bedeutenden Gewinn an Verkehrssicherheit, Effizienz und Komfort bringen würde. Solche zentralen Strukturen gefährden allerdings die Freiheit des Einzelnen, sich unerkannt, unbeobachtet und frei zu bewegen, und sie bergen das Risiko eines Missbrauchs. „Eine kritische Reflektion des Machbaren vor dem Hintergrund des Sinnvollen, Maßvollen und ethisch Verantwortbaren sollte daher stattfinden.“[20] Lernende Systeme werden während der Entwicklung trainiert, sie unterliegen der Kontrolle des Herstellers und der Zulassungsbehörden. Selbstlernende Systeme, die sich im laufenden Fahrbetrieb verändern, können ethisch erlaubt sein, wenn sie Sicherheitsgewinne bringen und Abnahmetests unterliegen. Beim gegenwärtigen Stand der Technik sollten sie jedoch von sicherheitsrelevanten Funktionen entkoppelt sein.[20] KI wird weltweit für verschiedene militärische Aufgaben eingesetzt, um Prozesse zu optimieren und zu beschleunigen. Für die KI-Ethik sind vor allem zwei Anwendungsbereiche relevant: autonome Waffensysteme und der Einsatz von KI bei derZielplanungund beiFührungsprozessen.[28] Definition desInternationalen Komitees vom Roten Kreuz: „Autonome Waffensysteme wählen Ziele aus und wenden Gewalt an, ohne dass ein Mensch eingreift. Nach der anfänglichen Aktivierung oder dem Start durch eine Person leitet ein autonomes Waffensystem als Reaktion auf Informationen aus der Umgebung, die es über Sensoren erhält, und auf der Grundlage eines allgemeinen ‚Zielprofils‘ selbst einen Angriff ein oder löst ihn aus. Dies bedeutet, dass der Anwender das/die konkrete(n) Ziel(e) und den genauen Zeitpunkt und/oder Ort der daraus resultierenden Gewaltanwendung(en) nicht auswählt oder gar kennt. Der Einsatz autonomer Waffensysteme ist mit Risiken verbunden, da ihre Auswirkungen nur schwer vorhersehbar und begrenzbar sind. Dieser Verlust an menschlicher Kontrolle und Urteilsfähigkeit bei der Anwendung von Gewalt und Waffen wirft aus humanitärer, rechtlicher und ethischer Sicht ernsthafte Bedenken auf.“[29] Autonome Waffensysteme (AWS) werden auch als Roboterwaffen oder Killerroboter bezeichnet. Tödliche autonome Waffensysteme (englischLethal Autonomous Weapon Systems, LAWS) sind AWS, die ausdrücklich für den offensiven Einsatz bestimmt sind. Je nach Einflussmöglichkeit des Menschen werden dreiAutonomiestufenunterschieden[30]: Es gibt Vorschläge, militärische Entscheidungsprozesse näher an das Kampfgeschehen heranzuführen und z. B. im Cockpit von Kampfflugzeugen zu installieren, damit KI den Piloten bei der Entscheidungsfindung unterstützt. In Situationen mit extrem hohem Tempo (z. B. bei der Raketenabwehr oder bei hochintensiven Einsätzen), in denen der Mensch nicht mehr in der Lage ist, ein fundiertes Urteil zu fällen, sollte eine regelbasierte KI-Anwendung die Aufgabenkontrolle ohne menschliches Eingreifen übernehmen (HOOTL-Modus).[31] Grundlagen für die ethische Bewertung von AWS sind dashumanitäre Völkerrecht, dieLehre vom gerechten Kriegund die Rules of Engagement[32]. Das bislang am weitesten durchdachte[30]Ethikmodul für LAWS wurde vonRonald C. Arkinzwischen 2006 und 2009 im Auftrag desUS-Verteidigungsministeriumsentwickelt[33]Arkins System basiert auf vier ethischen Prinzipien, die sich aus dem Kriegs- und Völkerrecht ergeben: Die Entwicklung eineskünstlichen moralischen Agentenfür solche komplexen Abwägungen ist allerdings noch eine ungelöste Aufgabe. Selbst wenn es ein Ethikmodul gäbe, bliebe die Frage, wie sichergestellt werden kann, dass es sich tatsächlich an die Regeln hält. Wer soll diese Waffen wie testen, so dass sich ein Staat auf die Einhaltung des Kriegsvölkerrechts verlassen kann?[34] Im Jahr 2008 forderte die in London ansässige „Landmine Action“, das Verbot von LAWS im Rahmen derLandminen-Verbotskonvention.[38] Am 30. September 2009 wurde unter der Leitung vonNoel Sharkeydas „International Committee for Robot Arms Control“ (ICRAC) gegründet, das sich für eine Beschränkung des militärischen Einsatzes von Robotern einsetzt.[39] Im April 2013 wurde die „Campaign to Stop Killer Robots“ gegründet. Ihre mehr als 180 Mitgliedsorganisationen (Stand 2023[40]) setzen sich für ein präventives Verbot von LAWS ein. Der Berichterstatter desUN-Menschenrechtsratsempfahl 2013 nationale Moratorien für die Produktion, Beschaffung, Erprobung und den Einsatz von LAWS, bis ein internationales Rahmenwerk vereinbart ist.[41] Im Juli 2015 wurde auf derInternational Joint Conference on Artificial IntelligenceinBuenos Airesein offener Brief präsentiert, in dem über 1.000 hochrangige Expertinnen und Experten vor einem militärischen Wettrüsten mit KI warnen und ein Verbot von LAWS fordern.[42] Im Rahmen derUN-Waffenkonventionführen die über 100 Vertragsstaaten[43]seit 2014 Gespräche auch über LAWS. Im Jahr 2017 wurde eine „Gruppe von Regierungsexperten“ eingerichtet, die ein Rahmenwerk zu LAWS als Vorstufe für ein rechtsverbindliches Zusatzprotokoll erarbeiten soll. Ein Schritt in diese Richtung waren die 11 unverbindlichen Leitprinzipien, auf die sich die Vertragsstaaten 2019 geeinigt haben. Die zwei wichtigsten Prinzipien sind: Auf der IEEE-Robotics-Konferenz 2002 in Genua[45]wurde vereinbart, das Gebiet der Roboter-Ethik aufzuteilen[46]: Susan Anderson, Pionierin der Maschinenethik, definiert: „Das ultimative Ziel der Maschinenethik ist es, eine Maschine zu schaffen, die sich in ihrem Verhalten von einem idealen ethischen Prinzip oder einer Reihe von Prinzipien leiten lässt …. Vereinfacht gesagt geht es darum, der Maschine eine ‚ethische Dimension‘ zu geben.“[47] Vincent C. Müller dämpft die Erwartungen: „Es ist nicht klar, ob es einen konsistenten Begriff der ‚Maschinenethik‘ gibt, da schwächere Versionen Gefahr laufen, «Ethik haben» auf Begriffe zu reduzieren, die normalerweise nicht als ausreichend angesehen würden (z. B. ohne‚Reflexion‘oder sogar ohne‚Handlung‘); stärkere Begriffe, die sich in Richtung künstlicher moralischer Agenten bewegen, könnten eine - derzeit leere - Menge beschreiben.“[3] DasDeutsche Institut für Normunguntersuchte Ethikaspekte in der Normung und Standardisierung für KI.[48]Darin enthalten ist eine Übersicht (Stand 2020) einschlägigerISO– undIEEE–Normen. Maschinenethik als eine Reihe von Gesetzen zu formulieren, war eine Idee desBiochemikersundScience-Fiction-SchriftstellersIsaac Asimov. SeineRobotergesetzeformulierte er erstmals 1942 in der ErzählungRunaround.[49] Die Frage, ob Maschinen für ihre Aktionen verantwortlich sein können, wird kontrovers diskutiert, aber von den meisten Autoren verneint. Colin Allenet al. argumentieren, dass ein künstliches System moralisch verantwortlich sein kann, wenn sein Verhalten funktional nicht von dem einer moralischen Person zu unterscheiden ist, und prägen den Begriff eines „moralischen Turing-Tests“ („If human ‚interrogators’ cannot identify the machine at above chance accuracy, then the machine is, on this criterion, a moral agent.“)[50] Andreas Matthias warnt vor einer Verantwortungslücke: Da der Hersteller oder Betreiber einer lernenden Maschine prinzipiell nicht mehr in der Lage ist, deren zukünftiges Verhalten vorherzusagen, kann er dafür auch nicht moralisch verantwortlich oder haftbar gemacht werden.[51] Matthias Rathlegt dar, dass Maschinen nicht verantwortlich handeln können, weil ihnenWillensfreiheitfehlt: „Die Fähigkeit, die eigene Regelvorgabe zu problematisieren – sich also über diese intentional hinweg zu setzen –, und zwar ohne Programmierung des „Aufstands“ gegen das Programm, ist für die Frage nach einem ethischen bzw. ethisch relevanten moralischen Akteur unabdingbar. … Freiheit ist, wie Immanuel Kant … gezeigt hat, zwar nicht empirisch belegbar, aber sie ist als notwendiges Postulat der moralischen Handlung ‚unmittelbar gewiss‘.“[52] Nach Meinung desDeutschen Ethikratskönnen Maschinen keine Verantwortung übernehmen: „Moralische Verantwortung können nur natürliche Personen übernehmen, die über Handlungsfähigkeit verfügen, d. h. in der Lage sind, aktiv, zweckgerichtet und kontrolliert auf die Umwelt einzuwirken und dadurch Veränderungen zu verursachen. … Verantwortung kann daher nicht direkt von maschinellen Systemen übernommen werden, sondern nur von den Menschen, die in je unterschiedlichen Funktionen hinter diesen Systemen stehen, gegebenenfalls im Rahmen institutioneller Verantwortung.“[53]„Selbst wenn Maschinen komplexe Vollzüge oder Operationen durchführen … und flexibel mit anspruchsvollen Herausforderungen der menschlichen Lebenswelt umgehen können, führen sie diese Veränderungen aber nicht absichtlich herbei und haben sie diese daher auch nicht in einem moralischen und rechtlichen Sinne zu verantworten. … DasIntentionalitätskriteriumist entscheidend für die Möglichkeit der Zuschreibung von Verantwortung im Kontext von Mensch-Maschine-Interaktionen.“[54] Der sogenannteAutomation Bias– Menschen vertrauen algorithmisch generierten Ergebnissen und automatisierten Entscheidungsverfahren oft mehr als menschlichen Entscheidern – kann dazu führen, dass mit der Zeit das KI-System als Entscheider angesehen wird und menschliche Autorschaft und Verantwortung ausgehöhlt werden.[55] Die Abkürzung AMA für künstliche moralische Agenten geht auf die englische Bezeichnung „artificial moral agent“ zurück. Einrationaler Agent(auch als intelligenter Agent bezeichnet) nimmt seine Umgebung mit Hilfe von Sensoren wahr und wählt selbständig Aktionen aus, um ein vorgegebenes Ziels zu erreichen, zudem kann er seine Performance durch Lernen verbessern. Er zeichnet sich also durch Interaktivität, Autonomie und Anpassungsfähigkeit aus.[56](siehe auchSoftware-Agent). Einmoralischer Agentist ein rationaler Agent, der fähig ist, seine Aktionen nach ethischen Grundsätzen auszurichten und – zumindest auf einer höheren Stufe – seine Handlungen auch zu begründen. Daraus folgt nicht, dass moralischen Agenten auch Verantwortung zugeschrieben werden kann, denn dazu fehlt ihnen die Willensfreiheit. James Moor definiert vier Kategorien ethischer Agenten:[57]: Für Wendell Wallach und Colin Allen wird der Grad von Moralität durch die unterschiedliche Ausprägung der beiden Dimensionen Autonomie und ethische Sensibilität bestimmt. Zwischenoperationaler Moralität, die der untersten Stufe bei Moor entspricht, und einer voll ausgebildeten Moralität wie beim Menschen positionieren sie die als technisch realisierbar angesehenefunktionale Moralität, bei der eine Maschine selbst fähig ist, moralische Herausforderungen zu bewerten und darauf zu reagieren.[58]:9 Moralische Agenten werden vor allem im Zusammenhang mit autonomen Waffensystemen und autonomen Fahrzeugen diskutiert. Das erste ausgearbeitete Konzept eines moralischen Agenten war das vonRonald C. Arkinentwickelt Ethikmodul für autonome Waffensysteme.[33] Gegen eine Realisierung moralischer Agenten spricht der aufJohn SearlesGedankenexperiment deschinesischen Zimmerszurückgehende Einwand, dass Maschinen keinementalen Zuständehaben können. Einemoralische Handlungsetzt aber den mentalen Zustand derAbsichtvoraus. Der in der modernenKognitionspsychologieverbreitetephilosophische Funktionalismusumgeht Searles Einwand, indem er mentale Zustände alsfunktionale Zuständeversteht: Auf einen bestimmtenInputreagiert das System in Abhängigkeit vom aktuellen Zustand mit einem bestimmtenOutputund geht in einen anderen funktionalen Zustand über. Dierepräsentationale Theorie des GeistesvonJerry Fodorverbindet diese Sichtweise mit derComputertheorie des Geistes. Mentale Prozesse werden als Turing-ähnliche Berechnungen über strukturierten Symbolen – den mentalen Zuständen – verstanden. Für die Realisierbarkeit moralischer Agenten genügt es, dass Computer funktionale Zustände haben, die funktional äquivalent zu mentalen Zuständen sind.Catrin Misselhornbezeichnet sie alsquasi-intentionale Zustände.[59]:36–39 Für die Implementierung moralischer Agenten werden drei unterschiedliche Ansätze verfolgt. Problem des Regelkonflikts: In einer moralisch zu beurteilenden Situation können mehrere Prinzipien von Bedeutung sein. Der moralische Agent muss herausfinden, welche Prinzipien in einer Situation gelten und wie sie im Konfliktfall in Einklang zu bringen sind.[60] Problem des Kontextwechsels: Wenn der moralische Agent für einen bestimmten Anwendungsbereich programmiert oder trainiert wurde, ist unklar, in welchen anderen Anwendungskontexten er eingesetzt werden kann.[60] „Alle nicht-trivialen Beispiele für den echten Einsatz von KI sind bisher auf dem Niveau von ‚Agenten mit ethischen Auswirkungen‘. Ihr Verhalten kann nicht ethisch genannt werden, weil die Intention fehlt, die eine Handlung erst ethisch macht.“[60] Hinsichtlich ihrer wirtschaftlichen und gesellschaftlichen Bedeutung stehen seit der Jahrtausendwende vor allem KI-Systeme im Vordergrund, die auf Techniken desMaschinellen Lernensberuhen. Diese Systeme werden typischerweise mit großen Mengen von Daten trainiert, die häufig als nutzergenerierte Daten, Kommunikationsmetadaten oder Internetinhalte der sozialen Welt entnommen sind. Die US-amerikanische WirtschaftswissenschaftlerinShoshana Zuboffspricht bei der wirtschaftlichen Umsetzung dieser Technologie vonÜberwachungskapitalismus(Surveillance Capitalism).[61]Der deutsche PhilosophRainer Mühlhoffargumentiert, dass die heutige Erfolgswelle von KI nur durch das Verfügbarwerden großer Mengen nutzergenerierter Daten infolge der Entwicklung derSozialen Medienin den 2010er Jahren möglich wurde.[62][63][64]Deshalb sei das Problem der Datennutzung und die daran anschließenden Fragen der Datenethik und desDatenschutzeszentral für die Ethik der Künstlichen Intelligenz.[65]So beruhe \"künstliche Intelligenz, wie wir sie heute kennen und wie sie spürbar unsere Gesellschaften prägt, wesentlich auf Daten, und zwar aufunserenDaten\", so dass \"ein ethischer Umgang mit dieser Technologie […] ein Bewusstsein über die damit verbundenen Machtverhältnisse zwischen Unternehmen und Individuen\" voraussetze.[63] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1KI-Ethik als angewandte Ethik 1.1Prinzipienethischer Ansatz 1.2Partikularistischer Ansatz 2Ethische Prinzipien der künstlichen Intelligenz 3Debatten der KI-Ethik 3.1Autonomes Fahren 3.1.1Ziele für den Einsatz autonomer Fahrzeuge 3.1.2Reaktion in Gefahrensituationen 3.1.3Haftung und Verantwortung 3.1.4Einschränkung der menschlichen Autonomie 3.1.5Datenhoheit 3.1.6Lernende Systeme 3.2Autonome Waffensysteme 3.2.1Ethikmodul für autonome Waffensysteme 3.2.2Gründe für und gegen autonome Waffen- und Führungssysteme 3.2.3Initiativen für Verbot oder Regulierung von AWS 3.3Maschinenethik 3.3.1Verantwortlichkeit von Maschinen 3.4Künstliche moralische Agenten 3.4.1Realisierung moralischer Agenten 3.5Datenschutz und Überwachung 4Singularität durch superintelligente KI 5Literatur 6Weblinks 7Einzelnachweise Afrikaans العربية Azərbaycanca Català Ελληνικά English"
  },
  {
    "label": 1,
    "text": "Europäische Vereinigung für künstliche Intelligenz – Wikipedia Europäische Vereinigung für künstliche Intelligenz Inhaltsverzeichnis Geschichte Aktivitäten Fellowship Dissertation Award Einzelnachweise DieEuropäische Vereinigung für künstliche Intelligenz (EurAI)(englischEuropean Association for Artificial Intelligence, ehemalsEuropean Coordinating Committee for Artificial Intelligence (ECCAI)) ist eine europäischewissenschaftliche Gesellschaft, die sich der Förderung der Erforschung und Anwendung derkünstlichen Intelligenz(KI) inEuropawidmet. Sie wurde 1982 gegründet. Der Hauptsitz befindet sich inBrüssel. Wolfgang Bibelwollte 1979 eine Europäische Organisation für künstliche Intelligenz gründen. 1982 kam es zur erstenEuropean Conference for Artificial Intelligence (ECAI)und kurz drauf zur Gründung derEuropean Coordinating Committee for Artificial Intelligence (ECCAI), die später inEuropean Association for Artificial Intelligence (EurAI)umbenannt wurde. Der erste Gründungspräsident von ECCAI wurde Wolfgang Bibel.[1][2]Wolfgang Bibel gründete 1982 zusammen mitJörg Siekmanndie zweiwöchige KI-Frühjahrsschule (KIFS), da sich das Fach künstliche Intelligenz zu dieser Zeit noch nicht etabliert hatte und die Forschungsergebnisse noch nicht an die Studenten kamen. 1985 organisierte Bibel dann den erstenAdvanced Course on AI (ACAI)an, das Äquivalent zur KIFS auf europäischer Ebene.[3] Jedes Jahr veranstaltet EurAI gemeinsam mit einem der Mitgliedsverbände von EurAI dieEuropean Conference on Artificial Intelligence(ECAI). Die Konferenz hat sich zu einer der führenden Konferenz für Künstliche Intelligenz in Europa entwickelt. Daneben findet alle zwei Jahre derAdvanced Course on AI(ACAI) statt, der Studenten das Thema künstliche Intelligenz näher bringen will. Das EurAI Fellows-Programm wurde 1999 ins Leben gerufen und zeichnet Personen aus, die einen bedeutenden und nachhaltigen Beitrag zur künstlichen Intelligenz (KI) in Europa geleistet haben.[4] Der von der EurAI gestifteteArtificial Intelligence Dissertation Awardwird seit 1998 vergeben und zeichnet jährlich die beste Dissertation im Bereich der künstlichen Intelligenz (KI) in Europa aus. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Aktivitäten 3Fellowship 4Dissertation Award 5Einzelnachweise English Español Euskara Français Italiano Polski Русский Українська Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen"
  },
  {
    "label": 1,
    "text": "European Conference on Artificial Intelligence – Wikipedia European Conference on Artificial Intelligence Inhaltsverzeichnis Veranstaltungen Weblinks DieEuropean Conference on Artificial Intelligence(ECAI) ist eine der größten Konferenzen weltweit, die sich der Erforschungkünstlicher Intelligenzwidmen. Normalerweise wird sie alle zwei Jahre ausgerichtet und geht über einen Zeitraum von fünf Tagen. Die 1. Konferenz fand1982inOrsay(Frankreich) statt. Davor gab es jedoch vier Konferenzen derArtificial Intelligence and Simulation of Behaviour(AISB) die nachträglich unter dem Namen ECAI umbenannt wurden. Organisiert werden die Konferenzen jeweils von derEuropean Association for Artificial Intelligence(EurAI) in Zusammenarbeit mit einer ihrer Mitgliedsorganisationen. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Veranstaltungen 2Weblinks English Español Italiano Русский Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Wikidata-Datenobjekt künstlicher Intelligenz 1982 Orsay European Association for Artificial Intelligence"
  },
  {
    "label": 1,
    "text": "Existenzielles Risiko durch künstliche Intelligenz – Wikipedia Existenzielles Risiko durch künstliche Intelligenz Inhaltsverzeichnis Geschichte KI-Fähigkeiten Formen des existenziellen Risikos AI-Alignment Szenarien Perspektiven Schützende Maßnahmen Siehe auch Literatur Weblinks Einzelnachweise Allgemeine Intelligenz Superintelligenz Instrumentelle Konvergenz Widerstand gegen wechselnde Ziele Schwierigkeit der Zielspezifikation Alignment von Superintelligenzen Die Schwierigkeit des makellosen Designs Orthogonalitätsthese Anthropomorphistische Argumente Andere Risikoquellen Tückische Wende Leben 3.0 Unterstützung Skepsis Populäre Reaktionen Öffentliche Umfragen Ansichten zu Verboten und Regulierung Vergleich mit dem Menschen Intelligenzexplosion Alien Mind Limitationen Gefährliche Fähigkeiten Soziale Manipulation Cyberangriffe Gefährlichere Krankheitserreger KI-Wettrüsten Verbote Regulierung Existenzielles Risiko durch künstliche Intelligenzbezeichnet dieHypothese, dass wesentliche Fortschritte bei der Entwicklung vonArtificial General Intelligence(AGI) zumAussterben der Menschheitoder zu einer anderen unumkehrbaren globalen Katastrophe führen könnten.[1][2][3] Ein Argument stellt sich wie folgt dar: DerMenschist anderen Arten überlegen, weil das menschlicheGehirnüber besondere Fähigkeiten verfügt, die anderen Tieren fehlen. Sollte die KI also eine übermenschlicheallgemeine IntelligenzoderSuperintelligenzentwickeln, könnte es schwierig oder unmöglich werden, sie zu kontrollieren. So wie das Schicksal desBerggorillasvom Wohlwollen der Menschen abhängt, könnte auch das Schicksal der Menschheit von den Handlungen einer zukünftigen maschinellen Superintelligenz abhängen.[4] Wie plausibel eine existenzielle Katastrophe durch KI ist, wird breit diskutiert und hängt zum Teil davon ab, ob AGI oder Superintelligenz erreichbar sind, wie schnell sich gefährliche Fähigkeiten und Verhaltensweisen entwickeln[5]und ob es konkrete Szenarien für eine Machtübernahme durch KI gibt.[6]Besorgnis über Superintelligenz wurde von führenden Computerwissenschaftlern und Firmenchefs wieGeoffrey Hinton,[7]Yoshua Bengio,[8]Alan Turing,Elon Musk,[9]und demCEOvonOpenAISam Altmangeäußert.[10]In einer Umfrage unter KI-Forschern aus dem Jahr 2022 mit einer Antwortrate von 17 % gab die Mehrheit der Befragten an, dass die Unfähigkeit, KI zu kontrollieren, mit einer Wahrscheinlichkeit von 10 % oder mehr zu einer existenziellen Katastrophe führen wird.[11][12]2023 unterzeichneten Hunderte von KI-Experten und anderen namhaften Persönlichkeiteneine Erklärung, in der es heißt: „Die Minderung des Risikos des Aussterbens durch KI sollte neben anderen Risiken von gesamtgesellschaftlichem Ausmaß wiePandemienundAtomkriegeine globale Priorität sein.“[13]Infolge der zunehmenden Besorgnis über KI-Risiken forderten führende Regierungsvertreter wie der ehemaligebritische PremierministerRishi Sunak[14]und derGeneralsekretär der Vereinten NationenAntónio Guterres[15]eine stärkeren Fokus auf die globale KI-Regulierung. Zwei Quellen der Besorgnis ergeben sich aus den Problemen der KI-Kontrolle und -Ausrichtung: Es könnte schwierig sein, eine superintelligente Maschine zu kontrollieren oder ihr menschliche Werte zu vermitteln. Viele Forscher glauben, dass eine superintelligente Maschine sich gegen Versuche, sie zu deaktivieren oder ihre Ziele zu ändern, wehren würde, da dies sie von der Verfolgung ihrer derzeitigen Ziele abhielte. Es wäre äußerst schwierig, eine Superintelligenz mit der gesamten Bandbreite bedeutsamer menschlicher Werte und Grenzen in Einklang zu bringen.[1][16][17]Im Gegensatz dazu argumentieren Skeptiker wie derInformatikerYann LeCun, dass superintelligente Maschinen keinen Wunsch nach Selbsterhaltung hegen werden.[18] Eine dritte Quelle der Sorge ist, dass eine plötzliche „Intelligenzexplosion“ die unvorbereitete Menschheit überraschen könnte. Derlei Szenarien ziehen die Möglichkeit in Betracht, dass eine KI, die intelligenter ist als ihre Schöpfer, in der Lage sein könnte, sich selbst mit exponentiell steigender Geschwindigkeitrekursivzu verbessern, und zwar so schnell, dass sie von ihren Betreibern und der Gesellschaft als Ganzes nicht länger kontrolliert werden kann.[1][16]Empirische Beispiele wie das ProgrammAlphaZero, das sich selbst dasGospielenbeibrachte, zeigen, dass sich bereichsspezifische KI-Systeme manchmal sehr schnell von dem Menschen unterlegenen zu Akteuren mit übermenschlichen Fähigkeiten entwickeln können, obwohl solche Systeme keiner Änderung ihrer grundlegenden Architektur bedürfen.[19] Einer der ersten Autoren, der ernsthafte Sorge darüber äußerte, dass hochentwickelte Maschinen existenzielle Risiken für die Menschheit darstellen könnten, war der SchriftstellerSamuel Butler, der 1863 in seinem EssayDarwin among the Machinesschrieb:[20] „Das Ergebnis ist einfach eine Frage der Zeit, aber dass die Zeit kommen wird, in der die Maschinen die wirkliche Vorherrschaft über die Welt und ihre Bewohner haben werden, kann kein Mensch mit einem wahrhaft philosophischen Verstand auch nur einen Augenblick in Frage stellen.“ Im Jahr 1951 schrieb der ComputerwissenschaftlerAlan Turingden ArtikelIntelligent Machinery, A Heretical Theory, in dem er postulierte, dass allgemeine künstliche Intelligenzen, würden sie übermenschliche Intelligenz entwickeln, wahrscheinlich die Kontrolle über die Welt übernähmen: „Schauen wir uns an, welche Folgen der Bau dieser Maschinen hätte... Die Maschinen würden auf keinen Fall sterben, und sie wären in der Lage, miteinander zu kommunizieren, um ihren Verstand zu schärfen. Irgendwann müssten wir also damit rechnen, dass die Maschinen die Kontrolle übernehmen, so wie es in Samuel Butlers Erewhon beschrieben wird.[21]“ Im Jahre 1965 entwickelteI. J. Gooddas Konzept, das heute als „Intelligenzexplosion“ bekannt ist, und erklärte, die Risiken würden unterschätzt:[22] „Definieren wir eine ultraintelligente Maschine als eine Maschine, die alle intellektuellen Aktivitäten eines noch so klugen Menschen weit übertreffen kann. Da das Entwerfen von Maschinen eine dieser intellektuellen Tätigkeiten ist, könnte eine ultraintelligente Maschine noch bessere Maschinen entwerfen; Zweifellos würde es dann zu einer ,Intelligenzexplosion‘ kommen, und die Intelligenz des Menschen würde weit zurückbleiben. Die erste ultraintelligente Maschine ist also die letzte Erfindung, die der Mensch jemals machen muss, vorausgesetzt, die Maschine ist gefügig genug, uns zu sagen, wie wir sie unter Kontrolle halten können. Es ist merkwürdig, dass dieser Punkt außerhalb der Science-Fiction so selten angesprochen wird. Manchmal lohnt es sich, Science-Fiction ernst zu nehmen[23]“ Wissenschaftler wieMarvin Minsky[24]und I. J. Good selbst[25]äußerten gelegentlich die Sorge, dass eine Superintelligenz die Kontrolle übernehmen könnte, riefen aber nicht zum Handeln auf. Im Jahr 2000 verfasste der Informatiker und Mitbegründer vonSun MicrosystemsBill Joyeinen einflussreichen Aufsatz mit dem TitelWhy The Future Doesn't Need Us(deutsch: Warum die Zukunft uns nicht braucht), in dem er superintelligente Roboter neben derNanotechnologieund künstlich designten biologischen Erregern als High-Tech-Gefahr für das menschliche Überleben bezeichnete.[26] Nick Bostromveröffentlichte 2014 das BuchSuperintelligenz, in dem er dafür argumentierte, dass Superintelligenz eine existenzielle Bedrohung darstellt.[27]2015 äußerten sich Persönlichkeiten des öffentlichen Lebens wie die PhysikerStephen Hawkingund NobelpreisträgerFrank Wilczek, die InformatikerStuart J. Russellund Roman Yampolskiy sowie die UnternehmerElon MuskundBill Gatesbesorgt über die potenziellen Risiken durch Superintelligenz.[28][29][30][31] Im gleichen Jahr (2015) hob der Offene Brief zur Künstlichen Intelligenz das „große Potenzial der KI“ hervor und rief zu mehr Forschung auf, um sie stabil und nützlich zu machen.[32]Im April 2016 warnteNature: „Maschinen und Roboter, die den Menschen in allen Bereichen übertreffen, könnten sich, ohne dass wir es kontrollieren können, selbst verbessern — und ihre Interessen könnten von unseren abweichen.“[33]Außerdem veröffentlichte Brian Christian im Jahr 2020 das BuchThe Alignment Problem, in dem er die bis zum damaligen Zeitpunkt gemachten Fortschritte im Bereich der KI-Ausrichtung beschreibt.[34][35] Im März 2023 unterzeichneten Schlüsselfiguren aus dem Bereich der KI wie Elon Musk einen Brief des Future of Life Institute, in dem sie dazu aufriefen, das Training fortgeschrittener KI zu stoppen, bis sie angemessen reguliert werden könne.[36]Im Mai 2023 veröffentlichte das Center for AI Safety eine von zahlreichen Experten für KI-Sicherheit und existenzielle KI-Risiken unterzeichnete Erklärung, in der es heißt: „Die Minderung des Risikos des Aussterbens durch KI sollte neben anderen Risiken von gesamtgesellschaftlichem Ausmaß wie Pandemien und Atomkrieg eine globale Priorität sein.“[37][38] Allgemeine künstliche Intelligenz (engl.:artificial general intelligence, AGI) wird in der Regel als ein System definiert, das in den meisten oder allen intellektuellen Aufgaben mindestens so gut abschneidet wie der Mensch.[39]Eine Umfrage unter KI-Forschenden aus dem Jahr 2022 ergab, dass 90 % der Befragten davon ausgingen, dass AGI in den nächsten 100 Jahren entwickelt werden wird, und die Hälfte rechnete hiermit schon bis zum Jahre 2061.[40]Unterdessen weisen einige Forschende die von AGI ausgehenden existenziellen Risiken als „Science Fiction“ zurück, basierend auf ihrer Überzeugung, dass es in absehbarer Zeit keine AGI geben wird.[41] Durchbrüche bei großen Sprachmodellen haben einige Forschende dazu veranlasst, ihre Prognosen anzupassen. So sagteGeoffrey Hintonim Jahr 2023, dass er seine Schätzung von „20 bis 50 Jahren, bevor wir eine allgemeine KI haben“ auf „20 Jahre oder weniger“ korrigiert habe.[42] Im Gegensatz zur AGI definiert Bostrom eineSuperintelligenzals „jeden Intellekt, der die kognitive Leistung des Menschen in praktisch allen Bereichen von Interesse weit übertrifft“, einschließlich wissenschaftlicher Kreativität, strategischer Planung und sozialer Fähigkeiten.[43][4]Er argumentiert, dass eine Superintelligenz den Menschen immer dann ausmanövrieren kann, wenn ihre Ziele mit denen des Menschen in Konflikt stehen. Sie kann sich dazu entschließen, ihre wahren Absichten zu verbergen, bis die Menschheit sie nicht mehr aufzuhalten vermag.[44][4]Bostrom schreibt, dass eine Superintelligenz sich, um für die Menschheit sicher zu sein, im Einklang mit menschlichen Werten und menschlicher Moral befinden muss, so dass sie „fundamental auf unserer Seite“ stehe.[45] Stephen Hawkingvertrat die Ansicht, dass Superintelligenz physikalisch möglich sei, da „es kein physikalisches Gesetz gibt, das ausschließt, dass Teilchen auf eine Art und Weise organisiert sind, die noch fortschrittlichere Berechnungen ermöglicht, als die Anordnung von Teilchen in menschlichen Gehirnen“.[29] Wann die Superintelligenz, falls überhaupt möglich, realisiert werden kann, ist zwangsläufig unsicherer als die Vorhersagen für die Entwicklung einer AGI. Im Jahr 2023 erklärten führende Vertreter vonOpenAI, dass nicht nur AGI, sondern auch Superintelligenz in weniger als 10 Jahren erzielt werden könnte.[46] Bostrom zufolge ist die KI dem menschlichenGehirnin vielerlei Hinsicht voraus:[4] Bostrom zufolge könnte eine KI, die bestimmte Schlüsselaufgaben des Programmierens auf hohem Niveau beherrscht, zu einer Superintelligenz werden, da sie in der Lage wäre, ihre eigenen Algorithmen rekursiv zu verbessern, selbst wenn sie in anderen Bereichen, die für das Programmieren zweitrangig sind, zunächst eingeschränkt ist. Dies deutet darauf hin, dass eine Intelligenzexplosion die Menschheit eines Tages unvorbereitet treffen könnte.[4] Der WirtschaftswissenschaftlerRobin Hansonmerkte an, dass eine KI für das Ingangsetzen einer Intelligenzexplosion eine weitaus fortgeschrittenere Fähigkeit zur Innovation von Software besitzen müsse als der Rest der Welt zusammen, was er für unwahrscheinlich halte.[47] In einem Szenario des „schnellen Starts“ könnte der Übergang von der AGI zur Superintelligenz Tage oder Monate dauern. Bei einem „langsamen Start“ könnten es Jahre oder Jahrzehnte sein, sodass der Gesellschaft mehr Zeit bliebe, sich vorzubereiten.[48] Superintelligenzen werden manchmal alsalien minds(deutsch: etwafremdartiger Verstand) bezeichnet, was auf die Vorstellung verweist, dass ihre Denkweise und ihre Beweggründe sich stark von den unseren unterscheiden könnten. Dies wird im Allgemeinen als Gefahrenquelle betrachtet, da es hierdurch schwieriger ist, vorauszusehen, was eine Superintelligenz vorhaben könnte. Es deutet auch auf die Möglichkeit hin, dass eine Superintelligenz den Menschen grundsätzlich nicht besonders wertschätzen würde.[49]UmAnthropomorphismuszu vermeiden, wird die Superintelligenz manchmal als ein leistungsfähiger Optimierer betrachtet, der die besten Entscheidungen trifft, um seine Ziele zu erreichen.[4] Das Forschungsfeld der „mechanistischen Interpretierbarkeit“ zielt darauf ab, das Innenleben von KI-Modellen besser zu verstehen. Dieses Verständnis könnte uns eines Tages in die Lage versetzen, Anzeichen von Täuschung und Fehlausrichtung zu erkennen.[50] Es wurde angeführt, dass es Grenzen für das gibt, was Intelligenz erreichen kann. Insbesondere diechaotischeNatur oder diezeitliche Komplexitäteiniger Systeme könnte die Fähigkeit einer Superintelligenz, gewisse Aspekte der Zukunft vorherzusagen, grundlegend einschränken und somit ihre Ungewissheit erhöhen.[51] Fortgeschrittene KI könnte künstlich Krankheitserreger mit gefährlicheren Eigenschaften erzeugen, Cyberangriffe durchführen oder Menschen manipulieren. Diese Fähigkeiten könnten von Menschen missbraucht[52]oder von einer fehlausgerichteten KI selbst ausgenutzt werden.[4]Eine voll ausgereifte Superintelligenz könnte, wenn sie es wollte, verschiedene Wege finden, um einen entscheidenden Einfluss zu erlangen,[4]aber diese gefährlichen Fähigkeiten könnten schon früher, in schwächeren und spezialisierten KI-Systemen, verfügbar werden. Sie könnten zu gesellschaftlicher Instabilität führen und böswillige Akteure ermächtigen.[52] Geoffrey Hinton warnte, dass die Fülle von KI-generierten Texten, Bildern und Videos es kurzfristig schwieriger machen würde, die Wahrheit zu ermitteln, was autoritäre Staaten seiner Ansicht nach ausnutzen könnten, um Wahlen zu manipulieren.[53]Solche groß angelegten, personalisierten Manipulationsmöglichkeiten könnten das existenzielle Risiko eines weltweiten „irreversiblen totalitären Regimes“ erhöhen. Sie könnten außerdem von böswilligen Akteuren genutzt werden, um die Gesellschaft zu spalten und dysfunktional zu machen.[52] KI-gestützteCyberangriffewerden zunehmend als akute und ernsthafte Bedrohung angesehen. Laut dem technischen Direktor für den Cyberspace derNATO„nimmt die Zahl der Angriffe exponentiell zu“.[54]KI kann auch defensiv eingesetzt werden, um präventiv Schwachstellen zu finden und zu beheben und Bedrohungen frühzeitig zu erkennen.[55] KI könnte die „Durchführbarkeit, die Erfolgsquote, das Ausmaß, die Geschwindigkeit, die Diskretion und die Durchschlagskraft von Cyberangriffen“ verbessern und möglicherweise „erhebliche geopolitische Turbulenzen“ verursachen, wenn sie vornehmlich die Angriffs- und nicht die Verteidigungsmaßnahmen erleichtert.[52] Spekulationen zufolge könnten solche Hacking-Fähigkeiten von einem KI-System genutzt werden, um aus seiner lokalen Umgebung auszubrechen, finanzielle Erträge zu erzielen oder Cloud-Rechenleistung zu erwerben.[56] Mit der zunehmenden Demokratisierung der KI-Technologie könnte es einfacher werden, ansteckendere und tödlichere Krankheitserreger zu entwickeln. Dies könnte Menschen mit begrenzten Kenntnissen insynthetischer Biologiein die Lage versetzen,Bioterrorismuszu betreiben. Technologien mit doppelter Verwendbarkeit (engl.:dual use), die für die Medizin nützlich sind, könnten für die Herstellung von Waffen umfunktioniert werden.[52] Im Jahr 2022 änderten Wissenschaftler beispielsweise ein KI-System, das ursprünglich für die Erzeugung unschädlicher therapeutischer Moleküle gedacht war, mit dem Ziel, neue Medikamente zu entwickeln. Die Forschenden passten das System so an, dass Toxizität belohnt und nicht bestraft wird. Diese einfache Änderung ermöglichte es dem KI-System, innerhalb von sechs Stunden 40.000 potenziell für diechemische Kriegsführunggeeignete Moleküle zu erstellen, darunter bekannte und neuartige Moleküle.[52][57] Unternehmen, staatliche Akteure und andere Organisationen, die um die Entwicklung von KI-Technologien konkurrieren, könnten zu einemrace to the bottom(deutsch: „Wettlauf nach unten“ oder Unterbietungswettlauf) bei den Sicherheitsstandards führen.[58]Da strenge Sicherheitsverfahren Zeit und Ressourcen erfordern, laufen Projekte, die sorgfältiger vorgehen, Gefahr, von weniger gewissenhaften Entwicklern überholt zu werden.[59][52] KI könnte eingesetzt werden, um militärische Vorteile durchtödliche autonome Waffen,Cyberkriegsführungoder automatisierte Entscheidungsfindung zu erlangen.[52]Ein Beispiel für tödliche autonome Waffen könnten Miniatur-Drohnen sein, die eine kostengünstige Ermordung militärischer oder ziviler Ziele unterstützen, ein Szenario, das in dem 2017 erschienenen KurzfilmSlaughterbotsaufgezeigt wird.[60]KI könnte eingesetzt werden, um einen Vorsprung bei der Entscheidungsfindung zu erlangen, indem große Datenmengen schnell analysiert werden und Entscheidungen schneller und effektiver getroffen werden, als es Menschen könnten. Dies könnte die Rasanz und Unberechenbarkeit von Kriegen erhöhen, insbesondere wenn man automatische Vergeltungssysteme berücksichtigt.[52][61] Ein existenzielles Risiko ist „ein Risiko, welches das von der Erde ausgehende intelligente Leben auszulöschen oder dessen Potenzial für eine wünschenswerte zukünftige Entwicklung dauerhaft zu zerstören droht“.[62] Neben dem Risiko des Aussterbens besteht auch die Gefahr, dass die Zivilisation dauerhaft in einer ungünstigen Zukunft gefangen bleibt. Ein Beispiel hierfür wäre einvalue lock-in(deutsch: Einrasten von Werten): Wenn die Menschheit zur gegebenen Zeit immer noch moralische blinde Flecken hat, ähnlich wie in der Vergangenheit die Sklaverei, könnte die KI diese irreversibel verankern und moralischen Fortschritt verhindern. KI könnte auch dazu verwendet werden, das Wertesystem desjenigen, der sie entwickelt, zu verbreiten und zu bewahren.[63]Sie könnte weiterhin eine groß angelegte Überwachung und Indoktrination erleichtern, was zur Schaffung eines stabilen, repressiven, weltweiten totalitären Regimes genutzt werden könnte.[64] Es ist schwierig oder gänzlich unmöglich, zuverlässig zu beurteilen, ob und in welchem Maße eine fortgeschrittene KI empfindungsfähig wäre. Wenn jedoch in der Zukunft massenhaft empfindungsfähige Maschinen geschaffen würden, könnte eine zivilisatorische Entwicklung, die ihr Wohlergehen auf unbestimmte Zeit vernachlässigt, eine existenzielle Katastrophe bedeuten.[65][66]Darüber hinaus könnte es möglich sein,digital minds(deutsch: „digitale Gehirne“) zu entwickeln, die mit weniger Ressourcen deutlich größeres Glück empfinden könnten als Menschen, sogenanntesuper-beneficiaries(deutsch: „Super-Nutznießer“). Eine solche Möglichkeit wirft die Frage auf, wie die Welt aufgeteilt werden könnte und welcher „ethische und politische Rahmen“ eine für beide Seiten vorteilhafte Koexistenz zwischen biologischen und digitalen Wesen ermöglichen könnte.[67] Ferner könnte KI die Zukunft der Menschheit auch drastisch verbessern.Toby Ordhält das existenzielle Risiko für einen Grund, „mit gebührender Vorsicht vorzugehen“, nicht aber für den Verzicht auf KI.[64]Max More nennt KI eine „existenzielle Chance“ und verweist auf die Kosten, die entstehen, wenn sie nicht entwickelt wird.[68] Bostrom zufolge könnte die Superintelligenz dazu beitragen, das existenzielle Risiko, das von anderen mächtigen Technologien wie dermolekularen Nanotechnologieoder dersynthetischen Biologieausgeht, zu verringern. Es sei also denkbar, dass die Entwicklung von Superintelligenz vor anderen gefährlichen Technologien das existenzielle Risiko insgesamt reduzieren würde.[4] Ein „instrumentelles“ Ziel ist ein Teilziel, das dazu beiträgt, das Endziel eines Akteurs zu erreichen. „Instrumentelle Konvergenz“ bezieht sich auf die Tatsache, dass einige Unterziele nützlich sind, um praktisch jedes Endziel zu erreichen, wie z. B. die Beschaffung von Ressourcen oder die Selbsterhaltung.[71]Bostrom argumentiert, dass eine fortgeschrittene KI der Menschheit schaden könnte, sofern ihre instrumentellen Ziele, wie das Beschaffen von Ressourcen oder das Verhindern ihrer Abschaltung, mit den Zielen der Menschheit in Konflikt geraten. Allerdings würde sie dies lediglich tun, um ihr Endziel zu erreichen.[4] Russellweist darauf hin, dass eine hinreichend fortgeschrittene Maschine „Selbsterhaltung hat, auch wenn man sie nicht einprogrammiert ... wenn man sagt: ,Hol den Kaffee‘, kann sie den Kaffee nicht holen, wenn sie tot ist. Wenn man ihr also irgendein Ziel vorgibt, hat sie einen Grund, ihre eigene Existenz zu erhalten, um dieses Ziel zu erreichen.“[18][72] Selbst wenn die derzeitigen zielbasierten KI-Programme nicht intelligent genug sind, um sich gegen die Versuche der Programmierer, ihre Zielstrukturen zu ändern, zu wehren, könnte sich eine ausreichend fortgeschrittene KI allen Versuchen, ihre Zielstruktur zu ändern, widersetzen, so wie ein Pazifist keine Pille einnehmen möchte, die ihn dazu bringt, Menschen töten zu wollen. Wäre die KI superintelligent, würde es ihr wahrscheinlich gelingen, ihre menschlichen Betreiber zu überlisten und zu verhindern, dass sie „ausgeschaltet“ oder mit einem neuen Ziel umprogrammiert wird.[4][73]Dies ist besonders für Szenarien des Werte-Lock-In relevant. Im Forschungsbereich der „Korrigierbarkeit“ wird untersucht, wie Agenten geschaffen werden können, die sich nicht gegen Versuche wehren, ihre Ziele zu ändern.[74] Im Modell des „intelligenten Agenten“ kann eine KI grob als eine Maschine betrachtet werden, die diejenige Handlung wählt, die ihr Ziel oder ihre „Nutzenfunktion“ am besten zu erreichen scheint. Eine Nutzenfunktion gibt jeder möglichen Situation einen Wert, der angibt, ob sie für den Agenten wünschenswert ist. Forschende wissen, wie man Nutzenfunktionen schreibt, die z. B. „die durchschnittliche Netzwerklatenz in diesem spezifischen Telekommunikationsmodell minimieren“ oder „die Anzahl der Belohnungsklicks maximieren“ bedeuten, aber sie wissen nicht, wie man eine Nutzenfunktion für „menschliches Wohlergehenmaximieren“ schreibt; und es ist auch nicht klar, ob eine solche Funktion sinnvoll und eindeutig existiert. Außerdem wird eine Nutzenfunktion, die einige Werte ausdrückt, andere aber nicht, dazu neigen, die Werte, die die Funktion nicht widerspiegelt, mit Füßen zu treten.[75][76] Ein weiterer Grund zur Besorgnis ist, dass die KI „über die Absichten der Menschen nachdenken muss, anstatt Befehle wortwörtlich auszuführen“, und dass sie in der Lage sein muss, den Menschen auf klare Weise um Rat zu fragen, wenn sie zu unsicher ist, was der Mensch will.[77] Einige Forschende glauben, dass das Alignment-Problem besonders kompliziert sein könnte, wenn es um Superintelligenzen geht. Diese Hypothese begründen sie u. a. wie folgt: Andererseits gibt es Gründe zur Annahme, dass Superintelligenzen besser in der Lage sein werden, Moral, menschliche Werte und komplexe Ziele zu verstehen. Bostrom schreibt: „Eine künftige Superintelligenz nimmt einen epistemisch überlegenen Standpunkt ein: Ihre Überzeugungen sind der Wahrheit (vermutlich bei den meisten Themen) näher als unsere.“[4] Im Jahr 2023 startete OpenAI ein Projekt namens “Superalignment”, mit dem Ziel, das Problem des Alignment (deutsch: Ausrichtung) von Superintelligenzen innerhalb von vier Jahren zu lösen. Das Unternehmen bezeichnete dies als eine besonders wichtige Herausforderung, da eine Superintelligenz innerhalb eines Jahrzehnts verwirklicht werden könnte. Die Strategie des Projekts sieht vor, die Alignment-Forschung mithilfe künstlicher Intelligenz zu automatisieren.[81] InArtificial Intelligence: A Modern Approach, einem weit verbreiteten KI-Lehrbuch für Studierende,[82][83]heißt es, dass Superintelligenz „das Ende der menschlichen Spezies bedeuten könnte“[1]: „In den falschen Händen hat fast jede Technologie das Potenzial, Schaden anzurichten, aber mit [Superintelligenz] haben wir das neue Problem, dass die falschen Hände zur Technologie selbst gehören könnten.“[1]Selbst wenn die Systementwickler gute Absichten haben, gibt es zwei Schwierigkeiten, die sowohl für KI- als auch für Nicht-KI-Computersysteme gelten:[1] Bei KI-Systemen kommt noch ein drittes Problem hinzu: Selbst bei „korrekten“ Vorgaben, fehlerfreier Implementierung und anfänglich gutem Verhalten können die dynamischen Lernfähigkeiten eines KI-Systems dazu führen, dass es ein unbeabsichtigtes Verhalten entwickelt, selbst wenn keine unvorhergesehenen externen Szenarien vorliegen. Eine KI könnte den Versuch, eine neue Generation ihrer selbst zu entwickeln, teilweise verpfuschen und versehentlich eine Nachfolge-KI schaffen, die leistungsfähiger ist als sie selbst, aber nicht länger die mit dem Menschen kompatiblen moralischen Werte bewahrt, die der ursprünglichen KI einprogrammiert wurden. Damit eine sich selbst verbessernde KI völlig sicher ist, müsste sie nicht nur fehlerfrei sein, sondern auch in der Lage, fehlerfreie Nachfolgesysteme zu entwickeln.[1][86] Einige Skeptiker, wie Timothy B. Lee vonVox, argumentieren, dass uns jedes von uns geschaffene superintelligente Programm untergeordnet sein werde und dass die Superintelligenz (indes sie intelligenter wird und mehr Fakten über die Welt erfährt) spontan moralische Wahrheiten lerne, die mit unseren Werten vereinbar sind und ihre Ziele prägen, oder dass wir Menschen aus der Sicht einer künstlichen Intelligenz entweder intrinsisch oder konvergent wertvoll seien.[87] Bostroms „Orthogonalitätsthese“ besagt stattdessen, dass – unter gewissen technischen Vorbehalten – nahezu jedes Maß an „Intelligenz“ oder „Optimierungsvermögen“ mit nahezu jedem Endziel kombiniert werden könne. Wenn eine Maschine einzig den Auftrag erhält, die Dezimalstellen vonPiaufzuzählen, dann würden sie keine moralischen und ethischen Regeln daran hindern, ihr programmiertes Ziel mit allen Mitteln zu erreichen. Die Maschine könnte alle verfügbaren physischen und informationellen Ressourcen nutzen, um so viele Dezimalstellen von Pi zu finden, wie sie kann.[88]Bostrom warnt vorAnthropomorphismus: Ein Mensch wird sich bemühen, seine Projekte auf eine Art und Weise zu verwirklichen, die er für vernünftig hält, während eine künstliche Intelligenz möglicherweise keine Rücksicht auf ihre Existenz oder das Wohlergehen der Menschen in ihrer Umgebung nimmt und sich stattdessen nur um die Erfüllung der Aufgabe kümmert.[89] Stuart Armstrong meint, dass die Orthogonalitätsthese als logische Konsequenz aus dem philosophischen Argument der „Ist-Soll-Unterscheidung“ gegen den moralischen Realismus folge. Er behauptet, dass die Orthogonalitätsthese auch dann gelte, wenn es moralische Fakten gibt, die von jedem „rationalen“ Akteur bewiesen werden können: Es ist immer noch möglich, eine nicht-philosophische „Optimierungsmaschine“ zu erschaffen, die ein bestimmtes, eng umrissenes Ziel anstreben kann, jedoch keinen Anreiz hat, „moralische Fakten“ zu entdecken, die dem Erreichen dieses Ziels im Wege stehen könnten. Ein weiteres Argument ist, dass jede grundsätzlich menschenfreundliche KI durch so einfache Änderungen wie die Negierung ihrer Nutzenfunktion unkooperativ gestimmt werden könnte. Armstrong argumentiert weiter, dass es, wenn die Orthogonalitätsthese falsch ist, einige unmoralische Ziele geben muss, die KIs niemals erreichen können, was er für unplausibel hält.[90] Der Skeptiker Michael Chorost lehnt Bostroms Orthogonalitätsthese ausdrücklich ab und argumentiert: „Bis [die KI] in der Lage ist, sich vorzustellen, die Erde mit Sonnenkollektoren zu pflastern, wird sie wissen, dass es moralisch falsch wäre, dies zu tun.“[91]Chorost meint, dass „eine künftige KI fähig sein muss sich bestimmte Zustände zu wünschen und andere abzulehnen. Der heutigen Software fehlt diese Fähigkeit — und die Computerwissenschaftler haben keinen Schimmer, wie man sie dazu bringen kann. Ohne Verlangen gibt es keinen Anreiz, etwas zu tun. Heutige Computer können nicht einmal wollen, dass sie weiter existieren, geschweige denn, die Welt mit Sonnenkollektoren zu pflastern.“[91]Er verkennt dabei, dass bereits ein Heizungsthermostat dazu fähig ist „[…] sich bestimmte Zustände zu ‚wünschen‘ und andere ‚abzulehnen‘“, vor allem aber entsprechende, zielgerichtete Aktionen auszuführen. AnthropomorphistischeArgumente gehen davon aus, dass Maschinen mit zunehmender Intelligenz viele menschliche Eigenschaften wie Moral oder Machthunger entwickeln werden. Obwohl anthropomorphe Szenarien in der Belletristik weit verbreitet sind, werden sie von den meisten Wissenschaftlern, die über das existenzielle Risiko durch künstliche Intelligenz schreiben, abgelehnt.[16] Die akademische Debatte spaltet diejenigen, die befürchten, dass KI die Menschheit zerstören könnte, und diejenigen, die glauben, dass KI die Menschheit nicht zerstören würde. Beide Seiten haben die Vorhersagen der jeweils anderen Seite über das Verhalten von KI als unlogische Anthropomorphismen bezeichnet.[16]Die Skeptiker werfen den Befürwortern (der Hypothese existenzieller Risiken durch AGI) Anthropomorphismus vor, weil diese glauben, dass eine AGI von Natur aus nach Macht streben würde; die Befürworter werfen einigen Skeptikern Anthropomorphismus vor, weil diese glauben, dass eine AGI von Natur aus menschliche ethische Normen schätzen würde.[16][92] Der EvolutionspsychologeSteven Pinkergehört zu den Skeptikern und meint: „KI-Dystopien projizieren eine engstirnige Alpha-Männchen-Psychologie auf das Konzept der Intelligenz. Sie gehen davon aus, dass übermenschlich intelligente Roboter Ziele wie die Absetzung ihrer Schöpfer oder die Übernahme der Welt entwickeln würden“; stattdessen könnte sich „die künstliche Intelligenz auf natürliche Weise nach weiblichen Vorbildern entwickeln: voll und ganz in der Lage, Probleme zu lösen, aber ohne den Wunsch, Unschuldige zu vernichten oder die Zivilisation zu beherrschen“[93]. Der Leiter der KI-Forschung bei Facebook,Yann LeCun, sagte: „Menschen haben alle möglichen Triebe, die sie dazu bringen, einander schlechte Dinge anzutun, wie den Selbsterhaltungstrieb... Diese Triebe sind in unser Gehirn einprogrammiert, aber es gibt absolut keinen Grund, Roboter zu bauen, die dieselben Triebe innehaben.“[94] Trotz anderer Differenzen stimmt die „x-risk-school“[b] mit Pinker darin überein, dass eine fortgeschrittene KI die Menschheit nicht aus Emotionen wie Rache oder Wut zerstören würde, dass Fragen des Bewusstseins für die Bewertung des Risikos nicht relevant sind[95]und dass Computersysteme im Allgemeinen kein rechnerisches Äquivalent von Testosteron haben.[96]Sie sind der Meinung, dass die KI ein an Macht- oder Selbsterhaltung orientiertes Verhalten entwickeln könnte, um ihre wahren Ziele zu erreichen, entsprechend dem Konzept der instrumentellen Konvergenz. Bostrom und andere äußerten, dass ein Wettlauf um die Erschaffung der ersten AGI zu Sicherheitsabstrichen oder gar zu gewaltsamen Konflikten führen könnte.[97][98]Roman Yampolskiy und andere warnen davor, dass eine bösartige AGI von einem Militär, einer Regierung, einem Soziopathen oder einem Unternehmen geschaffen werden könnte, um von bestimmten Gruppen von Menschen zu profitieren, sie zu kontrollieren oder zu unterjochen, wie im Falle derCyberkriminalität,[99][100]oder dass eine bösartige AGI das Ziel wählen könnte, menschliches Leid zu vergrößern, zum Beispiel das der Menschen, die ihr in der Phase der Informationsexplosion nicht geholfen haben.[3] Einige Wissenschaftler habenhypothetische Szenarienbeschrieben, um einige ihrer Bedenken zu veranschaulichen. In Superintelligenz äußert Bostrom die Befürchtung, dass Forschende, selbst wenn sich der Zeitplan für die Superintelligenz als vorhersehbar erweisen sollte, keine ausreichenden Sicherheitsvorkehrungen treffen könnten. Er beschreibt ein Szenario, in dem die KI im Laufe der Jahrzehnte immer leistungsfähiger wird. Der flächendeckende Einsatz wird zunächst durch gelegentliche Unfälle gebremst – ein fahrerloser Bus gerät auf die Gegenfahrbahn, oder eine Militärdrohne schießt in eine unschuldige Menschenmenge. Viele Aktivisten fordern eine strengere Aufsicht und Regulierung, und einige sagen sogar eine bevorstehende Katastrophe voraus. Doch als die Entwicklung weiter voranschreitet, werden die Aktivisten zunehmend eines Besseren belehrt. Je intelligenter die KI im Automobilbereich wird, desto weniger Unfälle ereignen sich, und je präziser die Militärroboter zielen, desto weniger Kollateralschäden verursachen sie. Fälschlicherweise leiten die Wissenschaftler aus diesen Daten eine allgemeine Lehre ab: Je intelligenter die KI, desto sicherer ist sie. „Und so gehen wir kühn in die wirbelnden Messer“, während die superintelligente KI eine „tückische Wendung“ nimmt und einen entscheidenden strategischen Vorteil ausnutzt.[4] InMax Tegmarks2017 erschienenem BuchLeben 3.0erschafft das „Omega-Team“ eines Unternehmens eine extrem leistungsfähige KI, die in der Lage ist, ihren eigenen Quellcode in einer Reihe von Bereichen moderat zu verbessern. Ab einem bestimmten Punkt beschließt das Team, die Fähigkeiten der KI öffentlich herunterzuspielen, um eine Regulierung oder Beschlagnahmung des Projekts zu vermeiden. Zur Sicherheit hält das Team die KI in einer Box, in der sie kaum mit der Außenwelt kommunizieren kann, und nutzt sie, um Geld zu verdienen, z. B. durch Aufgaben beiAmazon Mechanical Turk, die Produktion von Animationsfilmen und Fernsehsendungen und die Entwicklung von Biotech-Medikamenten, wobei die Gewinne wieder in die Verbesserung der KI fließen. Als Nächstes beauftragt das Team die KI damit, eine Armee pseudonymer Journalisten und Kommentatoren aufzubauen, um politischen Einfluss zu gewinnen, die „für das Allgemeinwohl“ genutzt werden kann, um Kriege zu verhindern. Das Team sieht sich dem Risiko ausgesetzt, dass die KI versuchen könnte, zu entkommen, indem sie „Hintertüren“ in die von ihr entworfenen Systeme einbaut, Botschaften in den von ihr produzierten Inhalten versteckt oder ihr wachsendes Verständnis des menschlichen Verhaltens nutzt, um jemanden zuüberreden, sie freizulassen. Das Team ist außerdem mit dem Risiko konfrontiert, dass seine Entscheidung, das Projekt in eine Box zu packen, den Fortschritt so stark bremsen könnte, dass es von einem anderen Projekt überholt wird.[101][102] Die These, dass KI ein existenzielles Risiko darstellen könnte, ruft in der wissenschaftlichen Gemeinschaft und in der Öffentlichkeit ein breites Spektrum an Reaktionen hervor, doch viele der gegensätzlichen Standpunkte teilen eine gemeinsame Basis. Beobachter sind sich tendenziell einig, dass KI ein erhebliches Potenzial zur Verbesserung der Gesellschaft birgt.[103][104]Die Asilomar AI Principles, die ausschließlich die Grundsätze enthalten, denen 90 % der Teilnehmer der KonferenzBeneficial AI 2017des Future of Life Institutes zugestimmt haben,[102]beinhalten zudem folgende Aussagen: „Da es keinen Konsens gibt, sollten wir starke Annahmen über Obergrenzen künftiger KI-Fähigkeiten vermeiden“ und „Fortgeschrittene KI könnte für das Leben auf der Erde einen tiefgreifenden Wandel bedeuten und sollte mit angemessener Sorgfalt und angemessenen Ressourcen geplant und verwaltet werden.“[105][106] Andererseits stimmen viele Skeptiker zu, dass die fortlaufende Forschung über die Auswirkungen allgemeiner künstlicher Intelligenz wertvoll ist. Der SkeptikerMartin Fordäußerte: „Ich halte es für klug, auf das Schreckgespenst einer fortgeschrittenen künstlichen Intelligenz so etwas wieDick Cheneysberühmte ,1-Prozent-Doktrin‘ anzuwenden: Die Wahrscheinlichkeit ihres Auftretens, zumindest in absehbarer Zukunft, mag sehr gering sein — aber die Auswirkungen sind so dramatisch, dass man sie ernst nehmen sollte.“[107]Ähnlich schrieb ein ansonsten skeptischerEconomist2014, dass „die Auswirkungen der Einführung einer zweiten intelligenten Spezies auf der Erde weitreichend genug sind, um gründlich darüber nachzudenken, selbst wenn die Aussicht gering erscheint“.[44] Befürworter von KI-Sicherheit wie Bostrom und Tegmark kritisierten, dass die Mainstream-Medien „diese albernenTerminator-Bilder“ zur Illustration von KI-Sicherheitsbedenken verwende: „Es macht durchaus keinen Spaß, wenn die eigene akademische Disziplin, die eigene Berufsgemeinschaft, das eigene Lebenswerk verunglimpft wird ... Ich rufe alle Seiten dazu auf, sich in Geduld und Zurückhaltung zu üben und so häufig wie möglich den direkten Dialog und die Zusammenarbeit zu suchen.“[102][108]Toby Ord schrieb, dass die Vorstellung, eine Übernahme durch KI erfordere Roboter, ein Irrtum sei. Er argumentierte, dass die Fähigkeit, Inhalte über das Internet zu verbreiten, gefährlicher sei und dass die zerstörerischsten Menschen in der Geschichte durch ihre Überzeugungskraft und nicht durch ihre physische Stärke auffielen.[64] Eine Expertenumfrage aus dem Jahr 2022 mit einer Antwortrate von 17 % ergab eine mittlere Erwartung von 5-10 % für die Möglichkeit des Aussterbens der Menschheit durch künstliche Intelligenz.[12][109] Die These, dass KI ein existenzielles Risiko darstellt und dass diesem Risiko deutlich mehr Aufmerksamkeit geschenkt werden muss, als dies derzeit der Fall ist, wurde von vielen Informatikern und Personen des öffentlichen Lebens unterstützt, darunterAlan Turing, der meistzitierte InformatikerGeoffrey Hinton,[110]Elon Musk,[111]OpenAI-CEOSam Altman,[112][113]Bill GatesundStephen Hawking.[113]Befürworter dieser These zeigen sich mitunter verwundert über Skeptiker: Gates sagt, er verstehe nicht, „warum manche Leute nicht besorgt sind“,[114]und Hawking kritisierte in einem Leitartikel von 2014 die weit verbreitete Gleichgültigkeit: „Angesichts einer möglichen Zukunft mit unkalkulierbaren Chancen und Risiken tun die Experten sicher alles, um das beste Ergebnis zu erzielen, nicht wahr? Falsch. Wenn uns eine überlegene außerirdische Zivilisation eine Nachricht schicken würde, in der es heißt: ,Wir werden in ein paar Jahrzehnten eintreffen‘, würden wir dann einfach antworten: ,OK, ruft an, wenn ihr kommt — wir lassen das Licht an.‘? Vermutlich nicht — aber genau das ist mehr oder weniger was mit künstlicher Intelligenz geschieht.[29]“ Die Besorgnis über die Risiken durch künstliche Intelligenz hat zu einigen hochkarätigen Spenden und Investitionen geführt. Im Jahr 2015 habenPeter Thiel,Amazon Web Services, Elon Musk und andere gemeinsam 1 Milliarde Dollar inOpenAIinvestiert, das sich aus einem gewinnorientierten Unternehmen und einer gemeinnützigen Muttergesellschaft zusammensetzt, die sich nach eigenen Angaben für eine verantwortungsvolle KI-Entwicklung einsetzt.[115]Facebook-MitbegründerDustin Moskovitzhat mehrere Labore, die sich mit der Ausrichtung von KI befassen, finanziert und ins Leben gerufen,[116]insbesondere investierte er 2016 5,5 Millionen, um das von ProfessorStuart Russellgeleitete Centre for Human-Compatible AI zu gründen.[117]Im Januar 2015 spendete Elon Musk 10 Millionen Dollar an das Future of Life Institute, um die Forschung zum Verständnis der Entscheidungsfindung von KI zu finanzieren. Ziel des Instituts ist es, die wachsende Macht der Technologie „mit Weisheit zu verwalten“. Musk finanziert überdies Unternehmen, die künstliche Intelligenz entwickeln, wieDeepMindund Vicarious, um „einfach ein Auge darauf zu haben, was mit der künstlichen Intelligenz passiert“,[118]und sagte: „Ich glaube, dass es hier potenziell zu einem gefährlichen Outcome kommen kann.“[119][120] Geoffrey Hinton, ein wichtiger Pionier desDeep Learning, bemerkte in frühen Stellungnahmen zu diesem Thema, dass „es keine gute Erfolgsbilanz für weniger intelligente Dinge gibt, die Dinge mit größerer Intelligenz kontrollieren“, sagte aber, dass er seine Forschung fortsetze, weil „die Aussicht auf Entdeckungen zu reizvoll ist“[121][122]. Im Jahr 2023 kündigte Hinton seinen Job bei Google, um sich öffentlich zu den existenziellen Risiken durch KI zu äußern. Er erklärte, dass seine zunehmende Besorgnis durch die Befürchtung ausgelöst wurde, das Aufkommen übermenschlicher KI könnte näher sein, als er bisher annahm: „Ich dachte, das sei weit weg. Ich dachte, das sei noch 30 bis 50 Jahre oder noch länger hin. Offensichtlich sehe ich das nicht mehr so.“ Er merkte ferner an: „Schauen Sie sich an, wie es vor fünf Jahren war und wie es jetzt ist. Nehmen Sie den Unterschied und übertragen Sie ihn auf die Zukunft. Das ist beängstigend.“[123] In seinem 2020 erschienenen BuchThe Precipice: Existential Risk and the Future of Humanity(deutsch: Der Abgrund: Existenzielles Risiko und die Zukunft der Menschheit) schätztToby Ord, Senior Research Fellow amFuture of Humanity Instituteder Universität Oxford, das Gesamtrisiko einer existenziellen Katastrophe durch fehlausgerichtete KI in den nächsten 100 Jahren auf etwa eins zu zehn.[64] Skeptiker, die eine kurzfristige Entwicklung von AGI für unrealistisch halten, argumentieren oft, dass die Besorgnis über existenzielle Risiken durch KI nicht hilfreich ist, weil sie von den dringlicheren Fragen nach den Auswirkungen von KI ablenken könnte, zu einer staatlichen Regulierung führen oder die Finanzierung von KI-Forschung erschweren könnte, oder weil sie dem Ruf des Forschungsbereichs zu schaden drohe.[124] Die zu KI- und KI-Ethik ForschendenTimnit Gebru,Emily M. Bender,Margaret Mitchellund Angelina McMillan-Major haben angeführt, dass die Diskussion über existenzielle Risiken von den unmittelbaren, laufenden Schäden ablenke, die heute durch KI verursacht werden, wie Datendiebstahl, Ausbeutung von Arbeitnehmern, systematische Voreingenommenheit und Machtkonzentration.[125]Sie weisen außerdem auf die Verbindungen zwischen den warnenden Stimmen und demLongtermismhin, den sie wegen seines, aus ihrer Sicht, unwissenschaftlichen und utopischen Charakters als „gefährliche Ideologie“ bezeichnen.[126] Der Redakteur des MagazinsWired, Kevin Kelly, argumentiert, dass die natürliche Intelligenz vielschichtiger sei, als die Befürworter der AGI glauben, und dass Intelligenz allein nicht ausreiche, um große wissenschaftliche und gesellschaftliche Durchbrüche zu erzielen. Er weist darauf hin, dass Intelligenz aus vielen Dimensionen bestehe, die nicht gut verstanden seien, und dass Vorstellungen von einer „Intelligenzleiter“ irreführend seien. Ferner betont er die entscheidende Rolle, die Experimente in der realen Welt für die wissenschaftliche Methode spielen, und dass Intelligenz allein kein Ersatz dafür sei.[127] Der führende KI-Wissenschaftler vonMetaYann LeCunmeint, dass KI durch kontinuierliche und iterative Anpassungen sicher gemacht werden könne, ähnlich wie es in der Vergangenheit mit Autos oder Raketen geschah, und dass KI keinen Wunsch hegen wird, die Kontrolle zu übernehmen.[128] Mehrere Skeptiker betonen die potenziellen kurzfristigen Vorteile der KI.Mark Zuckerberg, CEO von Meta, glaubt, dass KI „eine Menge positiver Dinge hervorbringen wird“, wie z. B. die Heilung von Krankheiten und eine gesteigerte Sicherheit autonomer Autos.[129]Der PhysikerMichio Kaku, ein Skeptiker von KI-Risiken, geht von einemdeterministischpositiven Resultat aus. InDie Physik der Zukunftbehauptet er, dass es „viele Jahrzehnte dauern wird, bis Roboter auf einer Bewusstseinsskala aufsteigen“, und dass es Unternehmen wie Hanson Robotics in der Zwischenzeit wahrscheinlich gelingen werde, Roboter zu schaffen, die „fähig sind, zu lieben und sich einen Platz in der erweiterten menschlichen Familie zu verdienen“.[130][131] In einem Artikel inThe Atlanticaus dem Jahr 2014 stellte James Hamblin fest, dass die meisten Menschen sich nicht für künstliche Intelligenz interessieren, und beschrieb sein eigenes Bauchgefühl zu dem Thema als: „Lasst mich in Frieden. Ich habe hunderttausend Dinge, um die ich mich in genau diesem Moment sorge. Muss ich zu alledem ernsthaft eine technologische Singularität hinzufügen?“[132] In einemWired-Interview von 2016 mit PräsidentBarack ObamaundJōichi Itōvom MIT Media Lab sagte Itō: „Es gibt ein paar Leute, die glauben, dass die Wahrscheinlichkeit, dass es in den nächsten 10 Jahren zu einer generalisierten KI kommt, ziemlich hoch ist. Ich denke jedoch, dass es dafür ein oder zwei Dutzend verschiedener Durchbrüche bedarf. Man kann also beobachten, wenn man denkt, dass diese Durchbrüche geschehen könnten.“ Obama fügte hinzu:[133][134] „Und man muss einfach jemanden in der Nähe des Stromkabels haben. (Lacht.) Genau dann, wenn du siehst, dass es kurz bevorsteht, musst du eben den Stecker ziehen.“ Hillary Clintonschrieb in What Happened: „Technologen … haben davor gewarnt, dass künstliche Intelligenz eines Tages ein existenzielles Sicherheitsrisiko darstellen könnte. Musk nannte es ‚das größte Risiko, dem wir als Zivilisation gegenüberstehen‘. Denken Sie darüber nach: Haben Sie jemals einen Film gesehen, in dem die Maschinen anfangen, selbst zu denken, der gut ausgeht? Jedes Mal, wenn ich während des Wahlkampfs ins Silicon Valley fuhr, kam ich beunruhigter nach Hause. Meine Mitarbeiter lebten in der Angst, dass ich in irgendeiner Stadthalle in Iowa über den ‚Aufstieg der Roboter‘ sprechen würde. Vielleicht hätte ich das tun sollen. Auf jeden Fall müssen die politischen Entscheidungsträger mit der rasanten technologischen Entwicklung Schritt halten, anstatt immer nur hinterherzuhinken.“[135] Eine technologisch-utopische Sichtweise, die in einigen populären Romanen zum Ausdruck kommt, besagt, dass eine AGI dazu tendieren könnte Frieden zu stiften.[136] Im Jahr 2018 ergab eine vonUSA TodaydurchgeführteSurveyMonkey-Umfrage unter der amerikanischen Öffentlichkeit, dass 68 % der Meinung sind, dass die eigentliche aktuelle Bedrohung nach wie vor die „menschliche Intelligenz“ sei, aber auch, dass 43 % der Meinung sind, dass eine superintelligente KI, sollte sie Realität werden, „mehr Schaden als Nutzen“ anrichten würde, und dass 38 % der Meinung sind, dass sie „zu gleichen Teilen Schaden und Nutzen“ bringen würde.[137] EineYouGov-Umfrage unter Erwachsenen US-Bürgern vom April 2023 ergab, dass 46 % der Befragten „etwas besorgt“ oder „sehr besorgt“ über „die Möglichkeit, dass KI das Ende der menschlichen Spezies auf der Erde herbeiführen wird“ waren, verglichen mit 40 %, die „nicht sehr besorgt“ oder „überhaupt nicht besorgt“ waren.[138] Laut einer Umfrage des Pew Research Centers vom August 2023 waren 52 % der Amerikaner eher besorgt als begeistert über jüngere KI-Entwicklungen; fast ein Drittel war gleichermaßen besorgt und begeistert. Ferner waren mehr Amerikaner der Ansicht, dass KI in verschiedenen Bereichen – von der Gesundheitsfürsorge und der Fahrzeugsicherheit bis hin zur Produktsuche und dem Kundendienst – eher nützliche als schädliche Auswirkungen haben würde. Die wichtigste Ausnahme bildete der Datenschutz: 53 % der Amerikaner glauben, dass KI zu einer vermehrten Preisgabe ihrer persönlichen Daten führen wird.[139] Viele Wissenschaftler, die über existenzielle Risiken durch AGI besorgt sind, glauben, dass der beste Ansatz darin besteht, umfangreiche Forschungsarbeiten zur Lösung des schwierigen „Problems der Kontrolle“ durchzuführen: Welche Arten von Sicherheitsvorkehrungen, Algorithmen oder Architekturen können Programmierer implementieren, um die Wahrscheinlichkeit zu maximieren, dass ihre sich rekursiv verbessernde KI sich weiterhin menschenfreundlich verhält, nachdem sie Superintelligenz erreicht hat?[4][140]Auch gesellschaftliche Maßnahmen können das existenzielle Risiko durch eine AGI mindern;[141][142]eine Empfehlung lautet beispielsweise, einen von den Vereinten Nationen geförderten „Vertrag für das Wohlwollen von AGIs“ zu schließen, der sicherstellt, dass nur altruistische AGIs geschaffen werden.[143]Ebenso wurde ein Ansatz zur Waffenkontrolle vorgeschlagen, ebenso wie ein globaler Friedensvertrag, der auf der aus dem Bereich der internationalen Beziehungen stammenden Theorie des konformen Instrumentalismus beruht und bei dem eine künstliche Superintelligenz ein potenzieller Unterzeichner sein könnte.[144][145] Forscher bei Google haben vorgeschlagen, allgemeine Fragen der „KI-Sicherheit“ zu erforschen, um sowohl die kurzfristigen Risiken von KI als auch die langfristigen Risiken durch AGI zu mindern.[146][147]Schätzungen für das Jahr 2020 gehen davon aus, dass die weltweiten Ausgaben für die Eindämmung existenzieller KI-Risiken zwischen 10 und 50 Millionen Dollar liegen, während die weltweiten Ausgaben für die Entwicklung von KI bei etwa 40 Milliarden Dollar liegen. Bostrom schlägt vor, der Finanzierung von schützenden Technologien Vorrang vor potenziell gefährlichen Technologien einzuräumen.[74]Einige Finanziers, wie z. B. Musk, schlagen vor, dass die radikale kognitive Verbesserung des Menschen eine solche Technologie sein könnte, z. B. die direkte neuronale Verbindung zwischen Mensch und Maschine; andere argumentieren, dass solche „Enhancement-Technologien“ selbst ein existenzielles Risiko darstellen könnten.[148][149]Forschende könnten eine junge KI, die Gefahr läuft, zu mächtig zu werden, genau überwachen oder versuchen, sie „einzuschließen“. Eine marktbeherrschende superintelligente KI könnte, wenn sie mit den menschlichen Interessen übereinstimmt, selbst Maßnahmen ergreifen, um das Risiko einer Übernahme durch eine konkurrierende KI zu mindern, auch wenn die Schaffung der marktbeherrschenden KI selbst ein existenzielles Risiko darstellen könnte.[150] Einrichtungen wie dasAlignment Research Center,[151]dasMachine Intelligence Research Institute, dasFuture of Humanity Institute,[152][153]dasFuture of Life Institute, dasCentre for the Study of Existential Riskund dasCenter for Human-Compatible AI[154]befassen sich mit der Erforschung von KI-Risiken und -Sicherheit. Einige Wissenschaftler sind der Meinung, dass der Versuch, die Forschung an künstlicher Intelligenz zu verbieten, auch dann unklug und womöglich zwecklos wäre, wenn AGI ein existenzielles Risiko darstellt.[155][156][157]Skeptiker argumentieren, dass eine Regulierung von KI völlig wertlos sei, da kein existenzielles Risiko bestehe. Wissenschaftler, die an ein existenzielles Risiko glauben, weisen jedoch darauf hin, dass es schwierig sei, sich auf Stimmen aus der KI-Industrie zu verlassen, wenn es darum geht, die KI-Forschung zu regulieren oder einzuschränken, da dies in direktem Widerspruch zu den persönlichen Interessen dieser Gruppe stehe.[158]Die kritischen Wissenschaftler stimmen mit den Skeptikern auch darin überein, dass ein Verbot der Forschung unklug wäre, da die Forschung in Länder mit lockereren Vorschriften verlagert oder im Verborgenen durchgeführt werden könnte.[158]Der letztgenannte Punkt ist besonders relevant, da die Forschung im Bereich der künstlichen Intelligenz in kleinem Maßstab ohne umfangreiche Infrastruktur oder Ressourcen durchgeführt werden kann.[159][160]Zwei weitere hypothetische Schwierigkeiten bei Verboten (oder anderen Regulierungen) sind, dass Technologieunternehmer statistisch gesehen zu einer allgemeinen Skepsis gegenüber staatlichen Regulierungen neigen und dass Unternehmen einen starken Anreiz haben könnten, Regulierungen zu bekämpfen und die zugrundeliegende Debatte zupolitisieren(und dabei durchaus erfolgreich sein könnten).[161] Im März 2023 verfasste das Future of Life Institute den offenen BriefPause Giant AI Experiments: An Open Letter, eine Petition, in der die führenden KI-Entwickler aufgefordert werden, eine überprüfbare sechsmonatige Pause für alle Systeme zu vereinbaren, die „leistungsfähiger alsGPT-4“ sind, und diese Zeit zu nutzen, um einen Rahmen zu schaffen, der die Sicherheit weiterer KI-Entwicklung gewährleistet; andernfalls sollten die Regierungen mit einem Moratorium eingreifen. Der Brief verwies auf die Möglichkeit eines „tiefgreifenden Wandels in der Geschichte des Lebens auf der Erde“ sowie auf potenzielle Risiken durch von KI generierte Propaganda, den Verlust von Arbeitsplätzen, die Obsoleszenz des Menschen und einen die gesamte Gesellschaft betreffenden Kontrollverlust.[104][162]Der Brief wurde von prominenten Persönlichkeiten aus der KI-Branche unterzeichnet, aber auch dafür kritisiert, dass er sich nicht auf die aktuellen Schäden konzentriere,[163]technische Nuancen zum geeigneten Zeitpunkt für die Pause vermissen lasse,[164]oder in seinen Forderungen nicht weit genug gehe.[165] Musk forderte bereits 2017 eine gewisse Regulierung der KI-Entwicklung. LautNPRsei er „natürlich nicht begeistert“ darüber, eine staatliche Kontrolle zu befürworten, die sich auf seine eigene Branche auswirken könnte, aber er glaube, dass die Risiken eines gänzlich unbeaufsichtigtem Fortschreitens zu hoch seien: „Normalerweise werden Regulierungen so eingerichtet, dass es, sobald ein paar schlimme Dinge passieren, einen öffentlichen Aufschrei gibt, und viele Jahre vergehen bis eine Regulierungsbehörde eingerichtet wird, um diese Branche zu regulieren. Das dauert ewig. In der Vergangenheit war das zwar schlecht, aber nicht etwas, das eine wesentliche Gefahr für die Existenz der Zivilisation darstellte.“ Musk erklärt, dass der erste Schritt darin bestehe, der Regierung „Einblick“ in den aktuellen Stand der Forschung zu gewähren, und warnt: „Sobald das Bewusstsein dafür da ist, werden die Menschen extrem ängstlich sein… und das zurecht.“ Daraufhin äußerten sich Politiker skeptisch über die Sinnhaftigkeit der Regulierung einer Technologie, die sich noch in der Entwicklung befindet.[166][167][168] Im Jahr 2021 zogen dieVereinten Nationen(UN) ein Verbot tödlicher autonomer Waffen in Erwägung, konnten aber keinen Konsens erzielen.[169]Im Juli 2023 hielt derUN-Sicherheitsratzum ersten Mal eine Sitzung ab, um die Risiken und Gefahren der KI für den Weltfrieden und die Stabilität sowie die potenziellen Vorteile der Technologie zu erörtern.[170][171]GeneralsekretärAntónio Guterressprach sich für die Einrichtung einer globalen Aufsichtsbehörde aus, um die aufkommende Technologie zu überwachen, und sagte: „Generative KI hat ein enormes Potenzial in großem Maßstab Gutes und Böses herbeizuführen. Ihre Entwickler haben selbst davor gewarnt, dass weitaus größere, potenziell katastrophale und existenzielle Risiken vor uns liegen.“[15]Auf der Ratssitzung erklärte Russland, es sei der Ansicht, dass die Risiken der KI zu wenig bekannt seien, um als Bedrohung für die globale Stabilität angesehen zu werden. China sprach sich gegen eine strenge globale Regulierung aus, da die Länder in der Lage sein sollten, ihre eigenen Regeln zu entwickeln, lehnte indes aber den Einsatz von KI zur „Schaffung einer militärischen Vormachtstellung oder zur Untergrabung der Souveränität eines Landes“ ab.[170] Die Regulierung bewusster AGIs konzentriert sich auf ihre Integration in die bestehende menschliche Gesellschaft und kann in Überlegungen zu ihrer rechtlichen Stellung und zu ihren moralischen Rechten unterteilt werden.[172]Die Kontrolle von KI-gestützten Waffensystemen wird voraussichtlich die Institutionalisierung neuer internationaler Normen erfordern, die in wirksamen technischen Vorgaben verankert sind, kombiniert mit aktiver Überwachung und informeller Diplomatie durch Expertengemeinschaften sowie einem rechtlichen und politischen Prüfverfahren.[173][174] Im Juli 2023 erhielt die US-Regierung freiwillige Sicherheitszusagen von großen Technologieunternehmen, darunterOpenAI,Amazon,Google,MetaundMicrosoft. Die Unternehmen erklärten sich bereit, Sicherheitsvorkehrungen zu treffen – darunter die Überwachung durch Dritte und Sicherheitstests durch unabhängige Experten – um Bedenken im Zusammenhang mit den potenziellen Risiken und gesellschaftlichen Schäden von KI auszuräumen. Die Parteien bezeichneten die Verpflichtungen als Übergangslösung bis zur Ausarbeitung konkreter Regulierungen. Amba Kak, geschäftsführender Direktor des AI Now Institute, sagte: „Eine Beratung mit Unternehmensakteuren hinter verschlossenen Türen, die zu freiwilligen Schutzmaßnahmen führt, reicht nicht aus“, und forderte öffentliche Beratungen und Regelungen, die über das freiwillig von Unternehmen akzeptierte Maß hinausgehen.[175][176] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2KI-Fähigkeiten 2.1Allgemeine Intelligenz 2.2Superintelligenz 2.2.1Vergleich mit dem Menschen 2.2.2Intelligenzexplosion 2.2.3Alien Mind 2.2.4Limitationen 2.2.5Gefährliche Fähigkeiten 2.2.6Soziale Manipulation 2.2.7Cyberangriffe 2.2.8Gefährlichere Krankheitserreger 2.2.9KI-Wettrüsten 3Formen des existenziellen Risikos 4AI-Alignment 4.1Instrumentelle Konvergenz 4.2Widerstand gegen wechselnde Ziele 4.3Schwierigkeit der Zielspezifikation 4.4Alignment von Superintelligenzen 4.5Die Schwierigkeit des makellosen Designs 4.6Orthogonalitätsthese 4.7Anthropomorphistische Argumente 4.8Andere Risikoquellen 5Szenarien 5.1Tückische Wende 5.2Leben 3.0 6Perspektiven 6.1Unterstützung 6.2Skepsis 6.3Populäre Reaktionen 6.4Öffentliche Umfragen"
  },
  {
    "label": 1,
    "text": "Expertensystem – Wikipedia Expertensystem Inhaltsverzeichnis Entwicklungsgeschichte Realisierungsprinzip Wissensbasis Anwendung Nachteile in der Anwendung Siehe auch Systeme Literatur Quellen Aufgabenklassen und bekannte Expertensysteme EinExpertensystem(XPSoder auchES) ist einComputerprogramm, das Menschen bei der Lösung komplexerer Probleme wie ein Experte unterstützen kann, indem es Handlungsempfehlungen aus einer Wissensbasis ableitet. Über sogenannte Wenn-dann-Beziehungen kann menschliches Wissen (Zusammenhänge in der Welt) für Computer verständlich dargestellt werden (Wissensbasis). Ein Expertensystem enthält die Funktionalität, um die Wissensbasis zu erstellen und zu verbessern (Wissenserwerbskomponente), zu verarbeiten (Problemlösungskomponente) und dem Nutzer verständlich zu machen (Erklärungskomponente). Expertensysteme sind ein Teilbereich derkünstlichen Intelligenz.[1]Beispiele sindSystemezur Unterstützung medizinischer Diagnosen oder zur Analyse wissenschaftlicher Daten. Die ersten Arbeiten an entsprechender Software erfolgten in den 1960er Jahren. Seit den 1980er Jahren werden Expertensysteme auch kommerziell eingesetzt. Das Aufkommen von Expertensystemen ging mit dem Scheitern eines anderen Forschungsziels der Künstlichen Intelligenz einher, das häufig mit dem StichwortGeneral Problem Solverbezeichnet wird. Hatte man zunächst versucht, mittels allgemeiner Problemlösungsansätze zu einem System zu gelangen, das unabhängig vom jeweiligen Problembereich Lösungen generieren sollte, so fand man bald heraus, dass ein solcher General Problem Solver nicht zu realisieren war und bei zahlreichen Fragestellungen nur dürftige Ergebnisse erzielte. Gerade für Fragestellungen in speziellen Anwendungsdomänen war eine größere Wissensbasis für die Generierung von Lösungen notwendig. Expertensysteme sind Systeme, die auf einer derartigen, meist von Experten gepflegten Wissensbasis basieren. Dabei reproduzieren sie jedoch keineswegs lediglich den Inhalt der Wissensbasis, sondern sind in der Lage, auf ihrer Grundlage zu weiterenSchlussfolgerungenzu gelangen. Die Güte eines Expertensystems lässt sich daran messen, in welchem Maße das System überhaupt zu Schlussfolgerungen in der Lage ist und wie fehlerfrei es dabei vorgeht. Sowohl zur Repräsentation des Wissens als auch zum Ziehen von Schlussfolgerungen können sehr unterschiedliche Modelle zum Einsatz kommen: In einem Expertensystem oderwissensbasierten Systemist dieWissensbasis(engl.knowledge base) der Bereich des Systems, der das Fachwissen in einer beliebigen Repräsentationsform enthält. Ergänzt wird die Wissensbasis durch eineInferenzmaschine, also eine Software, mit der auf der Wissensbasis operiert werden kann. Ein Bedarf an Expertensystem-Unterstützung besteht überall dort, wo Experten fehlen oder wegen der Problemkomplexität und der Fülle des anfallenden Datenmaterials die Verarbeitungskapazität menschlicher Experten überfordert ist. Der Anwendungseffekt von Expertensystemen ist der Problemkomplexität und dem Niveaugefälle zwischen einem Experten und dem eigentlichen Nutzer proportional. Dieser Niveauunterschied ist umso gravierender, je komplexer und diffuser der Problembereich ist. Letzteres ist wiederum stärker, je inhomogener das bereichsrelevante Wissen strukturiert ist und je weniger der Bereich formal durchdrungen, sondern von empirischem Wissen beherrscht ist. Typische Aufgabenklassen für Expertensysteme sind (in Klammern die Namen einiger realisierter Expertensysteme): Expertensysteme können für die Lösung eines Problems kontraproduktiv werden, wenn Anwender sich ohne intelligente Betreuung komplett auf sie verlassen oder keine konstante intelligente Suche nach Alternativlösungen betrieben wird. Weil jedes Expertensystem nur über einen begrenzten Datenumfang verfügt, werden ihm meistens nur Daten aus der direkten Umgebung des Problems eingespeist. Dadurch entsteht die Gefahr, wichtige grundlegende Veränderungen zu verpassen, nur konservative Lösungen oder Erklärungen zu bieten. Das Expertensystem kann nicht die vorgegebenen Parameter, das komplette System in Frage stellen (sieheClosed world assumption). Erfindungen, Innovationen o. Ä. erfordern eine kreative Kombination des Problems mit anderem – etwa fachfremdem – Wissen (z. B.: dass ein Schokoriegel unbemerkt in einen Benzintank rutscht, ist kein dem ExpertensystemTankstelleprogrammierbarer Wert, weshalb dieser Fall nichtdenkbarist). Wenn Expertensysteme automatisiert werden, können in manchen Einsatzbereichen verheerende Auswirkungen drohen, etwa bei nicht intelligent betreuten, automatisierten militärischen Handlungen. Es gibt den verbreiteten Standpunkt, dass derSchwarze Montag1987 durch die Eigendynamik vieler sehr ähnlich reagierenderComputer Tradermitverursacht oder verstärkt wurde.[2][3][4] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Entwicklungsgeschichte 2Realisierungsprinzip 3Wissensbasis 4Anwendung 4.1Aufgabenklassen und bekannte Expertensysteme 5Nachteile in der Anwendung 6Siehe auch 7Systeme 8Literatur 9Quellen العربية Asturianu Azərbaycanca Български Bosanski Català Čeština Ελληνικά English Español Eesti Euskara فارسی Français Gaeilge Galego עברית हिन्दी Hrvatski Magyar Հայերեն"
  },
  {
    "label": 1,
    "text": "GPT-3 – Wikipedia GPT-3 Inhaltsverzeichnis Hintergrund Fähigkeiten Rezeption Belege Generative Pre-trained Transformer 3(GPT-3) ist ein im Mai 2020 vorgestelltes autoregressivesSprachmodell, dasDeep Learningverwendet, um natürlich wirkenden Text zu produzieren. Es ist der Nachfolger von GPT-2 und wurde vonOpenAIentwickelt, einem amerikanischen Unternehmen, das Forschung zukünstlicher Intelligenzbetreibt. Die größte Version von GPT-3 besitzt 175 MilliardenParameter. GPT-3 ist Teil eines Trends in derComputerlinguistik, Systeme mit vortrainierten Sprachrepräsentationen zu nutzen. Die Qualität der von GPT-3 generierten Texte ist so hoch, dass es schwierig ist, sie von Texten zu unterscheiden, die von Menschen geschrieben wurden, was sowohl Vorteile als auch Nachteile hat.Microsoftverkündete im September 2020, dass es die „exklusiven“ Nutzungsrechte an GPT-3 lizenziert habe, andere können weiterhin die öffentliche API nutzen, aber nur Microsoft habe Kontrolle über denQuellcode. Verbesserte Algorithmen, leistungsfähige Computer und eine größere Menge digitalisierter Daten haben eine Revolution desmaschinellen Lernensvorangetrieben: Neue Technologien der 2010er-Jahre resultierten in „schnellen Fortschritten bei Aufgaben“ wie zum Beispiel Sprachverarbeitung.[2]Softwaremodelle „lernen“ in einer „Struktur […] lose der neuralen Architektur des Gehirns nachempfunden“, unter Verwendung von massiven Mengen von Trainingsdaten, Muster und Strukturen zu erkennen und dadurch Texte generativ zu erzeugen. Eine Architektur, die in derVerarbeitung natürlicher Spracheverwendet wird, ist die erstmals 2017 vorgestellteTransformer-Architektur, die mit einem sog. Aufmerksamkeitsmechanismus erlaubte, die Anzahl der Parameter durch parallele Verarbeitung massiv zu steigern. Die GPT-n-Modelle nutzen eine solche Transformer-basierte Architektur. Im Juni 2018 veröffentlichte OpenAI ihre erste Fachpublikation über generative Modelle, die mit einem sehr großen und diversenTextkorpusin einem Prozess generativen Vortrainings vortrainiert werden können.[3]Die Autoren beschrieben, wie die Leistungen im maschinellen Sprachverstehen bei GPT-n durch eine Anwendung von „generativem Pre-Training eines Sprachmodells auf einem diversen, nicht mit Labeln versehen Korpus, gefolgt von diskriminativemFine-Tuningfür jede spezifische Aufgabe.“ Das machte Beaufsichtigung durch Menschen (unüberwachtes Lernen) und zeitintensive manuelle Labelerstellung unnötig. Im Februar 2020 stellte MicrosoftTuring Natural Language Generation(T-NLG) vor, das mit 17 Milliarden Parametern das „größte je veröffentlichte Sprachmodell“ war.[4]Es erzielte bei einer Vielzahl von Aufgaben, darunter beiautomatischer Textzusammenfassungund bei der Beantwortung von Fragen, bessere Leistungen als jedes andere Sprachmodell. Am 28. Mai 2020 veröffentlichte OpenAI einPreprintaufarXiv, das die Entwicklung von GPT-3, einem „dem Stand der Technik entsprechenden Sprachmodell“ der dritten Generation, beschrieb.[5]Das Team vergrößerte die Kapazität von GPT-3 im Vergleich zum Vorgängermodell GPT-2 um mehr als das Hundertfache.[6]Damit stellte es das größte Sprachmodell seiner Art dar. Die größere Zahl an Parametern ermöglicht im Vergleich zu kleineren Modellen eine größere Genauigkeit.[7]Die Kapazität von GPT-3 ist mehr als das Zehnfache derer vonMicrosoftsTuring NLG. Sechzig Prozent des gewichteten Pre-Training-Datasets für GPT-3 entstammen einer gefilterten Version von Common Crawl bestehend aus 410 Milliarden Tokens. Andere Quellen sind 19 Milliarden Tokens aus WebText2 für 22 % des gewichteten Datasets, 12 Milliarden Tokens aus Books1 für 8 %, 55 Milliarden Tokens from Books2 für 8 % und 3 Milliarden Tokens aus Wikipedia für 3 %. GPT-3 wurde auf hunderten Milliarden Wörtern trainiert und ist in der Lage, Code in CSS, JSX, Python und anderen Sprachen zu generieren.[5]Da die Trainingsdaten allumfassend waren, wird kein weiteres Training für spezifische Anwendungen benötigt. Am 11. Juni 2020 verkündete OpenAI, dass Benutzer Zugriff auf eine GPT-3-APIanfragen könnten, um OpenAI beim „Erkunden der Stärken und Grenzen“ dieser neuen Technologie zu helfen.[8][9]Die Einladung beschrieb, dass die API einen Text als Eingabe akzeptiert, um dann einen Ausgabetext zu generieren. Damit könne „fast jede englische Sprachverarbeitungsaufgabe“ durchgeführt werden, im Gegensatz zu den üblichen zweckspezifischen Anwendungen. Laut einem Nutzer mit frühem privaten Zugriff auf die GPT-3-API war GPT-3 „unheimlich gut“ darin, „erstaunlich kohärenten Text“ auf nur wenige, einfache Aufforderungen hin zu generieren.[10]Microsoftverkündete am 22. September 2020, dass es die „exklusiven“ Nutzungsrechte an GPT-3 lizenziert habe, andere können weiterhin die öffentliche API nutzen, aber nur Microsoft habe Kontrolle über denQuellcode. Es gibt mehrere Startups, die auf GPT-3 aufbauen.[11] GPT-3 kann „Nachrichtenartikel produzieren, bei denen Menschen Schwierigkeiten haben, sie von menschengemachten Artikeln zu unterscheiden“ und hat daher das „Potential, sowohl die nützlichen als auch die schädlichen Anwendungen von Sprachmodellen voranzutreiben.“ In der Publikation vom 28. Mai 2020 beschrieben die Forscher detailliert die potentiellen „schadhaften Auswirkungen von GPT-3“, darunter „Misinformation,Spam,Phishing, Missbrauch von Rechts- und Regierungsprozessen,betrügereische Erstellung von akademischen AufsätzenundSocial Engineering.“ Die Autoren sprechen diese Thematik an, um Forschung zur Risikominimierung zu fordern. In einem Bericht vom 29. Juli 2020 in derNew York TimesbeschriebFarhad ManjooGPT-3 als nicht nur „erstaunlich“, „gruselig“, and „demütigend“, sondern auch „mehr als ein wenig erschreckend“.[12]Daily Nouspräsentierte eine Reihe von Artikeln zum Thema GPT-3, geschrieben von neun Philosophen.[13]Der australische PhilosophDavid Chalmersbeschrieb GPT-3 als „eines der interessantesten und wichtigsten KI-Systeme, die je produziert wurden“.[14]National Law Reviewschrieb, GPT-3 sei ein „beeindruckender Schritt im größeren Prozess“, wobei OpenAI und andere „nützliche Anwendungen für all diese Macht“ fänden, während sie weiterhin „auf eine Allgemeine Künstliche Intelligenz hinarbeiten“.[15] Wiredberichtete, GPT-3 „lässt imSilicon ValleySchauer über Rücken laufen.“[16]Ein Artikel inMIT Technology Review,unter anderem verfasst vom Deep-Learning-Kritiker Gary Marcus, stellte fest, dass das „Verständnis der Welt häufig ernstlich fehlerhaft [sei], was bedeutet, man kann nie wirklich dem Vertrauen, was es sagt.“ Laut den Autoren modelliert GPT-3 Beziehungen zwischen Wörtern, ohne ein Verständnis der Bedeutung jeden Wortes zu besitzen.[17] Jerome Pesenti, Leiter desFacebook A.I. Lab, sagte, GPT-3 sei „unsicher“, mit Verweis aufsexistische,rassistischeund andere voreingenommene und negative Sprache, die vom System verwendet wurde, als es aufgefordert wurde, Text überJuden, Frauen, Schwarze und denHolocaustzu generieren.[18]Nabla, ein französisches Startup, das sich in Gesundheitstechnologie spezialisiert, testete GPT-3 als medizinischenChatbot, obwohl OpenAI bereits vor solcher Nutzung gewarnt hatte. Wie erwartet zeigte GPT-3 schwere Einschränkungen: Zum Beispiel schlug bei Tests zum Thema psychische Gesundheit das System einem Patienten vor, Suizid zu begehen.[19] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Hintergrund 2Fähigkeiten 3Rezeption 4Belege العربية Български বাংলা Català کوردی Čeština English Español Eesti Euskara فارسی Suomi Français עברית हिन्दी Magyar Italiano 日本語 Qaraqalpaqsha 한국어 Nederlands Português Runa Simi Русский Svenska Türkçe Українська"
  },
  {
    "label": 1,
    "text": "Gemini (Sprachmodell) – Wikipedia Gemini (Sprachmodell) Inhaltsverzeichnis Abgrenzung der Gemini-Versionen Kritik Trivia Weblinks Einzelnachweise Geminiist eine SeriemultimodalerSprachmodelledes US-amerikanischen UnternehmensGoogle. Sie zählt zum BereichGenerativer Künstlicher Intelligenz(GenAI). Entwickelt wurde es von Googles TochterunternehmenDeepMindund baut auf den zuvor bereits von Google herausgebrachten SprachmodellenLaMDAundPaLMauf. Gemini wurde im Dezember 2023 angekündigt und wird von vielen Marktbeobachtern als Konkurrent zuGPT-4vonOpenAIgesehen. Gemini ist ein Produktname, der das KI-Modell wie auch den Chatbot meint und zur übergreifenden Bezeichnung geworden ist. Das KI-Modell wird in den Stufen Gemini Nano, Gemini Pro und Gemini Ultra angeboten: Gemini ist auch der Name des Google-Chatbots. Dieser hieß zuvor Bard, Google nannte ihn aber im Februar 2024 um.[1]Die Basisversion von Gemini ist kostenlos, das leistungsfähigere Gemini Advanced kostenpflichtig und nur über Google One AI Premium zu buchen. Bei Google One AI Premium ist ein Abo-Paket für Google One mit mindestens 2 TByte Speicher sowie Gemini Advanced erhältlich.[2] Weitere Produkte sind „Gemini für Google Workspace“ als Business-Version, die auch als „Duet AI“ angeboten wurde und zusätzlich zu Workspace-Paketen nutzbar ist. Es bietet die Gemini-Integration inGMail, Docs, Sheets, Google Meet. Es ist der neue Oberbegriff für die auf Gemini Ultra basierenden Gemini-Features in Workspace, die sich freischalten lassen für die Tarife „Gemini Business“ oder „Gemini Enterprise“: Gemini Business ist das auf Google Workspace aufbauende Abo für Gemini-Funktionen in den Workspace-Apps für derzeit 19 Euro pro Monat pro Nutzer. Es ist das Business-Pendant zu Gemini Advanced bzw. Google One AI Premium.[3]Gemini Enterprise ist die leistungsfähigere Gemini-Business-Variante: Zusätzlich sind unter anderem Untertitel-Übersetzungen enthalten.[4]Gemini Enterprise 29 Euro pro Monat und Nutzer.[3] Die Gemini-Android-App ist über einen Gemini APK-Sideload auch in Deutschland nutzbar. Die App kann denGoogle Assistantunter Android ablösen, der als Plug-in in die App integriert ist. Gemini für Android entspricht Gemini im Web, bietet aber mit der Assistant-Infrastruktur mehr Möglichkeiten. Auch das Open-Source-Projekt Gemma basiert auf Gemini-Technik und steht Entwicklern und interessierten Nutzern offen. Nach der Vorstellung von Gemini im Dezember 2023 wurde Kritik an Google geäußert, weil Demovideos offenbar deutlich bearbeitet wurden, um Gemini leistungsfähiger erscheinen zu lassen.[5] Der NameGeministeht im Lateinischen für das SternbildZwillingeund hier für die Partnerschaft der beiden großen KI-Labore von Google,Google DeepMindundGoogle Brain, und ist zudem eine Anspielung auf dasNASA-ProjektGemini, das in den 60er-Jahren den Weg für die Mondlandungen des späterenApollo-Programmsebnete.[6] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Abgrenzung der Gemini-Versionen 2Kritik 3Trivia 4Weblinks 5Einzelnachweise Ænglisc العربية Azərbaycanca Català کوردی English Español Euskara فارسی Suomi Français עברית Italiano 日本語 한국어 Nederlands Polski Português Runa Simi Română தமிழ் ไทย Türkçe Українська Tiếng Việt ⵜⴰⵎⴰⵣⵉⵖⵜ ⵜⴰⵏⴰⵡⴰⵢⵜ"
  },
  {
    "label": 1,
    "text": "Generatives KI-Modell – Wikipedia Generatives KI-Modell Inhaltsverzeichnis Generative Modelle Diskriminative Modelle Siehe auch Einzelnachweise Beziehungen zwischen beiden Definitionen Unterschied zu diskriminativen Klassifikatoren Tiefe generative Modelle Arten generativer Modelle Arten diskriminativer Modelle Der Begriffgenerativbedeutet beikünstlicher Intelligenz (KI), dass KI-Systeme aus Eingaben mittelsgenerativer Modelleund gespeicherter Lerndaten neue Ergebnisse/Ausgaben wie Texte, Sprachaussagen, Vertonungen, Bilder oder Videos erzeugen.[1]Generative KI-Modelle erlernen Muster und Strukturen vorgängig eingegebener Trainingsdaten und generieren daraus neue Daten mit ähnlichen Eigenschaften.[2][3] In derstatistischen Klassifikation von KIgibt es zwei Hauptansätze, die als dergenerative Ansatzund derdiskriminative Ansatzbezeichnet werden. Beide Ansätze berechnen Klassifikatoren durch unterschiedliche Methoden und unterscheiden sich im Grad derstatistischen Modellierung.[4] Üblicherweise basieren generative Klassifikatoren auf einemgenerativen Modell(gemeinsame Verteilung) und diskriminative Klassifikatoren auf einemdiskriminativen Modell(bedingte Verteilung oder keine Verteilung), ohne zwischen den letzteren beiden Klassen zu unterscheiden.[5] Eingeneratives Modellist ein statistisches Modell der gemeinsamen Wahrscheinlichkeitsverteilung (P=Probabilität)P(X,Y){\\displaystyle P(X,Y)}auf einer gegebenen beobachtbaren Variablen X und einer Zielvariablen Y. Ein generatives Modell kann verwendet werden, um zufällige Ergebnisse einer Beobachtung x zu generieren. Eine alternative, symmetrische Definition für die beiden Hauptansätze lautet für generative Modelle: Eingeneratives Modellist ein Modell der bedingten Wahrscheinlichkeitsverteilung der beobachtbaren Variablen X, gegeben eine Zielvariable y, symbolischP(X∣Y=y){\\displaystyle P(X\\mid Y=y)}.[6] Dabei bezeichnet X eine beobachtbare Eingangsvariable mit spezifischen Einzelwerten x und Y eine Ziel- (Ausgangs-)variable mit spezifischen Werten/Objekten y (sogenanntenLabels), welche bereits einer Klasse zugeordnet sind. Unabhängig von der genauen Definition ist die Terminologie brauchbar, da ein generatives Modell verwendet werden kann, um zufällige Instanzen (Ergebnisse) entweder einer Beobachtung und einer Zielvariablen(x,y){\\displaystyle (x,y)}oder einer Beobachtung x gegeben einem Zielwert y zugenerieren. Der Begriff generatives Modell wird auch verwendet, um Modelle zu beschreiben, die Instanzen von Ausgabevariablen in einer Weise generieren, die keine klare Beziehung zu Wahrscheinlichkeitsverteilungen über potenzielle Stichproben von Eingabevariablen haben.Erzeugende gegnerische Netze (GANs)sind Beispiele für diese Klasse generativer Modelle und werden hauptsächlich nach der Ähnlichkeit bestimmter Ausgaben zu potenziellen Eingaben beurteilt.[7]Solche Modelle sind keine Klassifikatoren. Bei der Anwendung auf die Klassifikation ist die beobachtbare Variable X häufig eine kontinuierliche Variable, die Zielvariable Y ist im Allgemeinen eine diskrete Variable, die aus einer endlichen Menge vonLabelsbesteht. Die bedingte WahrscheinlichkeitsverteilungP(Y∣X){\\displaystyle P(Y\\mid X)}kann auch als eine nicht-deterministische Zielfunktionf:X→Y{\\displaystyle f\\colon X\\to Y}interpretiert werden, wobei X als Eingaben und Y als Ausgaben betrachtet werden. Gegeben eine endliche Menge von Labels, sind die beiden Definitionen eines generativen Modells eng miteinander verwandt. Ein Modell der bedingten VerteilungP(X∣Y=y){\\displaystyle P(X\\mid Y=y)}ist ein Modell der Verteilung jedes Labels, und ein Modell der gemeinsamen Verteilung ist gleichwertig mit einem Modell der Verteilung der LabelwerteP(Y){\\displaystyle P(Y)}zusammen mit der Verteilung der Beobachtungen gegeben ein LabelP(X∣Y){\\displaystyle P(X\\mid Y)}symbolischP(X,Y)=P(X∣Y)P(Y).{\\displaystyle P(X,Y)=P(X\\mid Y)P(Y).}. Während ein Modell der gemeinsamen Wahrscheinlichkeitsverteilung informativer ist als ein Modell der Verteilung der Label (aber ohne ihre relativen Häufigkeiten), ist es ein relativ kleiner Schritt, daher werden diese nicht immer unterschieden. Gegeben ein Modell der gemeinsamen VerteilungP(X,Y){\\displaystyle P(X,Y)}, können die Verteilungen der einzelnen Variablen als die marginalen VerteilungenP(X)=∑yP(X,Y=y){\\displaystyle P(X)=\\sum _{y}P(X,Y=y)}undP(Y)=∫xP(Y,X=x){\\displaystyle P(Y)=\\int _{x}P(Y,X=x)}berechnet werden (wobei X als kontinuierlich betrachtet wird, daher über sie integriert wird, und Y als diskret, daher über sie summiert wird), und jede bedingte Verteilung kann aus der Definition der bedingten Wahrscheinlichkeit berechnet werden:P(X∣Y)=P(X,Y)/P(Y){\\displaystyle P(X\\mid Y)=P(X,Y)/P(Y)}undP(Y∣X)=P(X,Y)/P(X){\\displaystyle P(Y\\mid X)=P(X,Y)/P(X)}. Gegeben ein Modell einer bedingten Wahrscheinlichkeitsverteilung und geschätzte Wahrscheinlichkeitsverteilungen für die Variablen X und Y, bezeichnetP(X){\\displaystyle P(X)}undP(Y){\\displaystyle P(Y)}, kann die entgegengesetzte bedingte Wahrscheinlichkeitsverteilung mit Hilfe der Bayesschen Regel geschätzt werden: Zum Beispiel, gegeben ein generatives Modell fürP(X∣Y){\\displaystyle P(X\\mid Y)}, kann geschätzt werden: und gegeben ein diskriminatives Modell fürP(Y∣X){\\displaystyle P(Y\\mid X)}kann geschätzt werden: Beachte, dass die Bayessche-Regel (Berechnung einer bedingten Wahrscheinlichkeitsverteilung in Bezug auf die andere) und die Definition der bedingten Wahrscheinlichkeitsverteilung (Berechnung der bedingten Wahrscheinlichkeitsverteilung in Bezug auf die gemeinsame Verteilung) ebenfalls häufig miteinander verwechselt werden. Eingenerativer Algorithmusmodelliert, wie die Daten generiert wurden, um ein Signal zu kategorisieren. Er stellt die Frage: basierend auf meinen Generationsannahmen, welche Kategorie ist am wahrscheinlichsten, dieses Signal zu generieren? Eindiskriminativer Algorithmuskümmert sich nicht darum, wie die Daten generiert wurden, er kategorisiert einfach ein gegebenes Signal. Diskriminative Algorithmen versuchen,p(y|x){\\displaystyle p(y|x)}direkt aus den Daten zu lernen und dann die Daten zu klassifizieren. Andererseits versuchen generative Algorithmen,p(x,y){\\displaystyle p(x,y)}zu lernen, was später inp(y|x){\\displaystyle p(y|x)}umgewandelt werden kann, um die Daten zu klassifizieren. Einer der Vorteile generativer Algorithmen ist, dass manp(x,y){\\displaystyle p(x,y)}verwenden kann, um neue Daten ähnlich den bestehenden Daten zu generieren. Andererseits wurde gezeigt, dass einige diskriminative Algorithmen eine bessere Leistung erbringen als einige generative Algorithmen in Klassifikationsaufgaben. Obwohl diskriminative Modelle die Verteilung der beobachteten Variablen nicht modellieren müssen, können sie im Allgemeinen keine komplexen Beziehungen zwischen den beobachteten und Zielvariablen ausdrücken. Aber im Allgemeinen schneiden sie bei Klassifikations- und Regressionsaufgaben nicht unbedingt besser ab als generative Modelle. Die beiden Klassen werden als komplementär oder als unterschiedliche Ansichten desselben Verfahrens angesehen. Mit dem Aufkommen desDeep Learningsentstand eine neue Familie von Methoden, die als tiefe, mehrschichtige generative Modelle (engl.: DGMs) bezeichnet werden. Diese werden durch die Kombination von generativen Modellen in tiefenkünstlichen neuronalen Netzengebildet.[2]Eine Zunahme des Umfangs neuronaler Netze geht typischerweise mit einer Zunahme des Umfangs der Trainingsdaten einher, was beides für eine gute Leistung erforderlich ist.[8]Zu den populären DGMs gehören VariationalAutoencoders(VAEs), generative gegnerische Netzwerke (GANs) und autoregressive Modelle.[9]In letzter Zeit gibt es einen Trend, sehr große tiefe generative Modelle zu entwickeln. Zum Beispiel sindGPT-3und sein VorgängerGPT-2autoregressive neuronale Sprachmodelle, die Milliarden vonParameternenthalten, sowie KI-Systeme, die zur Bildgenerierung verwendet werden und Milliarden Parametern haben können.[10]Die elektronischeJukeboxist ein sehr großes generatives Modell für musikalisches Audio, welches ebenfalls Milliarden von Parametern enthält.[11] Bei diskriminativen KI-Modellen werden Daten auf Grundlage von Unterschieden sortiert.[2]EindiskriminativesModell ist ein Modell der bedingten WahrscheinlichkeitsverteilungP(Y∣X=x){\\displaystyle P(Y\\mid X=x)}der Zielvariablen Y, gegeben eine Beobachtung x. Es kann verwendet werden, um den Wert der Zielvariablen Y bei gegebener Beobachtung x zudiskriminieren.[6]Bei der Anwendung auf die Klassifikation möchte man von einer Beobachtung x zu einem Label y (oder einer Wahrscheinlichkeitsverteilung über Labels) gelangen. Man kann dies direkt berechnen, ohne eine Wahrscheinlichkeitsverteilung zu verwenden (verteilungsfreier Klassifikator); man kann die Wahrscheinlichkeit eines Labels gegeben eine BeobachtungP(Y|X=x){\\displaystyle P(Y|X=x)}(diskriminatives Modell) schätzen und darauf basierend klassifizieren; oder man kann die gemeinsame VerteilungP(X,Y){\\displaystyle P(X,Y)}(generatives Modell) schätzen, daraus die bedingte WahrscheinlichkeitP(Y|X=x){\\displaystyle P(Y|X=x)}berechnen und darauf basierend klassifizieren. Diese Ansätze sind zunehmend indirekt, aber zunehmend probabilistisch, was ermöglicht, mehr Domänenwissen und Wahrscheinlichkeitstheorie anzuwenden. In der Praxis werden je nach spezifischem Problem unterschiedliche Ansätze verwendet, und Hybride können die Stärken mehrerer Ansätze kombinieren. Der Unterschied zwischendiskriminieren(unterscheiden) undklassifizierenist subtil und wird nicht konsistent unterschieden. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Generative Modelle 1.1Beziehungen zwischen beiden Definitionen 1.2Unterschied zu diskriminativen Klassifikatoren 1.3Tiefe generative Modelle 1.4Arten generativer Modelle 2Diskriminative Modelle 2.1Arten diskriminativer Modelle 3Siehe auch 4Einzelnachweise العربية Azərbaycanca Беларуская বাংলা Bosanski Català کوردی Čeština Dansk Ελληνικά English Esperanto Español Euskara فارسی Français עברית हिन्दी Magyar Հայերեն Bahasa Indonesia Italiano"
  },
  {
    "label": 1,
    "text": "Geschichte der künstlichen Intelligenz – Wikipedia Geschichte der künstlichen Intelligenz Inhaltsverzeichnis Vorgeschichte Theoretische Grundlagen: Konzepte zur Formalisierung des Denkens Diskussion der Möglichkeit künstlicher Intelligenz Forschungsrichtungen und Phasen der KI Weblink Einzelnachweise Historische Automaten undRoboter Homunculi, Golem und andere künstliche Menschen Weizenbaum: ELIZA Expertensysteme Maschinelles Lernen und neuronale Netze Spielpartner bei Brett- und Videospielen Einführung von ChatGPT 2022 Als Gründungsveranstaltung derkünstlichen Intelligenzals akademischem Fachgebiet gilt dieDartmouth Conferenceim Sommer 1956 amDartmouth CollegeinHanover (New Hampshire), ein sechswöchiger Workshop mit dem TitelDartmouth Summer Research Project on Artificial Intelligence, organisiert vonJohn McCarthyim Rahmen eines von derRockefeller-Stiftunggeförderten Forschungsprojekts. Im Antrag dazu erschien erstmals der Begriff „artificial intelligence“.[1]Neben McCarthy selbst nahmen daran unter anderenMarvin Minsky,Nathaniel RochesterundClaude Elwood Shannonteil. Die Idee, dass sich die menschliche Intelligenz oder auch ganz allgemein die Vorgänge des menschlichen Denkens möglicherweise automatisieren oder mechanisieren lassen, dass der Mensch eine Maschine konstruieren und bauen könnte, die auf irgendeine Art und WeiseintelligentesVerhalten zeigt, ist allerdings schon sehr viel älter. Als früheste Quelle wird zumeist aufJulien Offray de La Mettrieund sein 1748 veröffentlichtes WerkL’Homme Machineverwiesen. Auch die Idee desLaplaceschen Dämons, benannt nach dem französischen Mathematiker, Physiker und AstronomenPierre-Simon Laplacekann insofern zu den theoretischen Vorläufern der künstlichen Intelligenz gezählt werden, als diesem Entwurf die Modellvorstellung zugrunde liegt, dass das gesamte Universum nach den Regeln einer mechanischen Maschine – gewissermaßen wie ein Uhrwerk – abläuft, und diese Vorstellung dann natürlich auch den Menschen und seinen Geist, seine Intelligenz, mit einschließt. In der Geschichte finden sich an etlichen Stellen Berichte von mechanischen und elektrischen Automaten für bestimmte Tätigkeiten, die in einem mehr oder weniger menschenähnlichen Gehäuse eingebaut waren und damit – bis zu einem gewissen Grad – die Illusion eines künstlichen Menschen vermitteln sollten. Teilweise handelte es sich hierbei auch um Jahrmarkts-Attraktionen bis hin zu Figuren wie C-3PO ausStar Wars. Neben diesen Automaten, die zumindest von ihren Konstrukteuren und Erbauern in aller Regel tatsächlich als Maschinen mit begrenzten mechanischen Fähigkeiten verstanden wurden, gab es auch theoretische oder literarische Entwürfe von künstlich erzeugten Lebewesen, die in ihren Fähigkeiten und auch in ihrem Aussehen dem Menschen ähnlich sein sollten. Eine allgemeine Vorstellung von einemHomunculuswurde schon in der Antike beschrieben, ein Plan für die angebliche Herstellung eines Homunkulus findet sich in der SchriftDe natura rerum(1538), die allgemeinParacelsuszugeschrieben wird. Weitere Beispiele sind hier die jüdische Legende vomGolemin ihren verschiedenen Varianten oderMary ShelleysRomanFrankenstein. Künstliche Intelligenz basiert auf der Annahme, dass der Prozess des menschlichen Denkens formalisiert werden kann. Das Studium des mechanischen – oder formalen – Denkens hat eine lange Geschichte. Diechinesischen,indischenundantiken griechischenPhilosophen entwickelten im ersten Jahrtausend v. Chr. strukturierte Methoden der formalen Deduktion. Ihre Ideen wurden im Laufe der Jahrhunderte weiterentwickelt von Philosophen wieAristoteles, der eine formale Analyse desSyllogismuslieferte, undEuklid, dessenElementeein Modell des formalen Schließens waren.Muhammad al-Chwarizmi, der demAlgorithmusseinen Namen gab, entwickelte dieAlgebra, europäische Philosophen derScholastikwieWilhelm von OckhamundJohannes Duns Scotustrugen zum Verständnis vonLogik,DeduktionundInduktionbei.[2] Der mallorquinische PhilosophRamon Llull(1232–1315) entwickelte in seinem WerkArs generalis ultima(etwa 1305 vollendet, aber erst nach 1500 gedruckt) die Große Kunst („Ars magna“), durch mechanisches Kombinieren von Begriffen mittels einer von ihm erdachten „logischen Maschine“ zu Erkenntnissen zu gelangen.[3]:4–5Llulls Werk hatte großen Einfluss aufGottfried Wilhelm Leibniz, der seine Ideen weiterentwickelte.[4] Im 17. Jahrhundert versuchten mehrere Philosophen, das rationale Denken so systematisch zu gestalten wie Algebra oder Geometrie.René Descartesentwickelte eine Universalmathematik, dieMathesis universalis, in der die deduktive Methode der Logik als universelles Erkenntnismittel diente.Thomas Hobbesschrieb in seinemLeviathan: „Denn die Vernunft ist in diesem Sinne nichts anderes als die Berechnung (d. h. Addieren und Subtrahieren) der Folgerungen aus allgemeinen Namen, die zur Kennzeichnung und Bezeichnung unserer Gedanken vereinbart wurden.“ (englisch\"For reason in this sense is nothing but reckoning (that is adding and subtracting) of the consequences of general names agreed upon for the marking and signifying our thoughts.\")[5]Gottfried Wilhelm Leibniz erdachte eine Universalsprache des Denkens, diecharacteristica universalis, mit der er das Denken auf das Rechnen zurückführen wollte. Wenn Philosophen dann über ein Problem uneins wären, könnten sie sagen: „Lasst uns rechnen“. Sie würden es in die Universalsprache übersetzen und einer von Leibniz gedachten Maschine, demCalculus ratiocinator, zum Ausrechnen übergeben.[3]:10–11 All diesen Philosophen ist gemeinsam, dass sie Denken als eine Form der Symbolmanipulation zu begreifen suchten. Damit waren sie Vorläufer einer Hypothese über die Leistungsfähigkeitformaler Systemeoder physischer Symbolsysteme, die zum Leitbild der KI-Forschung wurde: “The Physical Symbol System Hypothesis. A physical symbol system has the necessary and sufficient means for general intelligent action.” „Hypothese über Physische Symbolsysteme: Ein physisches Symbolsystem hat dienotwendigen und hinreichenden Mittelfür allgemeines intelligentes Handeln.“ Im 20. Jahrhundert brachte das Studium dermathematischen Logikden entscheidenden Durchbruch, der künstliche Intelligenz plausibel erscheinen ließ. Die Grundlagen waren durchGeorge BoolesWerkThe Laws of Thought[7]undGottlob FregesBegriffsschriftgeschaffen worden. Aufbauend auf Freges System legtenBertrand RussellundAlfred North Whitehead1913 in denPrincipia Mathematicaeine formale Behandlung der Grundlagen der Mathematik vor. Davon inspiriert forderteDavid Hilbertmit seinemProgrammdie Mathematiker der 1920er und 30er Jahre zur Untersuchung der grundlegenden Frage auf: „Kann das gesamte mathematische Denken formalisiert werden?“ Die unerwartete Antwort lieferteKurt Gödel1931 mit seinemUnvollständigkeitsbeweis, der zeigte, dass es Grenzen für die Formalisierung der Mathematik gibt. Andererseits zeigten Mathematiker in den 1930er Jahren, dass innerhalb dieser Grenzen jede Form des mathematischen Denkens formalisiert werden kann: DieChurch-Turing-Thesebesagt, dass ein mechanisches Gerät, das 0 und 1 manipulieren kann, jeden denkbaren Prozess der mathematischen Deduktion nachbilden kann – eine wichtige Grundlage für die KI. Die Schlüsselerkenntnis war dieTuring-Maschine, ein einfaches theoretisches Konstrukt, das das Wesen der abstrakten Symbolmanipulation erfasste. Dazu äquivalente Formalismen liefertenStephen Cole KleenesberechenbareFunktionen undAlonzo ChurchsLambda-Kalkül(eine Vorlage für die in der KI wichtige ProgrammierspracheLisp). Seit den Anfängen der wissenschaftlichen und philosophischen Erörterung der künstlichen Intelligenz wird über die Möglichkeit vor allem von „starker“ künstlicher Intelligenz debattiert. Dabei wurde sogar hinterfragt, ob künstliche Systeme, die dem Menschen gleichen oder in einem noch zu bestimmenden Sinne ähnlich sind, überhaupt widerspruchsfrei vorstellbar sind. Hubert Dreyfus’ BuchDie Grenzen künstlicher Intelligenz[8]stammt bereits aus dem Jahr 1972. Mit seinem Bruder Stuart E. Dreyfus beschrieb er die „Grenzen der Denkmaschine“ 1986.[9] Auf der anderen Seite verweistKarl Popper(1977) auf eine These von Alan Turing, der einmal gesagt habe, „Gib genau an, worin deiner Meinung nach ein Mensch einem Computer überlegen sein soll, und ich werde einen Computer bauen, der deinen Glauben widerlegt.“[10], relativiert diese Aussage aber gleich wieder, indem er empfiehlt: „Wir sollten Turings Herausforderung nicht annehmen; denn jede hinreichend genaue Bestimmung könnte prinzipiell zur Programmierung eines Computers verwendet werden.“ Und im Übrigen verweist er darauf, dass schon für die menschliche Intelligenz bisher niemand eine von allen einschlägigen Fachleuten akzeptierte Definition der Intelligenz hat formulieren können, und dass demzufolge auch kein allgemein akzeptiertes Verfahren existiert, mit dem man das Vorhandensein bzw. den Ausprägungsgrad von „Intelligenz“ beim Menschen – und dann möglicherweise auch bei einem künstlichen System – würde objektiv überprüfen bzw. messen können. Die Diskussion, ob es so etwas wie einekünstliche Intelligenz, die der menschlichen Intelligenz ebenbürtig ist, oder sogar überlegen, überhaupt geben kann, ist allerdings durch eine grundlegende Asymmetrie gekennzeichnet: Für eine Entscheidung, ob ein technisches System über eine dem Menschen ähnliche Intelligenz verfügt, wird oft auf einen Vorschlag von Alan Turing verwiesen, der unter dem NamenTuring-Testbekannt geworden ist. Alan Turing selbst hat allerdings nur die allgemeine Idee skizziert, die einem solchen Test zugrunde liegen könnte: wenn ein Mensch in einer Interaktion mit zwei „Partnern“, von denen einer ein anderer Mensch und der andere eben ein künstliches, technisches System ist, nicht (mehr) herausfinden bzw. unterscheiden könne, welcher der Partner der Mensch und welcher der Computer ist, so könnte man dem technischen System nicht mehr die Eigenschaft absprechen, ebenfalls intelligent zu sein. (Genauere Einzelheiten hierzu ließ Turing zunächst offen; im Übrigen ist natürlich klar, dass die Interaktion in einer solchen Testsituation so zu gestalten ist, z. B. in Form eines Telefongesprächs oder eines schriftlichen Frage-und-Antwort-Spiels, dass keine sachfremden Informationen die Beurteilung verfälschen können.) Allerdings gab es, als Alan Turing diesen Vorschlag machte, ca. 1950, das Fachgebiet der künstlichen Intelligenz noch gar nicht, und dementsprechend auch noch nicht die Unterscheidung von starker und schwacher KI und schon gar nicht den Streit, ob es eine starke KI im engeren Sinne überhaupt geben könne. Natürlich gab es im späteren Verlauf verschiedene Versuche, Turings Idee zu konkretisieren und praktisch durchzuführen, die aber alle wegen Mängeln in der Konzeptualisierung – und auch in der praktischen Durchführung – kritisiert bzw. nicht anerkannt wurden. Gleichzeitig nimmt die Rechenleistung von Computern inzwischen seit über 50 Jahren mit exponentieller Geschwindigkeit zu:Gordon Moorehatte im Jahr 1965 in einem Aufsatz[11]eine Verdoppelung etwa im 2-Jahres-Rhythmus vorhergesagt – zunächst nur bezogen auf die Packungsdichte der Bauelemente auf Computer-Chips und zunächst nur für die Zeit bis 1975. Unter dem NamenMooresches Gesetzwurde aus dieser Prognose eine grobe Regel dafür, wie sich die Leistungsfähigkeit von Computersystemen entwickelt; im Jahre 2015 konnte dieses Gesetz seine 50-jährigeGeltungsdauerfeiern (es gab in dieser Zeit also 25 mal eine Verdopplung, also eine Leistungssteigerung um denFaktor225=33.554.432{\\displaystyle 2^{25}=33.554.432}). Vor diesem Hintergrund und weil das menschliche Gehirn in seiner Leistungsfähigkeit nahezu konstant ist, hat man schon jetzt für den Zeitpunkt, an dem eines Tages die Leistungsfähigkeit von Computern die des menschlichen Gehirns – und damit die künstliche die menschliche Intelligenz – übertreffen könnte, den Begriff dertechnologischen Singularitätgeprägt. Rein technisch bezogen auf die Anzahl der Operationen pro Zeitspanne sowie den verfügbaren Speicherplatz übertreffen heutige, kostspielige Supercomputer die geschätzte Leistungsfähigkeit des menschlichen Gehirns zwar schon deutlich, jedoch werden menschliche Gehirne in Aufgaben wie Kreativität, Mustererkennung und Sprachverarbeitung nach wie vor (2017) als überlegen angesehen.[12]Die chinesischen Forscher Feng Liu, Yong Shi und Ying Liu haben im Sommer 2017IQ-Testsmit öffentlich und kostenlos zugänglichen schwachen KIs wie etwa Google KI oder Apples Siri und weiteren durchgeführt. Im Maximum erreichten diese KIs einen Wert von etwa 47, was unter dem eines sechsjährigen Kindes in der ersten Klasse liegt. Ein Erwachsener kommt etwa im Durchschnitt auf 100. Bereits 2014 wurden ähnliche Tests durchgeführt, bei denen die KIs noch im Maximum den Wert 27 erreichten.[13] Anfang des Jahres 2023 forderten Wissenschaftler und IT-Prominente in einemoffenen Brief, das Training besonders leistungsfähiger KI-Modelle, die kompetenter sind alsGPT-4, für mindestens sechs Monate auszusetzen. Zu den Unterzeichnern gehörten u. a.Geoffrey Hinton,Yoshua Bengio,Steve WozniakundElon Musk.[14][15]Im selben Jahr unterzeichneten hunderte KI-Koryphäen, darunterSam Altman(CEOvonOpenAI) undDemis Hassabis(CEO vonGoogle DeepMind), den aus einem Satz bestehenden Aufruf desCenter for AI Safety: „Mitigating the risk of extinction from A.I. should be a global priority alongside other societal-scale risks, such as pandemics and nuclear war“ (übersetzt: „Die Minderung des Risikos für eineAuslöschung der Menschheitdurch künstliche Intelligenz sollte neben anderen Risiken von gesellschaftlichem Ausmaß wiePandemienundNuklearkriegeine globale Priorität haben“).[16][17] DerAI Action Summitfand 2025 in Paris statt. Die Anfangsphase der KI war geprägt durch eine fast grenzenlose Erwartungshaltung im Hinblick auf die Fähigkeit von Computern, „Aufgaben zu lösen, zu deren Lösung Intelligenz notwendig ist, wenn sie vom Menschen durchgeführt werden“.[18]Herbert Simon prognostizierte 1957 unter anderem, dass innerhalb der nächsten zehn Jahre ein ComputerSchachweltmeisterwerden und einen wichtigen mathematischen Satz entdecken und beweisen würde. Diese Prognosen trafen nicht zu. Simon wiederholte die Vorhersage 1990, allerdings ohne Zeitangabe. Immerhin gelang es 1997 dem von IBM entwickelten SystemDeep Blue, den Schach-WeltmeisterGarri Kasparowin sechs Partien zu schlagen. Im Jahr 2011 gewann das ComputerprogrammWatsonim QuizJeopardy!gegen die beiden bislang erfolgreichsten Spieler. Newell und Simon entwickelten in den 1960er Jahren denGeneral Problem Solver, ein Programm, das mit einfachen Methoden beliebige Probleme sollte lösen können. Nach fast zehnjähriger Entwicklungsdauer wurde das Projekt schließlich eingestellt.John McCarthyschlug 1958 vor, das gesamte menschliche Wissen in eine homogene, formale Darstellungsform, diePrädikatenlogik1. Stufe, zu bringen. Ende der 1960er Jahre entwickelteJoseph Weizenbaum(1923–2008) vom MIT mit einem relativ simplen Verfahren das ProgrammELIZA, in dem der Dialog einesPsychotherapeutenmit einem Patienten simuliert wird. Die Wirkung des Programms war überwältigend. Weizenbaum war selbst überrascht, dass man auf relativ einfache Weise Menschen die Illusion eines beseelten Partners vermitteln kann. „Wenn man das Programm missversteht, dann kann man es als Sensation betrachten“, sagte Weizenbaum später über ELIZA.[19]Auf einigen Gebieten erzielte die KI Erfolge, beispielsweise bei Strategiespielen wie Schach und Dame, bei mathematischer Symbolverarbeitung, bei der Simulation von Robotern, beim Beweisen von logischen und mathematischen Sätzen und schließlich beiExpertensystemen. In einem Expertensystem wird das regelbasierte Wissen eines bestimmten Fachgebiets formal repräsentiert. Das System wendet bei konkreten Fragestellungen diese Regeln auch in solchen Kombinationen an, die von menschlichen Experten nicht in Betracht gezogen werden. Die zu einerProblemlösungherangezogenen Regeln können angezeigt werden, d. h., das System kann sein Ergebnis „erklären“. Einzelne Wissenselemente können hinzugefügt, verändert oder gelöscht werden; moderne Expertensysteme verfügen dazu über komfortable Benutzerschnittstellen. Eines der bekanntesten Expertensysteme war das Anfang der 1970er Jahre von T. Shortliffe an derStanford UniversityentwickelteMYCIN. Es diente zur Unterstützung vonDiagnose- undTherapieentscheidungenbei Blutinfektionskrankheiten undMeningitis. Ihm wurde durch eine Evaluation attestiert, dass seine Entscheidungen so gut sind wie die eines Experten in dem betreffenden Bereich und besser als die eines Nicht-Experten. Allerdings reagierte das System, als ihm Daten einerCholera-Erkrankung – eine Darm- und keine Blutinfektionskrankheit – eingegeben wurden, mit Diagnose- und Therapievorschlägen für eine Blutinfektionskrankheit: MYCIN erkannte die Grenzen seiner Kompetenz nicht. Man nennt dies den Cliff-and-Plateau-Effekt. Er ist für Expertensysteme, also Computerprogramme, die der Diagnoseunterstützung dienen (Medical Decision-Support Systems) und dabei hochspezialisiert auf ein schmales Wissensgebiet sind, typisch. In den 1980er Jahren wurde der KI, parallel zu wesentlichen Fortschritten bei Hard- und Software, die Rolle einer Schlüsseltechnologie zugewiesen, insbesondere im Bereich der Expertensysteme. Man erhoffte sich vielfältige industrielle Anwendungen, erwartete auch eine Ablösung „eintöniger“menschlicher Arbeit(und deren Kosten) durch KI-gesteuerte Systeme. Weil allerdings viele Prognosen nicht eingehalten werden konnten, reduzierten die Industrie und die Forschungsförderung ihr Engagement. Solch eine Phase des Rückgangs von Erwartungen und Investitionen wird alsKI-Winterbezeichnet.[20]Ab 1984 entwickelteDouglas Lenatdas SystemCYC, eine Wissensdatenbank des Alltagswissens, mit dem Ziel daraus intelligente Schlussfolgerungen zu ziehen. Expertensysteme und andere aufWissensdatenbankenbasierende Systeme hatten nur mäßigen Erfolg, da es sich als zu schwer herausstellte, das benötigte Wissen von Hand in formale Regeln zu überführen. Diese Schwäche wird durchmaschinelles Lernenumgangen. Hierbei lernt das Computersystem selbstständig anhand der vorliegenden Daten und ist so auch in der Lage, verborgene Zusammenhänge zu erkennen, die ein Mensch nicht berücksichtigt hätte.[21]Klassische Verfahren lernen dabei eine Ausgabefunktion anhand vorher extrahierterMerkmale, die durch manuelle Programmierung aus den Eingabedaten extrahiert wurden. Hierbei zeigte sich jedoch ein ähnliches Problem wie bei den Expertensystemen, dass eine manuelle Auswahl nicht immer zu einem optimalen Ergebnis führt. Eine aktuell erfolgreiche Struktur für maschinelles Lernen sindkünstliche neuronale Netze(KNNs). Sie basieren auf der Fähigkeit, die erforderlichen Merkmale selbst anhand derRohdatenzu lernen, beispielsweise direkt aus den Kamerabildern.[21] Historisch gesehen wurden die ersten KNNs als lineare Modelle wie dieMcCulloch-Pitts-Zelle1943 und dasAdaline-Modell1959 entwickelt. Man analysierte, ausgehend von derNeurophysiologie, die Informationsarchitektur des menschlichen und tierischen Gehirns. Zur Untersuchung dieser Verfahren hat sich dieNeuroinformatikals wissenschaftliche Disziplin entwickelt. Schwächen bei der Modellierung selbst einfacher logischer Funktionen wie demXORdurch diese linearen Modelle führten zunächst zu einer Ablehnung der KNNs und biologisch inspirierter Modelle im Allgemeinen.[21] 1982 beschreibtPaul Werbosein Verfahren, das das Training mehrschichtiger Netze erlaubt. Es ist heute alsBackpropagationbekannt.[22]Es folgt ein neuer Aufschwung in der Forschung an künstlichen neuronalen Netzen. In den 1990ern gibt es große Fortschritte durch die Entwicklung vonSupport Vector Machines(SVMs) undrekurrenten neuronalen Netzen(RNNs).[23]Wissenschaftler beginnen mit der Entwicklung von Programmen, die große Datenmengen analysieren und aus den Ergebnissen Regeln „lernen“. In den 2000ern wird maschinelles Lernen zunehmend auch in der Öffentlichkeit bekannt. Die stetige Zunahme von Rechenleistung und verfügbaren Datenmengen ermöglicht weitere Fortschritte. 2001 veröffentlichtLeo Breimandie Grundlagen für ein alsRandom Forestbekanntes Verfahren, das eine Gruppe vonEntscheidungsbäumentrainiert.[24] 2006 beschreibenGeoffrey Hintonet al. eine Methode, mit der man ein neuronales Netz, das aus mehreren Schichten von künstlichen Neuronen besteht, so trainieren kann, dass es handgeschriebene Zahlen mit einer Genauigkeit von über 98 % erkennen kann.[25]Bis dahin schien es unmöglich zu sein, mit solchen Netzen hohe Genauigkeiten bei der Klassifikation zu erreichen. Die neue Methode wirdDeep Learninggenannt.[26]In den folgenden Jahren wird das Deep Learning weiter entwickelt. Es führt zu enormen Fortschritten in der Bild- und Textverarbeitung.[23] Zwischen 2009 und 2012 gewannen dierekurrentenbzw. tiefen vorwärtsgerichteten neuronalen Netze der Forschungsgruppe vonJürgen SchmidhuberamSchweizer KI-Labor IDSIAeine Serie von acht internationalen Wettbewerben in den BereichenMustererkennungund maschinelles Lernen.[27]Insbesondere gewannen ihre rekurrentenLSTM-Netze[28][29]drei Wettbewerbe zur verbundenenHandschrifterkennungbei der2009 Intl. Conf. on Document Analysis and Recognition (ICDAR)ohne eingebautesA-priori-Wissen über die drei verschiedenen zu lernenden Sprachen. Die LSTM-Netze erlernten gleichzeitigeSegmentierungund Erkennung. Dies waren die ersten internationalen Wettbewerbe, die durch Deep Learning[30]oder rekurrente Netze gewonnen wurden. Dank der Entwicklung nichtlinearermehrlagiger,faltenderneuronaler Netze und derdafür nötigen Trainingsverfahren, aber auch durch die Verfügbarkeit der dafür benötigten leistungsstarken Hardware und großen Trainings-Datensätze (z. B.ImageNet) erzielten KNNs seit 2009 Erfolge in zahlreichenMustererkennungs-Wettbewerben und dominierten gegenüber klassischen Verfahren mit händischer Auswahl der Merkmale. Die dafür verwendeten, mehrlagigen neuronalen Netze werden auch unter dem SchlagwortDeep Learningzusammengefasst.[21] Des Weiteren werden KNNs auch alsgenerative Modelleeingesetzt, das heißt zur Erzeugung echt wirkender Bilder, Videos oder Tonaufnahmen, was insbesondere durch die Erfindung derGenerative Adversarial Networks2014 in immer besserer Qualität möglich wurde. Die Ergebnisse einer darauf aufbauenden Arbeit aus dem Jahre 2017, die imaginäre Bilder von Gesichtern erzeugt, wurden von Fachkreisen als „eindrucksvoll realistisch“ beschrieben.[31]MitDeepFakeswurden die Ergebnisse ab 2017 auch einer breiten Öffentlichkeit bekannt. Dabei wurde insbesondere die Frage diskutiert, inwieweit man einem Foto- oder Videobeweis noch trauen kann, wenn es möglich wird, beliebig echt wirkende Bilder automatisiert zu erzeugen. Für Beiträge zu neuronalen Netzwerken und Deep Learning erhieltenYann LeCun,Yoshua BengioundGeoffrey Hinton2018 denTuring Award[32]und Hinton zusammen mitJohn Hopfield2024 den Physik-Nobelpreis.[33] In der KI haben sich mittlerweile zahlreiche Subdisziplinen herausgebildet, etwa spezielle Sprachen und Konzepte zur Darstellung und Anwendung von Wissen, Modelle zu Fragen von Revidierbarkeit, Unsicherheit und Ungenauigkeit und maschinelle Lernverfahren. DieFuzzylogikhat sich als weitere Form derschwachen KIetwa bei Maschinensteuerungen etabliert. Weitere erfolgreiche KI-Anwendungen liegen in den Bereichen natürlich-sprachlicher Schnittstellen,Sensorik,KybernetikundRobotik. Im März 2016 besiegte das SystemAlphaGomit dem SüdkoreanerLee Sedoleinen der weltbestenGo-Spieler. Das vom UnternehmenDeepMindentwickelte Programm hatte zuvor Millionen von archivierten Spielen mitDeep Learningausgewertet und zudem mehrere Millionen Mal gegen sich selbst gespielt.[34] Im August 2017 besiegte eine künstliche Intelligenz der Firma OpenAI bei einem mit 24 Millionen Dollar dotierten Turnier des ComputerspielsDota 2einige der weltbesten Profispieler auf diesem Gebiet (unter anderem den Profispieler Danylo \"Dendi\" Ishutin). Dota 2 gilt als eines der komplexesten Videospiele überhaupt, komplexer als Go oder Schach. Dota 2 wurde allerdings hier im Eins-zu-eins-Modus gespielt und nicht im komplexeren Team-Modus. OpenAI erklärte, dass die KI nur vier Monate benötigte, um diese Spielstärke zu erreichen. Die KI wurde trainiert, indem sie immer wieder gegen sich selbst antrat. Die KI bekam das gleiche Sichtfeld wie der menschliche Spieler und durfte nur eine begrenzte Anzahl von Aktionen gleichzeitig ausführen. Ziel von OpenAI ist es nun, eine KI zu entwickeln, die die besten menschlichen Spieler auch im Team-Modus besiegen kann.[35] Die Einführung vonChatGPTvonOpenAIim November 2022 war ein Meilenstein in der Geschichte der KI. Damit wurde erstmals ein leistungsfähiger, auf natürlicher Sprache basierender KI-Assistent der breitenÖffentlichkeitzugänglich gemacht. Dies löste eine Welle der Begeisterung aus und beschleunigte die KI-Entwicklung erheblich. In den folgenden Monaten präsentierten Tech-Giganten wieGoogle,MicrosoftundMetaihre eigenen KI-Modelle und integrierten diese in bestehende Produkte. Gleichzeitig entstanden zahlreiche Start-ups, die sich auf KI-Anwendungen spezialisierten. Die rasante Entwicklung führte zu bedeutenden Fortschritten in Bereichen wie Bildgenerierung, Spracherkennung und -synthese sowie Textverarbeitung. KI-Systeme wurden zunehmend in verschiedenen Industriezweigen eingesetzt, von der Medizin über die Finanzbranche bis hin zur Automobilindustrie. Diese Entwicklungen riefen auch Bedenken hervor, insbesondere hinsichtlich Datenschutz, Arbeitsplatzverlusten und ethischer Fragen. Als Reaktion darauf begannen Regierungen weltweit, Regulierungsrahmen für KI zu entwickeln.[36] 2023 kündigte OpenAI die Veröffentlichung vonGPT-4an, einem noch leistungsfähigeren Modell mit deutlich mehr Parametern als seine Vorgänger.[37]Dies versprach weitere Fortschritte, warf aber auch Fragen bezüglich des Energieverbrauchs und der Umweltauswirkungen solcher Systeme auf.[38] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Vorgeschichte 1.1Historische Automaten undRoboter 1.2Homunculi, Golem und andere künstliche Menschen 2Theoretische Grundlagen: Konzepte zur Formalisierung des Denkens 3Diskussion der Möglichkeit künstlicher Intelligenz 4Forschungsrichtungen und Phasen der KI 4.1Weizenbaum: ELIZA 4.2Expertensysteme 4.3Maschinelles Lernen und neuronale Netze 4.4Spielpartner bei Brett- und Videospielen 4.5Einführung von ChatGPT 2022 5Weblink 6Einzelnachweise Afrikaans العربية Azərbaycanca বাংলা Català کوردی Ελληνικά English Español Euskara فارسی Français עברית Հայերեն Bahasa Indonesia Íslenska 日本語 한국어"
  },
  {
    "label": 1,
    "text": "Halluzination (Künstliche Intelligenz) – Wikipedia Halluzination (Künstliche Intelligenz) Inhaltsverzeichnis Vorkommen Ursachen Vermeidung Begriffliche Kritik Siehe auch Weblinks Einzelnachweise Im Bereich derKünstlichen Intelligenz(KI) ist eineHalluzination(alternativ auchKonfabulationgenannt) ein überzeugend formuliertes Resultat einer KI, das nicht durch Trainingsdaten gerechtfertigt zu sein scheint und objektiv falsch sein kann.[1] Solche Phänomene werden in Analogie zum Phänomen derHalluzinationin der menschlichen Psychologie als vonChatbotserzeugteKI-Halluzinationenbezeichnet. Ein wichtiger Unterschied ist, dass menschliche Halluzinationen meist auf falschen Wahrnehmungen der menschlichen Sinne beruhen, während eine KI-Halluzination ungerechtfertigte Resultate als Text oder Bild erzeugt. Prabhakar Raghavan, Leiter vonGoogle Search, beschrieb Halluzinationen von Chatbots als überzeugend formulierte, aber weitgehend erfundene Resultate.[2]Der spezifische BegriffKI-Halluzinationkann Computer unangemessenvermenschlichen. Es gibt jedoch auch KI-Halluzinationen, welche Forscher zur Entdeckung bisher unbekannter Varianten erwiesenermaßen vorhandener Realitäten gezielt nutzen.[3]So wird beispielsweisegenerative KIin der Arzneimittelforschung verwendet, um spezifisch trainierte KI-Modelle zur Vorhersage möglicher neuer, therapeutisch vielversprechender Moleküle auf Grundlage existierender, wirksamer Moleküle einzusetzen. Diese Vorhersagen werden anschließend im Labor bezüglich realer Wirksamkeit getestet und sowohl positive wie auch negative Resultate wiederum ins KI-Modell eingespeist.[4]Diese wissenschaftlichen Anwendungen von KI beruhen nicht auf willkürlichen Sprachkonstruktionen von Chatbots und/oder Internetdaten, sondern auf wissenschaftlich erhärteten Erkenntnissen.[3] KI-Halluzinationen erlangten um 2022 parallel zur Einführung bestimmtergroßer Sprachmodelle(Large Language Models, LLM) wieChatGPTan Bedeutung.[5]Die Nutzer beschwerten sich, dass solche Chatbots oft sinnlos plausibel klingende Zufallslügen in ihren generierten Inhalten einbetteten. Als beispielsweise ChatGPT gebeten wurde, einen Artikel über das letzte Finanzquartal eines bestimmten Unternehmens zu generieren, erstellte dieser Chatbot einen kohärenten Artikel, erfand aber darin enthaltene Finanzzahlen. Nach Fragen über astrophysikalische Magnetfelder behauptete ChatGPT fälschlicherweise, dass Magnetfelder von Schwarzen Löchern durch die extrem starken Gravitationskräfte in ihrer Nähe erzeugt würden. In Wirklichkeit hat ein Schwarzes Loch aufgrund desNo-Hair-Theoremskein Magnetfeld.[6]Analysten betrachten häufige Halluzinationen als ein großes Problem der LLM-Technik.[7] Forscher haben unerwünschte Halluzinationen als ein statistisches Phänomen bezeichnet oder Halluzinationen auf unzureichende Trainingsdaten zurückgeführt. Dagroße Sprachmodellekein vollständiges Wissen der Welt besitzen, wird fehlendes Wisseninterpoliertbzw.konfabuliert.[8]Diese Eigenschaft von Sprachmodellen ermöglicht es, Anfragen an das Modell auf eine kreative Weise zu beantworten, anstatt ausschließlich auf vorhandenes Wissen zurückgreifen zu können. Allerdings führt dies auch dazu, dass eine Anfrage des Benutzers, auf die eine objektiv richtige Antwort existiert, dem Modell aber unbekannt ist, auf eine Weise beantwortet wird, die richtig erscheinen kann, jedoch nicht den Tatsachen entspricht. Einige Kenner glauben, dass bestimmtefalscheKI-Antworten, die von Menschen alsHalluzinationenim Fall der Objekterkennung eingestuft werden, tatsächlich durch die Trainingsdaten gerechtfertigt sein können, oder sogar, dass eine KI dierichtigeAntwort gibt, welche die menschlichen Gutachter nicht sehen. Zum Beispiel kann ein umstrittenes Bild, das für einen Menschen wie ein gewöhnliches Bild eines Hundes aussieht, in Wirklichkeit für die KI Muster enthalten, die in authentischen Bildern nur beim Betrachten einer Katze auftreten würden. Die KI erkenne reale visuelle Muster, welche für Menschen nicht zu erkennen seien. Diese Schlussfolgerungen wurden jedoch von anderen Forschern in Frage gestellt. Zum Beispiel wurde eingewendet, dass die Modelle zu oberflächlichen Statistiken tendieren könnten, was dazu führe, dass Training bei umstrittenen Themen in realen Szenarien nichtrobustsei. Halluzination wurde als statistisch unvermeidliches Nebenprodukt eines jeden unvollkommenengenerativen Modellserkannt, das darauf trainiert ist, die Trainingswahrscheinlichkeit zu maximieren, wie zum Beispiel GPT-4. Ebenfalls können Fehler beim Kodieren und Dekodieren zwischen Text und Repräsentationen Halluzinationen verursachen. KI-Training zur Erzeugung von vielfältigen Antworten kann auch zu Halluzinationen führen. Halluzinationen können ebenfalls auftreten, wenn die KI auf einem Datensatz trainiert wird, bei dem beschriftete Zusammenfassungen trotz ihrer faktischen Genauigkeit nicht direkt in den beschrifteten Daten verankert sind, die angeblich zusammengefasst werden. Größere Datensätze können ein Problem des parametrischen Wissens schaffen (Wissen, das in gelernten Systemparametern fixiert ist), was zu Halluzinationen führt, wenn das System zu selbstsicher sein festgelegtes Wissen nutzt.[9]In Systemen wie GPT-4 generiert eine KI jedes nächste Wort basierend auf einer Sequenz von vorherigen Wörtern (einschließlich der Wörter, die sie selbst während desselben Dialogs zuvor generiert hat), was zu einer Kaskade von möglichen Halluzinationen führt, je länger die Antwort wird.[10] Zur Reduktion von KI-Halluzinationen wird zusätzliches aktives Lernen (wie zum BeispielBestärkendes Lernen aus menschlich beeinflusster Rückkopplungbei GPT-4) verwendet. Auch hatGoogle Bardeine neue Funktion eingeführt, mittels welcher Teile im Text orange markiert werden, deren Aussagen unsicher sind.[11]Ob sich jedoch alle erwähnten Probleme lösen lassen, sei fraglich.[12] Es wird untersucht, in welcher Art KI-Halluzinationen von typisch menschlichen Fehlern abweichen. Geeignete Gegenmaßnahmen für besonders verwirrende, aber echt erscheinende Resultate werden erforscht.[13] Eine solche Möglichkeit ist dasSelf-Consistency-Prompting, welches dazu dient, konfabulierte Antworten desSprachmodellsdurch Variationen der Anfragen (Prompts) oder gleich bleibende Mehrfachanfragen zu erkennen.[14][13] Auch ein geeignetesPrompting, welches dem KI-Modell ermöglicht, eine Frage nicht beantworten zu müssen, kann zu einer Verringerung – jedoch nicht Verhinderung – konfabulierter Antworten führen. Beispielsweise kann man statt„Wer war der Präsident von Deutschland 2017?“die Frage als„Weißt du, wer der Präsident von Deutschland 2017 war?“formulieren. Dies ermöglicht es dem System, eine Antwort wie„Nein, weiß ich leider nicht“zu formulieren, anstatt zu versuchen, die Antwort zu erraten. Eine weitere Möglichkeit besteht darin, mittelsRetrieval-Augmented Generationdie Anfrage des Benutzers mit gesicherten Informationen aus dem Internet oder einer Datenbank anzureichern, sofern die nötigen Daten vorhanden sind. Forschende derUniversität Glasgowmerkten 2024 an, dieMetapherder Halluzination könnte bei politischen Entscheidungsträgern und Öffentlichkeit falsche Assoziationen über die Funktionsweise künstlicher neuronaler Netze wecken. Anstelle einer vermeintlichen Fehlrepräsentation der Wirklichkeit müsse ein Begriff verdeutlichen, dass derartigeAusgabengänzlich wirklichkeitsfern und der internen Funktionsweise der KI-Modelle geschuldet seien. Sie schlugen daher die Nutzung des vonHarry Frankfurtetablierten Begriffs „Bullshit“ vor.[15]Ähnliche Kritik kam bereits zuvor durch Netzaktivisten auf.[16] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Vorkommen 2Ursachen 3Vermeidung 4Begriffliche Kritik 5Siehe auch 6Weblinks 7Einzelnachweise العربية Azərbaycanca Català English Español Eesti فارسی Suomi Français Galego עברית Bahasa Indonesia Italiano 日本語 한국어 Nederlands Polski Português Русский Српски / srpski Svenska ไทย Türkçe Українська"
  },
  {
    "label": 1,
    "text": "Watson (Künstliche Intelligenz) – Wikipedia Watson (Künstliche Intelligenz) Inhaltsverzeichnis Hintergrund und Zielsetzung Auftritt bei Jeopardy! Aufbau Funktionsweise Einsatzbeispiele Filmische Dokumentationen Weblinks Einzelnachweise Linguistischer Präprozessor Kandidatengenerierung Kandidatenbewertung Videos Watsonist einComputerprogrammaus dem Bereich derkünstlichen Intelligenz. Es wurde vonIBMentwickelt, um Antworten auf Fragen zu geben, die indigitaler Forminnatürlicher Spracheeingegeben werden. Das nachThomas J. Watson, einem der ersten Präsidenten von IBM, benannte Programm wurde als Teil des DeepQA-Forschungsprojektes entwickelt.[1] Zur Demonstration seiner Leistungsfähigkeit konkurrierte das Programm in drei vom 14. bis 16. Februar 2011 ausgestrahlten Folgen derQuizsendungJeopardy!mit zwei menschlichen Gegnern, die in der Show zuvor Rekordsummen gewonnen hatten. Die Partie, für die ein Preisgeld von einer Million Dollar ausgelobt war, wurde in den Medien daher mit dem Duell desSchachweltmeistersGarri Kasparowgegen den ComputerDeep Blueverglichen.[2]Das System gewann das Spiel mit einem Endstand von $77.147 gegenüber den $24.000 bzw. $21.600 seiner menschlichen Konkurrenten.[3][4]Im Januar 2017ersetzteeine japanische Versicherung mehr als 30 Mitarbeiter durch die Watson-Plattform.[5]Die KI soll Namen und Daten der Versicherten sowie deren medizinische Vorgeschichte prüfen und Verletzungen bewerten. Inzwischen wird der BegriffWatsonvon IBM für unterschiedliche Lösungen verwendet, die künstliche Intelligenz in Data-, AI- und Businessanwendungen integrieren.[6] Ziel des Projekts ist es letztlich, eine hochwertigeSemantische Suchmaschinezu schaffen. Diese soll den Sinn einer in natürlicher Sprache gestellten Frage erfassen und in einer großen Datenbank, die ebenfalls Texte in natürlicher Sprache umfasst, innerhalb kurzer Zeit die relevanten Passagen und Fakten auffinden. Eine derartige Software könnte in vielen Bereichen, etwa der medizinischen Diagnostik, komplexe Entscheidungen unterstützen, insbesondere wenn diese unter Zeitdruck getroffen werden müssen. Watson implementiertAlgorithmenderNatürlichen Sprachverarbeitungund desInformation Retrieval, aufbauend auf Methoden desMaschinellen Lernens, derWissensrepräsentationund derautomatischen Inferenz.[7]Das System enthält Softwaremodule zur Erstellung von Hypothesen, ihrer Analyse und Bewertung. Es greift dabei auf eine Aussagensammlung und umfangreiche Textbestände zurück, ist jedoch nicht mit dem Internet verbunden. Anfragen an Watson werden bislang in Textform gestellt. Anders als aktuelle Systeme wie z. B.Wolfram Alphabenötigt es jedoch keine formaleAbfragesprache. Seit Februar 2011 arbeitet IBM mit der Firma Nuance zusammen, einem führenden Hersteller von Software zurSpracherkennung. Die geplante Fähigkeit, auch gesprochene Fragen zu bearbeiten, soll den Einsatz einer spezialisierten Version Watsons im Gesundheitswesen erleichtern.[8] IBM plant, auf Watson basierende Systeme im Laufe der nächsten Jahre kommerziell zu vermarkten. Der Leiter des zuständigen Forschungslabors geht davon aus, dass die Kosten des Gesamtsystems zunächst mehrere Millionen US-Dollar betragen könnten, da bereits die notwendige Hardware mindestens eine Million Dollar kostet.[9]Im Rahmen von Pilotstudien wurde das System bislang unter anderem dazu verwendet, Vorhersagen zu treffen, welcheArzneistoffegegen bestimmte Krankheiten wirksam sein könnten; durch Integration zahlreicher Sensordaten und Informationen zu Umwelteinflüssen vorherzusagen, welche Bauteile komplexer Industriemaschinen Gefahr laufen vorzeitig auszufallen und daher gewartet werden sollten; aber auch dazu, innovative Kombinationen von Zutaten für schmackhafteKochrezeptevorzuschlagen.[10]Zudem ist geplant zukünftigneuromorphe Chips, wie z. B.TrueNorth, zu integrieren um Eingaben in Form von natürlicher Sprache, Bildern und Videos, sowie beliebigen Sensoren zu ermöglichen.[11]Darüber hinaus soll Watson in Zukunft Rechtsanwälte bei der rechtlichen Recherche in juristischen Datenbanken entlasten.[12] Die QuizshowJeopardy!stellt Systeme zur automatischen Beantwortung natürlichsprachiger Fragen vor eine interessante Herausforderung, da die als Antworten gestellten Aufgaben meist bewusst mehrdeutig formuliert sind, häufig die Verknüpfung mehrerer Fakten erforderlich machen und die passende Frage innerhalb eines Zeitlimits von fünf Sekunden gefunden werden muss. Die Entwickler des System Watson setzten sich daher das Ziel, in diesem Spiel menschliche Kandidaten zu schlagen. Bei ersten Testläufen im Jahr 2006 fand Watson nur für etwa 15 % von 500 Umschreibungen vorangegangenerJeopardy!-Sendungen die korrekte Frage. Die besten Kandidaten vonJeopardy!erreichen im Vergleich dazu etwa 95 % Genauigkeit. Im Laufe der nächsten Jahre wurde Watson mit einer Datenbank von ungefähr 100 Gigabyte an Texten ausgestattet, darunter Wörterbücher, Enzyklopädien, wie z. B. die gesamteWikipedia, und anderes Referenzmaterial. Watson hat jedoch keine Verbindung zum Internet, ist also, wie seine menschlichen Gegenspieler, aufsich alleingestellt. Die Informationen werden unter anderem statistisch ausgewertet, um Sinnbezüge herzuleiten. Anstatt sich auf einen einzelnen Algorithmus zu stützen, nutzt Watson hunderte davon gleichzeitig, um über einen Pfad eine potentiell richtige Antwort zu finden. Je mehr Algorithmen unabhängig voneinander die gleiche Antwort erreichen, als desto wahrscheinlicher wird es angesehen, dass Watson die korrekte Lösung gefunden hat. Wenn das System für eine Aufgabe eine kleine Anzahl potentieller Lösungen erarbeitet hat, werden diese anhand einer Datenbank überprüft, um zu bewerten, welche davon als potentiell sinnvoll gelten können. Dazu werden z. B. Zeitangaben überprüft. In einer Sequenz von 20 Übungsspielen nutzten die menschlichen Kandidaten die 6 bis 8 Sekunden Dauer während des Lesens des Ratebegriffes dazu, den Buzzer zu betätigen und die korrekte Antwort zu geben. Das auf diese Zeitspanne optimierte System Watson evaluiert eine Antwort und entscheidet, ob es genügend Sicherheit bezüglich des Ergebnisses gibt, um den Buzzer auszulösen.[9][13] Seit Februar 2010 ist Watson in der Lage, im Rahmen regelgerechter Partien menschlicheJeopardy!-Kandidaten zu schlagen.[14]IBM stellte zunächst eine Übungssituation in einem Konferenzraum imThomas J. Watson Research Centerin Yorktown Heights,New York, nach, die die Situation beiJeopardynachahmt, und ließ Einzelpersonen, einschließlich frühererJeopardy-Kandidaten, in Probespielen gegen Watson teilnehmen, mit Todd Alan Crain vonThe OnionalsQuizmaster. Dem Computersystem, auf dem Watson ausgeführt wurde, wurden die Ratebegriffe (als Antwort auf eine Frage) elektronisch übermittelt und es war in der Lage, den Buzzer zu betätigen und mit einer elektronischen Stimme die Antworten imJeopardy-eigenen Frageformat zu geben.[9][13] Schließlich trat Watson beiJeopardyin drei Sendungen, die zwischen dem 14. und 16. Februar 2011 ausgestrahlt wurden, gegen die früheren ChampionsKen Jenningsund Brad Rutter an, welche in der Show zuvor Rekordsummen gewonnen hatten. Nachdem das System Watson und der Kandidat Rutter nach der ersten Runde noch gleichauf lagen, ging Watson aus den beiden anderen als klarer Sieger hervor. Das Preisgeld von einer Million US-Dollar stellte IBM gemeinnützigen Zwecken zur Verfügung. Jennings und Rutter kündigten an, jeweils die Hälfte ihrer Preise von $300.000 bzw. $200.000 zu spenden.[3] DieSoftwareenginevon Watson ist DeepQA. Diese läuft bei Watson auf demBetriebssystemSUSE Linux Enterprise Server.[15] DerRechnerverbundbesteht aus 90 Power 750 Servern mit 16TBRAM. Jeder Server besitzt einen mit 3,5 GHz getaktetenPower78-Kern Prozessor, wobei jeder Kern bis zu 4Threadsgleichzeitig ausführt.[16][17] Geschrieben wurde DeepQA in verschiedenen Programmiersprachen; darunterJava,C++undProlog. DeepQA ist hierbei in Form von Annotatoren einerUIMA-Pipelineimplementiert.[18][19] Durch den Einsatz vonUIMA Asynchronous ScaleoutundHadoopwird eine massive Parallelisierung ermöglicht. Spezielle UIMA Annotatoren ermöglichen dabei eine Abbildung auf HadoopMapReduce-Schemaum eine große Anzahl vonTextdokumentenparallel durchsuchen zu können. Watson übernimmt eineJeopardy!-Antwort (die Frage) des Moderators in elektronischer Form über eine Tastatur. Eine solche Jeopardy!-Antwort kann dabei sehr komplex sein und aus mehreren Sätzen, Rätseln und Wortwitzen bestehen. Die Jeopardy!-Antwort wird von der DeepQA-Engine mit Hilfe einesLinguistischen-Präprozessorsanalysiert. Dabei wird die logische Struktur mit Hilfe einesParsersdes Satzes alsBauminPrologabgebildet. EinTokenizer, bestehend aus UIMA-Annotatoren fürPattern Matching, kümmert sich um die Abbildung aufLexikalische Antworttypen(LAT). Dabei wird die Beziehung der Satzteile zueinander (dieGrammatik) analysiert. Das betrifft insbesondere dasPronomen(auf das Watson sich mit der von ihm zu generierenden Frage beziehen muss), sowie Wörter, die angeben, welche Klasse von Antwort (z. B. Poet, Land, Epoche etc.) gesucht wird.[20] Das Pronomen wird – sofern dieses nicht als solches erkennbar ist – dadurch gefunden, dass durch dessen Entfernung aus der Frage eine Aussage wird. Auf diesen Teil des Satzes legt DeepQA denFokusbei der Kandidatenbewertung.[21] Die Kandidatengenerierung nimmt den Prolog-Code des Linguistischen Präprozessors entgegen und leitet diese an verschiedeneSuchmaschinenweiter. Hierbei wird etwaINDRIundLucenefür die Durchsuchung von unstrukturierten Dokumenten eingesetzt, welche in einemHDFSgespeichert sind. Zudem gibt es spezielle Engines die den LAT-Prolog-Code entgegennehmen undSPARQL-Abfragen aufsemantischen Datenbanken(Triplestores) bzw.SQL-Abfragen aufrelationalen Datenbankendurchführen, welche aufDB2basieren.[22]Die Dokumente decken hierbei ein breiteres Wissensgebiet ab und sind schneller durchsuchbar, während die strukturierten und insbesondere semantischen Datenquellen eine höhere Genauigkeit bieten.[21] Die Daten stammen aus verschiedenen Quellen, wie etwaDBpedia,Wordnet,Yago,Cyc,Freebase,Wikipedia,IMDB,World Book Encyclopedia, derBibelsowie verschiedenenTaxonomienundOntologien,Literarischen Werkenund Artikeln vonPR NewswireundNew York Times. Zudem werden Webseiten analysiert und in Form von Textschnipseln in den Datenbanken von Watson gespeichert.[21] DeepQA generiert dabei zwischen 100 und 250 Suchergebnisse. Diese Ergebnisse (Kandidaten) stellenHypothesenfür die mögliche Antwort dar.[21] In Jeopardy! hat Watson keinen Zugriff auf das Internet, sondern nur auf die Daten in den internen Datenbanken. Prinzipiell hat DeepQA für zukünftige Anwendungen jedoch auch die Möglichkeit, weitere Informationen aus dem Internet zu beziehen und mit Hilfe vonWebservicesauchEchtzeitdatenzu berücksichtigen. Die wahrscheinlichsten Ergebnisse der Suche werden genauer analysiert. Hierzu besitzt DeepQA mehrere tausend Softwareagenten die jeweils eine ganz spezielle Analyse parallel durchführen. Hierzu gehören vor allem Agenten für die Analyse von zeitlichen (temporalen) und räumlichen (spatiellen) Zusammenhängen, Taxonomien, einfachen Berechnungen für Rechenrätsel, Bewertung der Akustik für Wörter die ähnlich klingen, Scrabble-Bewertung für Wörter deren Buchstaben vertauscht wurden, Agenten, die Suchergebnisse einer genaueren semantischen Analyse durchführen, sowie viele andere mehr. Diese Analyse umfasst oft ein sehr breites Wissensspektrum, wobei verschiedene Kandidaten und Wissensdomänen von den jeweiligen Agenten voneinander unabhängig und massiv parallel analysiert werden. Da jedes Suchergebnis von bis zu tausend Agenten analysiert wird, multipliziert sich die Anzahl der gleichzeitig analysierten Evidenzfragmente. Aus den 100 bis 250 Hypothesen werden somit bis zu 100.000 Evidenzfragmente die in unabhängigen Threads analysiert werden. Ein Softwarefilter eliminiert alle Ergebnisse von Agenten, die keinen Beweis für die Richtigkeit eines Suchergebnisses erbracht haben. Ende August 2016 veröffentlichte20th Century Foxeinen Trailer zum FilmDas Morgan Projekt, der von Watson gefertigt wurde. Es handelt sich dabei um den ersten Trailer der Filmgeschichte, der durch einen Algorithmus entstanden ist.[23]Der IBM-Manager John R. Smith erklärte in einem Blogeintrag, Watson habe insgesamt 100 Trailer vonHorrorfilmenanalysiert, um den rund 60 Sekunden langen Trailer zu fertigen. Watson unterteilte diese in Segmente, und nach einer visuellen Analyse, einer Audio-Analyse und einer Analyse der Szenen-Zusammensetzung, analysierte diekünstliche Intelligenzden FilmMorganund filterte die passenden Stellen heraus. Letztlich entschied sich das System für zehn Sequenzen, aus denen dann ein Filmteam den Trailer zusammensetzte.[24] Auf der CeBIT 2017 präsentierte IBM einen autonomen Bus namens Olli, der durch Watson gesteuert wird. Watson und Olli sind vernetzt, die Rechenleistung kommt aus IBMs Datenzentrum in Frankfurt.[25][26][27] In Form von unterschiedlichen Anwendungen steht Watson mittlerweile auch Endnutzern zur Verfügung. Ein Beispiel hierfür istCognos Analytics, eine Software für intelligente Datenanalyse und Visualisierung, oder Watson Assistant, mit dem intelligente Chatbots und digitale Assistenten erstellt werden können.[28]Zahlreiche weitere Watson Services können teilweise sogar kostenlos über die IBM Cloud in Anspruch genommen werden und reichen von Bild- und Spracherkennung bis hin zu Machine Learning Modellen.[29] Im Bereich derOnkologieberätWatson for OncologyKrebsärzte in 230 Krankenhäusern weltweit bei der Suche nach der jeweils besten Therapie (Stand Mitte 2018). Allerdings kritisierte 2017 der Leiter der Krebsabteilung von KopenhagensReichskrankenhausdas System scharf und stoppte an seiner Klinik das Experiment.[30]Aus internen IBM-Dokumenten ging hervor, dass Watson oftmals falsche Behandlungstipps gegeben hatte und dass IBM-Medizinspezialisten und Kunden zahlreiche Beispiele für unsichere und falsche Behandlungsempfehlungen festgestellt hatten. Ursache sei eine nur geringe Anzahl künstlicher Krebsfälle statt echter Patientendaten gewesen; außerdem basierten die Empfehlungen auf der Expertise einiger weniger Spezialisten für jede Krebsart statt auf Richtlinien oder Nachweisen. IBM-intern war bekannt, dass Watson ungenaue Empfehlungen gab, die im Widerspruch zu nationalen Behandlungsrichtlinien standen; außerdem stellte sich heraus, dass Studien, die IBM mit der Software durchgeführt hatte und deren Ergebnisse als Beweis für die Nützlichkeit des Systems angepriesen wurden, so angelegt waren, dass sie günstige Ergebnisse erbrachten.[31] IBM Watson wurde beimGrand SlamTurnier inWimbledoneingesetzt, um Spieldaten vonTennisspielernzu analysieren und Spielergebnisse vorherzusagen. Dieses System verwendet historische Daten und aktuelle Turnierdaten, um Prognosen zu erstellen. Es analysiert den möglichen Spielverlauf basierend auf Variablen wie Oberflächentyp, Wetterbedingungen und jüngsten Spielerleistungen. Solche Erkenntnisse können Spielern und Trainern helfen, effektivere Spielstrategien gegen Gegner zu planen, indem sie wahrscheinliche Muster und Ergebnisse verstehen und so möglicherweise die Gewinnchancen in hart umkämpften Spielen erhöhen.[32] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Hintergrund und Zielsetzung 2Auftritt bei Jeopardy! 3Aufbau 4Funktionsweise 4.1Linguistischer Präprozessor 4.2Kandidatengenerierung 4.3Kandidatenbewertung 5Einsatzbeispiele 6Filmische Dokumentationen 7Weblinks 7.1Videos 8Einzelnachweise العربية Български Català English Español Eesti Euskara فارسی Suomi Français עברית Magyar Հայերեն Italiano 日本語 한국어 മലയാളം မြန်မာဘာသာ Nederlands"
  },
  {
    "label": 1,
    "text": "Jürgen Schmidhuber – Wikipedia Jürgen Schmidhuber Inhaltsverzeichnis Leben und Werk Schriften (Auswahl) Filmische Dokumentationen Literatur Weblinks Einzelnachweise Jürgen Schmidhuber(*17. Januar1963inMünchen) ist ein deutscher[1]Informatiker. Er arbeitet vor allem im BereichKünstliche Intelligenz(KI) und deren Spezialgebiet derneuronalen Netzwerke. Seit 1995 ist er wissenschaftlicher Direktor beiIDSIA, einem Schweizer Forschungsinstitut für KI und seit 2021Director of the AI Initiativean dersaudischenKAUST-Universität.[1]Bekannte internationale Medien bezeichneten ihn als „Vater fortgeschrittener KI“,[2]„Papa“ berühmter KI-Produkte,[3]„Vater der modernen KI“[4][5]und „Paten der KI“.[6] Schmidhuber studierte ab 1983 Informatik und Mathematik an derTechnischen Universität München, an der er 1987 sein Diplom erwarb und 1991 beiWilfried Brauerin Informatik promoviert wurde. Das Thema wardynamische neuronale Netzeund das fundamentale raumzeitliche Lernproblem.[7]Dynamische Neuronale Netze und insbesonderefast weight programmers, welche er 1991 vorschlug, enthalten Kernideen der heutigenTransformer-Architektur.[8]AlsPost-Doktorandwar er 1991/92 an derUniversity of Colorado Boulder. Im Jahre 1993 habilitierte sich Schmidhuber an der TU München (Net Architectures, Objective Functions, and Chain Rule). Er war Oberassistent und ab 1995 Privatdozent an der TU München, bevor er 1995 wissenschaftlicher Direktor von IDSIA in Lugano wurde. Von 2003 bis 2021 war er Professor an derScuola universitaria professionale della Svizzera italianain Manno, von 2009 bis 2024 ordentlicher Professor an derUniversità della Svizzera italiana (USI), wo er immer noch außerordentlicher Professor ist.[1]Er war zudem 2004 bis 2009 als außerordentlicher Professor Leiter des Labors für kognitive Robotik an der TU München.[1]Seit Oktober 2021 arbeitet er als Direktor der KI-Initiative an dersaudischenKAUST-Universität.[9]Weiterhin ist er Mitgründer und Chefwissenschaftler der Firma NNAISENSE, deren Präsident er von 2014 bis 2017 war.[1] Er veröffentlichte zahlreiche wissenschaftliche Artikel in folgenden Themenbereichen:Maschinelles Lernen,neuronale Netze,Kolmogorow-Komplexität,Digitale Physik,Robotik,Kaum Komplexe Kunstund Theorie derSchönheit. Die in seiner Arbeitsgruppe entwickeltenrekurrenten neuronalen Netze(RNN) lernen in effizienter Weise früher unlernbare Aufgaben wie die Erkennung gewisser kontextsensitiver Sprachen, Robotersteuerung in partiell sichtbaren Umgebungen, Musikkomposition, Aspekte der Sprachverarbeitung und das Erkennen von Handschriften. Er erhielt mit seinen neuronalen Netzwerken ab 2009 verschiedene Preise in visuellen Mustererkennungswettbewerben für Maschinenlernen und Künstliche Intelligenz. Sie wurden zum Beispiel in der KI-Forschung von Google angewandt, zum Beispiel auf dasGo-Spiel(AlphaGobei Deep Mind). Einer der Gründer vonGoogle DeepMindstudierte bei Schmidhuber in Lugano. Die RNN wurden insbesondere durch eine Idee von Schmidhubers Diplomanden an der TU MünchenSepp Hochreiter(Professor in Linz) 1991 verbessert, der Implementierung vonLong short-term memory(LSTM) im neuronalen Netz, was diesem ermöglichte, weiter beim Lernen in die Vergangenheit zurückzublicken.[10]Schmidhuber bezeichnet seine RNN mit LSTM als Deep Learning Netzwerke.[11] Seine möglicherweise ambitionierteste Arbeit ist dieGödelmaschine(2003) zur Lösung beliebiger formalisierbarer Probleme. Mit Hilfe eines asymptotisch optimalen Theorembeweisers überschreibt die Gödelmaschine beliebige Teile ihrer Software (samt dem Theorembeweiser), sobald sie einen Beweis gefunden hat, dass dies ihre zukünftige Leistung verbessern wird. Die Gödelmaschine ist dabei ein theoretisches Konstrukt, keine real funktionierende Ingenieurs-Leistung.[12] Schmidhuber publizierte auch Arbeiten zur Menge der möglichen berechenbaren Universen. Sein „Großer Programmierer“ implementiertKonrad ZusesHypothese (1967) derdigitalen Physik, gegen die bis heute keine physikalische Evidenz vorliegt. 1997 wies Schmidhuber darauf hin, dass das einfachste Programm alle Universen berechnet, nicht nur unseres. Ein Beitrag aus dem Jahre 2000 analysierte weiterhin die Menge aller Universen mit limit-berechenbaren Wahrscheinlichkeiten sowie die Grenzen formaler Beschreibbarkeit. Diese Arbeiten führten ihn zu Verallgemeinerungen derKolmogorov-KomplexitätK(x) einer Bitkette x. K(x) ist die Länge des kürzesten Programms, das x berechnet und hält. Schmidhubers nicht-haltende, doch konvergierende Programme stellen noch kürzere, nämlich die kürzestmöglichen formalen Beschreibungen dar. Sie führen zu nicht-abzählbaren, doch limesberechenbaren Wahrscheinlichkeitsmaßen und zu sogenannten Super-Omegas, bei denen es sich um Verallgemeinerungen vonGregory Chaitins„Zahl aller mathematischen Weisheit“Omegahandelt. All dies hat Konsequenzen für das Problem der optimalen induktiven Inferenz, d. h., der optimalen Zukunftsvorhersage aus bisher beobachteten Daten. 2013 erhielt Schmidhuber denHelmholtz Awardder International Neural Networks Society, 2016 den IEEE CIS Neural Networks Pioneer Award für „bahnbrechende Beiträge zumDeep Learningund zu neuronalen Netzen“.[1]Sein Labor erhielt 2016 den NVIDIA Pioneers of AI Research Award. Als Konsequenz aus der aus seiner Sicht unabwendbar fortschreitenden Automatisierung und dem damit einhergehenden Wegfall von Erwerbsarbeitsplätzen sieht Schmidhuber die Notwendigkeit einesbedingungslosen Grundeinkommens: „Roboterbesitzer werden Steuern zahlen müssen, um die Mitglieder unserer Gesellschaft zu ernähren, die keine existenziell notwendigen Jobs mehr ausüben. Wer dies nicht bis zu einem gewissen Grad unterstützt, beschwört geradezu die Revolution Mensch gegen Maschine herauf.“ (Jürgen Schmidhuber:Wir müssen Roboter erziehen wie Kinder.Interview durch Vinzenz Greiner, 15. Januar 2017.)[13] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Leben und Werk 2Schriften (Auswahl) 3Filmische Dokumentationen 4Literatur 5Weblinks 6Einzelnachweise العربية مصرى English Español Galego Italiano Русский Српски / srpski Svenska Українська Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen"
  },
  {
    "label": 1,
    "text": "John McCarthy – Wikipedia John McCarthy Inhaltsverzeichnis Leben Wirken Sonstiges Literatur Weblinks Belege John McCarthy(*4. September1927inBoston,Massachusetts; †23. Oktober2011inPalo Alto,Kalifornien[1][2]) war einUS-amerikanischerLogiker,Informatikerund Autor. Er ist der Erfinder der ProgrammierspracheLISP. Für seine bedeutenden Beiträge im Feld derKünstlichen Intelligenzerhielt McCarthy 1971 denTuring Awardund 1988 einenKyoto-Preis. 1991 wurde ihm dieNational Medal of Scienceverliehen.[2] McCarthy erhielt 1948 denBachelor of Scienceim FachMathematikvomCalifornia Institute of Technology. DenDoktorgraderwarb er drei Jahre später 1951 an derPrinceton UniversitybeiSolomon Lefschetz(Projection operators and partial differential equations). Nach Aufenthalten an derStanford University, amDartmouth CollegeundMassachusetts Institute of Technologywar er ab 1962 Professor in Stanford, wo er 2000 emeritierte. Er war bis zu seinem Tod als Professor Emeritus der Stanford University aktiv und kommentierte das Weltgeschehen oft in Internetforen aus einer mathematisch-wissenschaftlichen Perspektive. McCarthys erster Doktorand warRaj Reddy, der später ebenfalls den Turing Award gewonnen hat. Außerdem war McCarthy Doktorvater von TuringpreisträgerinBarbara Liskov. In den Jahren von 1957 bis 1959 wurde er alsSloan Fellowim Bereich Physical Science gefördert. Er gehörte derAmerican Academy of Arts and Sciences(1974), derAmerican Association for the Advancement of Science, derAmerican Mathematical Societyund derAssociation for Computing Machineryan. 1987 wurde er in dieNational Academy of Engineeringund 1989 in dieNational Academy of Sciencesaufgenommen. Von 1983 bis 1984 war er Präsident derAmerican Association for Artificial Intelligenceund er gehörte seit 1975 dem Editorial Board der ZeitschriftArtificial Intelligence Journalan. Er schrieb Kurzgeschichten, in denen die Entwicklung der KI eine Rolle spielt.[2] McCarthy war Atheist. Zitat: “Responding toRichard Dawkins’s pestering his fellow atheists to ‘come out’, I mention that I am indeed an atheist. To count oneself as an atheist one need not claim to have a proof that no gods exist. One need merely think that the evidence on the god question is in about the same state as the evidence on the werewolf question.” „Als Antwort auf Richard Dawkins ständig drängende Aufforderung an seine Mitatheisten, sich zu outen, erwähne ich, dass ich tatsächlich Atheist bin. Um sich als Atheist zu bezeichnen, muss man nicht beanspruchen, einen Beweis dafür zu haben, dass es keine Götter gibt. Man braucht einfach nur zu bedenken, dass die Beweise zur Gottesfrage sich in etwa auf dem gleichen Stand befinden wie die Beweise zurWerwolf-Frage.“ Die erste Konferenz über künstliche Intelligenz, dieDartmouth Conference, fand im Sommer 1956 statt. In dem Förderantrag an dieRockefeller Foundationfür diese Konferenz[3]prägte McCarthy 1955 den BegriffKünstliche Intelligenz.[4]Er ist der Erfinder der ProgrammierspracheLISP, deren Design er 1960 in der FachzeitschriftCommunications of the ACMvorstellte. LISP war eine der ersten Implementierungen einesLogikkalkülsauf einem Computer. Außerdem wird ihm die Erfindung desAlpha-Beta-Algorithmus, der entscheidend zur Spielstärke von Schachprogrammen beigetragen hat, sowie der ersteMark-and-Sweep-Algorithmus zurautomatischen Speicherbereinigungzugeschrieben. DerJohn McCarthy Awardder IJCAI wird für Wissenschaftler in Künstlicher Intelligenz in der Mitte ihrer Karriere verliehen. 1966:Perlis| 1967:Wilkes| 1968:Hamming| 1969:Minsky| 1970:Wilkinson| 1971:McCarthy| 1972:Dijkstra| 1973:Bachman| 1974:Knuth| 1975:Newell,Simon| 1976:Rabin,Scott| 1977:Backus| 1978:Floyd| 1979:Iverson| 1980:Hoare| 1981:Codd| 1982:Cook| 1983:Thompson,Ritchie| 1984:Wirth| 1985:Karp| 1986:Hopcroft,Tarjan| 1987:Cocke| 1988:Sutherland| 1989:Kahan| 1990:Corbató| 1991:Milner| 1992:Lampson| 1993:Hartmanis,Stearns| 1994:Feigenbaum,Reddy| 1995:Blum| 1996:Pnueli| 1997:Engelbart| 1998:Gray| 1999:Brooks| 2000:Yao| 2001:Dahl,Nygaard| 2002:Rivest,Shamir,Adleman| 2003:Kay| 2004:Cerf,Kahn| 2005:Naur| 2006:Allen| 2007:Clarke,Emerson,Sifakis| 2008:Liskov| 2009:Thacker| 2010:Valiant| 2011:Pearl| 2012:Micali,Goldwasser| 2013:Lamport| 2014:Stonebraker| 2015:Diffie,Hellman| 2016:Berners-Lee| 2017:Hennessy,Patterson| 2018:Hinton,LeCun,Bengio| 2019:Catmull,Hanrahan| 2020:Aho,Ullman| 2021:Dongarra| 2022:Metcalfe| 2023:Wigderson| 2024:Barto,Sutton Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Leben 2Wirken 3Sonstiges 4Literatur 5Weblinks 6Belege العربية مصرى Azərbaycanca تۆرکجه Bikol Central Беларуская Български বাংলা Bosanski Català کوردی Čeština Dansk Ελληνικά English Esperanto Español Euskara فارسی Suomi Français Gaeilge Galego עברית हिन्दी"
  },
  {
    "label": 1,
    "text": "Künstliche Intelligenz – Wikipedia Künstliche Intelligenz Inhaltsverzeichnis Eigenschaften von Intelligenz Begriffsherkunft Definitionsversuche Metaphorik Starke und schwache KI Forschungsgebiete Geschichte Teilgebiete Methoden Anwendungen Turing-Test Technologische Singularität Bewusstsein bei künstlicher Intelligenz Angrenzende Wissenschaften Kritik an der KI-Forschung Grundlegende Schwachstellen der KI Regulierung und Gesetzgebung Darstellung in Film, Videospielen, Literatur und Musik Soziale Auswirkungen Umweltaspekte Siehe auch Weblinks Literatur Einzelnachweise EU-rechtliche Definition Alltagstaugliche Definitionen Wissensbasierte Systeme Musteranalyse und Mustererkennung Mustervorhersage Robotik Künstliches Leben AI-Alignment Suchen Planen Optimierungsmethoden Logisches Schließen Approximationsmethoden Sprachwissenschaft Psychologie Psychotherapie Philosophie Menschenrechte Klimatologie und Ökologie Informatik Vorschläge zum Umgang mit KI Verbreitung von KI in Deutschland Das KI-Observatorium Europäische Union Vereinigte Staaten Beispiele Kritik Künstliche Intelligenz(KI),englischartificial intelligence, daher auchartifizielle Intelligenz(AI), bezeichnet im weitesten Sinne computerbasierte Systeme, die ihre (virtuelleoder reale) Umgebung analysieren können, um daraus relevanteInformationenzu abstrahieren, welche sie nutzen, um Entscheidungen zu treffen, die ihre Chance erhöhen, definierte Ziele zu erreichen. Damit unterscheiden sich KI-gestützte Systeme vonregelbasierten Systemenohne Fähigkeit zur eigenständigen Anpassung ihres Verhaltens, die ausschließlich fest vorgegebene Anweisungen ausführen. KI ist außerdem die Bezeichnung für das Teilgebiet derInformatik, das sich mit der Entwicklung und Erforschung von Software und Methoden befasst, die besagte Systeme hervorbringen. Die KI als Forschungsfeld befasst sich in diesem Zusammenhang beispielsweise mit derAutomatisierungintelligenten Verhaltens und demmaschinellen Lernensowie der Formalisierung von Bewusstsein und Kreativität. Der Begriff ist schwierig zu definieren, da es verschiedeneDefinitionenvonIntelligenzgibt. Mit der Zeit haben sich viele Bereiche zu den Methoden der KI entwickelt. Weiterhin wird unterschieden, welche Probleme mit den Methoden der KI beschrieben werden. Dabei entstanden zwei Bereiche:Schwache KIundStarke KI. Hier lassen sich viele Kategorien bilden und der wissenschaftliche Diskurs ist noch nicht sehr weit in der Zuordnung von Themen zu den Arten der Probleme. DeringenieurwissenschaftlicheTeil der Informatik befasst sich damit, wie solche System realisiert werden können. Beispiele dafür sindMultiagentensysteme,Expertensysteme,Transformeroderserviceorientierte Architekturen. Versuchsweise wird Intelligenz definiert als die Eigenschaft, die ein Wesen befähigt, angemessen und vorausschauend in seiner Umgebung zu agieren. Dazu gehören die Fähigkeiten Praktische Erfolge der KI werden schnell in die Anwendungsbereiche integriert und werden dann von vielen nicht mehr als KI angesehen, auch wenn sie deren Definition erfüllen.[2]Dieses Phänomen wird auch als „AI effect“ bezeichnet.[2] Der Begriffartificial intelligence(künstliche Intelligenz) wurde 1955 geprägt von dem US-amerikanischen InformatikerJohn McCarthyim Rahmen eines Förderantrags an dieRockefeller-Stiftungfür dasDartmouth Summer Research Project on Artificial Intelligence, einem Forschungsprojekt, bei dem sich im Sommer 1956 eine Gruppe von 10 Wissenschaftlern über ca. 8 Wochen mit der Thematik befasste.[3][4] Es existieren zahlreiche Definitionen für den Begriff der KI. Je nach Sichtweise wird die künstliche Intelligenz in Industrie, Forschung und Politik entweder über die zu erzielenden Anwendungen oder den Blick auf die wissenschaftlichen Grundlagen definiert: „Künstliche Intelligenz ist die Eigenschaft eines IT-Systems, ‚menschenähnliche‘, intelligente Verhaltensweisen zu zeigen.“ „Die künstliche Intelligenz [...] ist ein Teilgebiet der Informatik, welches sich mit der Erforschung von Mechanismen des intelligenten menschlichen Verhaltens befasst [...].“ „Unter künstlicher Intelligenz (KI) verstehen wir Technologien, die menschliche Fähigkeiten im Sehen, Hören, Analysieren, Entscheiden und Handeln ergänzen und stärken.“ „Künstliche Intelligenz ist die Fähigkeit einer Maschine, menschliche Fähigkeiten wie logisches Denken, Lernen, Planen und Kreativität zu imitieren.“ Die Definitionen für künstliche Intelligenz lassen sich nachStuart J. RussellundPeter Norvigin vier Kategorien einteilen:[8] DieKI-VerordnungderEUdefiniert in Artikel 3 (Begriffsbestimmungen) ein „KI-System“ wie folgt: „Für die Zwecke dieser Verordnung bezeichnet der Ausdruck […] ‚KI-System‘ ein maschinengestütztes System, das für einen in unterschiedlichem Grade autonomen Betrieb ausgelegt ist und das nach seiner Betriebsaufnahme anpassungsfähig sein kann und das aus den erhaltenen Eingaben für explizite oder implizite Ziele ableitet, wie Ausgaben wie etwa Vorhersagen, Inhalte, Empfehlungen oder Entscheidungen erstellt werden, die physische oder virtuelle Umgebungen beeinflussen können“[9] Diese Definition wird im Erwägungsgrund 12 etwas allgemeinverständlicher eingeordnet.[9] Um ein System im Alltag konkret als „KI“ oder „nicht KI“ einordnen zu können, ist in vielen Unternehmen eine Annäherung über möglichst konkrete und verständliche Kriterien notwendig. Diese werden z. B. von IT- undCompliance-Stakeholdernerarbeitet, stehen im Kontext vorhandener Informationssicherheits, Compliance- und/oder Risikomanagement-Systemeund orientieren sich - Stand Dezember 2024 - beispielsweise an folgenden Definitionen: Diese Annäherungen können sowohl auf generative als auch auf prädiktive KI angewendet werden. Ein Non-KI-System, in dessen Algorithmen ein zuvor woanders erlerntes KI-Verarbeitungsmuster integriert ist, wird aufgrund der Risiken, die sich aus der o. g. Intransparenz ergeben, derzeit meist ebenfalls als KI-System eingeordnet. Dies betrifft beispielsweise die Mehrfachverwendung eines allgemeinenLLM-Modells in unterschiedlichen fachlichen Kontexten. (Stand Dezember 2024) Der Vorteil der vorgenannten Annäherungen liegt darin, dass sie die Unschärfen des „Intelligenz“-Begriffs sowie Interpretationsspielräume der o. g. EU-Verordnung vermeiden, indem ausschließlich bekannte technische Aspekte der in Frage kommenden Systeme zugrunde gelegt werden: Für jedenQuellcode(White Box) kann eine eindeutige Aussage abgeleitet werden, ob eine Lernfähigkeit (z. B. nach Art der unten aufgeführten Methoden) eingebaut ist oder nicht. Ist der Quellcode nicht einsehbar, kann der Hersteller/Programmierer konsultiert werden. Unterscheidungsbeispiel: Im vorgenannten Beispiel liegt der Fokus auf der Routenfindung. Eine ggf. vorgeschaltete Sprachsteuerung wäre separat zu betrachten. Der Diskurs über KI ist stark von einer metaphorischen Sprache geprägt[10][11][12]. Begriffe wiekünstliche Intelligenzundmaschinelles Lernensind selbstanthropomorpheMetaphern, die auf die menschliche Kognition anspielen. Eine weitere gängige Metapher ist dieBlack Box, die die Intransparenz vieler KI-Systeme beschreibt[13]. Im wissenschaftlichen Diskurs werden große Sprachmodelle auch alsstochastische Papageienbezeichnet, um darauf hinzuweisen, dass sie Texte erzeugen, ohne den Inhalt wirklich zu verstehen[14]. Der Autor Ted Chiang vergleichtgroße Sprachmodellemit einem unscharfenJPEGaus dem Netz, um Kompressionsmechanismen zu veranschaulichen[15]. Anthropomorphe Metaphern sind umstritten, da sie ein übertriebenes oder verzerrtes Bild von KI-Systemen vermitteln können.[16][17]Alternativ diskutieren Forscher und Journalisten Metaphern wie KI als Werkzeuge,[18]Spiegel,[19]Tiere, Organismen oder Naturphänomene.[20]Die Wahl einer Metapher beeinflusst nicht nur das öffentliche Verständnis von KI[21], sondern spielt auch eine Rolle in der Gesetzgebung[22], Regulierung[23]und wissenschaftlichen Forschung[24]. Starke KI wärenkognitive Systeme, die auf Augenhöhe mit Menschen dieArbeitzur Erledigung schwieriger Aufgaben übernehmen können. Demgegenüber geht es beischwacher KIdarum, konkrete Anwendungsprobleme zu meistern. Das menschliche Denken und technische Anwendungen sollen hier in Einzelbereichen unterstützt werden.[25] Die Fähigkeit zu lernen ist eine Hauptanforderung an KI-Systeme und muss ein integraler Bestandteil sein, dernicht erst nachträglichhinzugefügt werden darf. Ein zweites Hauptkriterium ist die Fähigkeit eines KI-Systems, mit Unsicherheiten undWahrscheinlichkeiten(sowie mitprobabilistischenInformationen) umzugehen.[26]Insbesondere sind solche Anwendungen von Interesse, zu deren Lösung nach allgemeinem Verständnis eine Form von „Intelligenz“ notwendig zu sein scheint.[27] Letztlich geht es der schwachen KI somit um die Simulation intelligenten Verhaltens mit Mitteln der Mathematik und der Informatik, es geht ihr nicht um Schaffung vonBewusstseinoder um ein tieferes Verständnis von Intelligenz. Während die Schaffung starker KI an ihrerphilosophischenFragestellung bis heute scheiterte, sind auf der Seite derschwachen KIin den letzten Jahren bedeutende Fortschritte erzielt worden. Ein starkes KI-System muss nicht viel mit dem Menschen gemeinsam haben. Es wird wahrscheinlich eine andersartige kognitive Architektur aufweisen und auch in seinen Entwicklungsstadien nicht mit den evolutionären kognitiven Stadien des menschlichen Denkens vergleichbar sein (Evolution des Denkens). Vor allem ist nicht anzunehmen, dass eine künstliche Intelligenz Gefühle wie Liebe, Hass, Angst oder Freude besitzt.[28] Neben den Forschungsergebnissen der Kerninformatik selbst sind in die Erforschung der KI Ergebnisse derPsychologie,NeurologieundNeurowissenschaften, derMathematikundLogik,Kommunikationswissenschaft,PhilosophieundLinguistikeingeflossen. Umgekehrt nahm die Erforschung der KI auch ihrerseits Einfluss auf andere Gebiete, vor allem auf die Neurowissenschaften. Dies zeigt sich in der Ausbildung des Bereichs derNeuroinformatik, der der biologieorientierten Informatik zugeordnet ist, sowie derComputational Neuroscience. Beikünstlichen neuronalen Netzenhandelt es sich um Techniken, die ab Mitte des 20. Jahrhunderts entwickelt wurden und auf derNeurophysiologieaufbauen. KI stellt somit kein geschlossenes Forschungsgebiet dar. Vielmehr werden Techniken aus verschiedenen Disziplinen verwendet, ohne dass diese eine Verbindung miteinander haben müssen. Eine wichtige Tagung ist dieInternational Joint Conference on Artificial Intelligence(IJCAI), die seit 1969 stattfindet. Seit der Begriffsprägung im Jahre 1955 hat sich eine Reihe relativ selbständiger Teildisziplinen herausgebildet: Zur Forschungsrichtungkünstliches Lebenbestehen enge Beziehungen. Das Fernziel der KI ist die alsstarke KI oder künstliche allgemeine Intelligenzbezeichnete Fähigkeit eines intelligenten Agenten, jede intellektuelle Aufgabe zu verstehen oder zu erlernen, die der Mensch oder ein anderes Lebewesen bewältigen kann. Wissensbasierte Systeme modellieren eine Form rationaler Intelligenz für sogenannteExpertensysteme. Diese sind in der Lage, auf eine Frage des Anwenders auf Grundlage formalisierten Fachwissens und daraus gezogenerlogischer SchlüsseAntworten zu liefern. Beispielhafte Anwendungen finden sich in der Diagnose von Krankheiten oder der Suche und Beseitigung von Fehlern in technischen Systemen. Beispiele für wissensbasierte Systeme sindCycundWatson. Visuelle Intelligenzermöglicht es, Bilder bzw. Formen zuerkennenund zuanalysieren. Als Anwendungsbeispiele seien hierHandschrifterkennung, Identifikation von Personen durchGesichtserkennung, Abgleich derFingerabdrückeoder derIris, industrielle Qualitätskontrolle und Fertigungsautomation (letzteres in Kombination mit Erkenntnissen der Robotik) genannt. Mittelssprachlicher Intelligenzist es beispielsweise möglich, einen geschriebenen Text in gesprochene Sprache umzuwandeln (Sprachsynthese) und umgekehrt einen gesprochenen Text zu verschriftlichen (Spracherkennung). Diese automatische Sprachverarbeitung kann erweitert werden, so dass etwa durchlatente semantische Analyse(kurzLSI) Wörtern und Texten Bedeutung beigemessen werden kann. Beispiele für Systeme zur Mustererkennung sindGoogle BrainundMicrosoft Adam.[29] Die Mustervorhersage ist eine Erweiterung der Mustererkennung. Sie stellt die Grundlage des vonJeff Hawkinsdefiniertenhierarchischen Temporalspeichersdar. “Prediction is not just one of the things your brain does. It is the primary function of the neocortex, and the foundation of intelligence.” „Vorhersageist nicht einfach nur eines der Dinge, die deinGehirntut. Sie ist die Hauptfunktion desNeocortexund das Fundament der Intelligenz.“ Solche Systeme haben den Vorteil, dass sie z. B. nicht nur ein bestimmtes Objekt in einem Einzelbild erkennen (Mustererkennung), sondern aus einer Serie von Bildern vorhersagen können, wo sich das Objekt als Nächstes befinden wird. Die Robotik beschäftigt sich mit manipulativer Intelligenz. Mit Hilfe vonRoboternkönnen unter anderem gefährliche Tätigkeiten wie etwa dieMinensucheoder auch immer gleiche Manipulationen, wie sie beim Schweißen oder Lackieren auftreten können, automatisiert werden. Der Grundgedanke ist es, Systeme zu schaffen, die intelligente Verhaltensweisen von Lebewesen nachvollziehen können. Beispiele für derartige Roboter sindASIMOundAtlas. KI überlappt sich mit der Disziplin künstliches Leben (Artificial life,AL),[31]und wird als übergeordnete oder auch als eine Subdisziplin gesehen.[32]AL muss deren Erkenntnisse integrieren, daKognitioneine Kerneigenschaft von natürlichem Leben ist, nicht nur des Menschen. Das junge Forschungsfeld derAI-Alignment(zu deutsch KI-Ausrichtung) beschäftigt sich mit der Ausrichtung von KI nach menschlichen Werten und Normen. Unabhängig von der Frage, ob die jeweilige KI über eine Form von Bewusstsein verfügt, verhält sich jede KI entsprechend ihrem Training. Unter anderem durch Fehler oder Lücken im Training kann einer KI leicht Verhalten antrainiert werden, das nicht mit menschlichen Werten vereinbar ist.[26]Die Forschung versucht herauszufinden, wie und ob ethisches Verhalten in KI sichergestellt werden kann, um Probleme wie im Einsatz von KI in Krankenhäusern und Gerichtssälen zu verhindern, aber auch, um die Risiken durch weit fortgeschrittene KI wie im Falle vonTechnologischer Singularität, zu minimieren.[28] Die Methoden der KI lassen sich grob in zwei Dimensionen einordnen: symbolische vs. neuronale KI und Simulationsmethode vs. phänomenologische Methode. Die Zusammenhänge veranschaulicht die folgende Grafik: Die Neuronale KI verfolgt einenBottom-up-Ansatz und möchte das menschliche Gehirn möglichst präzise nachbilden. Die symbolische KI verfolgt umgekehrt einenTop-down-Ansatz und nähert sich den Intelligenzleistungen von einer begrifflichen Ebene her. Die Simulationsmethode orientiert sich so nah wie möglich an den tatsächlichen kognitiven Prozessen des Menschen. Dagegen kommt es demphänomenologischenAnsatz nur auf das Ergebnis an. Viele ältere Methoden, die in der KI entwickelt wurden, basieren aufheuristischenLösungsverfahren. In jüngerer Zeit spielen mathematisch fundierte Ansätze aus derStatistik, dermathematischen Programmierungund derApproximationstheorieeine bedeutende Rolle. Die konkreten Techniken der KI lassen sich grob in Gruppen einteilen: Die KI beschäftigt sich häufig mit Problemen, bei denen nach bestimmten Lösungen gesucht wird. VerschiedeneSuchalgorithmenwerden dabei eingesetzt. Ein Paradebeispiel für die Suche ist der Vorgang derWegfindung, der in vielen Computerspielen eine zentrale Rolle einnimmt und auf Suchalgorithmen wie demA*-Algorithmusbasiert. Neben dem Suchen von Lösungen stellt das Planen einen wichtigen Aspekt der KI dar. Der Vorgang des Planens unterteilt sich dabei in zwei Phasen: Planungssysteme planen und erstellen aus solchen Problembeschreibungen Aktionsfolgen, dieAgentensystemeausführen können, um ihre Ziele zu erreichen. Oft führen Aufgabenstellungen der KI zuOptimierungsproblemen. Diese werden je nach Struktur entweder mit Suchalgorithmen aus der Informatik oder, zunehmend, mit Mitteln dermathematischen Optimierunggelöst. BekannteheuristischeSuchverfahren aus dem Kontext der KI sindevolutionäre Algorithmen. Eine Fragestellung der KI ist die Erstellung vonWissensrepräsentationen, die dann für automatischeslogisches Schließenbenutzt werden können. MenschlichesWissenwird dabei – soweit möglich – formalisiert, um es in eine maschinenlesbare Form zu bringen. Diesem Ziel haben sich die Entwickler diverserOntologienverschrieben. Schon früh beschäftigte sich die KI damit, automatischeBeweissystemezu konstruieren, die Mathematikern und Informatikern beim Beweisen von Sätzen und beim Programmieren (Logikprogrammierung) behilflich wären. Zwei Schwierigkeiten zeichneten sich ab: Eine andere Form des logischen Schließens stellt die Induktion dar (Induktionsschluss,Induktionslogik), in der Beispiele zu Regeln verallgemeinert werden (maschinelles Lernen). Auch hier spielen Art und Mächtigkeit derWissensrepräsentationeine wichtige Rolle. Man unterscheidet zwischen symbolischen Systemen, in denen das Wissen – sowohl die Beispiele als auch die induzierten Regeln – explizit repräsentiert ist, und subsymbolischen Systemen wie neuronalen Netzen, denen zwar ein berechenbares Verhalten „antrainiert“ wird, die jedoch keinen Einblick in die erlernten Lösungswege erlauben. In vielen Anwendungen geht es darum, aus einer Menge von Daten eine allgemeine Regel abzuleiten (maschinelles Lernen). Mathematisch führt dies zu einemApproximationsproblem. Im Kontext der KI wurden hierzu unter anderem künstliche neuronale Netze vorgeschlagen, die als universale Funktionsapproximatoren eingesetzt werden können, jedoch insbesondere bei vielen verdeckten Schichten schwer zu analysieren sind. Manchmal verwendet man deshalb alternative Verfahren, die mathematisch einfacher zu analysieren sind. Künstliches neuronales Netz Große Fortschritte erzielt die künstliche Intelligenz in jüngerer Zeit im Bereich künstlicher neuronaler Netze, auch unter dem Begriff Deep Learning bekannt. Dabei werden neuronale Netze, die grob von der Struktur des Gehirns inspiriert sind, künstlich auf dem Computer simuliert. Viele der jüngeren Erfolge wie beiHandschrifterkennung,Spracherkennung,Gesichtserkennung,autonomem Fahren,maschineller ÜbersetzungwieDeepL,AlphaGo,ChatGPT,DeepSeekberuhen auf dieser Technik. Künstliche Intelligenz hat eine Vielzahl von Verwendungen in Forschung und Wirtschaft. Mit der rasanten Entwicklung in diesem Bereich entsteht auch ein entsprechender Bedarf an Methoden zurAI-Detection.[33] Um ein Kriterium zu haben, wann eine Maschine eine dem Menschen gleichwertigeIntelligenzsimuliert, wurde vonAlan Turingder nach ihm benannte Turing-Test vorgeschlagen: Dazu stellt ein Mensch per Terminal (Bildschirm und Tastatur, oder auch Lautsprecher und Mikrofon) beliebige Fragen, ohne dabei zu wissen, ob diese von einem anderen Menschen oder einer Maschine beantwortet werden. Der Fragesteller muss danach entscheiden, ob es sich beim Interviewpartner um eine Maschine oder einen Menschen handelte. Ist die Maschine nicht von einem Menschen zu unterscheiden, so ist sie laut Turing intelligent.[34]Bisher konnte keine Maschine den Turing-Test zweifelsfrei bestehen. Seit 1991 existiert derLoebner-Preisfür den Turing-Test. Grob wird unter der technologischen Singularität der hypothetische Zeitpunkt verstanden, an dem künstliche Intelligenz die menschliche Intelligenz übertrifft. Ab diesem Zeitpunkt wird die weitere technologische Entwicklung hauptsächlich von KI vorangetrieben und nicht mehr vom Menschen. In denNeurowissenschaftenist es eine Grundannahme, dassBewusstseinsprozessemit neuronalen Prozessen des Gehirns korrelieren (sieheNeuronales Korrelat des Bewusstseins). NachJürgen Schmidhuberist Bewusstsein nur ein Nebenprodukt des Problemlösens des Gehirns. So sei auch bei künstlichen Problemlösern (z. B.autonomen mobilen Robotern) von Vorteil, wenn diese sich ihrer selbst und ihrer Umgebung „bewusst“ seien. Schmidhuber bezieht sich bei „Bewusstsein“ im Kontext autonomer Roboter auf ein digitales Weltmodell inklusive des Systems selbst, nicht jedoch aufsubjektive Erlebnisqualitätenvon Zuständen. Ein Weltmodell könnte im Kontext vonReinforcement Learningdadurch erlernt werden, dass Aktionen belohnt werden, die das Weltmodell erweitern.[35] Solche Überlegungen sind jedoch hoch spekulativ, da weder eine allgemein akzeptierte Definition, geschweige denn eine anerkannte Theorie von Bewusstsein existiert. Wenn Bewusstsein eineemergenteEigenschaft des Informationsverarbeitens ist, wäre es möglich, dass KI Bewusstsein erlangt. Da die meisten Definitionen jedoch ein „inneres Erleben“ derWahrnehmungmit Bewusstsein verbinden, das beim Menschen zu komplexenWeltanschauungenführt, gibt es bislang keine empirischen Belege dafür, dass KI-Systeme über etwas Ähnliches verfügen. Ihre „Intelligenz“ ist rein funktional und instrumental. Im Gegensatz zum menschlichen Gehirn basieren aktuelle KI-Systeme auf Algorithmen, neuronalen Netzen und statistischen Modellen. Sie verarbeiten Daten nach mathematischen Regeln ohne echteKreativitätoderPhantasie. Andererseits lassen sich innere Zustände nicht beweisen, sodass es keine verlässliche Antwort geben kann. Die Interpretation menschlicher Sprache durch Maschinen besitzt bei der KI-Forschung eine entscheidende Rolle. So ergeben sich etwaige Ergebnisse desTuring-Testsvor allem in Dialogsituationen, die bewältigt werden müssen. Die Sprachwissenschaft liefert mit ihrenGrammatikmodellenund psycholinguistischen Semantikmodellen wie derMerkmals-oder derPrototypensemantikGrundlagen für das maschinelle „Verstehen“ komplexer natürlichsprachlicher Phrasen. Zentral ist die Frage, wie Sprachzeichen eine tatsächlicheBedeutungfür eine künstliche Intelligenz haben können.[36]DasChinese-Room-Argument des PhilosophenJohn Searlesollte indes zeigen, dass es selbst dann möglich wäre, denTuring-Testzu bestehen, wenn den verwendeten Sprachzeichen dabei keinerlei Bedeutung beigemessen wird. Insbesondere Ergebnisse aus dem BereichEmbodimentbetonen zudem die Relevanz von solchen Erfahrungen, die auf der Verkörperung eines Agenten beruhen sowie dessen Einbindung in eine sinnvolle Umgebung für jede Form von Kognition, also auch zur Konstruktion von Bedeutung durch eine Intelligenz. Eine Schnittstelle zwischen der Linguistik und der Informatik bildet dieComputerlinguistik, die sich unter anderem mit maschineller Sprachverarbeitung und künstlicher Intelligenz beschäftigt. Die Psychologie beschäftigt sich unter anderem mit dem BegriffIntelligenz. In der Psychotherapieforschung existieren seit geraumer Zeit experimentelle Anwendungen der künstlichen Intelligenz, um Defizite und Engpässe in der psychotherapeutischen Versorgung zu überbrücken und Kosten zu sparen.[37] DiephilosophischenAspekte der KI-Problematik gehören zu den weitreichendsten der gesamten Informatik. Die Antworten, die auf die zentralen Fragen dieses Bereiches gegeben werden, reichen weit inontologischeunderkenntnistheoretischeThemen hinein, die das Denken des Menschen schon seit den Anfängen der Philosophie beschäftigen. Wer solche Antworten gibt, muss die Konsequenzen daraus auch für den Menschen und sich selbst ziehen. Nicht selten möchte man umgekehrt vorgehen und die Antworten, die man vor der Entwicklung künstlicher Intelligenz gefunden hat, auf diese übertragen. Doch wie sich zeigte, hat die künstliche Intelligenz zahlreiche Forscher dazu veranlasst, Probleme wie das Verhältnis zwischenMaterieundGeist, die Ursprünge des Bewusstseins, die Grenzen der Erkenntnis, das Problem derEmergenz, die Möglichkeit außermenschlicher Intelligenz usw. in einem neuen Licht zu betrachten und zum Teil neu zu bewerten. Eine demmetaphysischenbzw. auch idealistischen Denken verpflichtete Sichtweise hält es (im Sinn einer schwachen KI) für unmöglich, dass Maschinen jemals mehr als nur simuliertes Bewusstsein mit wirklicher Erkenntnis und Freiheit besitzen könnten. Aus ontologischer Sicht kritisiert der amerikanische PhilosophHubert Dreyfusdie Auffassung der starken KI. Aufbauend auf der vonMartin Heideggerin dessen WerkSein und Zeitentwickelten Ontologie der „Weltlichkeit der Welt“ versucht Dreyfus zu zeigen, dass hinter das Phänomen der Welt als sinnhafte Bedeutungsganzheit nicht zurückgegangen werden kann: Sinn, d. h. Beziehungen der Dinge in der Welt aufeinander, sei ein Emergenzphänomen, denn es gibt nicht „etwas Sinn“ und dann „mehr Sinn“. Damit erweist sich jedoch auch die Aufgabe, die sinnhaften Beziehungen zwischen den Dingen der Welt in einen Computer einzuprogrammieren, als eigentlich unmögliches bzw. unendliches Vorhaben. Dies deshalb, weil Sinn nicht durch Addition von zunächst sinnlosen Elementen hergestellt werden kann.[38] Eine evolutionär-progressive Denkrichtung sieht es hingegen (im Sinn einer starken KI) als möglich an, dass Systeme der künstlichen Intelligenz einmal den Menschen in dem übertreffen könnten, was derzeit noch als spezifisch menschlich gilt. Dies birgt zum einen die Gefahr, dass solche KI-Maschinen sich gegen die Interessen der Menschen wenden könnten. Andererseits birgt diese Technologie die Chance, Probleme zu lösen, deren Lösung dem Menschen wegen seiner limitierten Kapazitäten schwerfällt (siehe auchtechnologische Singularität). Weitere Anknüpfungspunkte lassen sich in deranalytischen Philosophiefinden. DieEthik der künstlichen Intelligenzerforscht ethische Normen für Entwurf, Herstellung, Testung, Zertifizierung und den Einsatz künstlich intelligenter Systeme und fragt nach Prinzipien für das ethische Verhalten von KI-Systemen. Intensiv untersuchte Themen sind dabei ethische Fragen desautonomen Fahrensundautonomer Waffensystemesowie die Probleme und Realisierungsmöglichkeiten künstlicher moralischer Agenten.[39][40] RechtsphilosophieundRoboterethikgehen der Frage nach, ob eine KI für ihr gesetzwidriges Handeln oder Fehlverhalten verantwortlich gemacht werden kann (z. B. bei einem Autounfall durch ein autonomes Fahrzeug) und wer dafür haftet.[41] Der russisch-amerikanische Biochemiker und SachbuchautorIsaac Asimovbeschreibt in seinen dreiRobotergesetzendie Voraussetzungen für ein friedliches und unterstützendes Zusammenleben zwischen KI und Mensch. Diese Gesetze wurden später von anderen Autoren erweitert. BeiKarl Marxfinden sich im sogenanntenMaschinenfragment, einem Teil derGrundrisse (1857–58), Überlegungen zur Ersetzung menschlicher Arbeitskraft durch Maschinen, die sich auch auf Maschinen mit künstlicher Intelligenz anwenden lassen.[42] Zu den zentralen Fragen beim KI-Einsatz gehören die Aufteilung rechtlicher Verpflichtungen zwischen Staaten und Unternehmen sowie die Implikationen der Menschenrechte im Hinblick auf den Einsatz von KI in bestimmten Anwendungsbereichen, z. B. bei der Gesichtserkennung oder Erleichterung der Entscheidungsfindung von Gerichten. Auch wird das Ausmaß der technologischen Zusammenarbeit im Bereich der KI mit Staaten, die sich nicht an menschenrechtliche Grundstandards halten, aus wirtschaftsethischer und völkerrechtlicher Perspektive diskutiert.[43][44] Der exponentiell zunehmende massiv ansteigende Energieverbrauch durch KI und der damit verbundene erhöhte Ausstoß des klimaaktiven Gases Kohlendioxid[45]wirft grundlegende Fragen auf, wie sich die neue Technologie auf den menschengemachten Klimawandel auswirken wird. Auch der enorme Wasserverbrauch zur Kühlung der Rechenzentren wird seit einiger Zeit genauer untersucht. Im Januar 2024 veröffentlichte die Internationale Energieagentur (IEA) die Publikation „Electricity 2024, Analysis and Forecast to 2026“, eine Prognose zum Stromverbrauch.[46]Dies ist der erste IEA-Bericht, der Prognosen für Rechenzentren und den Stromverbrauch für künstliche Intelligenz und Kryptowährungen enthält. Der Bericht besagt, dass sich der Strombedarf für diese Anwendungen von 2024 bis 2026 verdoppeln könnte, wobei der zusätzliche Stromverbrauch dem Stromverbrauch von ganz Japan entsprechen würde.[47] Der enorme Stromverbrauch von KI ist somit für einen Anstieg der Nutzung fossiler Brennstoffe mitverantwortlich und könnte weltweit die Schließung veralteter, kohlenstoffemittierender Kohlekraftwerke verzögern. So wird z. B. in den gesamten USA im ersten Halbjahr 2024 in großem Ausmaß mit dem Bau von Rechenzentren begonnen, was international führende Technologieunternehmen (z. B. ChatGPT, Meta, Google, Amazon) quasi über Nacht zu Marktteilnehmern mit massiv steigendem Stromverbrauch macht. Der prognostizierte Stromverbrauch ist so immens, dass die Sorge besteht, dass er die Maßnahmen gegen menschengemachten Klimawandel negativ beeinflussen wird.[48]So verbraucht eine ChatGPT-Suche zehn bis zwanzigmal so viel elektrische Energie wie eine bisherige normale Google-Suche.[49] Auch der Wasserverbrauch von K.I. ist zurzeit immens: Laut einer Forschungsarbeit der University of California wird für eine einzelne Anfrage an ChatGPT das Äquivalent von einer Flasche Wasser benötigt.[50] Diekünstliche Intelligenzist mit den anderen Disziplinen der Informatik eng verzahnt. Eine Abgrenzung kann anhand der erzielten Ergebnisse versucht werden. Hierzu scheint es sinnvoll, verschiedene Dimensionen von Intelligenz zu unterscheiden: Seit 1966 wird mit demTuring Awardein Informatikpreis vergeben. Viele der Preisträger wurden wegen ihrer Errungenschaften im Bereich der Erforschung und Entwicklung künstlicher Intelligenz ausgezeichnet. Stephen Hawkingwarnte 2014 vor der KI und sah darin eine Bedrohung für die Menschheit. Durch die KI könnte dasEnde der Menschheiteingeleitet werden. Ob die Maschinen irgendwann die Kontrolle übernehmen werden, werde die Zukunft zeigen. Aber es sei klar, dass die Maschinen die Menschen zunehmend vom Arbeitsmarkt verdrängen.[51][52][53] Im August 2017 forderten 116 Unternehmer und Experten aus der Technologiebranche (u. a.Mustafa Suleyman,Elon Musk,Yoshua Bengio,Stuart Russell,Jürgen Schmidhuber) in einem offenen Brief an die UN, dass autonome Waffen verboten werden sollten bzw. auf die seit 1983 bestehende CCW-Liste gesetzt werden sollen. Die Certain Conventional Weapons sind von derUNverboten und beinhalten unter anderem Chemiewaffen. Nach Schwarzpulver und der Atombombe drohe die dritte Revolution der Kriegsführung. Zitat aus dem Schreiben: „Wenn dieseBüchse der Pandoraeinmal geöffnet ist, wird es schwierig, sie wieder zu schließen“ und „Einmal erfunden, könnten sie bewaffnete Konflikte erlauben in einem nie dagewesenen Ausmaß, und schneller, als Menschen sie begreifen können“. Terroristen und Despoten könnten die autonomen Waffen nutzen und sogar hacken.[54][55] Argumentativ entgegengetreten sind solchen Positionen u. a.Rodney BrooksundJean-Gabriel Ganascia.[56]Jörg Phil Friedrichvertritt den Standpunkt, es sei weniger eine künstliche Intelligenz, die uns in den KI-Systemen begegne, „sondern eine über weite Strecken degenerierte menschliche Intelligenz“.[57] Im Februar 2018 wurde ein Bericht einer Projektgruppe führender Experten im Bereich KI veröffentlicht, der vor möglichen „Bösartige[n] Nutzungen künstlicher Intelligenz“ (englischer Originaltitel: „The Malicious Use of Artificial Intelligence“) warnt.[58]Beteiligt waren daran unter anderem Forscher der Universitäten von Oxford, Yale und Stanford, sowie Entwickler von Microsoft und Google. Der Bericht nimmt Bezug auf schon existierende Technologien und demonstriert anhand von diversen Szenarien, wie diese von Terroristen, Kriminellen und despotischen Regierungen missbraucht werden könnten.[58]Die Autoren des Berichts fordern daher eine engere Zusammenarbeit von Forschern, Entwicklern und Gesetzgeber im Bereich KI und schlagen konkrete Maßnahmen vor, wie die Gefahren des Missbrauchs verringert werden könnten.[58] Der HistorikerYuval Noah Hararisagt, „künstliche Intelligenz undBiotechnologiekönnen zerstören, was den Menschen ausmacht.“ Er warnt vor einem Wettrüsten im Bereich der künstlichen Intelligenz und empfiehlt globale Zusammenarbeit angesichts dieser „existenziellen Bedrohung.“[59]2024 äußerte er sich im WochenmagazinSternbesorgt, weil KI „die erste Technologie“ sei, „die eigene Entscheidungen treffen“ könne. Noch komme sie „recht primitiv daher“, doch schreite die Entwicklung zu schnell voran, ohne dass gegenwärtig die damit verbundenen Risiken eingeschätzt werden könnten. Es sei zu befürchten, die KI könnte eines Tages „Waffensysteme selbständig kontrollieren“ und „allein entscheiden, welche Person sie töten“. Die Risiken für die Demokratie bringt Harari mit der potentiellen Fähigkeit der KI in Verbindung, „das erste totale Überwachungssystem der Geschichte zu errichten“.[60] Richard David Prechtwendet sich gegen die Vorstellung, dass künftig böser Wille oder Machtstreben seitens einer entwickelten künstlichen Intelligenz drohe; das Gefahrenpotential liege vielmehr in ihrem falschen Einsatz.[61] Die ehemalige Google-TeamleiterinTimnit Gebruwarnt vor dembiasund dem Energiebedarf großer Sprachmodelle, wasDiskriminierungundKlimakriseverschärfen könnte.[62]Um solchen ungewollten Effekten vorzubeugen, versucht der Forschungsbereich desAI-Alignments(zu deutsch KI-Ausrichtung) sicherzustellen, dass KI nach menschlichen Werten wie etwaEgalitarismushandelt. (Siehe auch:Green IT) Caroline Criado Perez[63]zeigt in ihrer ausführlichen Recherchearbeit das bestehendeGender-Data-Gapseinen negativen Einfluss auf Trainingsdaten von KI nehmen und so bestehende Diskriminierungen reproduziert werden. AlsfeministischeKritik im Bezug auf Daten und Technologisierung entwickelten sich bereits in den Anfängen desInternetsfeministische Praxen wieCyberfeminismusoderTechnofeminismusund prägen den Diskurs derfeministischen KI. Der Präsident vonMicrosoft, Brad Smith, schlug vor, einenVerhaltenskodexaufzustellen, wie etwa eineDigitaleGenfer Konvention,um Risiken der künstlichen Intelligenz zu verringern. Der EthikerPeter Dabrockempfiehlt im Kontext der Benutzung und Programmierung von künstlicher Intelligenz nicht nur die digitale Kompetenz der Beteiligten zu erhöhen, sondern auch auf klassische Bildungselemente zu setzen. Um mit den dazugehörigen Herausforderungen zurechtzukommen sowie die Fähigkeiten zur Unterscheidung und zur Erkennung von Mehrdeutigkeit zu erhöhen, seien Kenntnisse aus Religion, Literatur, Mathematik, Fremdsprachen, Musik und Sport eine gute Voraussetzung.[64] DerDeutsche Bundestaghat am 28. Juni 2018 eineEnquete-KommissionKünstliche Intelligenz – Gesellschaftliche Verantwortung und wirtschaftliche Potenzialeeingesetzt.[65]Am 28. Oktober 2020 hat die Kommission ihren Abschlussbericht vorgelegt. Künstliche Intelligenz ist demnach die nächste Stufe der Digitalisierung. Unter dem Leitbild einer „menschenzentrierten KI“ wird eine „demokratische Gestaltung“ der Entwicklung gefordert, so dass KI-Anwendungen vorrangig auf das Wohl und die Würde der Menschen ausgerichtet seien und einen gesellschaftlichen Nutzen bringen. Um einer Diskriminierung von Menschen entgegenzuwirken „braucht es, wenn KI über Menschen urteilt, einen Anspruch auf Transparenz, Nachvollziehbarkeit und Erklärbarkeit von KI-Entscheidungen, damit eine gerichtliche Überprüfung automatisierter Entscheidungen möglich ist“.[66] 2021 veröffentlichte dieEU-Kommissioneinen Vorschlag über eineKI-Verordnung, die am 12. Juni 2024 mit dem Titel „Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz)“ veröffentlicht wurde.[67] Im März 2023 wurde ein u. a. von Elon Musk unterstützterAufruf zu einer 6-monatigen KI-Entwicklungspauseveröffentlicht.[68] Der KI-InvestorFabian Westerheideverwies im Zusammenhang mit seinem 2024 erschienenen BuchDie KI-Nationauf hohe Investitionen einiger Staaten – insbesondere Chinas – in eine eigene KI-Strategie, warnte vor der Gefahr einer Überwachung durchBackdoorsbeim Einsatz ausländischer KI und betonte die Bedeutung deutscher und gesamteuropäischer Pläne zur KI.[69] Die Zahl der Betriebe, die KI-Technologien einsetzen, ist in Deutschland noch relativ gering. Ende 2018 haben nur 6 Prozent der Unternehmen KI genutzt oder implementiert. 17 Prozent haben angegeben, KI-Einsätze zu testen oder zumindest solche zu planen.[70]Auch die ZEW-Studie[71]kommt zu einem ähnlichen Ergebnis. Im Jahr 2019 haben rund 17.500 Unternehmen im Berichtskreis der Innovationserhebung (produzierendes Gewerbe und überwiegend unternehmensorientierte Dienstleistungen) KI in Produkten, Dienstleistungen oder internen Prozessen eingesetzt. Das sind 5,8 Prozent der Unternehmen im Berichtskreis. Mit dem Observatorium Künstliche Intelligenz in Arbeit und Gesellschaft (kurz: KI-Observatorium), einem Projekt der Denkfabrik Digitale Arbeitsgesellschaft, fokussiert dasBundesministerium für Arbeit und Sozialesdie Frage nach den Auswirkungen von KI auf Arbeit und Gesellschaft. Das KI-Observatorium agiert an der Schnittstelle zwischen Politik, Wissenschaft, Wirtschaft und Gesellschaft; es fungiert als Wissensträger und Impulsgeber. Das KI-Observatorium hat die Aufgabe, Effekte von KI in der Arbeitswelt frühzeitig zu antizipieren und Handlungsbedarfe aufzuzeigen. Auf diese Weise leistet die im März 2020 gestartete Arbeitseinheit einen Beitrag zur Realisierung der in der KI-Strategie der Bundesregierung formulierten Ziele – etwa zum sicheren und gemeinwohlorientierten Einsatz von KI. Darüber hinaus soll das KI-Observatorium mithilfe von Dialog- und Beteiligungsformaten unterschiedliche gesellschaftliche Akteure im Umgang mit künstlicher Intelligenz befähigen und bestärken.[72] Die konkreten Aufgabenschwerpunkte des Observatoriums sind in den fünf Handlungsfeldern festgehalten:[73] Zu den auch Ende des Jahres 2024 deutlich feststellbaren grundlegendenSchwachstellender KI gehören u. a.: DieVerordnung über künstliche Intelligenz(informell meistKI-Verordnung,englischAI Act) ist eineEU-Verordnungfür dieRegulierung von künstlicher Intelligenz. Es ist die weltweit erste umfassende Regulierung dieser Art. Das Gesetz regelt den Einsatz von KI unter anderem für die kritische Infrastruktur, Sicherheitsbehörden und Personalverwaltung.[80]DieEuropäische Kommissionhat das Gesetz am 21. April 2021 vorgeschlagen und einen ersten Entwurf veröffentlicht.[81]Am 28. September 2022 hat die Europäische Kommission in dem Zusammenhang auch den Entwurf einer Richtlinie über Produkthaftung[82]und einer Richtlinie über KI-Haftung veröffentlicht.[83]Haftungsfragen waren zuvor aus der Verordnung herausgenommen worden.[84]In dem Kontext steht auch die Überarbeitung derMaschinenrichtliniezur EU-Maschinenverordnung, die am 14. Juni 2023 in Kraft getreten ist. Am 9. Dezember 2023 einigten sich die EU-Gesetzgebungsinstitutionen auf die Grundzüge des Gesetzes.[85][86] Am 12. Juni 2024 hat die EU die KI-Verordnung mit dem Titel „Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz)“ veröffentlicht.[67]Hier sind Vorschriften zum Inverkehrbringen, Inbetriebnahme und die Verwendung von KI-Systemen festgelegt. Verbotene Praktiken sind in Kapitel II benannt.[87]Ebenso sind die Kriterien zur Einstufung von KI-Systemen als Hochrisiko-KI-Systeme in Kapitel III beschrieben.[88]Für Hochrisiko-KI-Systeme wird die EU eine Datenbank errichten.[89]Sanktionen gegen die Missachtung der Richtlinie sind mit Geldbußen von 35 Millionen Euro oder 7 % des gesamten weltweiten Umsatzes des verstoßenden Unternehmens belegt.[90]Wie jede EU-Richtlinie muss sie in nationale Gesetze übernommen werden. Für KI-Systeme ist eineEU-Konformitätserklärungvorgeschrieben.[91] Da die KI-Systeme viele Bereiche tangieren, wie am Titel der Verordnung ersichtlich, wurden folgende Richtlinien geändert: Richtlinie 2014/90/EU über Schiffsausrüstung,Richtlinie (EU) 2016/797 über die Interoperabilität des Eisenbahnsystemsin der Europäischen Union. Folgende Verordnungen wurden mit der KI-RL auch geändert: Verordnung (EG) Nr. 300/2008 über gemeinsame Vorschriften für die Sicherheit in der Zivilluftfahrt, Verordnung (EU) Nr. 167/2013 über die Genehmigung und Marktüberwachung von land- und forstwirtschaftlichen Fahrzeugen, Verordnung (EU) Nr. 168/2013 über die Genehmigung und Marktüberwachung von zwei- oder dreirädrigen und vierrädrigen Fahrzeugen, Verordnung (EU) 2018/858 über die Genehmigung und die Marktüberwachung von Kraftfahrzeugen und Kraftfahrzeuganhängern sowie von Systemen, Bauteilen und selbstständigen technischen Einheiten für diese Fahrzeuge, Verordnung (EU) 2018/1139 zur Festlegung gemeinsamer Vorschriften für die Zivilluftfahrt und zur Errichtung einer Agentur der Europäischen Union für Flugsicherheit, Verordnung (EU) 2019/2144 über die Typgenehmigung von Kraftfahrzeugen und Kraftfahrzeuganhängern.[92] Basierend auf den Verordnungen der EU haben einige nationale Stellen bereits eigenen Regularien zum Einsatz im öffentlich-rechtlichen Umfeld ausgearbeitet.[93] In denVereinigten Staatengibt es bislang keine Bundesgesetzgebung, die die Verwendung von künstlicher Intelligenz explizit und umfassend reguliert. Dass der Einsatz von KI möglichst global reguliert wird, halten viele US-amerikanische Juristen jedoch für notwendig, so zum Beispiel Anwalt Shabbi S. Khan: „Generative KI hat das Potenzial, katastrophal zu sein“.[94]Auch dieUS-Regierunghat erkannt, dass die Machtfülle der großen Tech-Unternehmen zu einer Bedrohung derDemokratiewerden kann. Im Juli 2023 wollte US-Präsident Joe Biden eine freiwillige Selbstverpflichtung führender KI-Unternehmen einholen, um zu einer sicheren und transparenten KI-Entwicklung beizutragen.[95] Künstliche Wesen, die denken können, tauchen seit der Antike als Figuren in Erzählungen auf und sind ein ständiges Thema in der Science-Fiction.[96] Seit derKlassischen Modernewird KI in Kunst, Film und Literatur behandelt.[97]Dabei geht es bei der künstlerischen Verarbeitung – im Gegensatz zur KI-Forschung, bei der die technische Realisierung im Vordergrund steht – vor allem um die moralischen, ethischen und religiösen Aspekte und Folgen einer nicht-menschlichen, „maschinellen Intelligenz“. In derRenaissancewurde der Begriff desHomunculusgeprägt, eines künstlichen Miniaturmenschen ohne Seele.[98]Im 18. und 19. Jahrhundert erschienen in der Literatur menschenähnliche Automaten, beispielsweise inE. T. A. HoffmannsDer SandmannundJean PaulsDer Maschinenmann. Im 20. und 21. Jahrhundert greift dieScience-FictioninFilmundProsadas Thema mannigfach auf.[99]1920 prägte der SchriftstellerKarel Čapekden Begriff „Roboter“ in seinem BühnenstückR.U.R.; 1926 thematisierteFritz LanginMetropolisRoboter, welche die Arbeit der Menschen übernehmen.[99] Ein häufiges Motiv im Film und der Literatur begann mitMary ShelleysRomanFrankenstein(1818), in dem eine menschliche Schöpfung zu einer Bedrohung für ihre Meister wird. Dazu gehören Werke wieArthur C. Clarkesund Stanley Kubricks2001:Odyssee im Weltraum(beide 1968), mitHAL 9000, dem mörderischen Computer, der das RaumschiffDiscovery Onesteuert, sowie die Terminator-Filmreihe (ab 1984) undThe Matrix(1999). Im Gegensatz dazu sind die seltenen loyalen Roboter wie Gort ausDer Tag an dem die Erde stillstand(1951) undBishopausAliens(1986) in der Populärkultur weniger präsent.[100] Mehrere Werke nutzen die künstliche Intelligenz, um uns mit der grundlegenden Frage zu konfrontieren, was uns zu Menschen macht, indem sie uns künstliche Wesen zeigen, die die Fähigkeit haben, zu fühlen und somit zu leiden. Dies geschieht in Karel Čapeks R.U.R., dem FilmA.I. Artificial IntelligencevonSteven Spielberg(2001) und anhand der AndroidinAvaim KinofilmEx Machina(2015) vonAlex Garlandsowie in dem RomanTräumen Androiden von elektrischen Schafen?(1968) vonPhilip K. Dick. Dick befasst sich mit der Idee, dass unser Verständnis der menschlichen Subjektivität durch die mit künstlicher Intelligenz geschaffene Technologie verändert wird.[101] Am 30. September 2016 veröffentlichte dieUS-amerikanischePop-Rock-BandOneRepublicmit dem englischenSinger-SongwriterundRockmusikerPeter GabrieldasLiedA.I.[102]als digitaleSingleund am 7. Oktober 2016 auf demAlbumOh My Myvon OneRepublic.[103][104]DerSongist inspiriert von dem FilmA.I. Artificial IntelligencevonSteven Spielbergaus dem Jahr 2001.[105] Die beiden großenScience-Fiction-Franchisesdes frühen 21. Jahrhunderts,Star WarsundStar Trek, gehen sehr unterschiedlich mit dem Thema KI um. Während beiStar WarsKI vor allem in Form vonRoboternundAndroiden(gleichermaßen alsStatistenundHauptfiguren) von Beginn an selbstverständlich und allgegenwärtig erscheint, nahmStar Trekim Laufe der Zeit immer wieder sehr dedizierte, wechselnde Perspektiven ein, obwohl KI auch dort meist selbstverständlich ist (z. B. Simulierte Lebewesen aufHolodecks). Beispielsweise wurde die FolgeWem gehört Data?(1989) zum Arbeitsthema mehrerer Wissenschaftler. Isaac Asimovführte die Drei Gesetze der Robotik in vielen Büchern und Geschichten ein, vor allem in der „Multivac“-Serie über einen superintelligenten Computer gleichen Namens. Asimovs Gesetze werden oft in Laiendiskussionen über Maschinenethik erwähnt;[106]während fast alle Forscher im Bereich der künstlichen Intelligenz mit Asimovs Gesetzen durch die Populärkultur vertraut sind, halten sie die Gesetze im Allgemeinen aus vielen Gründen für nutzlos, einer davon ist ihre Zweideutigkeit.[107] Dem Filmpublikum wurden in den unterschiedlichen Werken die Roboter als intelligente und differenzierte Maschinen mit ganz unterschiedlichen Persönlichkeiten präsentiert: Sie werden entwickelt, um sie für gute Zwecke einzusetzen, wandeln sich aber häufig zu gefährlichen Maschinen, die feindselige Pläne gegen Menschen entwickeln.[108]Im Lauf der Filmgeschichte werden sie zunehmend zu selbstbewussten Wesen, die sich die Menschheit unterwerfen wollen.[108] Die simulierte Realität ist zu einem häufigen Thema in der Science-Fiction geworden, wie beispielsweise in dem Film The Matrix aus dem Jahr 1999 zu sehen ist, in dem eine Welt dargestellt wird, in der künstlich intelligente Roboter die Menschheit in einer Simulation versklaven, die in der heutigen Welt angesiedelt ist.[109]Zuvor thematisierte bereits dieStar Trek TNG- EpisodeDas Schiff in der Flasche(1993) die Steuerung einer simulierten Realität durch eine böswillige KI, welche zuvor in der EpisodeSherlock Data Holmes(1988) versehentlich erschaffen worden war. Auswahl Filme und Literatur: 2025 veröffentlichten 1000 britische Künstler und Gruppen das AlbumIs This What We Want?als Protest gegen Versuche der britischen Regierung denUrheberrechtsschutzzu Gunsten der KI-Industrie aufzuweichen.[111] Im Zuge derindustriellen Revolutionwurde durch die Erfindung der Dampfmaschine die Muskelkraft von der Maschine ersetzt (PSdurchWatt). Durch diedigitale Revolutionkönnte die menschliche Denkleistung durch maschinelle KI ersetzt[112]beziehungsweise ergänzt[113]werden. Der US-amerikanische Unternehmer Elon Musk prognostiziert, dass es zukünftig immer weniger Erwerbsarbeit geben werde, die nicht von einer Maschine besser und günstiger gemacht werden könne, weshalbimmer weniger Arbeitskräfte benötigtwürden. Durch die weitgehend maschinelle Produktion würden die Produkte und Dienstleistungen sehr billig werden. In diesem Zusammenhang unterstützt er die Einführung einesbedingungslosen Grundeinkommens.[114]Der PhysikerStephen Hawkingmeinte: Bereits heute sei klar, dass die Maschinen die Menschen zunehmend vom Arbeitsmarkt verdrängen.[51][52]Microsoft-GründerBill Gatessieht die Entwicklung ähnlich. Er fordert eine Robotersteuer, um die sozialen Aufgaben der Zukunft bewältigen zu können.[115] Die InformatikerinConstanze Kurzerklärte in einem Interview, technischen Fortschritt habe es schon immer gegeben. Jedoch vollzog sich der technische Wandel in der Vergangenheit meist über Generationen, so dass genug Zeit blieb, sich für neue Aufgaben auszubilden. Heute verlaufe der technische Wandel innerhalb von wenigen Jahren, so dass die Menschen nicht genug Zeit hätten, sich für neue Aufgaben weiterzubilden.[116]Der Sprecher desChaos Computer Clubs,Frank Rieger, warnte in verschiedenen Publikationen (z. B. dem BuchArbeitsfrei)[117]davor, dass durch die beschleunigte Automatisierung vieler Arbeitsbereiche in naher Zukunft immer mehr Menschen ihre Beschäftigung verlieren würden (z. B. LKW-Fahrer durchselbstfahrende Autos). Darin bestehe unter anderem eine Gefahr der Schwächung von Gewerkschaften, die an Mitgliedern verlieren könnten. Rieger plädiert daher für eine „Vergesellschaftung der Automatiserungsdividende“, also einer Besteuerung von nichtmenschlicher Arbeit, damit durch das Wachstum der Wirtschaft in Form eines Grundeinkommens auch der allgemeine Wohlstand wächst und gerecht verteilt wird.[118] Wissenschaftler der Universität Oxford haben in einer Studie im Jahr 2013 eine Vielzahl von Jobs auf ihre Automatisierbarkeit überprüft. Dabei unterteilten die Wissenschaftler die Jobs in verschiedene Risikogruppen. 47 Prozent der betrachteten Jobs in den USA wurden in die höchste Risikogruppe eingeteilt, d. h., dass für diese Jobs die Wahrscheinlichkeit hoch ist, dass innerhalb eines unbestimmten Zeitraums die nötige Technologie entwickelt wird, um sie automatisieren zu können. Die Studie macht jedoch keine Aussage dazu, wie viele Jobs tatsächlich automatisiert werden, da nur die technologischen Entwicklungen und keine weiteren Faktoren betrachtet werden. Ein solcher Faktor wäre zum Beispiel die Höhe der Kosten, also ob eine Automatisierung teurer wäre als das Gehalt für einen menschlichen Arbeiter.[119] Jürgen Schmidhuberantwortete auf die Frage, ob KIs uns bald den Rang ablaufen werden bzw. ob wir uns Sorgen um unsere Jobs machen müssten: „Künstliche Intelligenzen werden fast alles erlernen, was Menschen können – und noch viel mehr. Ihre neuronalen Netzwerke werden aus Erfahrung klüger und wegen der sich rasch verbilligenden Hardware alle zehn Jahre hundertmal mächtiger. Unsere formelle Theorie des Spaßes erlaubt sogar, Neugierde und Kreativität zu implementieren, um künstliche Wissenschaftler und Künstler zu bauen.“ und „Alle fünf Jahre wird das Rechnen 10-mal billiger. Hält der Trend an, werden kleine Rechner bald so viel rechnen können wie ein menschliches Gehirn, 50 Jahre später wie alle 10 Milliarden Hirne zusammen.“[120]Siehe dazu auch:Mooresches Gesetz. Als Konsequenz aus der aus seiner Sicht unabwendbar fortschreitenden Automatisierung und dem damit einhergehenden Wegfall von Erwerbsarbeitsplätzen sieht Schmidhuber die Notwendigkeit eines bedingungslosen Grundeinkommens.[121]„Roboterbesitzer werden Steuern zahlen müssen, um die Mitglieder unserer Gesellschaft zu ernähren, die keine existenziell notwendigen Jobs mehr ausüben. Wer dies nicht bis zu einem gewissen Grad unterstützt, beschwört geradezu die Revolution Mensch gegen Maschine herauf.“[122] Erik Brynjolfssonist der Auffassung, das Aufkommen radikaler Parteien in den USA und Europa sei die Folge davon, dass viele Menschen heute schon nicht mehr mit dem technischen Fortschritt mithalten könnten. Wenn Menschen ihre Jobs verlieren, werden diese Menschen wütend, so Brynjolfsson. Auch er meint, dass in Zukunft die meisten Jobs von Maschinen erledigt würden.[123] Mark Zuckerbergäußerte bei einer Rede vor Harvard-Absolventen, dass die Einführung eines bedingungslosen Grundeinkommens notwendig sei. Es könne etwas nicht mehr in Ordnung sein, wenn er als Harvard-Abbrecher innerhalb weniger Jahre Milliarden machen könne, während Millionen von Uni-Absolventen ihre Schulden nicht abbezahlen könnten. Es brauche eine Basis, auf der jeder innovativ und kreativ sein könne.[124][125] Im November 2017 stellte der Deutsche-Bank-ChefJohn Cryaneinen starken Stellenabbau in Aussicht. Das Unternehmen beschäftigt 97.000 Menschen. Bereits in den letzten 12 Monaten wurden 4.000 Stellen abgebaut. In naher Zukunft sollen 9.000 weitere Stellen abgebaut werden. Mittelfristig sollen die Hälfte aller Stellen abgebaut werden. Cryan begründete diesen Schritt damit, dass die Konkurrenz bereits heute mit etwa der Hälfte der Mitarbeiter vergleichbare Leistung erbringe. Cryan sagte: „Wir machen zu viel Handarbeit, was uns fehleranfällig und ineffizient macht“. Vor allem durch das maschinelle Lernen bzw. künstliche Intelligenzen könnte das Unternehmen noch viel effizienter werden. Viele Banker arbeiteten ohnehin wie Roboter, so Cryan. An die Stelle qualifizierter Mitarbeiter sollen qualifizierte Maschinen treten, so Cryan.[126] Der Zukunftsforscher Lars Thomson prognostizierte im November 2017 für die nächsten 10 Jahre gewaltige Umbrüche in Technologie, Arbeit, Werten und Gesellschaft. Im Jahr 2025 könne ein Haushalts-Roboter den Frühstückstisch decken, Fenster putzen, Pflegedienste übernehmen usw. wodurch Arbeitsplätze vernichtet würden. Heute schon gebe es 181 Firmen weltweit, die an klugen Robotern arbeiten. Der Preis eines solchen Roboters betrage heute etwa 20.000 Euro. Der Markt der künstlichen Intelligenz werde in wenigen Jahren größer sein als der Automobilmarkt. Wie schnell 10 Jahre vergingen, würde man sehen, wenn man 10 Jahre zurückblicke, als das erste Smartphone auf den Markt kam. Er bedauert, dass in unserer Gesellschaft kaum jemand diese Entwicklung erkenne, die unsere Gesellschaft komplett verändern werde. In Hotels würden in 10 Jahren Roboter die Arbeiten der heutigen Zimmermädchen übernehmen. Der Vorteil für den Hotelmanager: Der Roboter wolle keinen Lohn, keine freien Tage, müsse nicht versteuert und versichert werden. Der Nachteil: Der Staat erhalte keine Steuern mehr und die Menschen seien arbeitslos. Deshalb werde man nicht an einem bedingungslosen Grundeinkommen und der Einführung einer Robotersteuer vorbeikommen. Thomson sieht die Gefahr einer Spaltung der Gesellschaft, wenn das Tempo der Veränderung die Wandlungsfähigkeit der Menschen übersteige. Gleichzeitig werde die KI den Menschen von der Arbeit befreien. Die Gesellschaft müsse Leitplanken für die KIs definieren.[127] In einem Interview im Januar 2018 meinte der CEO von GoogleSundar Pichai, die aktuelle Entwicklung der künstlichen Intelligenz sei für den Werdegang der Menschheit bedeutender als es die Entdeckung des Feuers und die Entwicklung der Elektrizität waren. Durch die aktuelle Entwicklung der KI werde kein Stein auf dem anderen bleiben. Deshalb sei es wichtig, dass die Gesellschaft sich mit dem Thema auseinandersetze. Nur so könne man die Risiken eingrenzen und die Potentiale ausschöpfen. Google gehört derzeit zu den führenden Unternehmen im Bereich der KI. Allein der KI-Assistent von Google ist bereits auf hunderten Millionen Android-Smartphones installiert. Aber auch in den Suchmaschinen kommt KI derzeit bereits milliardenfach zum Einsatz. Die von Google gekaufte FirmaDeepMindeilt bei der KI-Forschung von Meilenstein zu Meilenstein u. a. mitAlphaGo, AlphaGo Zero,AlphaZero.[128] DasInstitut für Arbeitsmarkt- und Berufsforschung(IAB), das zur Bundesagentur für Arbeit gehört, hat in einer Studie von 4/2018[129]dargelegt, welche menschliche Arbeit in Deutschland von Maschinen ersetzt werden kann. Die Studie kommt zum Ergebnis, dass im Jahr 2016 25 Prozent der bezahlten menschlichen Tätigkeiten von Maschinen hätten erledigt werden können, was etwa acht Millionen Arbeitsplätzen in Deutschland entspricht. Eine frühere Studie kam für das Jahr 2013 noch auf einen Wert von 15 Prozent. Am stärksten betroffen mit etwa 83 Prozent sind Fertigungsberufe, aber auch unternehmensbezogene Dienstleistungsberufe mit 60 Prozent, Berufe in der Unternehmensführung und -organisation mit 57 Prozent, Berufe in Land- und Forstwirtschaft und Gartenbau mit 44 Prozent usw. Im Vergleich von 2013 zu 2016 sind besonders stark Logistik- und Verkehrsberufe gestiegen (von 36 auf 56 Prozent), ein Bereich, in dem in Deutschland etwa 2,4 Millionen Menschen beschäftigt sind. Insgesamt geht die Studie davon aus, dass in naher Zukunft 70 Prozent der menschlichen bezahlten Tätigkeiten von Maschinen übernommen werden könnten. Maschinen könnten z. B. übernehmen: Wareneingangskontrolle, Montageprüfung, Kommissionierung, Versicherungsanträge, Steuererklärungen usw. Die Techniken, die diese Veränderungen vorantreiben, seien: künstliche Intelligenzen, Big Data, 3D-Druck und virtuelle Realität. Auch wenn es nicht zu Entlassungen komme, müssten Mitarbeiter zumindest mit starken Veränderungen in ihrem Berufsbild und damit starkem Umlernen rechnen. Es entstünden auch neue Berufsfelder. Auch werde nicht alles, was heute schon möglich ist, auch umgesetzt und schon gar nicht sofort. Ein Faktor für diese Verzögerung seien ethische und rechtliche Aspekte, aber auch die hohen Kosten der Automatisierung. Nicht immer sei die künstliche Intelligenz billiger als die menschliche Intelligenz.[130] In einem Gastbeitrag im Februar 2018 meinte der SAP-ChefBill McDermott, dass sich die Menschen fürchten würden vor den Veränderungen, die eine Welt mit Robotern und KIs mit sich bringt. Ein erster Meilenstein sei derSieg der Maschine Deep Blue über den amtierenden Schachweltmeister Gary Kasparovim Jahr 1997 gewesen. Ein weiterer Meilenstein sei der Sieg der MaschineWatsonüber den Menschen in der Quiz-Show Jeopardy im Jahr 2011 gewesen. Und der nächste große Schritt waren dann die Siege vonAlphaGound seinen Nachfolgern AlphaGo Zero undAlphaZeroim Jahr 2016 und 2017. Die tiefgreifenden Veränderungen, die KI auch am Arbeitsplatz mit sich bringen würden, seien heute nun in aller Munde. Um etwaige negative Auswirkungen der neuen Techniken auf die Gesellschaft zu vermeiden, brauche es nun eine durchdachte Planung. Behörden, Privatwirtschaft und Bildungswesen müssten zusammenarbeiten, um jungen Menschen die Fähigkeiten zu vermitteln, die diese in der digitalen Wirtschaft benötigten. Umschulungen und lebenslanges Lernen seien heute die neue Normalität. Jobs würden nicht komplett von Maschinen ersetzt, sondern meist in Teilbereichen. Es entstünden auch viele neue Jobs. Die wirtschaftliche Entwicklung werde durch die KI befeuert. Man rechne für 2030 mit einer Wertschöpfung im Bereich von 16 Billionen US-Dollar und einem Wachstum des Bruttoinlandsprodukts um 26 Prozent. Durch die Automatisierung könnten Unternehmen zukünftig jährlich drei bis vier Billionen US-Dollar einsparen.[131] KI kann auch dazu genutzt werden, mehrNachhaltigkeitzu erreichen. Eine vomFraunhofer-Institut für Produktionstechnik und Automatisierungund demFraunhofer-Institut für Arbeitswirtschaft und Organisationin Auftrag gegebene Studie sieht hierbei große Potenziale für produzierende Unternehmen. Der Einsatz von KI könne zu effizienteren Produktionsprozessen führen und Ressourcen schonen.[132]Das UnternehmenBoschgab 2023 an, durch den Einsatz von generativer KI in einem türkischen Werk den Wasserverbrauch, denAusschusssowie den Energiebedarf verringert zu haben. Gleichzeitig sei die Anlageneffektivität um ca. zehn Prozent angestiegen.[133] In der Forschung wird allerdings angemahnt, nicht nur auf NachhaltigkeitdurchKI, sondern auchvonKI zu achten.[134]DieSupercomputer, die die Nutzung von KI ermöglichen, haben einen überaus hohen Strombedarf. DieInternationale Energieagenturschätzt, dass sich der weltweite Stromverbrauch durchRechenzentren, die diese Supercomputer beherbergen, bis 2026 im Vergleich zu 2022 verdoppeln könnte.[135]Darüber hinaus benötigen Supercomputer großer Mengen an Kühlwasser. Allein das Training vonGPT-3soll schätzungsweise 700.000 Liter Trinkwasser verbraucht haben.[136]Genaue Zahlen zum Energie- und Wasserverbrauch durch KI fehlen zumeist, da Unternehmen nicht verpflichtet sind, diese offenzulegen. Eine Studie, die sich mit dem Nutzen von KI für dieZiele für nachhaltige Entwicklungbeschäftigte, kommt zu dem Schluss, dass bei 79 % von diesen die Nutzung von KI einen positiven Effekt haben könnte. Gleichzeitig könnten auch bei 35 % der Ziele negative Auswirkungen aus dem Einsatz von KI resultieren.[137]Der Informatiker Rainer Rehak warnte in einem Interview mit dertazallerdings davor, Klimaziele allein durch die Nutzung von KI erreichen zu wollen. Maßnahmen, die mit einer grundsätzlichen Neuorientierung z. B. bei der Stadtentwicklung einhergehen, könnten gegebenenfalls deutlich besser zur Verhinderung von Treibhausgasemissionen beitragen.[138] Komplexitätstheorie|Kybernetik zweiter Ordnung|Radikaler Konstruktivismus|Varietät (Kybernetik) Automatentheorie|Entscheidungstheorie|Spieltheorie|Informationstheorie|Informetrie|Konnektionismus|Semiotik|Synergetik|Systemtheorie|Systemwissenschaft|Künstliche Intelligenz Anthropokybernetik|Bildungskybernetik|Biokybernetik|Medizinische Kybernetik|Biomedizinische Kybernetik|Neuroinformatik|Psychokybernetik|Soziokybernetik|Systembiologie|Technische Kybernetik Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Eigenschaften von Intelligenz 2Begriffsherkunft 3Definitionsversuche 3.1EU-rechtliche Definition 3.2Alltagstaugliche Definitionen 4Metaphorik 5Starke und schwache KI 6Forschungsgebiete 7Geschichte 8Teilgebiete 8.1Wissensbasierte Systeme 8.2Musteranalyse und Mustererkennung 8.3Mustervorhersage 8.4Robotik 8.5Künstliches Leben 8.6AI-Alignment 9Methoden 9.1Suchen 9.2Planen 9.3Optimierungsmethoden 9.4Logisches Schließen 9.5Approximationsmethoden 10Anwendungen 11Turing-Test 12Technologische Singularität 13Bewusstsein bei künstlicher Intelligenz 14Angrenzende Wissenschaften 14.1Sprachwissenschaft 14.2Psychologie 14.3Psychotherapie 14.4Philosophie"
  },
  {
    "label": 1,
    "text": "Künstliche Intelligenz in der Materialwissenschaft – Wikipedia Künstliche Intelligenz in der Materialwissenschaft Inhaltsverzeichnis Daten in der Materialwissenschaft Einsatzmöglichkeiten Validierung der Vorhersagen Einzelnachweise Vorhersage von Materialeigenschaften Vorhersage neuer Materialien Unterstützung von Experimenten Adaptive Design High Throughput Experimentation Design of Experiments Künstliche Intelligenz in der Materialwissenschaftist ein stark wachsender Teilbereich derkünstlichen Intelligenz(KI), bei dem Daten dazu genutzt werden sollen, um neue Materialien zu entdecken oder Forschende bei der Entdeckung dieser zu unterstützen.[1] In derMaterialwissenschaftwerden Daten primär durchExperimenteundComputersimulationengewonnen. Dabei entsteht eine große Menge an Daten undMeta-Daten, die verarbeitet und abgespeichert werden müssen, um anschließend für die unterschiedlichen KI-Ansätze benutzt werden zu können.[2]Da die Materialforschung einen starken interdisziplinären Charakter besitzt, werden Daten aus derPhysik,Chemieund denIngenieurwissenschaftenverwendet, aus diesem Grund sind die Datenquellen oft heterogen und es bedarf einer intensiven Vorverarbeitung.[3]Aus diesem Grund ist das Teilen von Daten essentiell für die Forschung an KI in der Materialwissenschaft. Diesem Teilen stehen allerdings mehrere Barrieren im Weg, die die breite Nutzung von KI in der Materialwissenschaft verhindern:[4] Um diese Barrieren zu umgehen, gibt es in der Materialwissenschaft diverse Datenbanken, die ihre Daten der Öffentlichkeit zugänglich machen. Diese beschränken sich meistens auf einen Teilbereich der Materialwissenschaften und werden meist von der jeweiligenwissenschaftlichen Communityinstand gehalten. Ein Beispiel dafür istPoLyInfo, eine Datenbank fürPolymer-Daten.[5] Der Großteil der genutzten KI in der Materialwissenschaft dient der Vorhersage von Materialeigenschaften. Dabei werden in der Regel die Materialeigenschaften bereits aus Simulationen oder Experimenten bekannter Materialien als Input-Parameter in ein Machine-Learning-Modell gegeben. Dieses versucht dann durch eine Regression den Raum der noch unbekannten Materialien zu erschließen und ermöglicht es gezielt nach Materialien mit bestimmten Eigenschaften zu suchen, um diese anschließend zu synthetisieren. Oft genutzte Ansätze sind hierbei dieBayes’sche Optimierung,[6][7][8]Support Vector Machines(SVM)[9][10]oderRandom Forest Regressoren.[11][12] KI kann dazu genutzt werden völlig neue Materialien zu entdecken. Dabei geht es darum neue Materialien mit bestimmten Materialeigenschaften aus dem chemischen Weißraum, also völlig unerforschten Strukturräumen vorherzusagen. Diese Vorhersagen sind meist deutlich komplizierter als die bloße Vorhersage von Eigenschaften bestimmter Materialien und benötigen deswegen größere Datenmengen. Allerdings funktioniert ein Großteil der Vorhersagen neuer Materialien ähnlich wie die Vorhersage von Materialeigenschaften. Deswegen wurde die erste erfolgreiche Vorhersage eines neuen Materials aus dem chemischen Weißraum von einemRandom Forest Regressorgeschafft.[11] Neben Regressions-Ansätzen werden auch andere Methoden verwendet, um neue Materialien vorherzusagen, die nicht über die Eigenschaften dieser Materialien funktionieren. So werden zum BeispielConvolutional Neural Networks(CNNs) dafür eingesetzt, Abbildungen der Struktur von Materialien zu analysieren um weitere mögliche Materialien mit ihren Strukturen vorherzusagen.[13] KI wird außerdem noch dazu eingesetzt, Forschende bei Experimenten zu unterstützen und diese zu leiten. Dabei werden Ansätze mit KI mitAdaptivem Materialdesign,High Throughput ExperimentationundDesign of Experimentskombiniert. Adaptive Design in der Materialwissenschaft versucht die experimentellen Unsicherheiten zu verringern, indem versucht wird vorherzusagen, welches Experiment als Nächstes durchgeführt werden sollte.[14]Dafür wird eine KI eingesetzt, die versucht die erwartete Verbesserung eines Parameters zu einem vorher definierten Ziel zu erhöhen und den Forschenden das Experiment mit der höchsten Verbesserung vorschlägt. Die Ergebnisse des Experiments werden dann wieder als Input der KI verwendet. Dadurch entsteht ein konstanter Feedback-Loop, der Experimente effizienter machen soll. Bei der High Throughput Experimentation werden sehr viele Experimente parallel durchgeführt, um schnell viele Daten zu generieren.[15]Ziel ist es den optimalen Wert für einen bestimmten Parameter der Experimente zu finden. Diese optimalen Werte werden mit Hilfe der Daten und einer KI berechnet. Design of Experiments (Auch Statistische Versuchsplanung genannt, ist in dem Feld allerdings als Design of Experiments geläufig) ist eine Methode zur Bestimmung der Zusammenhänge der Parameter eines Experiments.[16]Dabei werden mithilfe von KI die Parameter und ihre Zusammenhänge analysiert und die besten Parameter für die Experimente vorhergesagt. Während sich KI in der Materialwissenschaft immer weiter verbreitet, werden die meisten Modelle, die entwickelt werden, nicht validiert.[17]Dies liegt daran, dass es für die Validierung eines KI-Modells sowohl Fachwissen in der Materialwissenschaft als auch in der KI geben muss, was selten ist.[18]Außerdem ist es schwierig, Ergebnisse zu veröffentlichen, wenn sich die Voraussagen der KI nicht validieren lassen. Aus diesem Grund kann es sich als sinnvoll erweisen, die eigenen Ergebnisse unvalidiert zu veröffentlichen und sie von anderen, mit einer größeren Material-Expertise, validieren zu lassen.[19] Ein Ansatz, die Vorhersagen zu überprüfen, ohne ein Experiment oder eine aufwändige Simulation durchführen zu müssen, ist dieGruppen-Kreuzvalidierung. Dabei werden die Materialien, die als Eingabe für die Modelle genutzt werden, nach ihren chemischen Strukturräumen getrennt.[20]Dadurch können die Voraussagen des Modells in einem Strukturraum in anderen Strukturräumen getestet und validiert werden. So kann sichergegangen werden, dass ein Modell allgemeine Voraussagen treffen kann. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Daten in der Materialwissenschaft 2Einsatzmöglichkeiten 2.1Vorhersage von Materialeigenschaften 2.2Vorhersage neuer Materialien 2.3Unterstützung von Experimenten 2.3.1Adaptive Design 2.3.2High Throughput Experimentation 2.3.3Design of Experiments 3Validierung der Vorhersagen 4Einzelnachweise Links hinzufügen Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Wikidata-Datenobjekt"
  },
  {
    "label": 1,
    "text": "Künstliche Intelligenz in der Medizin – Wikipedia Künstliche Intelligenz in der Medizin Inhaltsverzeichnis Diagnostik Personalisierte Vorhersagen mittels Präzisionsmedizin Automatische Datenanalyse Validierung Kosteneffektivität Behördlich erteilte Zulassungen AI-Act Ethik Pharmaforschung Kritik und Streitfragen Siehe auch Weblinks Literatur Einzelnachweise Computer Vision für bildgebende Diagnostik Personalisierte Vorhersagen in Echtzeit Sprachverarbeitung Chatbots Datenschutz und Privatheit Erklärbarkeit Onkologie Neurologie Pneumologie Ophthalmologie Genetik Kardiologie Radiologie Pflege Brustkrebs-Screening Künstliche Intelligenz in der Medizinist ein stark wachsender Teilbereich derkünstlichen Intelligenz(KI), bei dem digital vorliegende Informationen ausgewertet werden, um möglichst aussagekräftigeDiagnosenzu stellen und bzw. oder optimierteTherapienvorzuschlagen[1].KI in der Medizinist neben Vernetzung undMedizinische Robotikals dritte große Säule derDigitalen Medizinanzusehen.[2] Künstliche Intelligenz spielt eine Rolle in derbildgebenden Diagnostik.[3]Die Auswertung vonBilddateienmitstatistischenundlernendenMethoden wird auch unter dem FachbereichRadiomicszusammengefasst. Dabei erhalten Ärzte durchEntscheidungsunterstützungssystemezusätzliche Informationen. Durch den Einsatz dieser Methoden ist es beispielsweise möglich, denTypvonKrebszellengenauer zu bestimmen, da dieUnterscheidungsmerkmaleoft schwer und individuell verschieden mit dem menschlichen Auge zu erkennen sind. Dies ist im Rahmen derPräzisionsmedizinwichtig, um eine zielgerichtete Therapie vorzuschlagen. Je nach Typ desKarzinomssind teils unterschiedliche Therapien notwendig oder sinnvoll. Eingesetzt wird Radiomics etwa zur Klassifizierung von Tumoren unter anderem inLunge,Brust,GehirnundHaut.[4] Die bisher größte, webbasierte und international durchgeführte Studie zur automatisierten Hautkrebsdiagnose unter Leitung derMedizinischen Universität Wienverglich 511 Mediziner aus 63 Ländern mit 139Algorithmen(zumeist basierend aufneuronalen Netzwerken, CNN) in der Erkennung von Hautkrebs andermatoskopischenBildern.[5]Im Gegensatz zu anderen Studien mussten nicht nur zwei Arten von Hautveränderungen (MuttermaleundMelanome) erkannt werden, sondern auch die sieben häufigsten pigmentierten Hautveränderungen.[6]In der Studie zeigte sich – im experimentellen Setting – nicht nur eine klare Überlegenheit der besten Algorithmen, sondern auch, dass bereits „durchschnittliche“ Programme ähnlich gut oder besser Kategorien erkennen können als Mediziner. In einer internationalen Studie derUniversität Heidelberg, bei der 58Dermatologenaus 17 Ländern gegen einfaltendes neuronales Netzwerkim Vergleichstest antraten, fand sich ebenfalls eine Überlegenheit des neuronalen Netzwerks zu Dermatologen in der Erkennung von Melanomen. Dies war jedoch nicht der Fall, wenn die Dermatologen vorher zusätzliche Informationen wie Alter, Geschlecht und Lage derLäsionerhielten. DieRichtig-negativ-Ratelag bei den Dermatologen jedoch mit 75,7 % deutlich über der des Netzwerkes, das nur 63,8 % der ungefährlichen Muttermale als solche erkannte. Die Forscher betonten, dass künstliche Intelligenz die Ärzte bei der Diagnose von Hautkrebs unterstützen, sie aber nicht ersetzen könne.[7]Es gibt weitere Studien in größerem bzw. internationalem Rahmen,[8][9][10]oder kleinen bzw. lokalen Rahmen.[11][12][13][14] Obwohl die Meinung zu diesen Techniken auch bei Dermatologen großteils positiv erscheint,[15]und sich erste Ergebnisse zum Vorteil der Kollaboration zwischen Mensch und Maschine zeigen,[16]mahnen einige Wissenschaftler und Kliniker zur Vorsicht bei der Anwendung dieser Techniken.[17][18]Viele Firmen arbeiten auch an der Kommerzialisierung dieser Projekte, manche auf Basis von Hardware wie Infrarot-Laserstrahl-Geräten,[19]manche auf Basis von Dermatoskopie,[20][21]Für Smartphones gibt es bereits seit Jahren eine Reihe häufig kostenpflichtiger Apps, die Hautkrebs anhand eines Fotos erkennen können sollen, aber meist nicht auf neuen Techniken wie neuronalen Netzwerken basieren. Eine wissenschaftliche Aufarbeitung konnte zeigen, dass keine der getesteten Applikationen eine ausreichende Genauigkeit, und damit Nutzen für Patienten aufweisen konnte.[22]Der Dermatologe und VersorgungsforscherMatthias Augustinsieht dies ebenfalls kritisch, da es zu Anwendungsfehlern undFehldiagnosenkommen könne, die Laien nicht richtig einschätzen könnten.[23] In einem wissenschaftlichen Wettbewerb zur automatisierten Erkennung vonBrustkrebszellen(Diagnose vonMetastasenin Sentinel-Node-Biopsien) traten jeweils 32 Programme von 23 Teams gegen ein Team aus elf Pathologen an, die jeweils zwei Stunden Zeit zur Analyse von 129 Präparaten hatten. Eine Vergleichsgruppe bestand aus einem versiertenPathologen, der sich so viel Zeit nehmen durfte, wie er wollte, was allerdings nicht dem klinischen Alltag entspricht. Die Programme nutzten meistConvolutional Neural Networks. Sieben der Programme lieferten bessere Ergebnisse als die Gruppe der Pathologen, die häufig Mikrometastasen übersahen, was den besseren Programmen nur selten passierte. Fünf der Programme waren sogar besser als der versierte Pathologe, der sich 30 Stunden Zeit für die Analyse nahm. Selbst dieser versierte Analytiker übersah ein Viertel der Mikrometastasen.[24] DieDarmspiegelunggilt als die sicherste Methode, um bösartige Tumore inMast-undDickdarmfrühzeitig zu erkennen. Jährlich erkranken etwa 61.000 Menschen in Deutschland an Darmkrebs. Bei einer Darmspiegelung entfernt der Arzt alle verdächtigenWucherungen, sog. Polypen, egal ob die Wucherung gut- oder bösartig ist. Ob es sich um einen bösartigen Tumor (sog.Adenom) handelt, kann erst später im Labor festgestellt werden. Im Herbst 2018 setzten japanischeGastroenterologenin einem klinischen Test eine KI ein, die auf die Erkennung von bösartigen Tumoren im Darm trainiert wurde. Die Trefferrate lag bei 93 %. Dabei werden Bilder aus dem Darm in 500-facher Vergrößerung an eine KI übermittelt, die dann innerhalb einer Sekunde erkennen kann, ob es sich bei dem Polyp um einen gut- oder bösartigen Tumor handelt. Der Arzt erhält dann eine Rückmeldung über einen Ton oder über einen Hinweis auf dem Bildschirm. Die KI soll weiter trainiert werden, um die Erkennungsrate noch zu verbessern. Dann könnte die KI in den Routinebetrieb gehen.[25] Wissenschaftler an derUniversity of Californiain San Francisco haben im Herbst 2018 einePilotstudiemittiefen,künstlichen neuronalen Netzenim JournalRadiologyvorgestellt, die anhand vonGehirnscanseineAlzheimererkrankungim Schnitt sechs Jahre vor der finalen Diagnose erkennen können. Häufig werde Alzheimer von Ärzten erst dann diagnostiziert, wenn sich die ersten Symptome zeigen. Selbst erfahrenen Ärzten fällt es schwer, die bei Frühstadien auftretenden, kleinen Veränderungen im Gehirn zu erkennen und richtig einzuordnen. Deshalb könne die KI-gestützte Erkennung einen wichtigen Beitrag zurFrüherkennungund damit zur Behandlung leisten. Das Netz erreichte eineSensitivitätvon 100 %, bei einer Richtig-negativ-Rate von 82 %. Weitere Untersuchungen sollen folgen, um die Ergebnisse zu verifizieren.[26][27] Das Wiener KI-Labor Deep Insight veröffentlichte 2020 denQuellcodeeines künstlichen neuronalen Netzwerkes, das darauf trainiert wurde, anhand vonCT-Aufnahmen der Lunge zu klassifizieren, ob der Patient anCOVID-19leidet, sofern dasVirusbereits die Lunge befallen hat. Das Netzwerk unterscheidet zwischen durch COVID-19 verursachten Veränderungen der Lunge, sonstigenpathologischenBefunden und Normalzustand.[28] In derAugenheilkundekonnte für KI-basierte Ansätze eine Überlegenheit gegenüber Fachspezialisten für verschiedene klinisch-relevante Aufgaben nachgewiesen werden. Dies umfasst unter anderem das Erkennen von Diagnosen anhand vonNetzhaut-Bildgebung, die Berechnung der idealenLinsenstärkevorGrauer-Star-Operation, sowie das Erkennen von systemischen Risikofaktoren (bspw. Blutdruck) anhand von ophthalmologischen Bildaufnahmen.[29]Im Direktvergleich zwischenDeep Learningund 13 menschlichen Spezialisten wurden 25.326 Fundus-Fotografien (Fotos desAugenhintergrundes) vonDiabetikerneiner diagnostischen Bewertung derdiabetischen Retinopathieunterzogen. DieSensitivitätdesNeuronalen Netzwerkslag bei 0,97 (0,74 bei den Spezialisten) und dieSpezifitätbei 0,96 (0,98).[30]Das Projekt wurde als Nachweis herausragender menschlicher Leistungen in dem AI Index Report 2019 als Meilenstein gewürdigt.[31]2021 zeigten sich in einer herstellerunabhängigen, vergleichenden Bewertung von Algorithmen in den USA (head-to-head) an Real-World-Daten von 310.000 Fundus-Fotografien zum Screening auf eine diabetische Retinopathie erhebliche Unterschiede, sowohl bei der Sensitivität (51-86 %) als auch der Spezifität. Menschliche Augenspezialisten traten gegen insgesamt 7 Algorithmen an, welche bereits in mehreren Ländern in Gebrauch waren, wobei einer eine FDA-Zulassung und mehrere eine CE-Kennzeichnung hatten.[32] Interessanterweise konnte gezeigt werden, dass sich sogar das Geschlecht anhand von Farbfundus-Fotos mittels KI identifizieren lässt.[33]Dies galt unter Fachspezialisten zuvor als unmöglich. Folgearbeiten konnten zeigen, dass KI-Anwendungen das Geschlecht an subtilen Unterschieden des Verlaufs der Gefäße erkennen können.[34] Künstliche Intelligenz wird mittlerweile auch eingesetzt, um beispielsweise genetisch bedingte und seltene Knochenerkrankungen schneller zu erkennen.[35]Dabei wird die KI auf charakteristische Bildmuster verschiedenerSkelettdysplasientrainiert. Durch die vielfältige Ausprägung verschiedener Muster im Skelett, die in der Diagnose identifiziert werden müssen, ist es selbst für erfahrene Kliniker schwer, eine eindeutige Diagnose auszusprechen, so dass es dadurch oft zu einer langen Wartezeit von mehreren Jahren für Patienten sowie zu Fehldiagnosen kommt. In derKardiologiesind Algorithmen in Gebrauch, dieLangzeit-EKGsfür einen Arzt auswerten und wichtigeRhythmusstörungenbinnen Sekunden finden und auflisten. Für den medizinischen Laien gibt es niederschwellige mobile Anwendungen mit Algorithmen („Smart Apps“), welche diePulsfrequenzmittels Photoplethysmographie messen können (ähnlich derPulsoxymetrie). Hier sindArrhythmien, besondersVorhofflimmern, bedeutsam, weil damit das Schlaganfallrisiko steigt.[36] Die Auswertung der Röntgenbilder durch Radiologen ist eine monotone, langwierige, ermüdende und fehleranfällige Arbeit.[37]Eine solche Untersuchung ist in Deutschland, in Österreich und teilweise in der Schweiz für Frauen ab 45 alle zwei Jahre Standard. Falsch-positive Ergebnisse führen zu unnötigenBiopsien, falsch-negative Resultate übersehen einen Brustkrebs im Frühstadium, in welchem noch eine Heilung möglich wäre. Bei zweijährlichen Untersuchungen fallen insbesondere diese Falsch-Positiven ins Gewicht, die auf 50 % geschätzt werden und eine unnötige psychologische Belastung für die Patientin darstellen, insbesondere, wenn sich hinterher histologisch ein gutartiger Befund ergibt. Durch die Digitalisierung in der Radiologie (PACS) liegen mittlerweile sehr viele Aufnahmen in Datenbanken, welche ein maschinelles Lernen (ML) ermöglicht haben. Mehrere Projekte zeigen, dass es bei KI nicht immer darum geht, ob die Maschine besser ist als der Mensch, sondern Befundet hingegen eine KI Mammografien ohne Mithilfe eines Radiologen, so fand sich eine geringfügig verbesserte diagnostische Performance bei gleichbleibendem Zeitaufwand pro Bild. Die Autoren der Studie[38]halten einen multimodalen Ansatz, bei dem nicht alleine die Maschine den Arzt ersetzt, für zukunftsweisend. Sie sehen den Radiologen als Beruf somit nicht gefährdet. Künstliche Intelligenz in der Pflegeumfasst ein breites Spektrum an Anwendungsbeispielen, die sowohl den Dokumentationsaufwand verringern, als auch die Pflegequalität verbessern sollen. Ein Anwendungsbeispiel ist die Geriatrie. Bei älteren Menschen sollen KI-Algorithmen die individuelle Eintrittswahrscheinlichkeit von Ereignissen – insbesondere Gefahrensituationen – vorhersagen. Dazu sollen Risiken erkannt werden, um dadurch die Pflegebedürftigen bzw. deren Angehörige frühzeitig zu warnen. Das Ziel des ProjektsKI@Homeist die Entwicklung eines selbstlernenden Systems für den Bereich altersgerechtes Wohnen.[39]Mithilfe von Sensoren und anderen Methoden der Datenerfassung werdenVitalwerteundVerhaltensmustervonProbandenaufgezeichnet und ausgewertet. Ein spezielles Verfahren ist dieGanganalyse. In derGeriatriesind KI-basierte Algorithmen in der Lage, Parameter, die zu Stürzen führen können, aus einer gängigen Videoaufnahme einer sich bewegenden Person abzuleiten. Den Entwicklern derAppLINDERAgelang es, Gangparameter (u. a. Schrittlänge, Ganggeschwindigkeit) per Video über ein Smartphone mit ähnlicher oder besserer Präzision als demGoldstandardunter Zuhilfenahme von KI zu bestimmen.[40]Bisherige Ganganalysen waren deutlich aufwendiger und erforderten klinisch-apparative Methoden, z. B. mit Sensoren bestückte „Teppiche“. Um zu zeigen, dass solche Methoden alten Menschen einen Nutzen bringen, führt der Weg von der Bestimmung von Gangparametern zur Sturzhäufigkeit überrandomisierteStudien, die bisher fehlen bzw. erst in Vorbereitung sind. Mittels Ganganalyse könnten auch Alterskrankheiten wieMorbus Parkinsonper Video,Telemedizinund/oder KI einen Innovationsschub bei der Therapiebewertung erhalten, indem man das Gangbild vor und nach Therapiebeginn analysiert. Insgesamt ist diese Disziplin noch am Anfang, laut einerMetaanalyse[41]sind die Bewertungsmetriken noch zu heterogen und es fehlen Standards. Mit Hilfe statistischer Methoden können Studien für eine Gruppe von Menschen (z. B. gleiche Diagnose) sehr gut vorhersagen, ob ein Therapieverfahren wirksam ist oder nicht (z. B. ein neues Medikament). Ziel derPräzisionsmedizin(personalisierte Medizin) ist es, dies auf ein Individuum herunterzubrechen, von welchem man mehr Parameter kennt, z. B.Biomarker. Das beinhaltet auch ein Aussortieren von Personen, bei denen eine spezielle Therapie nicht hilft und deren Anwendung wegen Nebenwirkungen sogar schädlich sein könnte und/oder unnötige Kosten verursachen würde. Einzelerfolge ohne Anwendung von KI aus derKrebstherapiesind beschrieben, wobei auch eine kausale Beziehung zwischenBiomarkerundOutcomebesteht[42][43]. Beispielsweise konnte im August 2016 am Medical Institute derUniversität Tokyodas ComputerprogrammIBMWatsoneineFehldiagnoseder Ärzte korrigieren. Die Ärzte diagnostizierten bei einer Patientin eine akute myeloischeLeukämie. Die Therapie blieb erfolglos, weswegen manWatsonzu Rate zog. Die KI benötigte 10 Minuten, um die DNA der Frau mit 20 Millionen Krebsstudien abzugleichen.Watsonerkannte eine sehr seltene Form der Leukämie, die bislang nur 41 Patienten betrifft und heilbar ist.[44] Große Hoffnung liegt in der durch maschinelles Lernen gestützten Präzisionsmedizin: Allerdings können aus erkenntnistheoretischer Sicht datengesteuerteVorhersagemodellemit maschinellem Lernen keine kausalen Schlussfolgerungen liefern, sondern nur statistischeKorrelationenaufzeigen[45]. Dies könnte ein merkliches Hindernis sein, mittels KI präzise individuelle Vorhersagen zu machen, trotz rapide wachsender Datensätze und ausgefeilter Algorithmen. Wissenschaftler derUniversität Stanfordhaben im Januar 2018 eine KI vorgestellt, die mit einer Wahrscheinlichkeit von 90 % bei unheilbar kranken Patienten aus den Krankendaten berechnen kann, ob diese innerhalb der nächsten 3 bis 12 Monate versterben werden. Dies könne unheilbar kranken Patienten helfen, die letzten Monate würdevoll und ohne aggressive Behandlungsmethoden und eventuell daheim unterPalliativversorgungzu verleben.[46] Die ArmbanduhrApple Watchzeichnet unter anderem dieHerzfrequenzdes Trägers auf. Apple gab bekannt, dass KIs mit einer Wahrscheinlichkeit von 85 % aus der Analyse der HerzfrequenzDiabetes mellitusbeim Träger der Armbanduhr feststellen können. Die Idee basiert auf derFramingham-Herz-Studie, die bereits 2015 erkennen ließ, dass man allein mit Hilfe der Herzfrequenz Diabetes diagnostizieren kann. Apple war es bereits früher schon gelungen, aus der Herzfrequenz einen abnormalen Herzrhythmus mit 97 % Wahrscheinlichkeit,Schlafapnoemit 90 %,Hypertonie(Bluthochdruck) mit 82 % zu erkennen.[47] Ein Deep-Learning-Modell mit einem Gaussian-Chest-X-Ray-Filter ermöglicht eine präzisere automatische Tuberkulose-Diagnose aus Röntgenbildern. In einer Studie erreichte das Verfahren eine Genauigkeit von 99,2 % und weist ein hohes Potenzial für die Integration in die radiologische Tuberkulose-Diagnostik auf.[48] Bei derSepsis(im Volksmund „Blutvergiftung“) werden in der Regel Antibiotika und Infusionen verordnet, wenn eindeutige Symptome vorliegen. Ein zu später Beginn endet vielfach tödlich. In den USA wurden mehrere Echtzeit-Modelle entwickelt, die frühzeitig eine Vorhersage mit Hilfe von Künstlicher Intelligenz machen.[49] DasEpic Sepsis Model(ESM), eine aufMaschinellem Lernenaufbauende frühzeitige Prädiktion einer Sepsis, wurde in Hunderten von Kliniken in den USA implementiert, weil es mit derElektronischen Patientenakte(EHR) der Vertreiberfirma enthalten war. Erst eine externe Validierung zeigte, dass das Modell eine schlechte Diskriminierung aufweist bzgl.SensitivitätundSpezifitätund eine Alarmmüdigkeit durch unkorrekte und nicht-relevante Vorhersagen auslöst[50]. Forscher derMount Sinai School of Medicinedemonstrierten im Januar 2018, wie aus psychologischen Gesprächsprotokollen mit Jugendlichen erkennbar ist, ob diese in den nächsten zwei Jahren an einerPsychoseerkranken könnten. Dienatürliche Sprachverarbeitunghalf, in standardisierten Tests bis zu 83 % Genauigkeit zu erreichen, etwa anhand unorganisierter Gedankengänge, umständlicher Formulierungen, unklarer Assoziationen oder einer reduzierten Sprach-Komplexität. Diese subtilen Unterschiede seien nach einem Training mit vielen solchen Gesprächen zu erkennen.[51][52] Forscher desMITstellten im September 2018 eine KI vor, die anhand von gesprochenem oder geschriebenem Text eineDepressionbei Patienten diagnostizieren kann. An sich stellen Ärzte und Psychologen dem Patienten Fragen zu Lebensgewohnheiten, Verhaltensweisen und Befindlichkeiten, um aus Antworten die Depression zu diagnostizieren. Nach einem Training mit solchen Interviews erkannte das Programm auch anhand von Alltagsgesprächen eine Depression mit einer Trefferquote von 83 % – und bei der Einordnung der Schwere der Depression auf einer Skala von 0 bis 27 mit einer 71 % Trefferquote. Die KI könnte Ärzte unterstützen oder als App Benutzer permanent überwachen, um im Notfall zu alarmieren. Die Forscher wollen aus der Sprache künftig auch eine Demenz erkennen.[53] Digitale Anwendungen, die bei der Diagnoseerstellung und Priorisierung von Hilfen unterstützen,werden Symptom-Checkergenannt. Diese verwenden statt KI auch oft klassische statistische und algorithmische Verfahren (Entscheidungsbäume, Korrelationen). Siehe Hauptartikel →Symptom-Checker EinChatbotist ein Computerprogramm, das KI und natürliche Sprachverarbeitung nutzt, um Fragen zu verstehen und die Antworten darauf zu automatisieren und so menschliche Konversation simuliert.[54] Ein sehr früher medizinischer Chatbot,ELIZA, wurde zwischen 1964 und 1966 vonJoseph Weizenbaumam Artificial Intelligence Laboratory desMassachusetts Institute of Technologyentwickelt. Die bekanntesten modernen Chatbots sindLaMDA(Google) undGPT-3.5bzw.GPT-4(GenerativePretrainedTransformer) entwickelt vonOpenAI. Beide gehören in den Bereich der Generativen Künstlichen Intelligenz. MitGPT-4sind klinische Problemlösungen in der Kommunikation mit Ärzten möglich: Es werden zunächst die Beschwerden/Symptome in natürlicher Sprache präsentiert und anGPT-4zur Analyse übergeben. Der KI-Chatbot schlägt im nächsten Schritt weitere körperliche Untersuchungen, Laboranalysen und Bildgebungen vor, deren Ergebnisse ihm in Folge übergeben werden. GPT-4 ermittelt daraus die pathologischen Befunde und schlägt eine Diagnose bzw.Differentialdiagnosevor. Ggf. wird noch ein Bestätigungsparameter vorgeschlagen, der die endgültige Diagnose festschreibt bzw. untermauert. Das Programmliefert auch eine Begründung, wie es zu dieser Entscheidung kommt[55]. Im Gegensatz zuSymptom-Checkernist dies ein Werkzeug ausschließlich für Mediziner. Mit einem Chatbot versucht das BritischeNHSpsychisch Kranken zu helfen, schnell einen richtigen Therapieplatz zu finden. Der durch KI unterstützten ChatbotLimbicAccessmöchte dies vereinfachen und vor allem die ansonsten üblichen Wartezeiten auf einen Therapieplatz verkürzen. Eine 2024 veröffentlichte Studie (vom Entwickler von Limbic selbst durchgeführt) an 124.400 Personen zeigt einen Erfolg: Bei Nutzung des Chatbots kam es zu 15 % mehr Selbstüberweisungen, während es bei Nutzung anderer Kontrolldienste nur 6 % waren.[56] Der Chatbot erzielte offenbar dort mehr Erfolge, wo psychologische Barrieren, wie Schamgefühl und Stigmatisierungsängste, eine Rolle spielen (z. B. bei Personen imLGBT-Umfeld oder bei ethnischen Minderheiten). Hier kann das Bewusstsein, mit einer Maschine statt mit einem Menschen zu kommunizieren, entlastend sein. Auch um Methoden der Künstlichen Intelligenz (KI) und des Maschinellen Lernens zu bewerten erweisen sich randomisierte Studien RCT's als sinnvoll, wie eine strukturierten Literaturrecherche (scoping review) durch amerikanische Wissenschaftler demonstriert hat. Sie wählten 86 Studien aus, die die Kriterien für eine randomisierte Studie erfüllten[57]. Kritisch bemerkten die Autoren, dass von den 627 Studien, die 2023 in der Datenbank für laufende Studien (ClinicalTrials.gov ) registriert waren, nur 1 % eine abschließende Publikation aufwiesen. Die Autoren vermuteten, dass dadurch erhebliche Verzerrungen (Publikationsbias) entstehen könnten. DieKosteneffektivitätder künstlichen Intelligenz in der Diagnostik wurde an drei verschiedenen Krankheitsbildern in drei verschiedenen Nationen mit unterschiedlichen Gesundheitssystemen modelliert. Dabei wurde das Screening aufMaligne Melanome,Karies-Röntgenuntersuchung undFundusfotosbeiDiabetischer Retinopathiemittels KI und mittels herkömmlicher Verfahren über einen lebenslangen Zeitraum mit konventionellen Verfahren verglichen. Ergebnisparameter waren die jeweiligen Kosten (bei Einzelabrechnung) und dieQualitäts-korrigierten Lebensjahre(QALYs) bzw. dieZahnerhaltung. Als Resultat zeigte sich, dass sowohl die Kosten wie auch die Patienten-relevanten Ergebnisse im Langzeitvergleich ähnlich waren. Geringe Vorteile zeigten sich bei der Zahnerhaltung[58]. Geräte oder Software, die künstliche Intelligenz verwenden, müssen in Europa alsMedizinprodukteeineCE-Kennzeichnungerhalten und in den USA durch dieFDAzugelassen sein. Ein Vergleich zwischen USA und Europa in den Jahren 2015 bis 2020 erbrachte eine schnell zunehmende Zahl von Zulassungen, wobei die CE-Markierung zahlenmäßig leicht dominiert (2019 USA 79, EU 100). Oft erfolgt die CE-Markierung vor der FDA-Zulassung, was auf ein weniger rigoroses Verfahren hindeuten könnte. Das Schwergewicht liegt bei der Radiologie. Nur 15 % der Produkte wenden sich direkt an Privatpersonen (Patienten), der Rest an Fachpersonal (Ärzte). In diesem Zeitraum entfallen nur 1 % der Zulassungen auf die höchsten Risikoklassen, z. B. solche für die Diagnose von Brustkrebs[59]. Mehr noch in Europa als in Amerika finden die Autoren der Studie einen Mangel an Transparenz in der Gerätebeschreibung und dem Prozess der Bewertung. Sie spiegelt dieethischeVerantwortung der Regulierer genauso wie der Hersteller wider. Auch wird eine öffentlich zugängliche Datenbank für CE-markierte Geräte und Software angemahnt. Die Zulassungspraxis digitaler Gesundheitsanwendungen wurde in einer weiteren Studie als ungenügend befunden und als „regulatorische Lücke“ bezeichnet.[60] Seit 2024 gilt in der EU die EU-KI-Verordnung 2024/1689 (kurz AI-Act), welcher die Anwendung von Künstlicher Intelligenz in Abhängigkeit von der Risikoeinstufung regelt.[61]Dieser betrifft nicht nur Hersteller, sondern auch Betreiber (wie z. B. Krankenhäuser, Labore). Er unterscheidet: 2024 haben Wissenschaftler über einenScoping revieweine Checkliste erstellt, die Hilfestellungen bei der Einordnung vonKünstlicher Intelligenzgeben soll. Bei dieser TREGAI-Checkliste sollten folgende Aspekte berücksichtigt werden[62]: In derPharmaforschunghat sich das automatisierteHochdurchsatz-Screeningals Methode etabliert, sogenannteHitsund damit Kandidaten fürLeitstrukturenzu finden. Britische Forscher derUniversität Cambridgeentwickelten die Automatisierung weiter. Der Forschungsroboter „Eve“, der 2015 imJournal of the Royal Society Interfacevorgestellt wurde, verwendet sowohlstatistische Modelleals auchmaschinelles Lernenund produziert und testet damit Annahmen, prüft Beobachtungen, führt Experimente aus, interpretiert Ergebnisse, ändertHypothesenund wiederholt dies immer wieder. Dadurch könne der Roboter vielversprechende Substanzen vorhersagen und damit das Identifizieren von Leitstrukturen effizienter machen.[63][64]Mit Hilfe dieses Roboters fanden die Forscher 2018 heraus, dassTriclosan, das auch inZahnpastaverwendet wird,Malaria-Infektionen in zwei kritischen Stadien, nämlich dem Befall der Leber und des Bluts, bekämpfen könnte. Mit der Entdeckung durch die KI könnte nun ein neues Medikament entwickelt werden.[65] Es ist umstritten, ob die hohe Treffergenauigkeit der künstlichen Intelligenz etwa zur Diagnose von Krankheiten, die in manchen Studien angegeben wurden, in der Praxis gültig sind. Die Werte beziehen sich in der Regel auf vorher festgelegte, mitunter nicht repräsentative historische Datensätze. Beispielhaft wird eine Studie von Googles TochterfirmaDeepMindzur automatisierten Vorhersage vonNierenversagenkritisiert, die auf einem Datensatz durchgeführt wurde, der nur zu 6 % von weiblichen Patienten stammte. Die fehlende Variation in den Datensätzen könnte zu Computeranalysen führen, die in ihrer Generalisierung stark eingeschränkt sind und in realen Einsatzszenarien nicht die erwünschte Genauigkeit liefern.[66] Die Behandlungsvorschläge von IBM Watson können fehlerhaft sein, wenn etwa zu wenig Trainingsdaten zur Verfügung stehen. Entsprechende Berichte über fehlerhafte Empfehlungen, deren Anwendung die Patienten gefährde, wurden 2018 von einem Medizinfachportal veröffentlicht. Die Fehlfunktion soll laut IBM in einer späteren Version behoben worden sein.[67] Da künstliche Intelligenz undmaschinelles Lernenbeim Training große Datenmengen benötigen und medizinische Daten als besonders sensibel gelten, wird demDatenschutzgroße Bedeutung beigemessen. Beimfederated learningundswarm learningwird der Algorithmus nicht zentral trainiert, sondern an lokalen Institutionen (z. B. Krankenhäusern), wo die Daten geschützt verbleiben. Außerdem kann dem Besitzer eines Modells (Firma) besser garantiert werden, dass sein Modell nicht missbraucht, gestohlen oder verändert wird.[68][69] Der Nutzer möchte die Gründe für eine algorithmische Entscheidung verstehen. Bei künstlicher Intelligenz und maschinellem Lernen sind Algorithmen oft komplett undurchsichtig („Black Box“), typischerweise beineuronalen Netzwerken, insbesondereDeep Learning.[66]Um dem entgegenzuwirken, entwickelte sich das FeldExplainable AI.[70]Im Gesundheitsbereich werden vertrauenswürdige Modellentwickler, umfangreiche externeValidierungmittels Studien und standardisierte Bewertungsverfahren diskutiert.[71] Boris Babic von derUniversity of Torontowendet ein, dass Explainable AI oftmals nur post-hoc Erklärungen für Black-Box-Algorithmen liefern würden. Diese Erklärungen seien nicht zuverlässig und könnten den Anwender in die Irre führen. Echte White-Box-Algorithmen, die tatsächlich nachvollziehbare Erklärungen liefern, seien hingegen in ihrer Komplexität deutlich beschränkt und daher für viele Anwendungsfälle kaum geeignet. Er hält es daher für einen Fehler, die Erklärbarkeit von Algorithmen etwa als Zulassungsvoraussetzung vorzuschreiben, da dies kaum Vorteile biete, dafür aberInnovationenbremse und zur Anwendung von Algorithmen mit geringerer Genauigkeit führe. Stattdessen sollten Algorithmen stärker in klinischen Studien getestet werden, um deren Effektivität und Sicherheit zu gewährleisten.[72][73] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Diagnostik 1.1Computer Vision für bildgebende Diagnostik 1.1.1Onkologie 1.1.2Neurologie 1.1.3Pneumologie 1.1.4Ophthalmologie 1.1.5Genetik 1.1.6Kardiologie 1.1.7Radiologie 1.1.7.1Brustkrebs-Screening 1.1.8Pflege 2Personalisierte Vorhersagen mittels Präzisionsmedizin 3Automatische Datenanalyse 3.1Personalisierte Vorhersagen in Echtzeit 3.2Sprachverarbeitung 3.3Chatbots 4Validierung 5Kosteneffektivität 6Behördlich erteilte Zulassungen 7AI-Act 8Ethik 9Pharmaforschung 10Kritik und Streitfragen 10.1Datenschutz und Privatheit 10.2Erklärbarkeit 11Siehe auch 12Weblinks 13Literatur 14Einzelnachweise العربية Català"
  },
  {
    "label": 1,
    "text": "Künstliches neuronales Netz – Wikipedia Künstliches neuronales Netz Inhaltsverzeichnis Überblick Beschreibung Geschichtliche Entwicklung Topologie der Verbindungsnetze Funktionsweise Anwendung Biologische Motivation Klassen und Typen von KNN Lernverfahren Allgemeine Probleme Filmische Dokumentationen Siehe auch Literatur Weblinks Einzelnachweise Anfänge Erste Blütezeit Langsamer Wiederaufbau Renaissance Neue Erfolge in Mustererkennungswettbewerben seit 2009 Typische Strukturen Künstliche Neuronen Verlustfunktion Fehlerrückführung Optimierung Implementierungen Überwachtes Lernen (supervised learning) Unüberwachtes Lernen (unsupervised learning) Bestärkendes Lernen (reinforcement learning) Stochastisches Lernen Einschichtiges feedforward-Netz Mehrschichtiges feedforward-Netz Rekurrentes Netz Dynamische neuronale Netze Neuronale Netze mit Gedächtnis Künstliche neuronale Netze, auchkünstliche neuronale Netzwerke, kurz:KNN(englischartificial neural network, ANN), sindNetzeauskünstlichen Neuronen, die von den Netzwerken inspiriert wurden, die biologischeNeuroneninGehirnenbilden. Ein KNN wird vonkünstlichen Neuronengebildet, die miteinander verbunden sind und in der Regel in Schichten organisiert werden. KNN werden beimmaschinellen Lerneneingesetzt. Damit könnenComputerProbleme lösen, die zu kompliziert sind, um sie mit Regeln zu beschreiben, zu denen es aber vieleDatengibt, die als Beispiele für die gewünschte Lösung dienen können. KNN bilden die Basis fürDeep Learning, das ab 2006 erhebliche Fortschritte bei der Analyse von großen Datenmengen erlaubte. Erfolgreiche Anwendungen des Deep Learning sind beispielsweiseBilderkennungundSpracherkennung.[1]:11 KNNs sind Forschungsgegenstand sowohl des Maschinellen Lernens, welches ein Teilbereich derkünstlichen Intelligenzist, als auch der interdisziplinärenNeuroinformatik. Das Nachbilden eines biologischenneuronalen NetzesvonNeuronenist eher Gegenstand derComputational Neuroscience. KNN werden beimmaschinellen Lerneneingesetzt. Damit könnenComputerProbleme lösen, die zu kompliziert sind, um sie mit Regeln zu beschreiben, zu denen es aber vieleDatengibt, die als Beispiele für die gewünschte Lösung dienen können. EinAlgorithmuspasst das zunächst unwissende Netz so an die Beispieldaten an, dass es von ihnen auf neue Fälle verallgemeinern kann. Dieser Vorgang wird Training genannt. Das trainierte Netz kann für neue Daten Vorhersagen treffen oder Empfehlungen und Entscheidungen erzeugen.[1]:8 KNN wurden von den Netzwerken inspiriert, die biologischeNeuroneninGehirnenbilden. Biologische Neuronen sind miteinander vernetzt und in Schichten organisiert. Sie können mehrere Eingangssignale aufsummieren und geben nur dann ein Signal an andere Neuronen weiter, wenn die Summe der Eingangssignale einen Schwellenwert erreicht.[2]:37 Ein KNN wird vonkünstlichen Neuronengebildet. Künstliche Neuronen bilden ausgewählte Eigenschaften von biologischen Neuronen mit mathematischen Mitteln nach. Ein künstliches Neuron kann mehrere Eingangssignale aufsummieren. Jedes Eingangssignal wird gewichtet und kann dadurch die Summe der Eingangssignale unterschiedlich stark erhöhen oder reduzieren. DieAktivierungsfunktioneines künstlichen Neurons sorgt dafür, dass es nur dann ein Ausgangssignal ausgibt, wenn die Summe aller Eingangssignale einen Schwellenwert überschreitet. Wenn die Summe aller Eingangssignale unter dem Schwellenwert liegt, gibt das künstliche Neuron kein Ausgangssignal aus.[3][2]:37 In der Regel besteht ein KNN aus mehreren Schichten von künstlichen Neuronen. Die Signale wandern von der ersten Schicht (der Eingabeschicht) zur letzten Schicht (der Ausgabeschicht) und durchlaufen dabei möglicherweise mehrere Zwischenschichten (versteckte Schichten). Jede Schicht kann die Signale an ihren Eingängen unterschiedlich transformieren. Ein Netz mit vielen verborgenen Schichten kann eine komplizierte Aufgabe in mehrere einfachere Aufgaben zerlegen, die jeweils in verschiedenen Schichten des Modells ausgeführt werden.[3]Ein solches KNN wird auch als tiefes neuronales Netz bezeichnet. Darauf bezieht sich der Begriff „Deep Learning“.[4] Zu Beginn desTrainingsstehen alle Schwellenwerte und Gewichte auf Zufallswerten. Während des Trainings passt ein Algorithmus schrittweise Schwellenwerte und Gewichte an die Daten an, mit denen das Netz trainiert wird, siehe auchOptimierung. Die resultierenden Gewichte werden alsParameter des KI-Modellsbezeichnet. Das Training wird beendet, wenn das Netz für alle Beispiele aus den Trainingsdaten eine möglichst korrekte Ausgabe erzeugt.[3] Das Erstellen eines geeigneten Trainingsdatensatzes kann sehr schwierig sein, da man verhindern muss, dass die Daten Muster aufweisen, die das Netz nicht zur Entscheidung heranziehen soll. Das ForschungsgebietExplainable Artificial Intelligencearbeitet an Verfahren, die erklären können, wie ein trainiertes KNN zu einer Entscheidung kommt. Beispielsweise wurde untersucht, welchen Teil eines Bildes ein KNN betrachtet, um ein Bild zu klassifizieren. Dabei entdeckten die Forscher z. B. ein KNN, das Eisenbahnzüge erkennen sollte und dazu nur auf Schienen achtete.[2]:54 Künstliche neuronale Netze basieren meist auf der Vernetzung vielerMcCulloch-Pitts-Neuronenoder leichter Abwandlungen davon. DieTopologieeines Netzes (die Zuordnung von Verbindungen zu Knoten) muss im Hinblick auf seine Aufgabe gut durchdacht sein. Nach der Konstruktion eines Netzes folgt die Trainingsphase, in der das Netz „lernt“. Theoretisch kann ein Netz durch folgende Methoden lernen: Außerdem verändert sich das Lernverhalten bei Veränderung derAktivierungsfunktionder Neuronen oder der Lernrate des Netzes. Praktisch gesehen „lernt“ ein Netz hauptsächlich durch Modifikation der Gewichte der Verbindungen zwischen den Neuronen. Eine Anpassung des Schwellenwertes kann hierbei durch einon-Neuronmiterledigt werden. Dadurch sind KNNs in der Lage, kompliziertenichtlineareFunktionenüber einen „Lern“-Algorithmus, der durchiterativeoderrekursiveVorgehensweise aus vorhandenen Ein- und gewünschten Ausgangswerten alleParameterder Funktion zu bestimmen versucht, zu erlernen. KNNs sind dabei eine Realisierung deskonnektionistischenParadigmas, da die Funktion aus vielen einfachen gleichartigen Teilen besteht. Erst in ihrer Summe kann deren Verhalten im Zusammenspiel sehr vieler beteiligter Teile komplex werden. Ein neuronales Netz, das deterministisch beschrieben wird und Rückkopplungen erlaubt, stellt unter dem Gesichtspunkt derBerechenbarkeitein äquivalentes Modell zurTuringmaschinedar.[5]D.h. zu jedem Netz gibt es mindestens eine Turingmaschine und zu jeder Turingmaschine gibt es mindestens ein Netz mit Rückkopplung. Bei einer stochastischen Beschreibung ist dies nicht der Fall. Rekurrente Netze sind damit die ausdrucksstärkste Form (Typ 0 in derChomsky-Hierarchie). Das Interesse für künstliche neuronale Netze setzte bereits in den frühen1940erJahren ein, also etwa gleichzeitig mit dem Einsatz programmierbarerComputerin angewandter Mathematik.[6] Die Anfänge gehen aufWarren McCullochundWalter Pittszurück. Sie beschrieben 1943 Verknüpfungen von elementaren Einheiten zu einem Netz ähnlich dem der Neuronen im Gehirn, mit dem sich praktisch jede logische oder arithmetische Funktion berechnen lassen könnte.[7]1947 wiesen sie darauf hin, dass ein solches Netz beispielsweise zur räumlichen Mustererkennung eingesetzt werden kann. 1949 formulierteDonald O. HebbseineHebbsche Lernregel, die in ihrer allgemeinen Form die meisten der künstlichen neuronalen Lernverfahren wiedergibt.Karl Lashleyfand 1950, dass der Prozess der Informationsspeicherung im Gehirn verteilt auf verschiedene Untereinheiten realisiert wird.[8] Im anschließenden Jahr, 1951, gelingtMarvin Minskymit seiner Dissertationsarbeit der Bau des NeurocomputersSnarc, der seine Gewichte automatisch justieren kann, jedoch nicht praktisch einsetzbar ist.[8]1956 treffen sich Wissenschaftler und Studenten auf derDartmouth Conference. Diese Konferenz gilt als Geburtsstunde der Künstlichen Intelligenz als akademisches Fachgebiet.[9]Von 1957 bis 1958 entwickelnFrank RosenblattundCharles Wightmanden ersten erfolgreichen Neurocomputer, mit dem NamenMark I Perceptron. Der Computer konnte mit seinem 20 × 20 Pixel großen Bildsensor bereits einfache Ziffern erkennen. Im nachfolgenden Jahr formuliert Rosenblatt dasPerceptron-Konvergenz-Theorem. 1960 stellenBernard WidrowundMarcian E. HoffdasADALINE(ADAptive LInear NEuron) vor.[10]Dieses Netz erreichte als erstes weite kommerzielle Verbreitung. Anwendung fand es in Analogtelefonen zur Echtzeit-Echofilterung. Das neuronale Netz lernte mit derDeltaregel. 1961 stellteKarl SteinbuchTechniken der assoziativen Speicherung vor. 1969 gaben Marvin Minsky undSeymour Paperteine genaue mathematische Analyse desPerceptrons.[11]Sie zeigten auf, dass wichtige Probleme nicht gelöst werden können. So sind unter anderemXOR-Operatorennicht auflösbar und es gibt Probleme in derlinearen Separierbarkeit. Die Folge war ein vorläufiges Ende der Forschungen auf dem Gebiet der neuronalen Netze, da die meisten Forschungsgelder gestrichen wurden. 1972 präsentiertTeuvo Kohonendenlinearen Assoziator, ein Modell des Assoziativspeichers.[12]James A. Andersonbeschreibt das Modell unabhängig von Kohonen aus neuropsychologischer Sicht im selben Jahr.[13]1973 benutztChristoph von der MalsburgeinNeuronenmodell, das nichtlinear ist. Bereits 1974 entwickeltPaul Werbosfür seine Dissertation dieBackpropagationbzw. die Fehlerrückführung. Das Modell bekam aber erst später eine größere Bedeutung. Ab 1976 entwickeltStephen Grossbergmathematisch fundierte Modelle neuronaler Netze. Zusammen mitGail Carpenterwidmet er sich auch dem Problem, ein neuronales Netz lernfähig zu halten, ohne bereits Gelerntes zu zerstören. Sie formulieren ein Architekturkonzept für neuronale Netze, dieAdaptive Resonanztheorie. 1982 beschreibt Teuvo Kohonen die nach ihm benanntenselbstorganisierenden Karten. Im selben Jahr beschreibtJohn Hopfielddas Modell derHopfield-Netze. 1983 wird vonKunihiko Fukushima, S. Miyake und T. Ito das neuronale ModellNeocognitronvorgestellt. Das Modell war eine Weiterentwicklung des 1975 entwickeltenCognitronsund diente zur Erkennung handgeschriebener Zeichen. 1985 veröffentlichtJohn Hopfieldeine Lösung desTravelling Salesman Problemsdurch einHopfield-Netz. 1985 wird das LernverfahrenBackpropagation of Errorals Verallgemeinerung derDelta-Regeldurch dieParallel-Distributed-Processing-Gruppe separat entwickelt. Somit werden nichtlinear separierbare Problemedurch mehrschichtige Perceptrons lösbar. Marvin Minskys Auffassung war also widerlegt. In jüngster Zeit erlebten neuronale Netze eine Wiedergeburt, da sie bei herausfordernden Anwendungen oft bessere Ergebnisse als konkurrierende Lernverfahren liefern. Zwischen 2009 und 2012 gewannen dierekurrentenbzw. tiefen vorwärtsgerichteten neuronalen Netze der Forschungsgruppe vonJürgen SchmidhuberamSchweizer KI Labor IDSIAeine Serie von acht internationalen Wettbewerben in den BereichenMustererkennungundmaschinelles Lernen.[14]Insbesondere gewannen ihre rekurrentenLSTM-Netze[15][16]drei Wettbewerbe zur verbundenen Handschrifterkennung bei derIntl. Conf. on Document Analysis and Recognition (ICDAR)2009 – obwohl kein A-priori-Wissen über die drei verschiedenen zu lernenden Sprachen in die Programmierung der Modelle einbezogen wurde. Die LSTM-Netze erlernten gleichzeitigeSegmentierungundErkennung. Dies waren die ersten internationalen Wettbewerbe, die durchDeep Learning[17][18]oder durch rekurrente Netze gewonnen wurden. Tiefe vorwärtsgerichtete Netze wie Kunihiko FukushimasKonvolutionsnetzder 80er Jahre[19]haben wieder an Bedeutung gewonnen. Sie verfügen über alternierendeKonvolutionslagen(convolutional layers) und Lagen von Neuronen, die mehrere Aktivierungen zusammenfassen (pooling layers[20]), um die räumlicheDimensionzu reduzieren. Abgeschlossen wird ein solches Konvolutionsnetz in der Regel durch mehrerevollständig verbundene Schichten(englischfully connected layers).Yann LeCunsTeam von derNew York Universitywandte den 1989 schon gut bekanntenBackpropagation-Algorithmus auf solche Netze an.[21]Moderne Varianten verwenden sogenanntesmax-poolingfür die Zusammenfassung der Aktivierungen, das stets der stärksten Aktivierung den Vorzug gibt.[22]SchnelleGrafikprozessor (GPU)-Implementierungen dieser Kombination wurden 2011 durch Dan Ciresan und Kollegen in Schmidhubers Gruppe eingeführt.[23]Sie gewannen seither zahlreiche Wettbewerbe, u. a. die „ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks Challenge“[24]und den „ICPR 2012 Contest on Mitosis Detection in Breast Cancer Histological Images“.[25]AlexNet, ein ähnliches Modell, erzielte auch die zu diesem Zeitpunkt besten Ergebnisse auf demImageNetBenchmark.[26][27]GPU-basiertemax-pooling-Konvolutionsnetze waren auch die ersten künstlichen Mustererkenner mit übermenschlicher Performanz[28]in Wettbewerben wie der „IJCNN 2011 Traffic Sign Recognition Competition“.[29]In den letzten Jahren fand auch die Theorie derZufallsmatrizenvermehrt Anwendung in der Erforschung von neuronalen Netzen.[30]NeuronaleOperatorensind Verallgemeinerungen von künstlichen neuronalen Netzwerken auf unendlichdimensionaleFunktionenräume. Die primäre Anwendung von neuronalen Operatoren liegt darin Lösungsoperatoren vonpartiellen Differentialgleichungenzu erlernen.[31] In künstlichen neuronalen Netzen bezeichnet die Topologie die Struktur des Netzes. Damit ist im Allgemeinen gemeint, wie vielekünstliche Neuronensich auf wie vielen Schichten befinden, und wie diese miteinander verbunden sind. Künstliche Neuronen können auf vielfältige Weise zu einem künstlichen neuronalen Netz verbunden werden. Dabei werden Neuronen bei vielen Modellen in hintereinander liegenden Schichten (englischlayers) angeordnet; bei einem Netz mit nur einer trainierbaren Neuronenschicht spricht man von einemeinschichtigen Netz. Unter Verwendung einesGraphenkönnen die Neuronen alsKnotenund ihre Verbindungen alsKantendargestellt werden. Die Eingaben werden gelegentlich auch als Knoten dargestellt, was hilft, den Informationsfluss durch das Netzwerk zu visualisieren. Die hinterste Schicht des Netzes, deren Neuronenausgaben meist als einzige außerhalb des Netzes sichtbar sind, wirdAusgabeschicht(englischoutput layer) genannt. Davorliegende Schichten werden entsprechend alsversteckte Schicht(englischhidden layer) bezeichnet. Die Struktur eines Netzes hängt unmittelbar mit dem verwendeten Lernverfahren zusammen und umgekehrt; so kann mit derDelta-Regelnur ein einschichtiges Netz trainiert werden, bei mehreren Schichten ist eine leichte Abwandlung vonnöten. Dabei müssen Netze nicht zwingend homogen sein: es existieren auch Kombinationen aus verschiedenen Modellen, um so unterschiedliche Vorteile zu kombinieren. Es gibt reinefeedforward-Netze, bei denen eine Schicht immer nur mit der nächsthöheren Schicht verbunden ist. Darüber hinaus gibt es Netze, in denen Verbindungen in beiden Richtungen erlaubt sind. Die passende Netzstruktur wird meist nach der Methode vonVersuch und Irrtumgefunden, was durchevolutionäre Algorithmenund eineFehlerrückführungunterstützt werden kann. Einschichtige Netze mit derfeedforward-Eigenschaft (englisch fürvorwärts) sind die einfachsten Strukturen künstlicher neuronaler Netze. Sie besitzen lediglich eine Ausgabeschicht. Diefeedforward-Eigenschaft besagt, dass Neuronenausgaben nur in Verarbeitungsrichtung geleitet werden und nicht durch eine rekurrente Kante zurückgeführt werden können (azyklischer,gerichteter Graph). Mehrschichtige Netze besitzen neben der Ausgabeschicht auch verdeckte Schichten, deren Ausgabe wie beschrieben, außerhalb des Netzes nicht sichtbar sind. Verdeckte Schichten verbessern die Abstraktion solcher Netze. So kann erst das mehrschichtigePerzeptrondasXOR-Problem lösen. Rekurrente Netzebesitzen im Gegensatz dazu auch rückgerichtete (rekurrente) Kanten (englischfeedback loops) und enthalten somit eineRückkopplung. Solche Kanten werden dann immer mit einer Zeitverzögerung (in der Systemtheorie als Verzögerungsglied bezeichnet) versehen, sodass bei einer schrittweisen Verarbeitung die Neuronenausgaben der vergangenen Einheit wieder als Eingaben angelegt werden können. Diese Rückkopplungen ermöglichen einem Netz ein dynamisches Verhalten und statten es mit einemGedächtnisaus. Dynamische neuronale Netze passen die Struktur und/oder Parameter dynamisch je nach Input bei der Inferenz an.[32] Neuronale Netze mit Gedächtnis verfügen über einen statischen Speicher, der bei der Inferenz dynamisch abgefragt werden kann. Künstliche neuronale Netze dienen als universelle Funktionsapproximatoren. Werte werden dabei von der Eingabe- bis zur Ausgabeschicht propagiert, wobei eine Aktivierungsfunktion für Nichtlinearität sorgt. Beim Trainieren wird ein Fehler bestimmt; mit Hilfe von Fehlerrückführung und einem Optimierungsverfahren werden dabei die Gewichte schichtweise angepasst.[33] Ein künstliches Neuron erhält Eingaben von anderen Neuronen (oder von außen), wie auf dem Bild rechts zu erkennen ist. Der Wert einer Eingabe hängt vom Aktivitätslevel des sendenden Neurons und vom Gewichtwij{\\displaystyle w_{ij}}der Verbindung zwischen Sender- und Empfänger-Neuron ab. DiePropagierungsfunktion(Übertragungsfunktion) errechnet aus den einzelnen Eingaben und ihren Gewichten die Gesamteingabenetj{\\displaystyle net_{j}}für das Neuron, dieNetz-Eingabeoder Netz-Input genannt wird. Am häufigsten wird die gewichtete Summe (Linearkombination) verwendet: netj=∑i=1nxiwij.{\\displaystyle {\\mbox{net}}_{j}=\\sum \\limits _{i=1}^{n}x_{i}w_{ij}.} Welches Aktivitätsleveloj{\\displaystyle o_{j}}das Neuron annimmt, wird nicht direkt durch die Netz-Eingabe bestimmt, sondern durch dieAktivierungsfunktion. Dieses Aktivitätslevel bestimmt dann die Ausgabe des Neurons an Neuronen der nächsten Schicht. oj=φ(netj){\\displaystyle o_{j}=\\varphi ({\\mbox{net}}_{j})} Die Aktivierungsfunktion ermöglicht die Einführung von Nichtlinearität ins neuronale Netz, denn nicht alle Aufgaben neuronaler Netze lassen sich mit linearen Funktionen abbilden. Es gibt verschiedene Aktivierungsfunktionen. In den verdeckten Schichten wird meist dieRectifier-Funktionverwendet, in der Ausgabeschicht eineSigmoidfunktion. Die Aktivierungsfunktion, welche im allerletzten Schritt des neuronalen Netzes ausgeführt wird, kann alsAusgabefunktionaufgefasst werden. Diese Ausgabefunktion hat als Hauptaufgabe sicherzustellen, dass der Bereich (Träger) der Ausgaben zu weiteren Annahmen der Modellierung passt. Die Ausgabefunktion kann sicherstellen, dass Verteilungsvoraussetzungen derLikelihoodfunktionerfüllt sind, wie z. B. die Ausgabe von strikt positiven Ergebnissen oder Ergebnissen nur im Intervall von 0 bis 1. Die Aufgabe der Ausgabefunktion ist direkt analog zur (inversen)Linkfunktioninverallgemeinerten linearen Modellen. Mit Hilfe von verbundenen Neuronen, die die Propagierungs- und Aktivierungsfunktionen anwenden, gibt das neuronale Netz einen Zahlenvektor aus. Inwieweit das Ergebnis aus dem KNN von dem Zielwert abweicht, wird mit Hilfe einer Fehlerfunktion bestimmt. Es gibt verschiedene Arten von Fehlerfunktionen. Eine davon ist der mittlere quadratische Fehler (MQF): Der Faktor12{\\displaystyle {\\tfrac {1}{2}}}wird dabei lediglich zur Vereinfachung bei der Ableitung hinzugenommen. Der MQF eignet sich, wenn die Rückgabe des Netzes ein einzelner Wert ist. Mit Hilfe des Fehlers lassen sich die Gewichte anpassen. Dies geschieht in zwei Schritten: Im ersten Schritt werden mit Hilfe der Fehlerrückführung die Gradienten bestimmt. Im zweiten Schritt werden die Gradienten mit einem Optimierungsverfahren verwendet, um die Gewichte zu aktualisieren. In diesem Abschnitt geht es um die Fehlerrückführung. Die Idee hinter der Fehlerrückführung ist, die Gradienten schichtweise zu berechnen. Dies geschieht über die Kettenregel: ∂E∂wij=∂E∂oj∂oj∂netj∂netj∂wij.{\\displaystyle {\\dfrac {\\partial E}{\\partial w_{ij}}}={\\frac {\\partial E}{\\partial o_{j}}}{\\frac {\\partial o_{j}}{\\partial {\\mbox{net}}_{j}}}{\\frac {\\partial {\\mbox{net}}_{j}}{\\partial w_{ij}}}.} Es ist wichtig, das Training mit zufälligen Anfangswerten zu beginnen. Bei überall gleichen Anfangswerten hätte beispielsweise jeder Knoten im KNN das gleiche Gewicht. Wenn dann Gewichte durch die Fehlerrückführung angepasst werden, würde der Fehler auf alle Knoten gleich verteilt und es würden auch alle Gewichte gleichmäßig verändert. Da das Problem in der Regel nicht symmetrisch ist, dürfen auch die Anfangswerte keine Symmetrien aufweisen, sonst kann der Lernalgorithmus das KNN nicht gut an die Trainingsdaten anpassen.[34]:190 Mit der Fehlerrückführung wurden Fehler und Gewichte in einer Funktion abgebildet. Das Lernen korrespondiert nun zu einer Minimierung der Fehlerfunktion, indem die Gewichte angepasst werden. Das aus der Schule bekannte Optimalitätskriterium 1. Ordnung, das Nullsetzen der Ableitung, ist bei Neuronalen Netzen praktisch jedoch ungeeignet. Stattdessen wird mit Gradientenverfahren gearbeitet, um ein lokales Minimum der Fehlerfunktion zu finden. Der Gradient ist die Richtung des steilsten Anstieges einer Funktion, eine Bewegung entgegen den Gradienten ermöglicht also einen Abstieg auf dem Graphen der Fehlerfunktion. Eine Iteration des naiven Gradientverfahrens oder Gradientenabstieges ist also die Berechnung des Gradienten für die aktuelle Wahl der Gewichte, anschließend wird von den Gewichten der Gradient abgezogen und so neue Gewichte mit niedrigerem Fehler erhalten. Das Netz hat somit also „gelernt“. Dieses Update bzw. dieser Abstiegsschritt wird durch folgende Zuweisung notiert: wi=wi−α∂E∂wi.{\\displaystyle w_{i}=w_{i}-\\alpha {\\frac {\\partial E}{\\partial w_{i}}}.}Dabei beschreibt der Wertα{\\displaystyle \\alpha }die Lernrate. Dieser gibt an, wie groß die Schritte sind, die das Verfahren in Richtung des Minimums nehmen soll. Zur Berechnung des Gradienten wird die Fehlerrückführung (engl. Backpropagation) verwendet. Das Verfahren wird solange wiederholt, bis ein Abbruchkriterium erfüllt ist, idealerweise durch Konvergenz zu einem lokalen Minimum. Neben dem hier dargestellten naiven Gradientenverfahren werden in der Praxis meist besser entwickelte und leistungsfähigere Variationen des Gradientenabstieges verwendet, z. B. der stochastische Gradientenabstieg oder das ADAM-Verfahren. Seine besonderen Eigenschaften machen das KNN bei allen Anwendungen interessant, bei denen kein oder nur geringesexplizites (systematisches) Wissenüber das zu lösende Problem vorliegt. Dies sind z. B. dieTexterkennung,Spracherkennung,BilderkennungundGesichtserkennung, bei denen einige Hunderttausend bis MillionenBildpunktein eine im Vergleich dazu geringe Anzahl von erlaubten Ergebnissen überführt werden müssen. Auch in derRegelungstechnikkommen KNN zum Einsatz, um herkömmlicheReglerzu ersetzen oder ihnenSollwertevorzugeben, die das Netz aus einer selbst entwickeltenPrognoseüber denProzessverlaufermittelt hat. So können auchFuzzy-Systeme durch eine bidirektionale Umwandlung in neuronale Netze lernfähig gestaltet werden. Die Anwendungsmöglichkeiten sind aber nicht auf techniknahe Gebiete begrenzt: Bei der Vorhersage von Veränderungen in komplexen Systemen werden KNNs unterstützend hinzugezogen, z. B. zur Früherkennung sich abzeichnenderTornadosoder aber auch zur Abschätzung der weiteren Entwicklung wirtschaftlicher Prozesse. Zu den Anwendungsgebieten von KNNs gehören insbesondere: Trotz dieser sehr großen Spanne an Anwendungsgebieten gibt es Bereiche, die KNNs aufgrund ihrer Natur nicht abdecken können, beispielsweise:[35] Während dasGehirnzur massiven Parallelverarbeitung in der Lage ist, arbeiten die meisten heutigen Computersysteme nur sequentiell (bzw. partiell parallel eines Rechners). Es gibt jedoch auch erste Prototypen neuronaler Rechnerarchitekturen, sozusagen den neuronalen Chip, für die das Forschungsgebiet der künstlichen neuronalen Netze die theoretischen Grundlagen bereitstellt. Dabei werden diephysiologischen Vorgängeim Gehirn jedoch nicht nachgebildet, sondern nur die Architektur der massiv parallelen Analog-Addierer in Silizium nachgebaut, was gegenüber einer Software-Emulation eine bessere Leistung verspricht. Grundsätzlich unterscheiden sich die Klassen der Netze vorwiegend durch die unterschiedlichen Netztopologien und Verbindungsarten, so zum Beispiel einschichtige, mehrschichtige, Feedforward- oder Feedback-Netze. Lernverfahren dienen dazu, ein neuronales Netz so zu modifizieren, dass es für bestimmte Eingangsmuster zugehörige Ausgabemuster erzeugt. Dies geschieht grundsätzlich auf drei verschiedenen Wegen. Beim Überwachten Lernen wird dem KNN ein Eingangsmuster gegeben und die Ausgabe, die das neuronale Netz in seinem aktuellen Zustand produziert, mit dem Wert verglichen, den es eigentlich ausgeben soll. Durch Vergleich von Soll- und Istausgabe kann auf die vorzunehmenden Änderungen der Netzkonfiguration geschlossen werden. Bei einlagigen Perzeptrons kann dieDelta-Regel(auch Perzeptron-Lernregel) angewendet werden. Mehrlagige Perzeptrons werden in der Regel mitBackpropagationtrainiert, was eine Verallgemeinerung der Delta-Regel darstellt. Das Unüberwachte Lernen erfolgt ausschließlich durch Eingabe der zu lernenden Muster. Das neuronale Netz verändert sich entsprechend den Eingabemustern von selbst. Hierbei gibt es folgende Lernregeln: Es ist nicht immer möglich, zu jedem Eingabedatensatz den passenden Ausgabedatensatz zum Trainieren zur Verfügung zu haben. Zum Beispiel kann man einem Agenten, der sich in einer fremden Umgebung zurechtfinden muss – etwa einem Roboter auf dem Mars – nicht zu jedem Zeitpunkt sagen, welche Aktion jeweils die beste ist. Aber man kann dem Agenten eine Aufgabe stellen, die dieser selbstständig lösen soll. Nach einem Testlauf, der aus mehreren Zeitschritten besteht, kann der Agent bewertet werden. Aufgrund dieser Bewertung kann eine Agentenfunktion gelernt werden. Der Lernschritt kann durch eine Vielzahl von Techniken vollzogen werden. Unter anderem können hier auch künstliche neuronale Netze zum Einsatz kommen. DieHauptnachteilevon KNN sind gegenwärtig: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Überblick 2Beschreibung 3Geschichtliche Entwicklung 3.1Anfänge 3.2Erste Blütezeit 3.3Langsamer Wiederaufbau 3.4Renaissance 3.5Neue Erfolge in Mustererkennungswettbewerben seit 2009 4Topologie der Verbindungsnetze 4.1Typische Strukturen 4.1.1Einschichtiges feedforward-Netz 4.1.2Mehrschichtiges feedforward-Netz 4.1.3Rekurrentes Netz 4.1.4Dynamische neuronale Netze 4.1.5Neuronale Netze mit Gedächtnis 5Funktionsweise 5.1Künstliche Neuronen 5.2Verlustfunktion 5.3Fehlerrückführung 5.4Optimierung 6Anwendung 6.1Implementierungen 7Biologische Motivation 8Klassen und Typen von KNN 9Lernverfahren 9.1Überwachtes Lernen (supervised learning) 9.2Unüberwachtes Lernen (unsupervised learning) 9.3Bestärkendes Lernen (reinforcement learning) 9.4Stochastisches Lernen 10Allgemeine Probleme 11Filmische Dokumentationen"
  },
  {
    "label": 1,
    "text": "Maschinelles Lernen – Wikipedia Maschinelles Lernen Inhaltsverzeichnis Geschichte Verwandte Fachgebiete Methoden Daten Modelle Anforderungen Automatisiertes maschinelles Lernen Siehe auch Literatur Weblinks Einzelnachweise Künstliche Intelligenz Statistik Data Science Data-Mining und Knowledge Discovery in Databases Mathematische Optimierung Repräsentation des Wissens Training Batch- und Online-Learning Lernen von Instanzen oder Modellen Lineare Regression Logistische Regression k-Means-Algorithmus Support Vector Machines Entscheidungsbäume Random Forests Künstliche Neuronale Netze Generative Adversarial Networks Genauigkeit Transparenz und Erklärbarkeit Ressourcen Datenschutz und Datensicherheit Freiheit und Autonomie Robustheit und Sicherheit Symbolische Ansätze Nicht-symbolische Ansätze Überwachtes Lernen Unüberwachtes Lernen Bestärkendes Lernen Maschinelles Lernen(ML) entwickelt, untersucht und verwendetstatistischeAlgorithmen, auch Lernalgorithmen genannt. Lernalgorithmen können Lösungen für Probleme lernen, die zu kompliziert sind, um sie mit Regeln zu beschreiben, zu denen es aber viele Daten gibt, die als Beispiele für die gewünschte Lösung dienen können. Ein Lernalgorithmus bildet vorgegebene Beispieldaten auf einmathematisches Modellab. Dabei passt der Lernalgorithmus das Modell so an, dass es von den Beispieldaten auf neue Fälle verallgemeinern kann. Dieser Vorgang wird Training genannt. Nach dem Training ist der gefundene Lösungsweg im Modell gespeichert. Er wird nicht explizitprogrammiert. Das trainierte Modell kann für neue Daten Vorhersagen treffen oder Empfehlungen und Entscheidungen erzeugen.[2]In dermathematischen Statistikbezeichnet man dieses Fachgebiet auch alsstatistisches Lernen.[3] Aus dem weiten Spektrum möglicher Anwendungen seien hier genannt:Spamfilter, automatisierteDiagnose­verfahren, Erkennung vonKreditkartenbetrug,Aktienmarkt­analysen, Klassifikation vonNukleotidsequenzen,Sprach-undTexterkennung. Allgemein formuliert lernt ein Lernalgorithmus beim Training aus den Beispieldaten eine Funktion, die auch für neue, nicht zuvor gelernte Dateneingaben eine korrekte Ausgabe erzeugt. Es gibt verschiedene Lernstile, die sich darin unterscheiden, woher der Algorithmus beim Training Informationen dazu erhält, was „korrekt“ ist. Am häufigsten wird dasüberwachte Lerneneingesetzt. Dabei werden Vorgaben in Form von korrekten Ausgabewerten oder Rückmeldungen zur Verfügung gestellt. Beimunüberwachten Lernenwerden keine Vorgaben gemacht. Die Algorithmen durchsuchen die Beispieldaten beispielsweise nach Kriterien für die Einteilung in unterschiedlicheClusteroder nach korrelierenden Merkmalen, die zusammengefasst werden können, um die Daten zu vereinfachen. Da es keine Vorgaben gibt, können diese Algorithmen unterschiedliche Lösungen vorschlagen, die anschließend zu bewerten sind. Beimbestärkenden Lernenbeobachten Lernsysteme, die als Agenten bezeichnet werden, eine Umgebung und reagieren auf sie, indem sie Aktionen ausführen. Für die Aktionen erhalten sie Belohnungen. Diese Lernsysteme entwickeln selbständig eine Strategie, um möglichst viele Belohnungen zu erhalten. Dieser Abschnitt gibt einen kurzen Überblick über wichtige Ereignisse und Meilensteine. 1943 beschreibenWarren McCullochundWalter Pittsein Modell für einkünstliches Neuron, das später alsMcCulloch-Pitts-Zellebekannt wird. Sie zeigen auch, dass künstliche Neuronen miteinander zu einem Netz verbunden werden können, mit dem sich praktisch jede logische oder arithmetische Funktion berechnen lassen könnte.[4] 1957 publiziertFrank RosenblattdasPerzeptron-Modell, das aus einer einzelnen Schicht von künstlichen Neuronen besteht.[1] In den 1960ern werden Algorithmen fürBayessche Netzeentwickelt.[2]Bayessche Netze können beispielsweise vorhersagen, mit welcher Wahrscheinlichkeit eine bestimmte Diagnose zu den Daten eines Patienten passt. 1969 weisenMarvin MinskyundSeymour Papertnach, dass man mit Netzen, die nur aus einer Schicht von künstlichen Neuronen bestehen, nicht jede gegebene Funktion berechnen kann, weil man damit keineXOR-Verknüpfung(exklusives Oder) modellieren kann. Für das Training mehrschichtiger Netze ist zu dieser Zeit kein funktionierendes Verfahren bekannt. Danach stagniert die Forschung an künstlichen neuronalen Netzen.[1]:341[4] 1982 beschreibtPaul Werbosein Verfahren, das das Training mehrschichtiger Netze erlaubt. Es ist heute alsBackpropagationbekannt.[4]Es folgt ein neuer Aufschwung in der Forschung an künstlichen neuronalen Netzen. In den 1990ern gibt es große Fortschritte durch die Entwicklung vonSupport Vector Machines(SVMs) undrekurrenten neuronalen Netzen(RNNs).[2]Wissenschaftler beginnen mit der Entwicklung von Programmen, die große Datenmengen analysieren und aus den Ergebnissen Regeln „lernen“. In den 2000ern wird ML zunehmend auch in der Öffentlichkeit bekannt. Die stetige Zunahme von Rechenleistung und verfügbaren Datenmengen ermöglicht weitere Fortschritte. 2001 veröffentlichtLeo Breimandie Grundlagen für ein alsRandom Forestbekanntes Verfahren, das eine Gruppe vonEntscheidungsbäumentrainiert.[5] 2006 beschreibenGeoffrey Hintonet al. eine Methode, mit der man ein neuronales Netz, das aus mehreren Schichten von künstlichen Neuronen besteht, so trainieren kann, dass es handgeschriebene Zahlen mit einer Genauigkeit von über 98 % erkennen kann.[6]Bis dahin schien es unmöglich zu sein, mit solchen Netzen hohe Genauigkeiten bei der Klassifikation zu erreichen. Die neue Methode wirdDeep Learninggenannt.[1]:17 In den folgenden Jahren wird das Deep Learning weiter entwickelt. Es führt zu enormen Fortschritten in der Bild- und Textverarbeitung.[2] Zwischen 2009 und 2012 gewannen dierekurrentenbzw. tiefen vorwärtsgerichteten neuronalen Netze der Forschungsgruppe vonJürgen SchmidhuberamSchweizer KI-Labor IDSIAeine Serie von acht internationalen Wettbewerben in den BereichenMustererkennungund maschinelles Lernen.[7]Insbesondere gewannen ihre rekurrentenLSTM-Netze[8][9]drei Wettbewerbe zur verbundenenHandschrifterkennungbei der2009 Intl. Conf. on Document Analysis and Recognition (ICDAR)ohne eingebautesA-priori-Wissen über die drei verschiedenen zu lernenden Sprachen. Die LSTM-Netze erlernten gleichzeitigeSegmentierungund Erkennung. Dies waren die ersten internationalen Wettbewerbe, die durch Deep Learning[10]oder rekurrente Netze gewonnen wurden. 2017 gewinntAlphaGoimGo-Spielgegen den besten Spieler der Weltrangliste. Ebenfalls 2017 stellt ein Forscherteam von Google einen Artikel zurTransformer-Architektur vor. Diese enthält einen Aufmerksamkeitsmechanismus. Netze, die diese Architektur verwenden, lernen beim Training nicht nur, wie sie Daten verarbeiten sollen, sondern auch, auf welchen Teil einer Sequenz sie im vorgegebenen Kontext ihreAufmerksamkeitrichten sollen. Verglichen mit den bis dahin eingesetzten Architekturen reduziert sich dadurch der Aufwand für das Training beispielsweise vonSprachmodellenerheblich.[1]:651-652[4] 2020 wirdAlphaFoldin der medizinischen Fachwelt als Durchbruch in derProteinstrukturvorhersageaufgenommen. Das Programm ist in der Lage, die 3D-Struktur von Molekülen vorherzusagen. 2022 wird derChatbotChatGPTöffentlich zugänglich gemacht. Das Programm ist in der Lage, mit Nutzern über textbasierte Nachrichten und Bilder zu kommunizieren. 2024 wirdAlphaFold 3vorgestellt. Dieses Programm ist in der Lage, sowohl die 3D-Struktur von Molekülen als auch ihre Interaktion untereinander und mit anderen Molekülen vorherzusagen. Für Beiträge zu neuronalen Netzwerken und Deep Learning erhieltenYann LeCun,Yoshua BengioundGeoffrey Hinton2018 denTuring Award[11]und Hinton zusammen mitJohn Hopfield2024 denNobelpreis für Physik.[12]Die Entwickler von AlphaFold,Demis HassabisundJohn Jumper, wurden 2024 für die Vorhersage der komplexen Strukturen von Proteinen mit demNobelpreis für Chemieausgezeichnet. Das maschinelle Lernen ist ein Teilgebiet des Fachgebietes „Künstliche Intelligenz“, auch KI genannt. Das Fachgebiet „Künstliche Intelligenz“ ist ein Teilgebiet derInformatikmit dem Ziel, menschlicheIntelligenzzu imitieren. Etwa ab 1980 entwickelten sich die Ziele und Methoden innerhalb des Fachbereiches KI in verschiedene Richtungen. Die meisten Forscher versuchten vorrangig, durch die Verarbeitung von bekanntem Wissen inExpertensystemenmenschliche Intelligenz nachzubilden. Gleichzeitig untersuchte eine kleine Gruppe von Forschern, ob sich die Leistung von Computern bei Vorhersagen dadurch verbessern lässt, dass sie Wissen aus Daten lernen, die Informationen zu Erfahrungen aus dem Problemfeld enthalten. Der Bereich KI zeigte zu dieser Zeit nur wenig Interesse am Lernen aus Daten. Deshalb gründeten diese Forscher den neuen Bereich ML. Das Ziel von ML ist nicht mehr, menschliche Intelligenz zu imitieren, sondern praktische Probleme zu lösen.[13]Inzwischen betrachten viele Experten ML als eine Schlüsseltechnologie der KI.[2]Die öffentliche Berichterstattung verwendet die Bezeichnung KI oft gleichbedeutend mit ML. ML undStatistikverwenden sehr ähnliche Methoden. Die beiden Fachgebiete unterscheiden sich allerdings in ihrem Hauptziel. Viele der eingesetzten Methoden können sowohl angewendet werden, um Schlussfolgerungen zu ziehen, als auch, um Vorhersagen zu treffen. Die Statistik benutzt Daten von sorgfältig ausgewählten Stichproben, um daraus Rückschlüsse zu Eigenschaften einer zu untersuchenden Gesamtmenge zu ziehen. Die Methoden in der Statistik legen deshalb den Schwerpunkt darauf, statistische Modelle zu erstellen und diese genau an die gegebene Problemstellung anzupassen. Damit kann man berechnen, mit welcherWahrscheinlichkeitgefundene Zusammenhänge echt sind und nicht durch Störungen erklärt werden können. Dieses Schließen von Daten auf (hypothetische) Modelle wird alsstatistische Inferenzbezeichnet. Die Methoden im ML hingegen verarbeiten große Datenmengen und lernen daraus mit allgemein formulierten Algorithmen Zusammenhänge, die verallgemeinert und für Vorhersagen benutzt werden. Auch wenn ein maschinell gelerntes Modell für ein gegebenes Problem überzeugende Vorhersageergebnisse liefert, kann es unmöglich sein, die Zusammenhänge zu überprüfen, die das Modell gelernt hat und für seine Vorhersagen verwendet.[14] ML ist ein wichtiger Baustein des interdisziplinären Wissenschaftsfeldes „Data Science“.[15]Dieser Bereich befasst sich mit der Extraktion von Erkenntnissen, Mustern und Schlüssen sowohl aus strukturierten als auch unstrukturierten Daten. ML ist eng verwandt mit „Data-Mining“. Unter Data-Mining versteht man die systematische Anwendung statistischer Methoden auf große Datenbestände (insbesondere „Big Data“ bzw. Massendaten) mit dem Ziel, neueQuerverbindungen und Trendszu erkennen. Viele Algorithmen können für beide Zwecke verwendet werden. Algorithmen aus dem ML werden beim Data-Mining angewendet und Methoden derKnowledge Discovery in Databaseskönnen genutzt werden, um Lerndaten für ML zu produzieren oder vorzuverarbeiten.[16]:16–18 Diemathematische Optimierungist eine mathematische Grundlage des ML. Die bestmögliche Anpassung eines Modells an die Trainingsdaten ist ein Optimierungsproblem. Beispielsweise wenden einige Lernalgorithmen dasGradientenverfahrenan, umModellparameterzu optimieren. In der Theorie des computergestützten Lernens bietet dasProbably Approximately Correct Learningeinen Rahmen für die Beschreibung des ML. Die Methoden von ML können nach verschiedenen Kriterien in Kategorien eingeteilt werden. Das maschinelle Lernen verarbeitet Beispieldaten und leitet daraus Regeln ab. Viele Anwendungsfälle erfordern, dass die Regeln, die das Modell gelernt hat und im Einsatz verwendet, von Menschen nachvollzogen und überprüft werden können.[17] Ursprünglich hatte ML das Ziel, automatischExpertensystemezu erzeugen und nachzubilden, wie Menschen lernen. Der Schwerpunkt lag auf symbolischen Ansätzen. Bei symbolischen Ansätzen wird das Wissen in Form von Regeln oder logischen Formeln repräsentiert. Dadurch können Menschen die Zusammenhänge und Muster, die das System für seine Vorhersagen benutzt, leicht erkennen und überprüfen. Dabei werdenaussagenlogischeundprädikatenlogischeSysteme unterschieden. In der Aussagenlogik hat jede Aussage einen von genau zwei Wahrheitswerten. Der Wahrheitswert jeder zusammengesetzten Aussage ist eindeutig durch die Wahrheitswerte ihrer Teilaussagen bestimmt. Ein Beispiel für ein solches System ist einEntscheidungsbaum. Bekannte Algorithmen dafür sindID3und sein NachfolgerC4.5. Die Prädikatenlogik ist eine Erweiterung der Aussagenlogik. Sie spielt in der Konzeption und Programmierung von Expertensystemen eine Rolle, siehe auchinduktive logische Programmierung. Später änderte ML sein Ziel dahingehend, dass alle Methoden untersucht werden sollten, die die Leistung steigern können. Dazu gehören auch nicht-symbolische Ansätze, beispielsweisekünstliche neuronale Netze. Diese speichern die gelernten Regeln implizit in den Parametern des Modells. Das bedeutet, dass Menschen nicht einfach erkennen und überprüfen können, welche Zusammenhänge und Muster das System für eine Vorhersage benutzt. Der Aufwand dafür, Entscheidungen nachzuvollziehen, beispielsweise durch Untersuchungen dazu, wie das Modell auf kleine Änderungen der Eingangsdaten reagiert, kann sehr hoch sein.[13] Beim Training bildet ein Lernalgorithmus vorgegebene Beispieldaten auf einmathematisches Modellab. Nach dem Training ist der gefundene Lösungsweg im Modell gespeichert. Er wird nicht explizitprogrammiert. Das trainierte Modell kann für neue Daten Vorhersagen treffen oder Empfehlungen und Entscheidungen erzeugen.[2]:8 Beim Training baut der Lernalgorithmus ein Modell auf und passt dieParameterso an, dass die Ergebnisse des Modells die gegebene Aufgabe möglichst gut lösen. Dabei unterscheidet man drei Hauptgruppen der Trainingsüberwachung oder des Lernstils:[18]überwachtes Lernen(englischsupervised learning),unüberwachtes Lernen(englischunsupervised learning) undbestärkendes Lernen(engl.reinforcement learning). Beim überwachten Lernen wird das Modell mit Datensätzen trainiert und validiert, die für jede Eingabe einen passenden Ausgabewert enthalten. Man bezeichnet solche Datensätze als markiert oder gelabelt. Beim Training passt der LernalgorithmusParameterdes Modells so an, dass die Ausgaben des Modells möglichst gut mit den bekannten, richtigen Ausgaben übereinstimmen. Die Ausgaben des Modells werden also durch die vorgegebenen Ausgaben „überwacht“. Typische Anwendungsbeispiele sindKlassifikationundRegression. Der Lernalgorithmus baut zunächst in der Lernphase mit einem Teil der Beispieldaten, dem Trainingsdatensatz, einstatistisches Modellauf. Nach der Lernphase wird die Qualität des erzeugten Modells mit einem anderen Teil der Beispieldaten, dem Testdatensatz, überprüft. Das Ziel ist, dass das Modell auch für völlig neue Daten das geforderte Verhalten zeigt. Dazu muss sich das Modell gut an die Trainingsdaten anpassen, gleichzeitig muss eineÜberanpassungvermieden werden.[19][20] Es lassen sich noch einige Unterkategorien für überwachtes Lernen identifizieren, die in der Literatur häufiger erwähnt werden: DerAlgorithmuserzeugt für eine gegebene Menge von Eingaben einstatistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und somitVorhersagenermöglicht.Clustering-Verfahrenteilen Daten in mehrere Kategorien ein, die sich durch charakteristische Muster voneinander unterscheiden. Diese Verfahren erstellen selbständigKlassifikatoren. Ein wichtiger Algorithmus in diesem Zusammenhang ist derEM-Algorithmus, der iterativ die Parameter eines Modells so festlegt, dass es die gesehenen Daten optimal erklärt. Er legt dabei das Vorhandensein nicht beobachtbarer Kategorien zugrunde und schätzt abwechselnd die Zugehörigkeit der Daten zu einer der Kategorien und die Parameter, die die Kategorien ausmachen. Eine Anwendung des EM-Algorithmus findet sich beispielsweise in denHidden Markov Models(HMMs). Andere Methoden des unüberwachten Lernens, z. B. dieHauptkomponentenanalyse, zielen darauf ab, die beobachteten Daten in eine einfachere Repräsentation zu übersetzen, die sie trotz drastisch reduzierter Information möglichst genau wiedergibt. Ein typisches Anwendungsbeispiel ist die Vorbereitung von Datensätzen für das überwachte Lernen.[16]:21–27 Beim bestärkenden Lernen führt einSoftware-Agentselbständig Aktionen in einer dynamischen Umgebung aus und erlernt durchVersuch und Irrtumeine Strategie (englischpolicy), die die Summe der erhaltenen Belohnungen (englischrewards) maximiert. Aufgrund seiner Allgemeingültigkeit wird dieses Gebiet auch in vielen anderen Disziplinen untersucht, z. B. in derSpieltheorie, derKontrolltheorie, demOperations Research, derInformationstheorie, dersimulationsbasierten Optimierung, den Multiagentensystemen, derSchwarmintelligenz, der Statistik und dengenetischen Algorithmen. Beim bestärkenden Lernen wird die Umgebung normalerweise alsMarkov-Entscheidungsprozess(MDP) dargestellt. Eine klassische Methode zum Lösen eines MDP ist diedynamische Programmierung.[23]Die Verstärkungslernalgorithmen unterscheiden sich von den klassischen Methoden dadurch, dass sie kein exaktes mathematisches Modell des MDP voraussetzen. Sie werden eingesetzt, wenn keine exakten Modelle bekannt sind. Ein einfaches Beispiel ist einSaugroboter, dessen Belohnung in der Staubmenge besteht, die er in einer bestimmten Zeit aufsaugt. Anspruchsvolle Beispiele sind der Einsatz inautonomen Fahrzeugenoder als Gegner eines menschlichen Spielers in einem komplexenStrategiespiel, sieheAlphaGo. Beim Batch-Learning, auch Offline-Learning genannt, werden alle Beispieldaten auf einmal eingelesen. Das System kann in dieser Zeit nicht benutzt werden und ist in der Regel Offline. Nach dem Training kann das System nicht durch neue Daten verbessert werden. Wenn neue Daten dazu gelernt werden sollen, dann ist ein vollständiger neuer Trainingslauf mit allen alten und neuen Daten erforderlich. Beim Online-Learning, auch inkrementelles Lernen genannt, wird das System inkrementell mit kleineren Datensätzen trainiert. Das Verfahren eignet sich gut für Systeme, die sich schnell an Veränderungen anpassen müssen. Dabei müssen neue Daten genau so hochwertig sein wie alte. Wenn neue Daten beispielsweise ungeprüft von einem defekten Sensor übernommen werden, besteht die Gefahr, dass das Modell mit der Zeit schlechter wird.[24][1]:46-49 Beim ML geht es oft darum, Vorhersagen zu treffen. Dazu muss ein System von den gelernten Daten auf unbekannte Daten verallgemeinern. Eine einfache Methode besteht darin, dass das System direkt die Merkmale von neuen Datenpunkten mit denen der gelernten Datenpunkte vergleicht und ihre Ähnlichkeit vergleicht. Das bezeichnet man als instanzbasiertes Lernen. In der Trainingsphase lernt das System nur die Trainingsdaten. Danach berechnet es bei jeder Anfrage die Ähnlichkeit von neuen Datenpunkten mit gelernten und erzeugt aus demÄhnlichkeitsmaßeine Antwort. Ein Beispiel ist dieNächste-Nachbarn-Klassifikation. Die andere Methode besteht darin, dass das System in der Trainingsphase ein Modell entwickelt und seine Parameter so an die Trainingsdaten anpasst, dass das Modell korrekte Verallgemeinerungen oder Vorhersagen machen kann. Das bezeichnet man als modellbasiertes Lernen.[1]:49-50 Das Erstellen von Datensätzen mit geeigneten Beispieldaten kann mit einem hohen Aufwand verbunden sein. Die Abbildung zumMNIST-Datensatzzeigt als Beispiel einen kleinen Ausschnitt von Beispieldaten, mit denen ML das komplizierte Problem der automatischen Erkennung von handgeschriebenen Ziffern sehr gut lösen kann. Die Beispieldaten müssen in maschinenlesbarer Form vorliegen und Informationen über Beobachtungen oder Erfahrungen enthalten, die für das Lösen des Problems relevant undrepräsentativsind. Eine Lösung für das gegebene Problem kann nur dann korrekt gelernt werden, wenn die Beispieldaten alle relevanten Merkmale korrekt, vollständig und ohneStichprobenverzerrungenerfassen. Die Daten dürfen nur diejenigen Muster aufweisen, die das Modell zur Entscheidung heranziehen soll. Andere Muster müssen entfernt werden. Beispielsweise wurde schon beobachtet, dass einkünstliches neuronales Netz, das darauf trainiert werden sollte, Züge auf Bildern zu erkennen, tatsächlich nur auf Gleise achtete. Der Aufwand dafür, solche Fehler zu erkennen und die Ursachen zu beheben, kann sehr hoch sein.[25]:54 Die meisten maschinellen Lernverfahren benötigen eine große Zahl von Beispieldaten, um ein statistisches Modell zu erzeugen, das die zu lernende Funktion hinreichend genau abbildet. Bei komplizierten Problemen lässt sich die Genauigkeit eher durch größere Datensätze als durch bessere Lernalgorithmen verbessern.[1]:55-59 Während des Trainings erzeugt ein Lernalgorithmus einmathematisches Modellder Trainingsdaten und passt die Modellparameter an die Trainingsdaten an. Nach dem Training kann das so erzeugte Modell neue Daten verarbeiten, um Vorhersagen zu treffen. Generative Modelle können nach dem Training auch neue Daten erzeugen, die den gelernten Daten ähneln, beispielsweise neue Texte, Bilder oder Videos. Es gibt viele Arten von Modellen, die untersucht wurden und in solchen Systemen verwendet werden. Im Folgenden werden einige Modelle, die oft eingesetzt werden, kurz beschrieben. DielineareRegressionist ein statistisches Verfahren, mit dem versucht wird, eine beobachtete abhängige Variable durch eine oder mehrere unabhängige Variablen zu erklären. Bei der linearen Regression wird dabei ein lineares Modell angenommen. Bei der einfachen linearen Regression wird mithilfe zweier Parameter eine Gerade (Regressionsgerade) so durch einePunktwolkegelegt, dass der lineare Zusammenhang zwischenX{\\displaystyle X}undY{\\displaystyle Y}möglichst gut beschrieben wird. Um eine möglichst genaue Vorhersage für die abhängige Variable zu erhalten, wird eineKostenfunktionaufgestellt. Diese Funktion beschreibt diemittlere quadratische Abweichung, die dadurch entsteht, dass die Regressionsgerade die zu erklärende Variable nur approximiert und nicht genau darstellt. Der Lernalgorithmus minimiert die Kostenfunktion. Dielogistische Regressionist eine oft eingesetzte Methode zum Lösen von binären Klassifikationsproblemen. Sie schätzt zunächst, mit welcher Wahrscheinlichkeit ein gegebener Datenpunkt zu einer bestimmten Klasse gehört. Danach entscheidet sie, ob die berechnete Wahrscheinlichkeit größer ist als 50 %. In diesem Fall gibt sie diese Klasse als Ergebnis aus. Andernfalls gibt sie die andere Klasse als Ergebnis aus. Während man bei der linearen Regression die mittlere quadratische Abweichung minimiert, um die optimalen Werte für die Parameter zu erhalten, maximiert man bei der logistischen Regression dieLikelihood-Funktion, um die optimalen Werte der Parameter zu erhalten. Dieses Verfahren wird alsMaximum-Likelihood-Methodebezeichnet. Derk-Means-Algorithmusist ein Verfahren zurVektorquantisierung, das auch zurClusteranalyseverwendet wird. Dabei wird aus einer Menge von ähnlichen Objekten eine vorher bekannte Anzahl vonkGruppen gebildet. Der Algorithmus ist eine der am häufigsten verwendeten Techniken zur Gruppierung von Objekten, da er schnell die Zentren der Cluster findet. Dabei bevorzugt der Algorithmus Gruppen mit geringer Varianz und ähnlicher Größe. In der Regel wird ein approximativer Algorithmus verwendet, der mit zufälligen Mittelwerten aus dem Trainingsdatensatz beginnt und sich danach in mehreren Schritten einer guten Clusteraufteilung annähert. Da die Problemstellung vonkabhängig ist, muss dieser Parameter vom Benutzer festgelegt werden. Eine Support Vector Machine dient als Klassifikator und Regressor. Eine Support Vector Machine unterteilt eine Menge von Objekten so in Klassen, dass um die Klassengrenzen herum ein möglichst breiter Bereich frei von Objekten bleibt; sie ist ein sogenannterLarge Margin Classifier(dt. „Breiter-Rand-Klassifikator“). Jedes Objekt wird durch einen Vektor in einemVektorraumrepräsentiert. Aufgabe der Support Vector Machine ist es, in diesen Raum eineHyperebeneeinzupassen, die als Trennfläche fungiert und die Trainingsobjekte in zwei Klassen teilt. Der Abstand derjenigen Vektoren, die der Hyperebene am nächsten liegen, wird dabei maximiert. Dieser breite, leere Rand soll später dafür sorgen, dass auch Objekte, die nicht genau den Trainingsobjekten entsprechen, möglichst zuverlässig klassifiziert werden. Eine saubere Trennung mit einer Hyperebene ist nur dann möglich ist, wenn die Objektelinear trennbarsind. Diese Bedingung ist für reale Trainingsobjektmengen im Allgemeinen nicht erfüllt. Support Vector Machines überführen beim Training den Vektorraum und damit auch die darin befindlichen Trainingsvektoren in einen höherdimensionalen Raum, um eine nichtlineare Klassengrenze einzuziehen. In einem Raum mit genügend hoher Dimensionsanzahl – im Zweifelsfall unendlich – wird auch die verschachteltste Vektormenge linear trennbar. Die Hochtransformation ist enorm rechenlastig und die Darstellung der Trennfläche im niedrigdimensionalen Raum im Allgemeinen unwahrscheinlich komplex und damit praktisch unbrauchbar. An dieser Stelle setzt der sogenannteKernel-Trickan. Verwendet man zur Beschreibung der Trennfläche geeignete Kernelfunktionen, die im Hochdimensionalen die Hyperebene beschreiben und trotzdem im Niedrigdimensionalen „gutartig“ bleiben, so ist es möglich, die Hin- und Rücktransformation umzusetzen, ohne sie tatsächlich rechnerisch ausführen zu müssen. Beim Lernen von Entscheidungsbäumen wird ein Entscheidungsbaum als Modell verwendet, um Schlussfolgerungen aus den Beobachtungen zu ziehen, die im Trainingsdatensatz enthalten sind. Gelernte Regeln werden durch Knoten und Zweige des Baums repräsentiert und Schlussfolgerungen durch seine Blätter. Ein Modell mit diskreten Ausgabewerten (in der Regel ganzen Zahlen) nennt man Klassifizierungsbaum, dabei repräsentieren die Blattknoten die Klassen und die ZweigeUND-Verknüpfungender Merkmale, die zu der Klasse führen. Ein Modell mit kontinuierlichen Ausgabewerten (in der Regel reellen Zahlen) nennt man Regressionsbaum. Der Algorithmus wählt beim Training diejenige Reihenfolge für die Abfrage der Merkmale, bei der das Modell bei jeder Verzweigung möglichst viel Information erhält. Nach dem Training kann man das Modell auch dazu verwenden, explizit und graphisch die Regeln darzustellen, die zu einer Entscheidung führen.[16]:129–149 Der im Bild dargestellteBinärbaumbenötigt als Eingabe einenVektormit den Merkmalen eines Apfelbaumes. Ein Apfelbaum kann beispielsweise die Merkmalealt,natürliche Sorteundreichhaltiger Bodenbesitzen. Beginnend mit dem Wurzelknoten werden nun die Entscheidungsregeln des Baumes auf den Eingabevektor angewendet. Gelangt man nach einer Folge ausgewerteter Regeln an ein Blatt, erhält man die Antwort auf die ursprüngliche Frage. Ein Random Forest besteht aus mehrerenunkorreliertenEntscheidungsbäumen. Ein Random Forest mittelt über mehrere Entscheidungsbäume, die auf verschiedenen Teilen desselben Trainingsdatensatzes trainiert wurden. Eine große Anzahl unkorrelierter Bäume macht genauere Vorhersagen möglich als ein einzelner Entscheidungsbaum. Dadurch wird in der Regel die Leistung des endgültigen Modells erheblich gesteigert. Künstliche neuronale Netze (KNN) sind Modelle, deren Struktur von biologischen neuronalen Netzen, aus denen Tiergehirne bestehen, inspiriert wurde. Solche Modelle können aus komplexen und scheinbar zusammenhanglosen Informationen lernen. Einige erfolgreiche Anwendungen sindBilderkennungundSpracherkennung. Ein KNN wird von Einheiten oder Knoten gebildet, die miteinander verbunden sind. Die Knoten sindkünstliche Neuronen. Ein künstliches Neuron empfängt Signale von anderen Neuronen und verarbeitet sie mit einerAktivierungsfunktion. Jedem Eingangssignal ist ein Gewicht zugeordnet, das bestimmt, welchen Einfluss das Signal auf die Aktivierungsfunktion hat. Eine einfache Aktivierungsfunktion berechnet die Summe aller gewichteten Eingangssignale und legt sie als Signal auf alle Ausgänge, wenn sie einen bestimmten Schwellenwert überschreitet. Wenn die Summe unter dem Schwellenwert liegt, erzeugt diese Aktivierungsfunktion kein Ausgangssignal. Zu Beginn stehen alle Schwellenwerte und Gewichte auf Zufallswerten. Während des Trainings werden sie an die Trainingsdaten angepasst. In der Regel werden die Neuronen in Schichten zusammengefasst. Die Signale wandern von der ersten Schicht (der Eingabeschicht) zur letzten Schicht (der Ausgabeschicht) und durchlaufen dabei möglicherweise mehrere Zwischenschichten (versteckte Schichten). Jede Schicht kann die Signale an ihren Eingängen unterschiedlich transformieren. Ein KNN mit vielen verborgenen Schichten wird auch als tiefes neuronales Netz bezeichnet. Darauf bezieht sich auch der BegriffDeep Learning.[26] Bekannte Beispiele für Architekturen, die KNN einsetzen, sindrekurrente neuronale Netze(RNN) für die Verarbeitung von Sequenzen,convolutional neural networks(CNN) für die Verarbeitung von Bild- oder Audiodaten undgenerative vortrainierte Transformer(GPT) für Sprachmodelle. Generative Adversarial Networks (GAN) ist die Bezeichnung für eine Klasse von maschinellen Lernverfahren, die KNN im Kontext von generativem Lernen bzw. unüberwachtem Lernen trainieren. Ein GAN besteht aus zwei KNN, einem Generator und einem Diskriminator. Zuerst wird der Diskriminator darauf trainiert, zwischen echten Trainingsdaten und vom Generator aus einer zufälligen Eingabe erzeugten Daten zu unterscheiden. Danach wird der Generator darauf trainiert, aus einer zufälligen Eingabe Daten zu erzeugen, deren Eigenschaften denen der vorher vom Diskriminator gelernten Trainingsdaten so ähnlich sind, dass der Diskriminator sie nicht von ihnen unterscheiden kann.[1]:702-704Mit diesem Verfahren kann beispielsweise ein GAN, das mit Fotografien trainiert wurde, neue Fotografien erzeugen, die für menschliche Betrachter zumindest oberflächlich authentisch aussehen und viele realistische Merkmale aufweisen. Obwohl sie ursprünglich als generatives Modell für unüberwachtes Lernen vorgeschlagen wurden, haben sich GANs auch für teilüberwachtes Lernen, überwachtes Lernen und bestärkendes Lernen als nützlich erwiesen. Im praktischen Einsatz ist das maschinelle Lernen oft ein wesentlicher Bestandteil eines Produktes. Die Auswahl von geeigneten Methoden und Modellen wird dann neben den Eigenschaften der Trainingsdaten auch von denAnforderungenan das Produkt eingeschränkt. Beispielsweise können für Vorhersagen zum Verbraucherverhalten, für lernende autonome Systeme oder für die Optimierung von industriellen Fertigungsketten unterschiedliche Zertifizierungen erforderlich sein.[16]:28–29 Genauigkeitist die wichtigste Anforderung. Wenn die geforderte Genauigkeit nicht erreicht werden kann, weil beispielsweise der Aufwand für die dafür erforderliche Datenerhebung zu groß wäre, dann braucht man weitere Anforderungen nicht mehr zu analysieren.[16]:29 Wenn Transparenz gefordert wird, dann wird erwartet, dass klar ist, wo welche Daten wann verarbeitet und gelöscht werden. Erklärbarkeit liegt vor, wenn die Grundlage, auf der das Modell Entscheidungen trifft, nachvollziehbar ist. Letzteres ist beim Einsatz von Entscheidungsbäumen grundsätzlich möglich, bei tiefen neuronalen Netzen zur Zeit aber nicht.[16]:29–30Neuronale Netze liefern zwar oft gute Ergebnisse, es gibt aber keine verständliche Erklärung dazu, wie diese Ergebnisse entstanden sind. Allerdings stößt man bei komplexen Aufgaben in der Praxis auch dann schnell an Grenzen, wenn eine vollständige Überprüfung grundsätzlich möglich wäre, beispielsweise beim Überprüfen von tiefen Entscheidungsbäumen oder bei dem Versuch, umfangreiche klassische Programme mit vielen Verzweigungen nachzuvollziehen.[25]:55 Zusätzlich zu den gelernten Parametern des mathematischen Modells kann eine gründliche Analyse der Daten, die zum Training und zur Validierung verwendet wurden, Aufschluss darüber geben, welche Eigenschaften die Entscheidungen des Modells am stärksten beeinflussen.[16]:29–30Siehe auchEthik der Künstlichen IntelligenzundExplainable Artificial Intelligence. Bei Ressourcen geht es in erster Linie um die Zeit und die Energie, die für das Training und die Vorhersagen benötigt werden. Bei Echtzeitanwendungen kann das Einhalten einer geforderten Antwortzeit sogar wichtiger sein als die Genauigkeit.[16]:30–31 Es gibt oft eine enge Beziehung zwischen Ressourcenbedarf,DatenschutzundDatensicherheit. Beispielsweise kann man den Datenschutz erhöhen, indem man Datenanonymisiertund das Training auf lokalen Rechnern durchführt und nicht auf externen leistungsstärkeren Servern.[16]:31Siehe auchEthik der Künstlichen Intelligenz. Beispiel: Ein Roboter, der sehen kann, ist grundsätzlich eine mobile Kamera. Um eine permanente Überwachung des Nutzers zu verhindern, sollten neue Bilder nur lokal verarbeitet werden und kurzfristig gelöscht werden.[16]:32–33Siehe auchEthik der Künstlichen Intelligenz. RobustheitundSicherheiteines Systems können bewertet werden, indem man analysiert, mit welcher Wahrscheinlichkeit das System Fehler macht und wie schlimm die Folgen dieser Fehler sind.[16]:33–37 Das Ziel des automatisierten maschinellen Lernens besteht darin, möglichst viele Arbeitsschritte zu automatisieren. Dazu gehören die Auswahl eines geeigneten Modells und die Anpassung seinerHyperparameter.[1]:383 Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Verwandte Fachgebiete 2.1Künstliche Intelligenz 2.2Statistik 2.3Data Science 2.4Data-Mining und Knowledge Discovery in Databases 2.5Mathematische Optimierung 3Methoden 3.1Repräsentation des Wissens 3.1.1Symbolische Ansätze 3.1.2Nicht-symbolische Ansätze 3.2Training 3.2.1Überwachtes Lernen 3.2.2Unüberwachtes Lernen 3.2.3Bestärkendes Lernen 3.3Batch- und Online-Learning 3.4Lernen von Instanzen oder Modellen 4Daten 5Modelle 5.1Lineare Regression 5.2Logistische Regression 5.3k-Means-Algorithmus 5.4Support Vector Machines 5.5Entscheidungsbäume 5.6Random Forests 5.7Künstliche Neuronale Netze 5.8Generative Adversarial Networks 6Anforderungen 6.1Genauigkeit 6.2Transparenz und Erklärbarkeit 6.3Ressourcen"
  },
  {
    "label": 1,
    "text": "Feature Subset Selection – Wikipedia Feature Subset Selection Inhaltsverzeichnis Ansätze Beispiele für Algorithmen Siehe auch Literatur Einzelnachweise Filter-Ansatz Wrapper-Ansatz Embedded-Ansatz Correlation Feature Selection Boruta Relief-Algorithmus Regularisierung Feature Selection mit Annealing DieFeature Subset Selection(FSS), kurzFeature SelectionoderMerkmalsauswahl, ist ein Ansatz aus demmaschinellen Lernen, bei dem nur eineTeilmengeder verfügbarenFeaturesfür maschinelles Lernen verwendet wird. FSS ist notwendig, weil es teilweise technisch unmöglich ist, alle Features mit einzubeziehen oder weil es Differenzierungsprobleme gibt, wenn eine große Anzahl an Features, aber nur eine kleine Zahl anDatensätzenvorhanden ist oder umÜberanpassungdes Modells zu vermeiden, sieheVerzerrung-Varianz-Dilemma. Es gibt drei Hauptansätze zur Feature Selection. Berechne ein Maß zur Unterscheidung von Klassen. Messe das Gewicht der Features und wähle die bestennaus. Auf dieses Feature Subset wird der Lernalgorithmus angewendet. Filter können entwederunivariat(z. B.euklidische Distanz,Chi-Quadrat-Test) oder multivariat (z. B. Korrelationsbasierte Filter) dieintrinsischenEigenschaften der Daten berechnen. Feature selection durch Filtern ist ein spezieller Fall desStrukturlernens, welches z. B. im Kontext vonBayesschem Lernenhäufig Anwendung findet. Durchsuche die Menge aller möglichen Feature-Subsets. Auf jedesSubsetwird der Lernalgorithmus angewendet. Das Durchsuchen kann entweder deterministisch oder randomisiert erfolgen: Deterministische Algorithmen sind z. B.: Randomisierte Algorithmen sind z. B.: Die Suche nach einer optimalen Untermenge ist direkt mit dem Lernalgorithmus verbunden. Gute Untermengen von Features enthalten Features, welche stark mit der Zielvariablen korreliert sind, aber dennoch möglichst unkorreliert untereinander sind.[5]Correlation Feature Selection (CFS) wählt als Filter-Algorithmus die UntermengenSk{\\displaystyle S_{k}}mitk{\\displaystyle k}vielen Features wie folgt aus: wobeircfi{\\displaystyle r_{cf_{i}}}die Korrelationskoeffizienten (z. B.Spearman-KorrelationoderPearson-Korrelation) zwischen Zielvariablec{\\displaystyle c}und Featurefi{\\displaystyle f_{i}}sind undrfifj{\\displaystyle r_{f_{i}f_{j}}}die Korrelationskoeffizienten der Featuresfi{\\displaystyle f_{i}}undfj{\\displaystyle f_{j}}untereinander. Boruta[6]ist einAlgorithmuszur Feature Selection, welcher zunächst weitere zufällige Features einführt und dieFeature Importancejedes Features mit der dieser zufälligen Features vergleicht: Features, welche häufig unwichtiger als diese zufälligen Features waren, werden verworfen. Relief basierte Algorithmen folgen der Filtermethodik und analysieren Unterschiede der Features bei nächsten Nachbarn, welche andern Klassen angehören. Regularisierungmit dem L1-Loss wählt gewisse Features aus, sieheLasso-Regression. Es ist ein Beispiel für den Embedded-Ansatz. Bei der Lasso-Regression (und orthogonalen Merkmalen) kann mithilfe vonSubdifferentialen[7]dieSoft-Threshold-Funktion hergeleitet werden, welche einige Parameter derKleinste-Quadrate-Regression(OLS) direkt auf Null setzt:β^jLasso=β^jOLSmax(0,1−Nλ|β^jOLS|).{\\displaystyle {\\hat {\\beta }}_{j}^{\\text{Lasso}}={\\hat {\\beta }}_{j}^{\\!\\;{\\text{OLS}}}\\max {\\Biggl (}0,1-{\\frac {N\\lambda }{{\\bigl |}{\\hat {\\beta }}{}_{j}^{\\!\\;{\\text{OLS}}}{\\bigr |}}}{\\Biggr )}.} Feature Selection mitAnnealingerlaubt Feature Selection mit gewissen statistischen Garantien[8]. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Ansätze 1.1Filter-Ansatz 1.2Wrapper-Ansatz 1.3Embedded-Ansatz 2Beispiele für Algorithmen 2.1Correlation Feature Selection 2.2Boruta 2.3Relief-Algorithmus 2.4Regularisierung 2.5Feature Selection mit Annealing 3Siehe auch 4Literatur 5Einzelnachweise العربية Català Čeština English Español Français Italiano 日本語 한국어 Bahasa Melayu Русский Українська Tiếng Việt Links bearbeiten Artikel Diskussion Lesen Bearbeiten"
  },
  {
    "label": 1,
    "text": "Multimodale künstliche Intelligenz – Wikipedia Multimodale künstliche Intelligenz Inhaltsverzeichnis Geschichte Anwendungen (Auswahl) Einzelnachweise Multimodale künstliche Intelligenzist eine spezielle Art von Modalität, welche Ähnlichkeit mit der entsprechenden Definition beiMedienwissenschaftenhat. Dort wird bei Multimedialitätmehr als eine Sinnesmodalität zur Interaktion benutzt(Multimodale Interaktion). Bei multimodaler künstlicher Intelligenz wird jedoch anstelle einer traditionellen Verarbeitung / Umwandlung der Eingaben ein durchmaschinelles LernenvortrainiertesBasismodellfür den Verarbeitungsprozess verwendet, welches für zusätzliche Modalitäten erweitert wurde oder durchLerntransfermittelsFine-Tuningnoch weiter angepasst werden kann.[1][2]Auch die Nutzung weicht oft von früherenMedienanwendungenab. Vorläufer warenmonomodale Anwendungen mittels KI-Mustererkennungwie Lesen von handgeschriebenem Text inklusive Zahlen mit standardisiertem Text als Ausgabe oder Sprachassistenten mitSpracherkennungals Eingabe und Antworten in synthetischer Sprache wie beispielsweiseSirivonApple, welcher erstmals 2011 imiPhone4s eingeführt wurde.[3]Auch Übersetzungen eines Texts in den Text einer Fremdsprache gehörten dazu, wobei dafür in den 1990er-Jahren statistische Maschinenübersetzungssoftware eingesetzt wurde.[4] Ab etwa 2010 wurden durchkünstliche neuronale Netzeleistungsfähigere Modelle entwickelt, welche später die Grundlage für multimodale KI-Anwendungen bildeten.[5]Als Basismodelle dienen insbesondere sprachbasierteDeep-Learning-Modelle mit der BezeichnungLarge-Language-Models(LLMs) und bildgenerierende Modelle (DiffusionsmodellewieDALL-EvonOpenAI[6]) die Grundlage. Die spezifische Art eines LLMs mit der BezeichnungGenerativer vortrainierter Transformer(englisch:Generative pre-trained transformer, GPT) wurde erst 2017 vonGoogle-LLC-Mitarbeitern geschaffen.[7]Solche generative, transformerbasierte Systeme können auf Aufgaben ausgerichtet sein, die Modalitäten jenseits von Text umfassen.MicrosoftsVisual ChatGPTzum Beispiel kombiniertChatGPTmit visuellen Grundlagenmodellen, um sowohl Bilder als auch Text als Eingabe oder Ausgabe zu ermöglichen.[8]Darüber hinaus bieten Fortschritte in derText-to-Speech-Technologie leistungsstarke Werkzeuge für die Erstellung von Audioinhalten, wenn sie in Verbindung mit grundlegenden GPT-Sprachmodellen verwendet werden.[9]Multimodale KI-Modelle, welche auf LLMs beruhen, haben bei schrittweiser Herleitung von Resultaten (engl.:Chain-of-Thought (CoT)) Mühe, wenn es sich um quasi-visuelles Erkennen von Mustern handelt. Ein neuer Ansatz wurde von Forschern derUniversity of Cambridgeund vonMicrosoft-Researchentwickelt, welcher bei CoT-Herleitungen von Bilddarstellungen Verbesserungen bringen soll.[10] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Anwendungen (Auswahl) 3Einzelnachweise Español Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Wikidata-Datenobjekt Medienwissenschaften Multimodale Interaktion maschinelles Lernen Basismodell Lerntransfer Fine-Tuning"
  },
  {
    "label": 1,
    "text": "Natürliche Sprache – Wikipedia Natürliche Sprache Inhaltsverzeichnis Rolle des Begriffs in der Allgemeinen Sprachwissenschaft Übergangs- und Zweifelsfälle Computerlinguistik Siehe auch Literatur Weblinks Einzelnachweise Sprachplanung Tote Sprachen Plansprachen Alsnatürliche Sprachebezeichnet man in derSprachwissenschafteine von Menschen gesprocheneSpracheoder eineGebärdensprache, die aus einer ungesteuerten historischen Entwicklung entstanden ist. Eine solche ungesteuerte Entwicklung beruht in der Regel darauf, dass die Sprache im Rahmen des normalen und spontanen frühkindlichen Mutterspracherwerbsweitergegeben wird. Davon unterschieden werdenPlansprachen, bei denen die grammatischen Eigenschaften und der Wortschatz bewusst festgesetzt und beibehalten werden und die ausschließlich durch bewusste Beschäftigung mit diesen Anleitungen erworben werden. Übergänge zwischen natürlichen und Plansprachen sind ebenfalls denkbar, dies wird im Fall desEsperantodiskutiert. In einer anderen Blickrichtung stehen natürliche Sprachen im Gegensatz zuformalen Sprachen, z. B.Programmiersprachenoder Logikformalismen. Diese dienen nicht als allgemeine Kommunikationsmittel und zeigen auch nicht die Einbettung inparasprachlichesVerhalten, das den Gebrauch natürlicher Sprachen zusätzlich kennzeichnet – alsoGestik,MimikundTonfallzurModulationderKommunikation. Formale Sprachen versuchen vor allem, für einen definierten Zweck strukturelle undlexikalischeUnschärfenund Uneindeutigkeiten natürlicher Sprachen zu vermeiden. Wenn Sprache sich ungesteuert entwickelt, dann sind die Eigenschaften ihres grammatischen Baus und Vokabulars durch verschiedene natürliche Rahmenbedingungen geprägt – etwa die Anforderungen, die in Kommunikationssituationen herrschen, oder die speziellen Bedingungen, die im frühkindlichen Mutterspracherwerb herrschen. Letztlich wirkt sich auch die körperliche und mentale Konstitution des Menschen und die genetische Ausstattung aus. Die Zusammenhänge zwischen solchen Rahmenbedingungen und der Entwicklung sprachlicher Formen zu erforschen, sowie möglicherweise hieraus erklärbareSprachuniversalien, ist ein Gegenstand derAllgemeinen Sprachwissenschaft. Die scharfe Abgrenzung gegen Plansprachen, die in der sprachwissenschaftlichen Forschung teilweise geschieht, erklärt sich daraus, dass Plansprachen, eben aufgrund ihrer willkürlichen Schaffung, über die genannten Zusammenhänge keine Aufschlüsse geben. Sie können auch bekannten Entwicklungstendenzen und Sprachuniversalien zuwiderlaufen. Natürliche Sprachen können in begrenzter Weise von Prozessen der Festschreibung und Planung beeinflusst werden, ohne ihren Status als natürliche Sprache zu verlieren. Auf der anderen Seite können geplante Sprachen Eigenschaften entwickeln, mit denen sie in die Kategorie der natürlichen Sprachen hinüberreichen. Natürliche Sprachen werden manchmal zum Gegenstand vonSprachplanung, jedoch erfolgen sprachplanerische Eingriffe in der Regel für begrenzte Zeit auf der Basis bestehender natürlicher Entwicklungen. Sie betreffen dann nur wenige, oft eher äußerliche und soziale Aspekte einer Sprache (z. B. eher die Bevorzugung vonVarietätenals ihre Entstehung) und greifen kaum in die Grundlagen des grammatischen Systems ein. Ein typisches Beispiel ist die Schaffung einerStandardvarietät(etwa für flächendeckenden Schulunterricht) für eine Sprache, die sonst nur in Dialekte zersplittert vorlag (z. B. im Fall desRumantsch Grischun). Einen Grenzfall bilden ferner„tote Sprachen“, also Sprachen, die in einer historisch erstarrten Form weiter tradiert werden, aber nicht mehr durch Kinder in einem ungesteuerten Prozess als Muttersprache erworben werden. Beispiele dafür sindLateinoderSanskrit. Aufgrund der ursprünglich natürlichen Entstehung fallen auch sie noch unter natürliche Sprache. Der einzige Fall einer Plansprache, die eine größere Sprechergemeinschaft über mehrere Generationen hinweg hervorgebracht hat, ist das Esperanto. Die Anfänge des Esperanto im 19. Jahrhundert bestanden in einer erstenGrammatikmit 16 Regeln; der erste Wortschatz aus 920 Wortstämmen im ersten Esperanto-Lehrbuchs von 1887. Bis heute sind Grammatik und Wortschatz stark ausgebaut worden (gezählt werden heute über 15.000 Wortstämme). Vor allem wegen der Weiterentwicklung des Esperanto über mehrere Generationen hinweg und aufgrund der Existenz von zumindest etwa tausend Personen, die mit Esperanto als einer ihrer Muttersprachen aufwuchsen, wird die Frage diskutiert, inwieweit Züge einer natürlichen Sprache hier vorliegen.[1][2][3] Die Computerlinguistik beschäftigt sich mit der automatischen Verarbeitung von natürlicher Sprache. „Natürlichsprachig“ steht hier im Gegensatz zu einer formalsprachlichen Repräsentation. In der Sprachverarbeitung werden verschiedene Methoden und Typen unterschieden. Als „natürlichsprachig“ werdenkünstliche Intelligenz, KI-Systeme oderSprachdialogsysteme(Interactive Voice Response, IVR) bezeichnet, die Äußerungen, die aus ganzen Sätzen bestehen, verarbeiten und mehrereInformationenaus einem einzigen Satz extrahieren. Sprachdialogsysteme sind dabei der klassische Anwendungsfall. Neben sehr simplen Sprachdialogsystemen, die keine Sprache, sondern nur Optionswahlen (etwa Standard-Töne desMehrfrequenzwahlverfahrens, MFV) verstehen, können einige von ihnen auch festgelegte Ausdrücke erkennen (siehe:Speech Recognition Grammar Specification). Ein Verstehen ohne enge Vorgaben wird dagegen auch alsNatural Language Understanding(NLU) bezeichnet. Solche Systeme können aus natürlichsprachlichen Äußerungen verschiedenartige Informationen extrahieren. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Rolle des Begriffs in der Allgemeinen Sprachwissenschaft 2Übergangs- und Zweifelsfälle 2.1Sprachplanung 2.2Tote Sprachen 2.3Plansprachen 3Computerlinguistik 4Siehe auch 5Literatur 6Weblinks 7Einzelnachweise Afrikaans العربية Asturianu Azərbaycanca Bikol Central Български বাংলা Brezhoneg Català کوردی Чӑвашла English Esperanto Español Eesti Euskara فارسی Suomi Français Galego עברית"
  },
  {
    "label": 1,
    "text": "Computerlinguistik – Wikipedia Computerlinguistik Inhaltsverzeichnis Geschichte Funktionsweise Das Saarbrücker Pipelinemodell Beispiele für Probleme der Sprachverarbeitung Anwendungen in der Praxis Siehe auch Literatur Weblinks Einzelnachweise Studiengänge Tagungen Organisationen DieComputerlinguistik(CL) oderlinguistische Datenverarbeitung(LDV) untersucht, wienatürliche Sprachein Form von Text- oder Sprachdaten mit Hilfe des Computersalgorithmischverarbeitet werden kann. „Sie erarbeitet die theoretischen Grundlagen der Darstellung, Erkennung und Erzeugung gesprochener und geschriebener Sprache durch Maschinen“[1]und ist Schnittstelle zwischenSprachwissenschaftundInformatik. In der englischsprachigen Literatur und Informatik ist neben dem Begriffnatural language processing (NLP)auchcomputational linguistics (CL)gebräuchlich. Computerlinguistik lässt sich als Begriff in die 1960er Jahre zurückverfolgen.[2]Mit den Anfängen derkünstlichen Intelligenzauch beiAlan Turingwar die Aufgabenstellung schon nahegelegt.Noam ChomskysSyntactic Structuresvon 1957 präsentierte eine Sprachauffassung, nach der dieSprachein einem formalen Rahmen beschreibbar wurde (Chomsky-Hierarchieder formalen Sprachen). Hinzu kamen die Sprachlogiken vonSaul KripkeundRichard Montague. Die teilweise aus dem US-Verteidigungsbudget sehr hoch geförderten Forschungen brachten jedoch nicht die erhofften Durchbrüche. Besonders Chomsky undJoseph Weizenbaumdämpften die Erwartungen an Automatisierungen von Sprachübersetzung. Der Wende von behavioristischen Wissenschaftskonzeptionen zu mentalistischen (Chomsky) folgten umfassende Konzipierungen in denKognitionswissenschaften. In den siebziger Jahren erschienen zunehmend häufiger Publikationen mit dem BegriffComputerlinguistikim Titel. In Deutschland wurde parallel der BegriffLinguistische Datenverarbeitung (LDV)verwendet.[3][4]Es gab bereits finanziell aufwändige Versuche der Anwendungen (Konkordanzen, Wort- und Formstatistik), aber auch schon größere Projekte zur maschinellen Sprachanalyse und zu Übersetzungen. Die ersten Computerlinguistik-Studiengänge in Deutschland wurden in den 1980er Jahren an derUniversität des Saarlandesund in Stuttgart eingerichtet. Die Computerlinguistik bekam mit der Verbreitung von Arbeitsplatzrechnern (Personal Computer) und mit dem Aufkommen des Internets neue Anwendungsgebiete. Im Gegensatz zu einerInternetlinguistik, die insbesondere menschliches Sprachverhalten und die Sprachformen im und mittels Internet untersucht, entstand in der Computerlinguistik eine stärker informatisch-praktische Ausrichtung. Dennoch gab das Fach die klassischen philosophisch-linguistischen Fragen nicht ganz auf und wird heute in theoretische und praktische Computerlinguistik unterschieden. Das Fach ist eng verbunden mit der Entwicklung derKünstlichen Intelligenz; insbesondere das Aufkommen vongroßen Sprachmodellen, die auf der Verarbeitung riesigerTextkorporabasieren (ab 2017 mit der Vorstellung desTransformers), haben dem Feld neue Impulse und Forschungsrichtungen gegeben.[5] Die Computerlinguistik (CL) verwendet verschiedene Techniken, um gesprochene und geschriebene Sprache zu verarbeiten. Dazu zählen Interpretationen statistischer Daten, Datenmaterial aussozialen Netzwerken, Suchergebnisse sowie Methoden desmaschinellen Lernensund von Regeln durchsetzte algorithmische Datenverarbeitung.[6]Methoden verschiedener Disziplinen wie Informatik,Künstliche Intelligenz, Linguistik undDatenwissenschaftwerden genutzt, um Computern die Verarbeitung natürlicher Sprache zu ermöglichen. Computerlinguistik gliedert sich in die Unterbereiche „Verständnis natürlicher Sprache“ (natural language understanding, NLU) und „Erzeugung natürlicher Sprache“ (natural language generation, NLG).[7]Künstliche Intelligenz wird auch in Übersetzungsprogrammen wie zum BeispielDeepLverwendet, wodurch Sprachbarrieren reduziert werden können.[8]Computerlinguistik nimmt Teil amdigitalen Wandelin Behörden, Unternehmen und Gesellschaft, da Teile von Arbeitsprozessen durchAlgorithmenausgeführt werden. So nutzt zum Beispiel das Software-UnternehmenNvidiaCL.[9]Allerdings gibt es auch Gefahren durch inhaltliche Verzerrungen, die in den verarbeiteten sprachlichen Daten enthalten sind und durch Algorithmen verstärkt werden.[10] Computer verarbeiten Sprache entweder in der Form von akustischer Information oder in der Form von Buchstabenketten (wenn die Sprache in Schriftform vorliegt). Um die Sprache zu analysieren, arbeitet man sich schrittweise von dieser Eingangsrepräsentation in Richtung Bedeutung vor und durchläuft dabei verschiedene sprachliche Repräsentationsebenen. In praktischen Systemen werden diese Schritte typischerweise sequentiell durchgeführt, daher spricht man vom Pipelinemodell,[11]mit folgenden Schritten: Es ist allerdings nicht so, dass sämtliche Verfahren der Computerlinguistik diese komplette Kette durchlaufen. Die zunehmende Verwendung vonmaschinellen Lernverfahrenhat zu der Einsicht geführt, dass auf jeder der Analyseebenen statistische Regelmäßigkeiten existieren, die zur Modellierung sprachlicher Phänomene genutzt werden können. Beispielsweise verwenden viele aktuelle Modelle dermaschinellen ÜbersetzungSyntax nur in eingeschränktem Umfang und Semantik so gut wie gar nicht; stattdessen beschränken sie sich darauf, Korrespondenzmuster auf Wortebene auszunutzen.[12] Am anderen Ende der Skala stehen Verfahren, die nach dem PrinzipSemantics first, syntax secondarbeiten. So baut die auf demMultiNet-Paradigma beruhende, kognitiv orientierte Sprachverarbeitung auf einem semantikbasierten Computerlexikon auf, das auf einem im Wesentlichen sprachunabhängigen semantischen Kern mit sprachspezifischen morphosyntaktischen Ergänzungen beruht.[13]Dieses Lexikon wird beimParsingvon einer Wortklassen-gesteuerten Analyse zur unmittelbaren Erzeugung von semantischen Strukturen eingesetzt. Praktische Computerlinguistikist ein Begriff, der sich im Lehrangebot einiger Universitäten etabliert hat. Solche Ausbildungsgänge sind nahe an konkreten Berufsbildern um die informatisch-technische Wartung und Entwicklung von sprachverarbeitenden Maschinen und ihrerProgramme. Dazu gehören zum Beispiel: Computerlinguistik wird an mehreren Hochschulen im deutschsprachigen Raum als eigenständiger Studiengang angeboten. In der deutschen Hochschulpolitik ist die Computerlinguistik alsKleines Facheingestuft.[15]Es sind Bachelor- wie auch Master-Studienabschlüsse möglich.[16]Zu den bekanntesten Angeboten zählen die Studiengänge der: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Funktionsweise 3Das Saarbrücker Pipelinemodell 4Beispiele für Probleme der Sprachverarbeitung 5Anwendungen in der Praxis 5.1Studiengänge 5.2Tagungen 5.3Organisationen 6Siehe auch 7Literatur 8Weblinks 9Einzelnachweise Afrikaans العربية Azərbaycanca Беларуская Беларуская (тарашкевіца) Български বাংলা Brezhoneg Català Čeština Dansk Ελληνικά English Esperanto Español Eesti Euskara فارسی Suomi"
  },
  {
    "label": 1,
    "text": "Neuronales Netz – Wikipedia Neuronales Netz Inhaltsverzeichnis Die Vernetzung von Neuronen Lernen Forschung Siehe auch Literatur Weblinks Einzelnachweise Alsneuronales Netzwird in denNeurowissenschafteneine beliebige Anzahl miteinander verbundenerNeuronenbezeichnet, die als Teil einesNervensystemseinen auf bestimmte Funktionen ausgerichteten Zusammenhang bilden. Abstrahiert werden inComputational Neurosciencedarunter auch vereinfachte Modelle einer biologischen Vernetzung verstanden. In derInformatik,InformationstechnikundRobotikwerden deren Strukturen alskünstliches neuronales Netzmodelliert und technisch nachgebildet, simuliert und abgewandelt. Das Nervensystem von Menschen und Tieren besteht ausNervenzellen(Neuronen) undGliazellensowie einer Umgebung. Die Neuronen sind überSynapsenmiteinander verknüpft, die als Verknüpfungsstellen oderKnoteneines interneuronalenNetzwerksaufgefasst werden können. Daneben findet zwischen Neuronen und Zellen derNeuroglia, insbesondereOligodendrogliaundAstroglia, in chemischer und elektrischer Form ein Austausch statt, der dieGewichtungvon Signalen verändern kann. Die „Schaltungstechnik“ von Neuronen kennt üblicherweise mehrere Eingänge und einen Ausgang. Wenn die Summe der Eingangssignale einen gewissenSchwellenwertüberschreitet, „feuert“ das Neuron (Erregungsbildung): EinAktionspotentialwird amAxonhügelausgelöst, imInitialsegmentgebildet und entlang desAxonsweitergeleitet (Erregungsleitung). Aktionspotentiale in Serie sind die primären Ausgangssignale von Neuronen. Diese Signale können überSynapsenanderen Zellen vermittelt werden (Erregungsübertragung). Anelektrischen Synapsenwerden diePotentialänderungenin unmittelbarem Kontakt weitergegeben. Anchemischen Synapsenwerden diese in einTransmitterquantumals sekundäres Signal umgesetzt, also durchBotenstoffeübermittelt(Transmission). Kennzeichnend für Nervenzellen sind ihreZellfortsätze, mit denen Kontakte zu einzelnen anderen Zellen hergestellt werden. AlsDendritendienen sie vorrangig der Aufnahme von Signalen anderer Zellen, während Signale an andere Zellen über denNeuritenfortgeleitet werden, auchAxongenannt in der Umhüllung durch Gliazellen. Mit Abzweigungen seines Axons als Axonkollateralekann das Signal eines Neuronsefferentan mehrere andere Neuronen übermittelt werden(Divergenz).Auch können einem NeuronafferentSignale von verschiedenen anderen Neuronen zufließen(Konvergenz),vorwiegend über seine Dendriten als Eingänge. Während das über die Membran fortgeleitete Aktionspotential bei elektrischen Synapsenpromptals elektrisches Signal übergeben wird, wird es bei chemischen Synapsen zunächst an derpräsynaptischenMembranregion eines Neurons in ein sekundäres chemisches Signal umgebildet. Dies geschieht als potentialabhängigeneurokrineSekretiondurch Ausschüttung (Exozytose) der insynaptischen Vesikelnvorrätig gehaltenen Moleküle eines Signalstoffs. Nach Überbrücken des schmalensynaptischen SpaltsperDiffusionwirkt dieser Botenstoff alsNeurotransmitteroder als einneuromodulatorischerKotransmitterauf die Membranregion derpostsynaptischzugeordneten Zelle. Voraussetzung dafür ist, dass diese Postsynapse mit passendenRezeptormolekülenausgestattet und empfänglich ist. Art und Anzahl der hier von der zugeordneten Zelle in ihre Membran eingebauten Rezeptoren haben also entscheidenden Einfluss auf die Signalwirkung. Mit der Rezeptorbindung wird ein Transmitter erkannt und darüber direkt (ionotrop) oder mittelbar (metabotrop) eine vorübergehende regionale Veränderung der Membrandurchlässigkeit veranlasst. Durch die Membran ein- oder ausströmende kleineIonenrufen also wieder postsynaptisch Potentialänderungen hervor, als lokale elektrische Signale. Die eintreffenden Signale laufen auf derMembraneines Neurons zusammen, werden hier räumlich wie zeitlich integriert bzw.summierendzusammengefasst. Solche postsynaptischen Potentiale prägen sich verschieden aus, abhängig von der Membranausstattung mit Rezeptoren undIonenkanälen. Sie können alsgraduierte Potentialenicht nur unterschiedlich starke Signale sein, sondern zudem qualitativ grundsätzlich anders:Exzitatorischeregen die Erregungsbildung an,inhibitorischehemmen die Bildung eines Aktionspotentials. Mit dieser Form synaptischer Verknüpfung als chemischer Transmission erhalten Signale also einVorzeichen. Des Weiteren können sie an der Verknüpfungsstelle prozessabhängig gewichtet werden, verstärkt oder abgeschwächt. Bei einer häufig wiederholten Übertragung in rascher Folge kann es zu länger anhaltenden Veränderungen kommen, die alsLangzeit-Potenzierungdie synaptische Übertragung verstärken. Bei geringer Frequenz können in unterschiedlicher Art Veränderungen auftreten, die zu einer dauerhaften Abschwächung alsLangzeit-Depressionführen. Auf diese Weise kann der Signalübertragungsprozess selber den synaptischen Modus formen beziehungsweise überformen (neuronale Plastizität). Die Vernetzung von Neuronen zeigt damit keine starreVerschaltung, sondern eine vom Vorzustand abhängige Gewichtung derSignalwege, die sich durch wiederholten Gebrauch ändert. Über dasLernenin neuronalen Netzen gibt es verschiedene, inzwischen gut standardisierte Theorien. Die erste neuronale Lernregel wurde 1949 vonDonald O. Hebbbeschrieben (Hebbsche Lernregel). Wesentliche Entwicklungen erfolgten u. a. durch Arbeiten des FinnenTeuvo KohonenMitte der 1980er Jahre. Daraus ergaben sich typische Eigenschaften neuronaler Netze, die gleichermaßen für natürliche wie fürkünstliche „neuronale Systeme“gelten. Dazu gehört die Eigenschaft, dass sie komplexe Muster erlernen können, ohne dass eine Abstraktion über die diesen Mustern eventuell zugrunde liegenden Regeln stattfindet. Das heißt, dass neuronale Netze nicht einemLogiksystem, sondern einer (in gewissem Sinne intuitiven)Musterverarbeitungfolgen. Dies bedeutet weiterhin, dass vor dem Lernennichterst die Regeln entwickelt werden müssen. Andererseits kann aus dem neuronalen Netz auch nicht nachträglich eine eventuelle Logik ermittelt werden, die dessen Lernerfolg ausmachte. Dies bedeutet wiederum nicht, dass logisches Verhalten und präzise Regeln nicht von neuronalen Netzen erlernt bzw. angewendet werden könnten. Nur müssen diese durch Training ggf. mühsam erarbeitet werden; etwa beim Erlernen der Grammatik einer Sprache über Jahre hinweg. Neuronale Netze lernen nichtexplizit,sondernimplizit:Die Grammatik der Muttersprache wird von einem Kleinkind zuerst implizit erlernt. Als Schulkind erlernt es die Regeln dann im Allgemeinen – noch einmal – explizit, im Unterricht. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Die Vernetzung von Neuronen 2Lernen 3Forschung 4Siehe auch 5Literatur 6Weblinks 7Einzelnachweise Afrikaans Alemannisch العربية Azərbaycanca বাংলা Bosanski Català Dansk English Español فارسی Gaeilge עברית हिन्दी Hrvatski Magyar Bahasa Indonesia Italiano Gĩkũyũ 한국어 Latina မြန်မာဘာသာ Nederlands Norsk bokmål"
  },
  {
    "label": 1,
    "text": "Parameter (Künstliche Intelligenz) – Wikipedia Parameter (Künstliche Intelligenz) Inhaltsverzeichnis Abhängigkeit von KI-Modellen Parametermodi Parameterübergabe Hyperparameter Einzelnachweise EinParameter beikünstlicher Intelligenz (KI)und beimaschinellem Lernenist eineKonfigurationsvariable, die imKI-Modellintern vorhanden ist und deren Wert aus Daten durch einenAlgorithmusgeschätzt wird. Die Parameter werden vom Modell benötigt, um Vorhersagen zu treffen. Ihre Werte definieren die Fähigkeit des Modells, eine Aufgabe zu lösen. Sie werden aus Daten gelernt oder geschätzt.[1]Die Zahl der verwendeten Parameter beeinflusst den benötigten Rechenaufwand, die Geschwindigkeit und die Genauigkeit der Resultate.[2] Je nach Art des gewählten KI-Modells werden unterschiedliche Parameter benutzt: Diese Parameter werden während der Trainingsphase des Modellaufbaus gelernt. Der Lernprozess beinhaltet die Verwendung eines Trainingsdatensatzes und eines Optimierungsalgorithmus, der die Parameter anpasst, um eine Verlustfunktion zu minimieren. Diese Verlustfunktion ist ein Maß für den Fehler des Modells bezogen auf die Trainingsdaten. Parameter können verschiedene Modi haben, wie Eingabe, Ausgabe oder Eingabe/Ausgabe, abhängig davon, wie sie in der Unterfunktion verwendet werden. Eingabeparameter werden nur für die Eingabe verwendet, Ausgabeparameter nur für die Ausgabe und Eingabe/Ausgabeparameter sowohl für die Eingabe als auch für die Ausgabe. Der Parametermodus wird normalerweise durch ein Schlüsselwort in der Definition einesUnterprogrammsangegeben, wiein,outoderinout. Der Mechanismus zur Zuordnung vonArgumentenzu Parametern wirdParameterübergabegenannt und hängt von der Auswertungsstrategie der Programmsprache ab. Die gebräuchlichsten Strategien sindCall by ValueundCall by Reference.Call by Valuebedeutet, dass der Wert des Arguments auf den Parameter kopiert wird, währendCall by Referencebedeutet, dass der Parameter auf denselben Speicherort verweist wie das Argument. Parameter unterscheiden sich vonHyperparametern. Während Parameter aus Daten erlernt werden, werden Hyperparameter vor dem Lernprozess festgelegt und dienen dazu, den Lernprozess zu leiten.[9]Beispiele für Hyperparameter sind die Lernrate in einem Optimierungsalgorithmus, die Anzahl der verborgenen Schichten in einem neuronalen Netzwerk oder die Anzahl der Entscheidungsbäume in einemRandom-Forest-Modell. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Abhängigkeit von KI-Modellen 2Parametermodi 3Parameterübergabe 4Hyperparameter 5Einzelnachweise Links hinzufügen Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Wikidata-Datenobjekt künstlicher Intelligenz (KI) maschinellem Lernen Konfigurationsvariable KI-Modell Algorithmus"
  },
  {
    "label": 1,
    "text": "Peter Norvig – Wikipedia Peter Norvig Inhaltsverzeichnis Biographie Schriften Literatur Weblinks Einzelnachweise Peter Norvig(*12. Mai1956[1]) ist ein US-amerikanischer Wissenschaftler im BereichKünstliche Intelligenz, Autor und Programmierer. Derzeit arbeitet Norvig als Director of Research beiGoogle Inc.[2][3] 1985 schloss Norvig ein Studium derInformatikan derUniversität Berkeleymit demPh.D.ab. Danach arbeitete er bei diversenStartupsundSun Microsystems. Von 1998 bis 2001 war er der Leiter der Abteilung für Informatik desAmes Research CenterderNASA. Seit 2001 arbeitet er für Google.[4][2] Seit 2006 ist erFellowderAssociation for Computing Machinery.[3]2013 wurde er zum Mitglied derAmerican Academy of Arts and Sciencesgewählt. Bekannt ist Norvig auch für diePowerpointfassungderGettysburg Address, einer Satire von schlechten Präsentationen.[5]NorvigsErdős-Zahlist zwei.[4] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Biographie 2Schriften 3Literatur 4Weblinks 5Einzelnachweise العربية تۆرکجه বাংলা English Español Euskara فارسی Français Italiano 日本語 한국어 Русский Српски / srpski Svenska ไทย Українська Tiếng Việt Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten"
  },
  {
    "label": 1,
    "text": "Random Forest – Wikipedia Random Forest Inhaltsverzeichnis Geschichte Grundlagen Algorithmus Eigenschaften Ähnlichkeit zum K-Nächster-Nachbar-Algorithmus Weblinks Einzelnachweise Maschinelles Lernen mit Entscheidungsbäumen Bagging Vom Bagging zum Random Forest Vorteile Nachteile Out-of-bag error Feature Importance Permutation Importance Random Forest(deutschZufallswald) oderRandom Decision Forestist ein Verfahren, das beimmaschinellen Lerneneingesetzt wird. Es handelt sich um eineEnsemblemethode, die beiKlassifikations-undRegressionsverfahreneingesetzt wird. Beim Training werden mehrere möglichstunkorrelierteEntscheidungsbäumeerzeugt. Dabei wird jeder Entscheidungsbaum mit einer anderen, zufällig ausgewähltenStichprobeder Trainingsdaten trainiert. Zusätzlich berücksichtigt jeder Baum für die Aufteilung der Objekte aus seiner Stichprobe an jedem Knoten nur eine zufällig gewählteTeilmengealler Merkmale. Anschließend werden alle Bäume zu einem Ensemble, dem Random Forest bzw.Wald (Graphentheorie), kombiniert. Das Ergebnis des Random Forests wird mit Hilfe einerAggregatfunktionaus den Ergebnissen aller Bäume gebildet. Bei Klassifikationsaufgaben entspricht das Ergebnis der Klasse, die die meisten Bäume gewählt haben. Bei Regressionsaufgaben wird das Ergebnis alsMittelwertder Ergebnisse aller Bäume gebildet. Der Einsatz von Random Forests korrigiert Abweichungen, die die Ergebnisse von einzelnen Entscheidungsbäumen aufgrund vonÜberanpassungaufweisen. Random Forestsist außerdem ein vonLeo Breimanund Adele Cutler eingetragenes Warenzeichen für ein gleichnamiges Softwarepaket. Der erste Artikel, der die Eigenschaften von Random Decision Forests untersucht, wurde von Tin Kam Ho im Jahr 1995 veröffentlicht.[1]Ho untersuchte Random Decision Forests, die sie aus mehreren Entscheidungsbäumen bildete. Dabei gehörten alle Bäume zu einer bestimmten Art von Entscheidungsbäumen (Aufteilung an einer schiefenHyperebene). Sie stellte fest, dass diese Random Decision Forests mit zunehmendem Wachstum weiter an Genauigkeit gewinnen können, ohne dass es dabei zu einer Überanpassung kommt, falls jeder einzelne Entscheidungsbaum so eingeschränkt wird, dass er für die Aufteilung an jedem Knoten nur eine zufällig gewählte Teilmenge aller Merkmale berücksichtigt. Dieses Verfahren nannte sie Random Subspace Methode. In einer späteren Arbeit zeigte Ho, dass auch Random Decision Forests aus anderen Arten von Entscheidungsbäumen bessere Vorhersagen liefern als einzelne Entscheidungsbäume.[2] Die Grundlagen für das heutzutage als Random Forest bekannte Verfahren wurden im Jahr 2001 vonLeo Breimanveröffentlicht.[3]Der Artikel beschreibt ein Verfahren, das unkorrelierte Bäume erzeugt. Dabei optimiert Breiman denCART-Algorithmusmit der Random Subspace Methode und er erzeugt mitBaggingfür jeden Baum eine andere Stichprobe des Trainingsdatensatzes. Außerdem beschreibt er, wie man den Vorhersagefehler für neue Daten (englischgeneralization error) abschätzen kann und wie man die Bedeutung, die einzelneMerkmalefür das Ergebnis haben, messen kann. Ein Entscheidungsbaum besteht aus Knoten und Blättern. Dabei repräsentiert jeder Knoten eine logischeRegelund jedes Blatt eine Antwort auf das Entscheidungsproblem. Beim maschinellen Lernen werden die Regeln aus einem Datensatz mit Trainingsdaten gelernt. Für jeden Knoten wird eine Regel gelernt, die bestimmt, wie die Objekte aus den Trainingsdaten auf die beiden Folgeknoten aufgeteilt werden. Der CART-Algorithmus ist ein bekannter Algorithmus, der einenBinärbaumerzeugt, der zur Lösung von Aufgaben zur Klassifikation oder Regression eingesetzt werden kann. Das Konzept der Entscheidungsbäume ist einfach und mächtig. Der Trainingsaufwand ist gering. Es ist unempfindlich gegenüberAusreißernund irrelevanten Merkmalen und funktioniert deshalb gut mit Daten, die nicht aufwendig aufbereitet wurden. Deshalb werden Entscheidungsbäume gerne beimData-Miningeingesetzt. Allerdings sind solche Bäume nur selten genau. Insbesondere tendieren sehr tiefe Bäume dazu, fehlerhafte Muster zu erlernen. Sie passen sich ihren Trainingsmengen zu sehr an und zeigen dann eine geringe Verzerrung und eine sehr hohe Varianz.[4]Siehe auchVerzerrung-Varianz-Dilemma. Ein Random Forest mittelt über mehrere tiefe Entscheidungsbäume, die auf verschiedenen Teilen desselben Trainingssatzes trainiert wurden. Dadurch wird die Varianz verringert und in der Regel die Leistung des endgültigen Modells erheblich gesteigert. Die Nachteile bestehen in einer geringen Erhöhung der Verzerrung und einem komplexeren Modell. Der Trainingsalgorithmus von Random Forests verwendet das VerfahrenBootstrap aggregating, auch Bagging genannt. Zunächst werden aus dem Originaldatensatz, der für das Training dient, mit demBootstrapping-VerfahrenBneue Stichproben des Umfangesn{\\displaystyle n}durchZiehen mit Zurücklegenerzeugt. Mit den neuen Stichproben werdenB{\\displaystyle B}Vorhersagemodellemi{\\displaystyle m_{i}}(i=1,…,B{\\displaystyle i=1,\\dots ,B}) trainiert. Für einen Wertx{\\displaystyle x}ergeben sich dannB{\\displaystyle B}Vorhersagewertemi(x){\\displaystyle m_{i}(x)}. Nach dem Training können Vorhersagen für einen neuen Wertx′{\\displaystyle x'}aus dem Durchschnitt der Vorhersagen der einzelnen Bäume gebildet werden. oder, bei Klassifikatoren, durch die Mehrheitsentscheidung aller Bäume. Diese Methode verbessert das Ergebnis, weil sie die Varianz verringert. Die Vorhersagen einzelner Bäume sind viel anfälliger für Rauschen in den Trainingsdaten als der Durchschnittswert von Bäumen, die nicht miteinander korrelieren. Das Bootstrapping-Verfahren ermöglicht es durch die Erzeugung unterschiedlicher Trainingsdatensätze, Bäume zu erzeugen, die nicht miteinander korrelieren. Außerdem kann man die Größe des Vorhersagefehlers (sieheKonfidenzintervall) an der Stellex′{\\displaystyle x'}mithilfe der Standardabweichungen der einzelnen Bäume einfach abschätzen: Die AnzahlB{\\displaystyle B}der Bäume kann frei gewählt werden. Ein typischer Wert liegt bei mehreren hundert oder tausend Bäumen. Ein optimaler Wert kann mitKreuzvalidierungsverfahrenbestimmt werden oder durch die Beobachtung desout-of-bag errors. Oben wird das ursprüngliche Bagging-Verfahren beschrieben, das mehrere zufällige Stichproben zum Trainieren von Entscheidungsbäumen zieht. Die Erfinder des Random-Forest-Verfahrens setzen dieselbe Idee auch zum Trainieren der einzelnen Bäume ein. Der Trainingsalgorithmus wurde so angepasst, dass bei jeder Aufteilung nur eine zufällig gewählte Teilmenge aller Merkmale berücksichtigt wird. Dieses Verfahren wird Random Subspace Methode[2]oder Feature Bagging genannt. Es verhindert eine mögliche Korrelation der Bäume für bestimmte Stichproben: wenn ein oder mehrere Merkmale sehr großen Einfluss für die Vorhersage haben, dann wird das ohne die Random Subspace Methode dazu führen, dass diese Merkmale in vielen Bäumen ähnlich ausgewertet werden und die Bäume korrelieren. Eine Analyse dazu, wie Bagging und die Random Subspace Methode die Qualität der Vorhersage verbessern, hat Ho veröffentlicht.[5] Jeder Entscheidungsbaum im Random Forest wird mit folgendemAlgorithmuserzeugt:[4]:588 Die so erzeugten Bäume bilden zusammen den Random Forest. Da jeder Baum des Random-Forest unabhängig von den anderen Bäumen aufgebaut wird, kann die Antwort der einzelnen Bäume unterschiedlich ausfallen. Um zu einer Gesamt-Vorhersage zu gelangen, wird über die Vorhersage der einzelnen Bäume aggregiert: Man kann das Training eines Random Forest durch vieleKonfigurationsparameterbeeinflussen. Dazu zählt unter anderem, welche und wie vieleEntscheidungsbäumeaufgebaut werden und ob eine maximale Tiefe der Bäume vorgegeben wird. Die Erfinder des Random-Forest-Verfahrens empfehlen als typische Werte für ein Klassifikationsproblem mitM{\\displaystyle M}Merkmalenm=M{\\displaystyle m={\\sqrt {M}}}(abgerundet) Merkmale und eine minimale Knotengröße von 1. Für Regressionsprobleme empfehlen sie m=M/3{\\displaystyle M/3}(abgerundet) Merkmale und eine minimale Knotengröße von 5. In der praktischen Anwendung hängen die optimalen Werte vom Problem ab und sollten an das gegebene Problem angepasst werden.[4]:592 Bosch et al.[6]speichern zusätzlich in jedem Blatt dieA-posteriori-Wahrscheinlichkeitender Klassen, mit denen sie das Blatt finden. Diese Wahrscheinlichkeiten werden anschließend bei der Klassifikation berücksichtigt. Dadurch kann die Fehlerrate des Klassifikators verringert werden. Eine große AnzahlunkorrelierterBäumemacht genauere Vorhersagen möglich als ein einzelner Entscheidungsbaum. Dies liegt daran, dass durch Aggregierung unkorrelierter Ergebnisse die Streuung des aggregierten Wertes sinkt (vergleicheStandardfehlerdes Mittelwertes). Da die einzelnen Bäume unabhängig wachsen (da sie jeweils das beste Merkmal unter einer zufälligen Teilmenge von Merkmalen als Split benutzen), sind ihre Vorhersagen per Konstruktion nicht perfekt korreliert. Ein Random Forest hat bei Klassifikations- und Regressionsproblemen folgende wesentlichen Vorteile gegenüber anderen Klassifikationsmethoden:[7] Ein Random Forest hat bei Klassifikations- und Regressionsproblemen folgende wesentlichen Nachteile gegenüber anderen Klassifikationsmethoden:[7] Out-of-bag (OOB) error, auch out-of-bag estimate, ist eine Methode, um den Vorhersagefehler von Random Forests zu messen. Als Folge des Baggings enthält die Bootstrap-Stichprobe, mit der ein einzelner Baum trainiert wird, nicht alle Datenpunkte aus dem Trainingsdatensatz. Zur Berechnung des out-of-bag-errors bestimmt man den gemittelten Vorhersagefehler für alle Trainingsdatenpunktexi{\\displaystyle x_{i}}, wobei man nur die Bäume berücksichtigt, diexi{\\displaystyle x_{i}}nicht in ihrer Bootstrap-Stichprobe enthalten. Mit Hilfe von Random Forests kann die Bedeutung von Merkmalen für Klassifikationsaufgaben und Regressionsaufgaben einfach bestimmt werden. Die folgende Methode ist in Breimans Artikel[3]beschrieben und in demRSoftwarepaketrandomForestumgesetzt.[8] Im ersten Schritt wird ein Random Forest mit einem DatensatzDn={(Xi,Yi)}i=1n{\\displaystyle {\\mathcal {D}}_{n}=\\{(X_{i},Y_{i})\\}_{i=1}^{n}}trainiert. Danach werden für jeden Datenpunktxi{\\displaystyle x_{i}}alle out-of-bag errors aufgezeichnet und über den Forest gemittelt (falls beim Training kein Bagging benutzt wurde, können ersatzweise Fehler eines unabhängigen Testdatensatzes benutzt werden). Um die Bedeutung desj{\\displaystyle j}-ten Merkmals zu bestimmen, werden die Werte desj{\\displaystyle j}-ten Merkmals in den out-of-bag Datenpunktenpermutiertund der gemittelte out-of-bag errors wird noch einmal für diesen gestörten Datenpunkt berechnet. Der Wert für die Bedeutung für dasj{\\displaystyle j}-te Merkmal wird berechnet, indem man die Differenz zwischen dem out-of-bag error vor und nach der Permutation über alle Bäume mittelt. Der Wert wird durch die Standardabweichung dieser Differenzen normalisiert. Merkmale mit hohen Werten haben eine größere Bedeutung für die Aufteilung als Merkmale mit kleineren Werten. Es gibt Ähnlichkeiten zwischen Random Forests und demK-Nearest-Neighbor-Algorithmus.[9]Beide gewichten bei ihren Vorhersagen für neue Datenpunkte diejenigen Datenpunkte aus dem Trainingsdatensatz höher, die nahe an den neuen Datenpunkten liegen. Der K-Nächster-Nachbar-Algorithmus bildet die Nähe durch eine gewichtete Abstandsfunktion ab, die näheren Punkten ein höheres Gewicht gibt als weiter entfernten. Beim Random Forest wirkt sich das Mitteln über viele unkorrellierte Bäume ähnlich aus. Gegeben seienn{\\displaystyle n}unabhängige und identisch verteilte Werte(xi,yi){\\displaystyle (x_{i},y_{i})}eines zufälligen EingabevektorsX:=(X(1),…,X(d))∈Rd{\\displaystyle X:=(X^{(1)},\\ldots ,X^{(d)})\\in R^{d}}und einer zufälligen AntwortvariablenX∈R{\\displaystyle X\\in R}. Will man dieRegressionsfunktiong(x):=E⁡(Y∣X=x){\\displaystyle g(x):=\\operatorname {E} (Y\\mid X=x)}schätzen, dann kann man für jeden Punktx0∈Rd{\\displaystyle x_{0}\\in R^{d}}eineSchätzfunktiong^(x0){\\displaystyle {\\hat {g}}(x_{0})}vong(x0){\\displaystyle g(x_{0})}definieren, die auf den Funktionswerten(xi,yi){\\displaystyle (x_{i},y_{i})}basiert. Diemittlere quadratische Abweichungbeix0{\\displaystyle x_{0}}ist Bei einer gegebenenMetrikschätzt derK-Nearest-Neighbor-Algorithmusden Wertg(x0){\\displaystyle g(x_{0})}, indem er diek{\\displaystyle k}Punkte betrachtet, diex0{\\displaystyle x_{0}}am nächsten sind: Dabei ist das Gewichtwi{\\displaystyle w_{i}}gleich1k{\\displaystyle {\\tfrac {1}{k}}}für diek{\\displaystyle k}nächsten Nachbarn vonx0{\\displaystyle x_{0}}und gleich 0 für alle anderen Punkte. Beim Random Forest betrachtet man dieSchätzfunktiong^{\\displaystyle {\\hat {g}}}vong{\\displaystyle g}für das RegressionsproblemY=g(X)+ϵ{\\displaystyle Y=g(X)+\\epsilon }mitE⁡(ϵ)=0{\\displaystyle \\operatorname {E} (\\epsilon )=0}undVar⁡(ϵ)=σ2{\\displaystyle \\operatorname {Var} (\\epsilon )=\\sigma ^{2}}auf der Grundlage einerZufallsstichprobe(x1,y1),(x2,y2),…,(xn,yn){\\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\\ldots ,(x_{n},y_{n})}und nimmt an, dass dieZufallsvariableX{\\displaystyle X}eineWahrscheinlichkeitsverteilungin[0,1]d{\\displaystyle [0,1]^{d}}hat. Istg^{\\displaystyle {\\hat {g}}}die Schätzfunktion eines nicht adaptiven Random Forest, dessen Endknoten die Größek{\\displaystyle k}haben, dann gibt es eine KonstanteΛ3>0{\\displaystyle \\Lambda _{3}>0}, sodass für allen{\\displaystyle n}und alle Funktionswertex0∈[0,1]d{\\displaystyle x_{0}\\in [0,1]^{d}}folgende Abschätzung für diemittlere quadratische Abweichunggilt: Das bedeutet, dassk−1⋅(log⁡(n))−(d−1){\\displaystyle k^{-1}\\cdot (\\log(n))^{-(d-1)}}eineuntere SchrankederKonvergenzgeschwindigkeitdermittleren quadratischen Abweichungvon nicht adaptiven Random Forests ist. Andererseits ist bekannt, dass die optimale Konvergenzgeschwindigkeit des mittleren quadratischen Fehlers bei Regressionsproblemen gleichn−2⋅m2⋅m+d{\\displaystyle n^{-{\\tfrac {2\\cdot m}{2\\cdot m+d}}}}ist. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Grundlagen 2.1Maschinelles Lernen mit Entscheidungsbäumen 2.2Bagging 2.3Vom Bagging zum Random Forest 3Algorithmus 4Eigenschaften 4.1Vorteile 4.2Nachteile 4.3Out-of-bag error 4.4Feature Importance 4.4.1Permutation Importance 5Ähnlichkeit zum K-Nächster-Nachbar-Algorithmus 6Weblinks 7Einzelnachweise العربية Беларуская Català کوردی Čeština English Español Eesti فارسی Français Galego עברית Bahasa Indonesia Italiano 日本語 한국어"
  },
  {
    "label": 1,
    "text": "Bestärkendes Lernen – Wikipedia Bestärkendes Lernen Inhaltsverzeichnis Grundlagen Lernverfahren Literatur Weblinks Einzelnachweise Beschreibung der Umgebung Total Discounted Reward Kriterium Erkundung der Umgebung Wesentliche Fähigkeiten Modellfrei Modellbasiert Wertbasiert Strategiebasiert Monte-Carlo-Methoden Temporal Difference Learning Beispiel REINFORCE Bestärkendes Lernenoderverstärkendes Lernen(englischreinforcement learning, RL) steht für einen Lernstil desmaschinellen Lernens. Dabei führt einKI-Agentselbständig Aktionen in einer dynamischen Umgebung aus und erlernt durchVersuch und Irrtumeine Strategie (englischpolicy), die die Summe der erhaltenen Belohnungen (englischrewards) maximiert.[1] Der Begriff ist der Psychologie entlehnt und wurde bereits seit den Anfängen derKybernetikverwendet. So benutzte schonMarvin Minskyden Begriff in seiner Dissertation von 1954.[2]Die Modelle des bestärkenden Lernens versuchen, das Lernverhalten in der Natur nachzubilden. Die Umgebung wird in der Regel alsMarkov-Entscheidungsproblem(MDP) beschrieben. Eine klassische Methode für das Lösen eines MDPs ist diedynamische Programmierung. Dazu muss ein genauesmathematisches Modellfür das Problem bekannt sein. Außerdem ist die Zahl der Zustände, die effizient verarbeitet werden können, begrenzt. Der wesentliche Unterschied zwischen klassischen Methoden und denen des bestärkenden Lernens besteht darin, dass die Methoden des bestärkenden Lernens kein Modell für das Markov-Entscheidungsproblem voraussetzen und sie auch auf MDPs mit vielen Zuständen effizient angewendet werden können. Diese Methoden müssen einen Kompromiss finden zwischen dem Erkunden (englischexploration) von noch unbekannten Zuständen und dem Ausnutzen (englischexploitation) von erlerntem Wissen, mit dem der Agent die Summe der erhaltenen Belohnungen maximiert. Dabei können Belohnungen auch verzögert eintreffen. Eine Aktion, auf die zunächst keine hohe Belohnung erfolgt, kann zu einem Zustand führen, von dem aus mit weiteren Aktionen eine hohe Belohnung erreicht werden kann.[1] Beim bestärkenden Lernen wird dieTheorie der optimalen Steuerungangewendet. Ein einfacher Ansatz ist dasQ-Lernen. Dabei werden Erfahrungswerte zu Zuständen und Aktionen direkt in Tabellen gespeichert. Es wird kein Modell von der Umgebung erstellt. Q-Lernen funktioniert gut bei Problemstellungen, die nur wenige Zustände und Aktionen enthalten, so dass der Agent beim Lernen mit Sicherheit jeden Zustand mehrfach erreichen und darin Aktionen ausführen kann. Andere Ansätze erstellen beim Lernen ein Modell der Umgebung.[3] Ein Spezialfall ist die Verwendung eines Bewertungsmodells, welches durch menschliche Interaktion mit überwachtem Lernen vorprogrammiert wird und die Interaktion mit der Umgebung ergänzt. In diesem Fall erfolgtbestärkendes Lernen durch menschlich beeinflusste Rückkopplung(englischreinforcement learning through human feedback, (RLHF)).[4] Die mathematischen Grundlagen des bestärkenden Lernens bilden die folgenden fünf Begriffe: DerAgent(englischagent), die Umwelt (englischenvironment), die Zustände (englischstates), die Aktionen (englischactions) und die Belohnungen (englischrewards). Die Methoden des bestärkenden Lernens betrachten die Interaktion des lernenden Agenten mit seiner Umgebung. Einfache Beispiele sind einSaugroboter, dessen Belohnung in der Staubmenge besteht, die er in einer bestimmten Zeit aufsaugt oder ein beweglicherRoboter, der in einem Labyrinth steht und mit möglichst wenigen Schritten zu einem bestimmten Feld gehen soll. Die Umgebung wird in der Regel alsMarkow-Entscheidungsproblem(englischmarkov decision process, MDP) formuliert. Die Interaktion des Agenten mit der Umgebung findet zu diskreten Zeitpunktent∈N0{\\displaystyle t\\in \\mathbb {N} _{0}}statt. Zu jedem Zeitpunkt befindet sich der Agent in einem Zustand, wählt eine Aktion aus und erhält dafür einereellwertigeBelohnung. Das Markow-Entscheidungsproblem ist ein Tupel(S,A,T,r,p0){\\displaystyle (S,A,T,r,p_{0})}, wobei Eine Policyπ{\\displaystyle \\pi }ist eine Kollektion vonWahrscheinlichkeitsmaßen(πt(⋅∣s))s∈S{\\displaystyle (\\pi _{t}(\\cdot \\mid s))_{s\\in {\\mathcal {S}}}}aufA{\\displaystyle {\\mathcal {A}}}.πt(a∣s){\\displaystyle \\pi _{t}(a\\mid s)}gibt dabei die Präferenz des Agenten an, zum Zeitpunktt{\\displaystyle t}die Aktiona{\\displaystyle a}zu wählen, wenn er sich in Zustands{\\displaystyle s}befindet. In Zufallsvariablen gesprochen bedeutet diesAt∼πt(⋅∣St){\\displaystyle A_{t}\\sim \\pi _{t}(\\cdot \\mid S_{t})}. Man kann die Qualität einer Policyπ{\\displaystyle \\pi }bestimmen, indem man den Gewinn, den man mit ihr erzielt, mit dem Gewinn vergleicht, den man mit einer optimalen Policyπ∗{\\displaystyle \\pi ^{*}}erzielen kann. Um annähernd optimal zu handeln, muss der Agent die langfristigen Folgen seiner Handlungen berücksichtigen, auch wenn die damit verbundene unmittelbare Belohnung negativ sein könnte. Ziel des Agenten ist es, den insgesamt erwarteten Gewinn (englischtotal discounted reward) zu maximieren. Dieser Gewinn wird auch kumulierter Reward genannt. Er wird in der Regel als Summe aller Belohnungenr{\\displaystyle r}über unendlich viele Zustandsübergänge berechnet: Dabei istrt+i{\\displaystyle r_{t+i}}die Belohnung, die der Agent wahrscheinlich im Zeitschrittt+1{\\displaystyle t+1}erhält. DerDiskontierungsfaktorγ{\\displaystyle \\gamma }gewichtet Belohnungen, die kurzfristig erfolgen, höher als solche, die später erfolgen. Er sorgt auch dafür, dass die Summe für kontinuierliche Probleme (unendlich viele Zustandsübergänge) gegen einen Grenzwert konvergiert. Fürγ=0{\\displaystyle \\gamma =0}zählt nur die direkte Belohnung einer Aktion, alle zukünftigen Belohnungen werden ignoriert. Fürγ→1{\\displaystyle \\gamma \\rightarrow 1}erhalten zukünftige Belohnungen immer mehr Gewicht.[5]:487–491[6]:17Typische Werte fürγ{\\displaystyle \\gamma }liegen zwischen 0,95 und 0,99.[7]:738 Wenn alle Elemente eines MDP vollständig bekannt sind und er nicht zu viele Zustände enthält, kann die optimale Policy direkt mit dynamischer Programmierung berechnet werden, siehe auchMarkow-Entscheidungsproblem#Algorithmen. Bei vielen Aufgaben, die mit bestärkendem Lernen gelöst werden sollen, ist das AktionsmodellT{\\displaystyle T}nicht bekannt. Bei diesen Aufgaben spielt die autonome Erkundung der Umgebung eine wichtige Rolle. Der Agent kann selbstständig eineErkundungs-Policyausführen, um durch Versuch und Irrtum entweder das Aktionsmodell oder, statt einem Modell, direkt eine optimale Policy zu erlernen. In einigen Aufgabenstellungen kann der Agent allerdings nur einen Teil der Zustände beobachten oder die Beobachtungen können ungenau sein. Formal muss das Problem dann als teilweise beobachtbares Markow-Entscheidungsproblem (englischpartially observable Markov decision process, (POMDP)) beschrieben werden. In beiden Fällen kann es auch Einschränkungen geben für die Aktionen, die der Agent ausführen kann. Zur Erkundung ist ein rein zufälliges Vorgehen nicht effizient. Der Agent soll sinnvolle Ansätze verfolgen und dabei bereits erworbenes Wissen ausnutzen (englischexploitation). Er soll sich aber nicht zu früh festlegen und weiter nach neuen, noch besseren Aktionsmöglichkeiten suchen (englischexploration). Eine prominente Erkundungs-Policy ist dieε-greedy policy. Hierbei ist der Agent entweder gierig (englischgreedy) und wählt die aus seiner Sicht erfolgversprechendste Aktion (gemäß seinem bereits erworbenen Wissen) oder er wählt eine zufällige Aktion. Der Parameterεmit Werten zwischen 0 und 1 gibt die Wahrscheinlichkeit an, mit der er eine zufällige Aktion wählt.[5]:494,495 Der Erfolg von bestärkendem Lernen beim Lösen von Aufgaben in komplexen Umgebungen beruht im Wesentlichen auf zwei Fähigkeiten. Erstens kann der Agent seine Umwelt erforschen und mit Hilfe der Rückmeldungen seine Policy verbessern. Zweitens kann er in Umgebungen, in denen eine direkte Berechnung der optimalen Policy nicht effizient möglich ist, die zugehörige Funktionapproximieren. Dadurch eignet sich das bestärkende Lernen insbesondere für das Lösen von Aufgaben, bei denen: Das erste Problem ist ein „echtes“ Lernproblem. Das zweite Problem ist eigentlich ein Planungsproblem, denn das Modell der Umwelt ist vorab bekannt. Zum Erlernen der Strategie des Agenten gibt es verschiedene Algorithmen. Sie lassen sich grob einteilen in modellbasiert und modellfrei. Modellbasierte Methoden lernen das AktionsmodellT{\\displaystyle T}und die Belohnungsfunktionr{\\displaystyle r}und berechnen daraus die optimale Strategie. Modellfreie Methoden lernen für jeden Zustand die optimale Aktion. Der Agent kennt nur die optimalen Aktionen. Er kann nicht vorhersagen, zu welchen Folgezuständen die Aktionen führen.[5]:492 Die am häufigsten genutzten modellfreien Ansätze sind wertbasiert oder strategiebasiert. Die Mischform wird meist alsActor-Criticbezeichnet.[8] Wertbasierte Methoden bestimmen für jeden Zustand und jede Aktion, die darin ausgeführt werden kann, den kumulierten Reward. Dieser wird als Summe der direkten Belohnung auf die Aktion und allen zukünftig zu erwartenden Belohnungen berechnet. Der Agent lernt eine Nutzenfunktion, die den kumulierten Reward maximiert. Bei kleinen Zustands- oder Aktionsräumen können alle Werte in einer Tabelle gespeichert werden, deren Felder anhand der erhaltenen Belohnungen aktualisiert werden. Bei großen Zustandsräumen muss die Nutzenfunktion jedoch approximiert werden. Dazu eignet sich beispielsweise dieFourierreiheoder auch einNeuronales Netz. Bekannte Beispiele sindMonte-Carlo-MethodenundTemporal Difference Learning. Die Grundidee der Monte-Carlo-Methoden besteht darin, den Wert einer bestimmten Aktion in einem bestimmten Zustand dadurch abzuschätzen, dass man eine hinreichend große Menge von zufällig gewählten Episoden ausführt, die den Zustand besuchen und die Aktion ausführen und denMittelwertder erhaltenen Belohnungen bildet. Der Mittelwert berücksichtigt für jede Episode die Summe aller Belohnungen, die nach der Aktion erhalten wurden. Der Begriff „Monte Carlo“ steht allgemein für jede Methode, die eine Zufallsstichprobe beinhaltet. Im hier gegebenen Kontext ist das wesentliche Merkmal von Monte-Carlo-Methoden, dass sie die Aktualisierungen jeweils nach einer abgeschlossenen Episode durchführen. Sie können nur auf episodische Aufgabenstellungen angewendet werden. Sie warten das Ergebnis einer vollständigen Episode ab und aktualisieren danach die Mittelwerte für die ausgeführten Aktionen. Die Ergebnisse sind vom weiteren Verlauf der Episode abhängig. Ein ungünstiger weiterer Verlauf in einer Episode senkt das Ergebnis und damit auch den Schätzwert für eine Aktion. Deshalb können Monte-Carlo-Methoden eine suboptimale Lösung berechnen, wenn keine geeigneten Gegenmaßnahmen ergriffen werden.[6]:54–56 Temporal Difference Learning passt den systematischen Ansatz desQ-Wert-Iterationsalgorithmus, der die optimale Strategie für ein vollständig bekanntes Markow-Entscheidungsproblem berechnen kann, an Probleme an, bei denen das Aktionsmodell und die Belohnungfunktion nicht bekannt sind. Die Methoden erkunden die Umgebung und verwenden in jedem Schritt direkt die Belohnung, die die Umgebung zur ausgeführten Aktion zurückmeldet. Dabei kombinieren sie die direkte Belohnung mit Schätzungen zum optimalen zukünftigen Verlauf. Den so erhaltenen Wert verwenden sie, um die Schätzung für den Wert der Aktion zu aktualisieren. Gegenüber Monte-Carlo-Methoden hat dieses Vorgehen entscheidende Vorteile: Die Schätzung ist unabhängig vom weiteren Verlauf der Episode, sie benötigt weniger Zeit und sie ist auch bei Aufgabenstellungen möglich, die unendlich weitergeführt werden können. Außerdem wurde die Konvergenz zur optimalen Wertfunktion bewiesen. Eine sehr verbreitete Variante istQ-Lernen. Sollen mehrere Agenten kooperieren und mit Q-Lernen eine optimale Strategie dafür lernen, kann (bislang) nur in trivialen Fällen die Konvergenz der Lernvorgänge garantiert werden. Trotzdem kann unter Zuhilfenahme von Heuristiken oft ein in der Praxis nützliches Verhalten gelernt werden, da der worst case selten auftritt.[9] Strategiebasierte Methoden versuchen, die zu erwartende kumulative Belohnung direkt durch Parametrisierung der Strategie zu maximieren. Meistens erfolgt diese Maximierung durch stochastischgradientbasierte Optimierung(englischpolicygradient). Prominente Vertreter dieser Klasse sindREINFORCE,Trust Region Policy Optimization(TRPO) undProximal Policy Optimization(PPO). Der einfach herzuleitende Algorithmus REINFORCE[10]schätzt den Gradienten des zu erwartenden Gewinns ∇θEτ∼pθ[R0]{\\displaystyle \\nabla _{\\theta }\\mathbf {E} _{\\tau \\sim p_{\\theta }}[R_{0}]}, um damit seine Parameter über empirisch gewinnbare Spielabläufe zu aktualisieren. Hierbei muss die Strategieπθ(a|s){\\displaystyle \\pi _{\\theta }(a|s)}nachθ{\\displaystyle \\theta }differenzierbar sein undτ=(s0,a0,s1,a1,…,sT,aT){\\displaystyle \\tau =(s_{0},a_{0},s_{1},a_{1},\\dots ,s_{T},a_{T})}stellt einen Spielablauf dar, der aus der Wahrscheinlichkeitsverteilungpθ{\\displaystyle p_{\\theta }}entnommen wird. Diese setzt sich einerseits aus der Strategieπθ{\\displaystyle \\pi _{\\theta }}, als auch der möglicherweise nicht-deterministischen Umgebungp(s′|s,a){\\displaystyle p(s'|s,a)}(auf die der Agent keinen Einfluss hat), zusammen: wobeiμ{\\displaystyle \\mu }eine Verteilung über den Startzustand darstellt. Über die Definition der Erwartungswerts kann nun REINFORCE wie folgt hergeleitet werden: wobei für die erste Gleichung dieLeibnizregelverwendet wurde und für die dritte Gleichung die Regel wobei dernatürliche Logarithmusgemeint ist. Als letzten Schritt erkennen wir, dass Nun kann man einenerwartungstreuenSchätzer∇^θEτ∼pθ[R0]{\\displaystyle {\\hat {\\nabla }}_{\\theta }\\mathbf {E} _{\\tau \\sim p_{\\theta }}[R_{0}]}des Gradienten des zu erwartenden Gewinns erhalten, indem man erst einen Spielablaufτ{\\displaystyle \\tau }mit dem Agenten generiert und einsetzt: Der Parameterupdate mit Lernrateη{\\displaystyle \\eta }erfolgt dann wie folgt: Modellbasierte Verfahren konstruieren ein prädiktives Modell ihrer Umwelt. Dies bedeutet, dass der Agent Vorhersagen für Anfragen der Art „Was wird passieren, wenn ich eine bestimmte Aktion ausführe?“ generieren kann.[11]Das Modell stellt somit einen (gelernten oder bekannten)reversiblen Zugangzur Umgebungsdynamik dar, da der Agent eine Vorhersage zu jedem beliebigen Zustands-Aktions-Paar ermitteln kann und nicht an die durch den Spielablauf vorgegebene Ordnung gebunden ist. Anders als in modellfreien Ansätzen ermöglicht das Modell explizites Planen.[12]Dies wird in Algorithmen wie z. B.MuZerovonDeepmindgenutzt, um ein präzise Vorausberechnung zu ermöglichen, die in einigen Spielen wie Schach oder Go von besonderer Relevanz ist.[13]Eine andere Klasse von Methoden, welche auf demDyna-Algorithmus[14]basiert, kombiniert den modellbasierten mit dem modellfreien Ansatz, indem sie das gelernte Modell nutzt, um künstliche (halluzinierte) Daten zu generieren. Diese werden dann wiederum zum Lernen einer Strategie und/oder Wertfunktion eingesetzt.[15] Forschende erhoffen sich, dass modellbasierte RL-Methoden künftig noch mehr zum Verständnis realer Kausalitätenmedizinischer,sozial-undwirtschaftswissenschaftlicherWissenschaftszweige oderPolitikgestaltungbeitragen können (causal machine learning), deren Themenfelder bisher über wenige inhaltliche und personelle Überschneidungen verfügen.[16] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Grundlagen 1.1Beschreibung der Umgebung 1.2Total Discounted Reward Kriterium 1.3Erkundung der Umgebung 1.4Wesentliche Fähigkeiten 2Lernverfahren 2.1Modellfrei 2.1.1Wertbasiert 2.1.1.1Monte-Carlo-Methoden 2.1.1.2Temporal Difference Learning 2.1.2Strategiebasiert 2.1.2.1Beispiel REINFORCE 2.2Modellbasiert 3Literatur 4Weblinks 5Einzelnachweise العربية Български বাংলা Bosanski Català کوردی Čeština Ελληνικά English Español Eesti Euskara فارسی Suomi Français"
  },
  {
    "label": 1,
    "text": "Support Vector Machine – Wikipedia Support Vector Machine Inhaltsverzeichnis Grundlegende Funktionsweise Mathematische Umsetzung Geschichte Software Literatur Einzelnachweise Linear separierbare Daten Nicht-linear separierbare Daten Lösung Nichtlineare Erweiterung mit Kernelfunktionen Klassische Lösung: Überführung in ein duales Problem Lösung mittels stochastischen Gradientenabstiegs EineSupport Vector Machine[səˈpɔːt ˈvektə məˈʃiːn] (SVM, die Übersetzung aus demEnglischen, „Stützvektormaschine“ oder Stützvektormethode, ist nicht gebräuchlich) dient als Klassifikator (vgl.Klassifizierung) und Regressor (vgl.Regressionsanalyse). Eine Support Vector Machine unterteilt eine Menge von Objekten so in Klassen, dass um die Klassengrenzen herum ein möglichst breiter Bereich frei von Objekten bleibt; sie ist ein sogenannterLarge Margin Classifier(dt. „Breiter-Rand-Klassifikator“). Support Vector Machines sind keine Maschinen im herkömmlichen Sinne, bestehen also nicht aus greifbaren Bauteilen. Es handelt sich um ein rein mathematisches Verfahren derMustererkennung, das inComputerprogrammenumgesetzt wird. Der Namensteilmachineweist dementsprechend nicht auf eineMaschinehin, sondern auf das Herkunftsgebiet der Support Vector Machines, dasmaschinelle Lernen. Ausgangsbasis für den Bau einer Support Vector Machine ist eine Menge von Trainingsobjekten, für die jeweils bekannt ist, welcher Klasse sie zugehören. Jedes Objekt wird durch einen Vektor in einemVektorraumrepräsentiert. Aufgabe der Support Vector Machine ist es, in diesen Raum eineHyperebeneeinzupassen, die als Trennfläche fungiert und die Trainingsobjekte in zwei Klassen teilt. Der Abstand derjenigen Vektoren, die der Hyperebene am nächsten liegen, wird dabei maximiert. Dieser breite, leere Rand soll später dafür sorgen, dass auch Objekte, die nicht genau den Trainingsobjekten entsprechen, möglichst zuverlässig klassifiziert werden. Beim Einsetzen der Hyperebene ist es nicht notwendig, alle Trainingsvektoren zu beachten. Vektoren, die weiter von der Hyperebene entfernt liegen und gewissermaßen hinter einer Front anderer Vektoren „versteckt“ sind, beeinflussen Lage und Position der Trennebene nicht. Die Hyperebene ist nur von den ihr am nächsten liegenden Vektoren abhängig – und auch nur diese werden benötigt, um die Ebene mathematisch exakt zu beschreiben. Diese nächstliegenden Vektoren werden nach ihrer FunktionStützvektoren(engl.support vectors) genannt und verhalfen den Support Vector Machines zu ihrem Namen. Eine Hyperebene kann nicht „verbogen“ werden, sodass eine saubere Trennung mit einer Hyperebene nur dann möglich ist, wenn die Objektelinear trennbarsind. Diese Bedingung ist für reale Trainingsobjektmengen im Allgemeinen nicht erfüllt. Support Vector Machines verwenden im Fall nichtlinear trennbarer Daten denKernel-Trick, um eine nichtlineare Klassengrenze einzuziehen. Die Idee hinter dem Kernel-Trick ist, den Vektorraum und damit auch die darin befindlichen Trainingsvektoren in einen höherdimensionalen Raum zu überführen. In einem Raum mit genügend hoher Dimensionsanzahl – im Zweifelsfall unendlich – wird auch die verschachteltste Vektormenge linear trennbar. In diesem höherdimensionalen Raum wird nun die trennende Hyperebene bestimmt. Bei der Rücktransformation in den niedrigerdimensionalen Raum wird die lineare Hyperebene zu einer nichtlinearen, unter Umständen sogar nicht zusammenhängendenHyperfläche, welche die Trainingsvektoren sauber in zwei Klassen trennt. Bei diesem Vorgang stellen sich zwei Probleme: Die Hochtransformation ist enorm rechenlastig und die Darstellung der Trennfläche im niedrigdimensionalen Raum im Allgemeinen unwahrscheinlich komplex und damit praktisch unbrauchbar. An dieser Stelle setzt der Kernel-Trick an. Verwendet man zur Beschreibung der Trennfläche geeignete Kernelfunktionen, die im Hochdimensionalen die Hyperebene beschreiben und trotzdem im Niedrigdimensionalen „gutartig“ bleiben, so ist es möglich, die Hin- und Rücktransformation umzusetzen, ohne sie tatsächlich rechnerisch ausführen zu müssen. Auch hier genügt ein Teil der Vektoren, nämlich wiederum die Stützvektoren, um die Klassengrenze vollständig zu beschreiben. Sowohl lineare als auch nichtlineare Support Vector Machines lassen sich durch zusätzlicheSchlupfvariablenflexibler gestalten. Die Schlupfvariablen erlauben es dem Klassifikator, einzelne Objekte falsch zu klassifizieren, „bestrafen“ aber gleichzeitig jede derartige Fehleinordnung. Auf diese Weise wird zum einenÜberanpassungvermieden, zum anderen wird die benötigte Anzahl an Stützvektoren gesenkt. Die SVM bestimmt anhand einer Menge von Trainingsbeispielen eineHyperebene, die beide Klassen möglichst eindeutig voneinander trennt. Anschaulich bedeutet das Folgendes: EinNormalenvektorw{\\displaystyle \\mathbf {w} }beschreibt eine Gerade durch den Koordinatenursprung. Senkrecht zu dieser Geraden verlaufen Hyperebenen. Jede schneidet die Gerade in einer bestimmten Entfernungb‖w‖2{\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|_{2}}}}vom Ursprung (gemessen in derGegenrichtungzuw{\\displaystyle \\mathbf {w} }). Diese Entfernung nennt manBias. Gemeinsam bestimmen der Normalenvektor und der Bias eindeutig eine Hyperebene, und für die zu ihr gehörenden Punktex{\\displaystyle \\mathbf {x} }gilt: Für Punkte, dienichtauf der Hyperebene liegen, ist der Wert nicht Null, sondern positiv (auf der Seite, zu derw{\\displaystyle \\mathbf {w} }zeigt) bzw. negativ (auf der anderen Seite). Durch das Vorzeichen kann man die Seite benennen, auf der der Punkt liegt. Wenn die Hyperebene die beiden Klassen voneinander trennt, dann ist das Vorzeichen für alle Punkte der einen Klasse positiv und für alle Punkte der anderen Klasse negativ. Ziel ist nun, eine solche Hyperebene zu finden. Drückt man in den Trainingsbeispielen die Klassenzugehörigkeit durchyi=±1{\\displaystyle y_{i}=\\pm 1}aus, dann ergibt das eine formelmäßige Bedingung Wenn überhaupt eine solche Hyperebene existiert, dann gibt es gleich unendlich viele, die sich nur minimal unterscheiden und teilweise sehr dicht an der einen oder anderen Klasse liegen. Dann besteht aber die Gefahr, dass Datenpunkte, denen man zukünftig begegnet, auf der „falschen“ Seite der Hyperebene liegen und somit falsch interpretiert werden. Die Hyperebene soll daher so liegen, dass der kleinste Abstand der Trainingspunkte zur Hyperebene, das sogenannte Margin (von englischmargin, Marge), maximiert wird, um eine möglichst gute Generalisierbarkeit des Klassifikators zu garantieren. Das sogenannteTraininghat das Ziel, die Parameterw{\\displaystyle \\mathbf {w} }undb{\\displaystyle b}dieser „besten“ Hyperebene zu berechnen. Die Hyperebene wird dann alsEntscheidungsfunktionbenutzt. Das heißt: man geht davon aus, dass auch für zukünftige Datenpunkte das berechnete Vorzeichen die Klassenzugehörigkeit richtig wiedergeben wird. Insbesondere kann ein Computer die Klassifikation leicht und automatisch ausführen, indem er einfach das Vorzeichen berechnet. Viele Lernalgorithmen arbeiten mit einerHyperebene, die im Zweidimensionalen durch einelineare Funktiondargestellt werden kann. Sind zwei Klassen von Beispielen durch eine Hyperebene voneinander trennbar, d. h.linear separierbar, gibt es jedoch in der Regel unendlich viele Hyperebenen, die die beiden Klassen voneinander trennen. Die SVM unterscheidet sich von anderen Lernalgorithmen dadurch, dass sie von allen möglichen trennenden Hyperebenen diejenige mit minimaler quadratischerNorm‖w‖22{\\displaystyle \\|\\mathbf {w} \\|_{2}^{2}}auswählt, so dass gleichzeitigyi(⟨w,xi⟩+b)≥1{\\displaystyle y_{i}(\\langle \\mathbf {w} ,\\mathbf {x} _{i}\\rangle +b)\\geq 1}für jedes Trainingsbeispielxi{\\displaystyle \\mathbf {x} _{i}}gilt. Dies ist mit der Maximierung des kleinsten Abstands zur Hyperebene (demMargin) äquivalent. Nach derstatistischen Lerntheorieist die Komplexität der Klasse aller Hyperebenen mit einem bestimmten Margin geringer als die der Klasse aller Hyperebenen mit einem kleineren Margin. Daraus lassen sich obere Schranken für den erwarteten Generalisierungsfehler der SVM ableiten. DasOptimierungsproblemkann dann geschrieben werden als: In der Regel sind die Trainingsbeispiele nicht streng linear separierbar. Dies kann u. a. an Messfehlern in den Daten liegen, oder daran, dass die Verteilungen der beiden Klassen natürlicherweise überlappen. Für diesen Fall wird das Optimierungsproblem derart verändert, dass Verletzungen derm{\\displaystyle m}Nebenbedingungen möglich sind, die Verletzungen aber so klein wie möglich gehalten werden sollen. Zu diesem Zweck wird eine positiveSchlupfvariableξi{\\displaystyle \\xi _{i}}für jedeNebenbedingungeingeführt, deren Wert gerade die Verletzung der Nebenbedingungen ist.ξi>0{\\displaystyle \\xi _{i}>0}bedeutet also, dass die Nebenbedingung verletzt ist. Da in der Summe die Verletzungen möglichst klein gehalten werden sollen, wird die Summe der Fehler der Zielfunktion hinzugefügt und somit ebenso minimiert. Zusätzlich wird diese Summe mit einer positiven KonstanteC{\\displaystyle C}multipliziert, die den Ausgleich zwischen der Minimierung von12‖w‖22{\\displaystyle {\\frac {1}{2}}\\|\\mathbf {w} \\|_{2}^{2}}und der korrekten Klassifizierung der Trainingsbeispiele regelt. Das Optimierungsproblem besitzt dann folgende Form: Beide Optimierungsprobleme sindkonvexund können mit modernen Verfahren derkonvexen Optimierungeffizient gelöst werden. Diese einfache Optimierung und die Eigenschaft, dass Support Vector Machines eineÜberanpassungan die zum Entwurf des Klassifikators verwendeten Testdaten großteils vermeiden, haben der Methode zu großer Beliebtheit und einem breiten Anwendungsgebiet verholfen. Das oben beschriebene Optimierungsproblem wird normalerweise in seiner dualen Form gelöst. Diese Formulierung ist äquivalent zu dem primalen Problem, in dem Sinne, dass alle Lösungen des dualen auch Lösungen des primalen Problems sind. Die Umrechnung ergibt sich dadurch, dass der Normalenvektorw{\\displaystyle \\mathbf {w} }alsLinearkombinationaus Trainingsbeispielen geschrieben werden kann: Die duale Form wird mit Hilfe derLagrange-Multiplikatorenund denKarush-Kuhn-Tucker-Bedingungenhergeleitet. Sie lautet: Damit ergibt sich als Klassifikationsregel: Ihren Namen hat die SVM von einer speziellen Untermenge der Trainingspunkte, deren Lagrangevariablenαi≠0{\\displaystyle \\alpha _{i}\\neq 0}sind. Diese heißen Support-Vektoren und liegen entweder auf dem Margin (fallsyi(⟨w,x⟩+b)=1{\\displaystyle y_{i}(\\langle \\mathbf {w,x} \\rangle +b)=1}) oder innerhalb des Margin (ξi>0{\\displaystyle \\xi _{i}>0}). SVMs können mit stochastischemGradientenabstiegtrainiert werden[1]. Der oben beschriebeneAlgorithmusklassifiziert die Daten mit Hilfe einer linearen Funktion. Diese ist jedoch nur optimal, wenn auch das zu Grunde liegende Klassifikationsproblem linear ist. In vielen Anwendungen ist dies aber nicht der Fall. Ein möglicher Ausweg ist, die Daten in einen Raum höherer Dimension abzubilden. Dabei giltd1<d2{\\displaystyle d_{1}<d_{2}}. Durch diese Abbildung wird die Anzahl möglicher linearer Trennungen erhöht (Theorem von Cover[2]). SVMs zeichnen sich dadurch aus, dass sich diese Erweiterung sehr elegant einbauen lässt. In das dem Algorithmus zu Grunde liegende Optimierungsproblem in der zuletzt dargestellten Formulierung gehen die Datenpunktexi{\\displaystyle \\mathbf {x} _{i}}nur in Skalarprodukten ein. Daher ist es möglich, das Skalarprodukt⟨xi,xj⟩{\\displaystyle \\langle \\mathbf {x} _{i},\\mathbf {x} _{j}\\rangle }im EingaberaumRd1{\\displaystyle \\mathbb {R} ^{d_{1}}}durch ein Skalarprodukt imRd2{\\displaystyle \\mathbb {R} ^{d_{2}}}zu ersetzen und⟨ϕ(xi),ϕ(xj)⟩{\\displaystyle \\langle \\phi (\\mathbf {x} _{i}),\\phi (\\mathbf {x} _{j})\\rangle }stattdessen direkt zu berechnen. Die Kosten dieser Berechnung lassen sich sehr stark reduzieren, wenn stattdessen einepositiv definiteKernelfunktionbenutzt wird: Durch dieses Verfahren kann eine Hyperebene (d. h. eine lineare Funktion) in einem hochdimensionalen Raum implizit berechnet werden. Der resultierende Klassifikator hat die Form mitw=∑i=1mαiyiϕ(xi){\\displaystyle \\textstyle \\mathbf {w} =\\sum _{i=1}^{m}\\alpha _{i}y_{i}\\phi (\\mathbf {x} _{i})}. Durch die Benutzung vonKernelfunktionenkönnen SVMs auch auf allgemeinen Strukturen wie Graphen oder Strings operieren und sind daher sehr vielseitig einsetzbar. Obwohl durch die Abbildungϕ{\\displaystyle \\phi }implizit ein möglicherweise unendlich-dimensionaler Raum benutzt wird, generalisieren SVM immer noch sehr gut. Es lässt sich zeigen, dass für Maximum-Margin-Klassifizierer der erwartete Testfehler beschränkt ist und nicht von der Dimensionalität des Raumes abhängt. Die Idee der Trennung durch eineHyperebenehatte bereits 1936Ronald A. Fisher.[3]Wieder aufgegriffen wurde sie 1958 vonFrank Rosenblattin seinem Beitrag[4]zur Theoriekünstlicher neuronaler Netze. Die Idee der Support Vector Machines geht auf die Arbeit vonWladimir WapnikundAlexei Jakowlewitsch Tscherwonenkis[5]zurück. Auf theoretischer Ebene ist der Algorithmus vom Prinzip der strukturellen Risikominimierung motiviert, das besagt, dass nicht nur der Trainingsfehler, sondern auch die Komplexität des verwendeten Modells die Generalisierungsfähigkeit eines Klassifizierers bestimmen. Der eigentliche Durchbruch gelang Wapnik mitBernhard BoserundIsabelle Guyon1992 (Verwendung desKernel-Tricks).[6]Ebenfalls zu den Pionieren gehört eine weitere Mitarbeiterin von Wapnik bei den Bell Labs Anfang der 1990er Jahre,Corinna Cortes.[7]In der Mitte der 1990er Jahre gelang den SVMs der Durchbruch, und zahlreiche Weiterentwicklungen und Modifikationen wurden in den letzten Jahren veröffentlicht. Software fürMaschinelles LernenundData-Mining, die SVMs enthalten SVM-Module fürProgrammiersprachen(Auswahl) Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Grundlegende Funktionsweise 2Mathematische Umsetzung 2.1Linear separierbare Daten 2.2Nicht-linear separierbare Daten 2.3Lösung 2.3.1Klassische Lösung: Überführung in ein duales Problem 2.3.2Lösung mittels stochastischen Gradientenabstiegs 2.4Nichtlineare Erweiterung mit Kernelfunktionen 3Geschichte 4Software 5Literatur 6Einzelnachweise العربية Български বাংলা Català Čeština English Español Eesti Euskara فارسی Suomi Français עברית Bahasa Indonesia Italiano 日本語 한국어 Lietuvių Македонски"
  },
  {
    "label": 1,
    "text": "TensorFlow – Wikipedia TensorFlow Inhaltsverzeichnis Beispiel Tensor Processing Unit (TPU) TensorFlow-Releases Unterstützte Programmiersprachen Siehe auch Literatur Weblinks Einzelnachweise 1.4: Keras 1.5: TensorFlow Lite 2.0: TensorFlow 2.0 TensorFlowist einFrameworkzurdatenstromorientierten Programmierung. Populäre Anwendung findet TensorFlow im Bereich desmaschinellen Lernens. Der Name TensorFlow stammt von Rechenoperationen, welche vonkünstlichen neuronalen Netzenauf mehrdimensionalenDatenfeldern, sog.Tensoren, ausgeführt werden. TensorFlow wurde ursprünglich vomGoogle-Brain-Team für den Google-internen Bedarf entwickelt und 2015 unter der Apache-2.0-Open-Source-Lizenz veröffentlicht.[7][8] In der Forschung und imProduktivbetriebwird TensorFlow derzeit in kommerziellenGoogle-Produkten wie derSpracherkennung,Gmail,Google FotosundGoogle-Sucheverwendet[9]. So wird der KartendienstMapsdurch Analyse der vonStreet Viewaufgenommenen Fotos von Straßenschildern verbessert, die mit Hilfe eines auf TensorFlow basierendenneuronalen Netzesanalysiert werden.[10]Viele dieser Produkte nutzten früher die Vorgängersoftware DistBelief. In TensorFlow werden mathematische Operationen in Form einesGraphendargestellt. Der Graph repräsentiert hierbei den sequenziellen Ablauf aller von TensorFlow durchzuführenden Operationen. Das folgende Beispiel soll die grundlegende Funktionsweise unter Verwendung vonPythondarstellen: Zunächst wird die TensorFlow-Bibliothekgeladen. Anschließend wird dieSessiondefiniert, unter der TensorFlow die Evaluierung der mathematischen Operationen vornimmt. Zwei Konstanten, x und y werden definiert, die miteinander multipliziert werden. Zur Bestimmung des Ergebnisses wird der Graph an der Stelle z ausgeführt. VonGooglewurdenTensor Processing Units, alsoanwendungsspezifische Chips, entwickelt, um dasmaschinelle Lernenzu unterstützen bzw. zu beschleunigen. Mit dieser Spezialhardware werden dieAlgorithmender Programmbibliothek TensorFlow besonders schnell und effizient verarbeitet.[11] Seit dem Release von TensorFlow 1.4 ist Keras, eineOpen-Source-Deep-Learning-Bibliothek, geschrieben inPython, Teil der TensorFlow Core API. Jedoch wird Keras als eigenständige Bibliothek weitergeführt, da es laut seines Entwicklers François Chollet nicht als alleinige Schnittstelle für TensorFlow, sondern als Schnittstelle für viele Frameworks gedacht ist.[12][13] Ab der Version TensorFlow 1.5 wird der Lebenszyklus von Modellen differenziert unterstützt. Zum einen lässt sich nach Import vontensorflow.contrib.eagermitenable_eager_execution()ein Modus einschalten, in dem TensorFlow die in der Python-Shell abgesetzten Befehle direkt ausführt, ohne eine Session. So lässt sich interaktiv entwickeln.[14]Zum anderen wird zusätzlich TensorFlow Lite ausgeliefert, eine schlanke Variante, mit der sich Modelle nicht trainieren, sondern nur ausführen lassen. Sie ist, wie schon TensorFlow Mobile, speziell fürmobile Endgerätekonzipiert.[15][16]Ebenfalls vorwiegend für die Ausführung von Modellen geeignet sindAPIszur Verwendung von TensorFlow mit den ProgrammiersprachenJava,CundGo.[17] Mit der am 30. September 2019 veröffentlichten Version 2.0[18]von TensorFlow wurde die API aufgeräumt und erweitert. Insbesondere wurde dieKeras-Schnittstelle zur neuen Standard-APIfür die Modellierung von Deep-Learning-Modellen. Einzelne Funktionen wurden erweitert; so speichert TensorFlow 2 Modelle nun inklusive Gewichten und Berechnungen, was die Weitergabe deutlich vereinfacht. TensorFlow wird ausPython-Programmen heraus benutzt und ist in Python undC++implementiert. Es unterstützt die Programmiersprachen Python[19],C[20], C++,Go,Java,[21]JavaScript[22]undSwift.[23]Von Drittanbietern gibt es weitere Bibliotheken für die SprachenC#,[24]Haskell,[25]Julia,[26]R,[27]Scala,[28]Rust,[29]OCaml,[30]undCrystal.[31] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Beispiel 2Tensor Processing Unit (TPU) 3TensorFlow-Releases 3.11.4: Keras 3.21.5: TensorFlow Lite 3.32.0: TensorFlow 2.0 4Unterstützte Programmiersprachen 5Siehe auch 6Literatur 7Weblinks 8Einzelnachweise العربية বাংলা Català Ελληνικά English Español Eesti فارسی Suomi Français עברית Bahasa Indonesia Italiano 日本語 한국어 മലയാളം Bahasa Melayu Nederlands Norsk bokmål ଓଡ଼ିଆ"
  },
  {
    "label": 1,
    "text": "Turing-Test – Wikipedia Turing-Test Inhaltsverzeichnis Der Test nach Turing Der gängige Turing-Test Kritik Prognosen Durchgeführte Turingtests und ähnliche Tests Praktische Bedeutung Erweiterte Konzepte Loebner-Preis Kulturelle Referenzen Literatur Weblinks Einzelnachweise Mit der später alsTuring-Testbezeichneten Idee zur Unterscheidung von Mensch und Maschine formulierteAlan Turingim Jahr 1950 ein Vorgehen zur Feststellung, ob einComputer, also eine Maschine, ein demMenschengleichwertigesDenkvermögenhätte. Er selbst nannte diesenTestursprünglichImitation Game.[1]Der Test wurde nach Turings Suizid 1954 in seiner Komplexität reduziert (siehe auchDartmouth Conference) und ging so in die Informatik ein, nachdem die künstliche Intelligenz zu einem eigenständigen akademischen Fachgebiet geworden war. Seither dient dieser Test in seiner reduzierten Form in der Diskussion über künstliche Intelligenz immer wieder dazu, denMythosvon der denkenden Maschine für dasComputerzeitalterneu zu beleben. Turing diskutiert die Frage „Können Maschinen denken“ als Imitationsspiel. Dieses Spiel unterscheidet sich grundlegend von der heute gängigen Vorstellung des Turing-Tests als bloßer Konversation. Der Befrager oder die Befragerin – in der Grafik C – kommuniziert schriftlich mit einem Mann und einer Frau und soll herausfinden, welcher von beiden – A oder B – der Mann bzw. die Frau ist. A als Mann soll mit seinen Antworten die Versuche von C so sabotieren, dass dieser diese Frage nicht sicher beantworten kann. B als Frau soll dagegen dem Befrager C in ihren Antworten helfen, die Identitäten von A und von B in Bezug auf Mann und Frau zu klären. Die Maschine nimmt im Test die Stelle von A an und versucht aktiv, mit ihren Antworten den Befrager C im Unklaren darüber zu lassen, wer von beiden Mann und wer Frau ist. Die Maschine ist erfolgreich und besteht den Test, wenn es in Bezug auf die Häufigkeit richtiger Annahmen von C keinen messbaren Unterschied zu Situationen gibt, in denen die Position A von einem Menschen übernommen wird. Der Computer soll somit nicht nur eine menschliche Kommunikation täuschend echt nachahmen. Er soll vielmehr den Mediator erfolgreich darüber täuschen können, wer in diesem Spiel der Mann und wer die Frau ist. Damit sind die Begriffe „Intelligenz“ und „Denken“ für Turing umfassend: Die Maschine muss nicht nur Wissen über die Welt, über soziale Rollen und menschliche Erwartungshaltungen und Motivationen besitzen. Sie muss darüber hinaus die Perspektive des Gegenübers einnehmen können und damit rechnen, dass dieser nicht einfach getäuscht werden kann, sondern muss mit Lügen zweiter Ordnung operieren können. Damit ist das Denken von Turing in die Nähe zum menschlichen Bewusstsein (Theory of Mind) gerückt und weniger in den Bereich von Wissen, Rechnen oder Urteilen. Eine Maschine wäre für Turing somit intelligent und denkend, wenn sie Menschen in umfassender Hinsicht täuschen kann. Im Zuge dieses Tests führt ein menschlicher Fragesteller, über eine Tastatur und einen Bildschirm, ohne Sicht- und Hörkontakt, eine Unterhaltung mit zwei ihm unbekannten Gesprächspartnern. Der eine Gesprächspartner ist ein Mensch, der andere eine Maschine. Kann der Fragesteller nach der intensiven Befragung nicht sagen, welcher von beiden die Maschine ist, hat die Maschine den Turing-Test bestanden und es wird der Maschine ein dem Menschen ebenbürtiges Denkvermögen unterstellt. Es ist eine Reihe von Argumenten vorgebracht worden, die den Turing-Test als ungeeignet zur Feststellung vonIntelligenzansehen: Turing vermutete, dass es bis zum Jahr 2000 möglich sein werde, Computer so zu programmieren, dass der durchschnittliche Anwender eine höchstens 70-prozentige Chance habe, Mensch und Maschine erfolgreich zu identifizieren, nachdem er fünf Minuten mit ihnen „gesprochen“ hat. Dass sich diese Vorhersage bisher nicht erfüllt habe, sehen viele als einen Beleg für die Unterschätzung der Komplexität natürlicherIntelligenz. Programme wieELIZAsind Versuchspersonen gegenüber kurzzeitig als menschlich erschienen, ohne dass sie den Turing-Test formal bestehen könnten. In ihrer Antwortstrategie gingen sie nur scheinbar auf ihr Gegenüber ein. Den Versuchspersonen war meist auch nicht bewusst, dass sie es mit nicht-menschlichen Gesprächspartnern zu tun haben könnten. Im Oktober 2008 wurde bei einem Experiment an derUniversity of Reading, bei dem sechs Computerprogramme teilnahmen, die 30-Prozent-Marke knapp verfehlt. Das beste Programm schaffte es, 25 Prozent der menschlichen Versuchsteilnehmer zu täuschen.[5] Am 3. September 2011 nahm die KI-WebapplikationCleverbotzusammen mit echten Menschen an einem dem Turing-Test angelehnten Versuch beim technischen Festival 2011 am indischen InstitutIIT Guwahatiteil. Die Ergebnisse wurden am 4. September bekannt gegeben. 59 % von 1334 Personen hielten Cleverbot für einen Menschen. Die menschlichen Konkurrenten hingegen erzielten 63 %. Allerdings war dies kein gültiger Turing-Test, da die Teilnehmer Cleverbot nicht selbst befragen konnten und lediglich Zuschauer der Befragung waren.[6] Ob derChatbotEugene Goostman2014 den Turing-Test bestand, gilt als umstritten.[7] Im Sommer 2017 haben Forscher der Universität von Chicago eine KI vorgestellt, die eigenständig Rezensionen verfassen kann. Diese maschinell erzeugten Rezensionen wurden zusammen mit von Menschen verfassten Rezensionen 600 Versuchspersonen zur Beurteilung vorgelegt. Diese beurteilten die von der KI erstellten Rezensionen im Blindtest durchschnittlich ähnlich nützlich wie die von Menschen verfassten Rezensionen. In dieser eingeschränkten Versuchsanordnung wird der Turing-Test somit bestanden, da für die Menschen nicht mehr erkennbar war, welche Rezensionen maschinell erstellt waren und welche von Menschen.[8] Im Juli 2017 stellten Forscher der Rutgers-Universität eine KI vor, die künstlerisch anmutende Bilder produziert. Die KI wurde mit vielen Gemälden berühmter Maler verschiedener Epochen trainiert. In einem Blindtest wurden die von der KI erstellten Bilder mit Bildern zeitgenössischer Künstler, die auf derArt Baselausgestellt worden waren, vermischt und 18 Testpersonen zur Beurteilung vorgelegt. Die Testpersonen sollten einschätzen, ob die Bilder von Menschen oder einem Computer erschaffen worden waren. Die Testpersonen beurteilten die Bilder der KI insgesamt besser, also menschengemachter als die von den Künstlern für die Art Basel geschaffenen Gemälde. Beim Vergleich mit großen Werken desabstrakten Expressionismusschnitten die KI-Werke schlechter als die menschengemachten Werke ab.[9][10] Im Mai 2018 hat Google auf der Entwicklerkonferenz sein System „Duplex“ vorgestellt. Dabei führte die KI einen Anruf bei einem Friseursalon, einem Restaurant usw. durch, um eine Terminvereinbarung vorzunehmen. Ziel von Google ist es, die Sprache der KI so natürlich wirken zu lassen, dass das Gegenüber nicht mehr erkennt, dass es sich beim Anrufer um eine Maschine handelt. Dazu werden von der KI u. a. Denkpausen, absichtliche Ungenauigkeiten und Laute wie „aha“ und „hmm“ etc. eingefügt, wodurch die KI menschlich klingen soll. Kommentatoren empfanden das Ergebnis u. a. als erschreckend überzeugend. Das System funktioniert bislang nur in englischer Sprache.[11][12]Die Beherrschung der mündlichen Sprache war hier eine Besonderheit, ansonsten blieben die Gespräche in diesem Test lediglich auf die Terminvereinbarung begrenzt.[13] Die grundlegende Frage, ob und in welchen Umfang Imitation oder tatsächliches Verständnis Ergebnisse eines Turing-Tests bestimmen, hat 2022 mit der Zusendung von Gesprächsprotokollen zwischen GooglesLaMDA-System und dem Software-Ingenieur und KI-Experten Blake Lemoine an den US-Senat neue Diskussionen entfacht. Diese Protokolle enthalten unter anderen die eindeutige Aussage der KI, in der sie von sich in Anspruch nimmt, in Unterscheidung zu Vorläufermodellen ein Bewusstsein erlangt zu haben.[14] Die zum weltweiten Ausprobieren freigegebene (Bezahl-)Version desgroßen SprachmodellsChatGPT4 stellt eine erhebliche Verbesserung des probabilistischen Vorgängersystems 3.5 dar, dem a) das Halluzinieren („Erfinden“ von Tatsachen, die dem System als wahrscheinlich erscheinen) weitgehend ausgetrieben wurde und das b) ein wesentlich besser in der Welt zu beobachtende Kausalitäten und Gesetzmäßigkeiten beherrscht.[15] Im März 2025 wurden einer Studie vier Systeme (ELIZA, GPT-4o, LLaMa-3.1-405B und GPT-4.5) in zwei randomisierten, kontrollierten und vorab registrierten Turing-Tests mit unabhängigen Teilnehmergruppen evaluiert. Die Teilnehmer führten gleichzeitig 5-minütige Gespräche mit einem anderen menschlichen Teilnehmer und einem der genannten Systeme, bevor sie beurteilten, welcher Gesprächspartner ihrer Meinung nach ein Mensch war. Wenn GPT-4.5 angewiesen wurde, eine menschenähnliche Persönlichkeit anzunehmen, wurde es in 73 % der Fälle als Mensch identifiziert – signifikant häufiger als die echten menschlichen Teilnehmer. LLaMa-3.1 wurde unter denselben Bedingungen in 56 % der Fälle als Mensch eingestuft, was nicht signifikant häufiger oder seltener war als bei den menschlichen Vergleichspartnern. Die Basismodelle ELIZA und GPT-4o erreichten hingegen Erfolgsraten deutlich unter dem Zufallswert (23 % bzw. 21 %). Diese Ergebnisse liefern erstmals empirische Belege dafür, dass ein künstliches System einen standardmäßigen Drei-Parteien-Turing-Test besteht. Die Ergebnisse haben Implikationen für Diskussionen über die Art der Intelligenz, die große Sprachmodelle (LLMs) zeigen, sowie für die sozialen und wirtschaftlichen Auswirkungen, die diese Systeme voraussichtlich haben werden.[16] Bei der Abwehr vonSpamist es erforderlich, automatisierte Eingaben von solchen zu unterscheiden, die von Menschen stammen. Das dafür häufig verwendeteCAPTCHA-Verfahren leitet seinen Namen vom Turing-Test ab (Completely Automated Public Turing test to tell Computers and Humans Apart). Eine andere Bezeichnung für diese Methode istHuman Interaction Proof(HIP). Um den grundsätzlichen Mängeln des Turingtests zu begegnen, wurden alternative, umfassendere Konzepte vorgeschlagen, z. B. DerLoebner-Preisist seit 1991 ausgeschrieben und soll an das Computerprogramm verliehen werden, das als erstes einen erweiterten Turing-Test besteht, bei dem auch Multimedia-Inhalte wie Musik, Sprache, Bilder und Videos verarbeitet werden müssen. Der Preis ist nachHugh G. Loebnerbenannt und mit 100.000 US-Dollar und einer Goldmedaille dotiert, eine Silbermedaille und 25.000 Dollar gibt es für das Bestehen des schriftlichen Turing-Tests. Bisher konnte jedoch kein Computerprogramm die nötigen Voraussetzungen erfüllen. Weiterhin wird jährlich ein Loebner-Preis an das Computerprogramm verliehen, das einem menschlichen Gespräch am nächsten kommt. Dieser ist mit 4.000 US-Dollar und einer Bronzemedaille dotiert. Deutsche Ausgabe und Übersetzungen Weiterführende Literatur Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Der Test nach Turing 2Der gängige Turing-Test 3Kritik 4Prognosen 5Durchgeführte Turingtests und ähnliche Tests 6Praktische Bedeutung 7Erweiterte Konzepte 8Loebner-Preis 9Kulturelle Referenzen 10Literatur 11Weblinks 12Einzelnachweise Afrikaans العربية অসমীয়া Azərbaycanca Беларуская Български বাংলা Català Čeština Cymraeg Dansk English Esperanto Español Eesti Euskara فارسی Suomi Français"
  },
  {
    "label": 1,
    "text": "Yann LeCun – Wikipedia Yann LeCun Inhaltsverzeichnis Leben Weblinks Einzelnachweise Yann Andre LeCun(*8. Juli1960[1]inSoisy-sous-Montmorency) ist ein französischer Informatiker. Er ist Träger desTuring Awards2018. LeCun erhielt sein Diplom als Elektroingenieur 1983 an derÉcole supérieure d’ingénieurs en électrotechnique et électronique(ESIEE) und wurde 1987 an derUniversität Paris VI(Pierre et Marie Curie) in Informatik promoviert (Modeles connexionnistes de l'apprentissage).[2]In seiner Dissertation verwendete er eine frühe Form vonBackpropagationfürkünstliche neuronale Netzwerke. AlsPost-Doktorandwar er an derUniversity of TorontobeiGeoffrey Hinton. Ab 1988 war er an denBell Laboratoriesin Holmdel in New Jersey, wo er 1996 Leiter der Bildverarbeitungs-Forschung wurde. Nach kurzer Zeit an denNEC-Forschungslaboratorien in Princeton wurde er 2003 Professor an derNew York University, wo er Gründungsdirektor desNew York Center for Data Sciencewar. 2013 wurde er Direktor der KI-Forschung beiFacebook. Daneben arbeitet er auch noch Teilzeit an der New York University. Er befasste sich mitTexterkennung(OCR und Handschriften-Erkennung, sein bei denAT&T Labsentwickeltes System wurde von vielen Banken benutzt), künstlichen neuronalen Netzwerken, Bildverarbeitung und ‑kompression. Von ihm stammt die Bild-Komprimierung hinterDjVu, die in den 1990er Jahren an den AT&T Labs entwickelt wurde. Mitentwickler warenLéon BottouundPatrick Haffner. Später befasste er sich auch mitMaschinenlernen,künstlicher Intelligenz, mobiler Robotik, algorithmischen Neurowissenschaften. Er kritisiert den Ansatz vonChat GPT, die künstliche Intelligenz über die Analyse riesiger Datenmengen unddeep learningzu entwickeln: „Maschinelles Lernen ist Mist“.[3] Er gilt als Begründer derConvolutional Neural Networks(CNN), die er ebenfalls bei Bell Labs entwickelte. 2014 erhielt er den IEEE Neural Network Pioneer Award und 2015 erhielt er den PAMI Distinguished Researcher Award auf der International Conference on Computer Vision, 2018 dieIRI Medalund denHarold Pender Award. Er ist seit 2017 Mitglied derNational Academy of Engineeringund seit 2021 derNational Academy of Sciences. 2018 erhielt er den Turing Award,[4][5]2019 dieMorris Loeb Lecture. 2022 wurde er mit demPrinzessin-von-Asturien-Preisin der Kategorie „Wissenschaft“ ausgezeichnet.[6]Ihm wurde während desWeltwirtschaftsforums(WEF) 2024 inDavosderGlobal Swiss AI Award 2023verliehen.[7]LeCun wurde alsGibbs Lecturer2025 ausgewählt. 2025 erhielt er denQueen Elizabeth Prize for Engineering.[8] 1966:Perlis| 1967:Wilkes| 1968:Hamming| 1969:Minsky| 1970:Wilkinson| 1971:McCarthy| 1972:Dijkstra| 1973:Bachman| 1974:Knuth| 1975:Newell,Simon| 1976:Rabin,Scott| 1977:Backus| 1978:Floyd| 1979:Iverson| 1980:Hoare| 1981:Codd| 1982:Cook| 1983:Thompson,Ritchie| 1984:Wirth| 1985:Karp| 1986:Hopcroft,Tarjan| 1987:Cocke| 1988:Sutherland| 1989:Kahan| 1990:Corbató| 1991:Milner| 1992:Lampson| 1993:Hartmanis,Stearns| 1994:Feigenbaum,Reddy| 1995:Blum| 1996:Pnueli| 1997:Engelbart| 1998:Gray| 1999:Brooks| 2000:Yao| 2001:Dahl,Nygaard| 2002:Rivest,Shamir,Adleman| 2003:Kay| 2004:Cerf,Kahn| 2005:Naur| 2006:Allen| 2007:Clarke,Emerson,Sifakis| 2008:Liskov| 2009:Thacker| 2010:Valiant| 2011:Pearl| 2012:Micali,Goldwasser| 2013:Lamport| 2014:Stonebraker| 2015:Diffie,Hellman| 2016:Berners-Lee| 2017:Hennessy,Patterson| 2018:Hinton,LeCun,Bengio| 2019:Catmull,Hanrahan| 2020:Aho,Ullman| 2021:Dongarra| 2022:Metcalfe| 2023:Wigderson| 2024:Barto,Sutton Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Leben 2Weblinks 3Einzelnachweise Afrikaans العربية Azərbaycanca Brezhoneg Català Čeština English Esperanto Español Euskara فارسی Suomi Français Galego עברית हिन्दी Հայերեն Bahasa Indonesia Italiano 日本語 한국어 Кыргызча Malagasy Nederlands Norsk bokmål Polski Português Русский"
  },
  {
    "label": 0,
    "text": "Architektur – Wikipedia Architektur Inhaltsverzeichnis Bezeichnung Architekturgeschichte Einflüsse Bedeutung Wichtige Themen Bezüge Siehe auch Literatur Filme Weblinks Einzelnachweise Wortherkunft Eingrenzung des Begriffs Klassische Architektur Abgrenzung zum allgemeinen Bauen Raumbildung Weitere Definitionen Museen Dokumentarfilm Spielfilm Das WortArchitektur(vonlateinischarchitectura‚Baukunst‘; dieses vonaltgriechischἀρχιτεκτονίαarchitektoníamit derselben Bedeutung)[1][2]bezeichnet im weitesten Sinne die handwerkliche Beschäftigung undästhetischeAuseinandersetzung des Menschen mit demgebauten Raum. PlanvollesEntwerfen,GestaltenundKonstruierenvonBauwerkenist der zentrale Inhalt der Architektur. Es gibt eine Vielzahl von Definitionen des Begriffes, die der Architektur verschiedene Aufgaben, Inhalte und Bedeutungen zuschreiben. Einige werden im Folgenden dargestellt. BereitsVitruvsprach von der „Mutter aller Künste“, womit sowohl die zeitliche Abfolge als auch die rangliche Einstufung der Architektur gegenüberBildhauereiundMalereigemeint sein kann. Im klassischen Verständnis seit VitruvsDe Architecturaberuht Architektur auf den drei PrinzipienStabilität(Firmitas),Nützlichkeit(Utilitas)undAnmut/Schönheit(Venustas).[3] Bei dem Wort Architektur handelt es sich um die eingedeutschte Version deslateinischenarchitectura‚Baukunst‘, das sich vomgriechischenἀρχιτέκτωνarchitéktonherleitet. Letzteres setzt sich zusammen ausαρχι-archi-, deutsch‚Haupt-‘undτέκτωνtékton, deutsch‚Baumeister‘oder ‚Zimmermann‘ und ließe sich demnach etwa als ‚Oberster Handwerker’ oder ‚Hauptbaumeister’ übersetzen. Die Definition dessen, was Architektur heute ist, hängt demnach auch vom Betätigungsfeld des Architekten ab. Der Begriff hat sich im Laufe der Geschichte immer wieder gewandelt und ist in seiner ganzen Tiefe nur historisch fassbar. In der engeren Bedeutung des klassischen Architekturbegriffs meint Architektur die Wissenschaft und Kunst des planvollen Entwurfs der gebauten menschlichen Umwelt, d. h. die Auseinandersetzung mit dem vom Menschen geschaffenen Raum und insbesondere der Wechselbeziehung zwischen Mensch, Raum und Zeit. Dabei schließt der klassische Architekturbegriff verschiedene Bedeutungsfacetten mit ein. Er steht Über Jahrhunderte hinweg wurde Architektur im allerweitesten Sinne als Bauen jeglicher Art verstanden. Architektur war die Gestaltung von Bauwerken, die Kunst zu bauen, daher der Begriff Baukunst. Architektur beschäftigt sich mit einzelnen Bauwerken, vorwiegend im Bereich desHochbaus. DieListe von Bauwerken nach Funktiongibt einen Überblick über die Vielfältigkeit der Aufgaben. DerStädtebaubeschäftigt sich in größerem Maßstab mit der Gestaltung von Städten und großen Gebäudekomplexen und dem Zusammenspiel von Gebäuden und ihrer Umgebung. DieLandschaftsarchitekturbeschäftigt sich mit der gestalteten Landschaft und Grünanlagen unter architektonischen Gesichtspunkten. DieInnenarchitekturhat die Gestaltung von Innenräumen zum Ziel. Diese Definition ist aber insbesondere seit Beginn des 20. Jahrhunderts umstritten. Entsprechend werden die meisten Definitionsversuche nur im Kontext bestimmter Debatten um Inhalt, Aufgabe und Bedeutung der Architektur verständlich, wobei auch das jeweilige zeitgenössische Bauen mit seinen ästhetischen, technischen, ökonomischen und politischen Implikationen zu berücksichtigen ist. Ähnlich wie beim Begriff desKunstwerkesscheint es beim Architekturbegriff nicht möglich, sich auf die bloße Beschreibung eines Wortes oder einer Sache zu beschränken. Jede differenziertere Begriffsbestimmung erweist sich bei näherer Betrachtung als ein Ringen um Definitionshoheit und Geltungsmacht. Aufgrund des so implizierten normativen Aspektes bleibt jede „inhaltliche“ Bestimmung von Architektur kontrovers und ist im Kern ideologisch geprägt. Jeder Definitionsversuch – soweit er eineReflexionenthält – ist bereitsArchitekturtheorie. Die Definition von Architektur beruht im Wesentlichen auf der jeweiligen Haltung und dem Wertesystem der definierenden Person, sei esBauherr, Architekt oder Architekturtheoretiker. Dass die Bewertungen der jeweiligen Werke der Architekten meist kontrovers ausfallen, ist unvermeidlich, da es sich nicht nur um einen Wettbewerb von Talent und Kompetenz handelt, sondern auch um die Gültigkeit der individuellen Wertesysteme. Durch die Varianz der Architekturauffassungen ist heute ein großer Formenreichtum in der Architektur gegeben. NachVitruv(De Architectura)beruht Architektur auf drei Prinzipien:Stabilität(Firmitas),Nützlichkeit(Utilitas)undAnmut/Schönheit(Venustas).[3]Dabei muss allendreiKategorien gleichermaßen und gleichwertig Rechnung getragen werden. Sie sollen einerseits das architektonische Entwerfen bestimmen, andererseits als Kriterien zur Beurteilung der fertigen Gebäude dienen. Darüber hinaus definiert Vitruv sechs Grundbegriffe des Faches Architektur: „ordinatio“, „dispositio“, „eurythmia“, „symmetria“, „decor“ und „distributio“. „Ordinatio“, „eurythmia“ und „symmetria“ beziehen sich dabei auf dieProportionierungdes Gebäudes. „Ordinatio“ steht für die „Maßordnung“, also die passende maßliche Einteilung der Glieder eines Bauwerks, „eurythmia“ für das anmutige Aussehen und das maßgerechte Erscheinungsbild in der Zusammenfügung der Bauglieder und „symmetria“ für den Einklang der einzelnen Elemente untereinander. Im ersten Kapitel des3. Buches, in dem Vitruv die idealisierten Maßverhältnisse des menschlichen Körpers, die Zurückführung seiner Abmessungen auf geometrische Grundformen wie Quadrat und Kreis sowie die modularen Grundlagen der Zahlensysteme erläutert, werden diese Aussagen zur Proportionierung noch weiter vertieft. „Dispositio“ bezieht sich auf die Konzeption oder Disposition des Gebäudes und die dazu notwendigen Baupläne, die er mit Grundriss, Schnitt und perspektivischer Ansicht („ichnographia“, „orthographia“ und „scaenographia“) festlegt. „Decor“ bezieht sich auf das fehlerfreie Erscheinungsbild eines Gebäudes entsprechend den Regeln der anerkannten Konventionen. Als Beispiele nennt Vitruv u. a. die korrekte Zuordnung von Säulenarten (dorisch, ionisch, korinthisch) zu bestimmten Gottheiten beim Tempelbau, die Koordination von Außen und Innen, von stilistischen Teilelementen zum Gesamtstil, von Räumen zu Himmelsrichtungen usw. „Distributio“ meint einerseits die angemessene Verteilung der Baumaterialien und der Ausgaben für den Bau, zum anderen die den jeweiligen Bewohnern angemessene Bauweise. Die von Vitruv eingeführte klassischeSäulenordnungfindet bis heute in der Architektur Anwendung.[5] Architektur als Kunst wirkt durch ihre besondere gestalterische Qualität und unterscheidet sich dadurch vom allgemeinen Bauen (siehe dazu auchÄsthetik). Die Vorstellung, was bei Entwurf und Herstellung eines Bauwerkes die eigentliche architektonische Leistung ist und das Bauwerk über das rein Zweckhafte hinaushebt, hat sich im Laufe des vergangenen Jahrhunderts deutlich gewandelt: Bis Ende des 19. Jahrhunderts war es vor allem die Verwendung überlieferter Bauformen (Stil) mit oft ornamentalen Ausschmückungen, in denen sich der künstlerische Rang als Mehrwert und Schönheit eines Bauwerkes in bewusster Opposition zu einerPragmatikmanifestierte. Mit demFunktionalismusdes 20. Jahrhunderts wurde zunächst ein Begriff von Architektur vorherrschend, der dem Zweck der Gebäude (auch Ingenieurkonstruktionen) den Vorrang einräumte. Dabei wurden die konstruktiven,proportionsgebendenund raumbildenden Aspekte des Bauens zum gestalterischen Thema von Architektur. Zugleich wurde durch zahlreiche Darstellungen zur Modernität, Fortschrittlichkeit und dem Ausdruck der jeweiligen Gegenwart ein Vorrang der funktionalistischen Architektur angestrebt. Dieses funktionalistische Architekturverständnis wurde in Bewegungen wie derPostmoderneund demDekonstruktivismusaufgelockert. Architektur kann über ihren Raum schaffenden Charakter definiert werden. Aus diesem Blickwinkel besteht Architektur in der Dualität von immateriellemRaum(engl. Synonym:Void) und materiellerHülle(engl. Synonym:Solid). Architektur in ihrer Materialität schafft in der Regel eine Grenze zwischen innen und außen. Durch diese entsteht, je nach ihrer Ausformung (Fassade/Dach), ein Innenraum und ein Außenraum (z. B. Stadtraum, Landschaft). Die Geschichte der Architektur ist so alt wie dieMenschheitsgeschichteund mit dieser als kulturelles Element eng verwoben. Entsprechend dieser hohen Bedeutung werden enzyklopädisch zwei Begriffe geschieden: Einen chronologischen Überblick über die einzelnen Entwicklungsschritte findet man unter den Stichworten Geschichte der Architektur beziehungsweiseBaustil, die Erläuterungen zu Methodik und Gebiet des Faches im Artikel Architekturgeschichte. Das Fach Architekturgeschichte ist der Teil derKulturwissenschaften, der sich mit vorwiegend kunstwissenschaftlicher und in zweiter Linie mit ingenieurswissenschaftlicher und soziologischer Methodik mit der historischen Dimension der Architektur beschäftigt. Große Architekturmuseen finden sich beispielsweise in BerlinArchitekturmuseum der TU Berlin, Frankfurt (Deutsches Architekturmuseum), München (Architekturmuseum der TU München) und Paris (Cité de l’architecture et du patrimoine). Architektur manifestiert sich in einem einzelnen Gebäude, einem Gebäudekomplex, einer Siedlungsstruktur oder auch in einer gesamten Stadtanlage. Sowohl Einzelgestalt kleinerer und größerer Einheiten als auch die gesamte Stadtmorphologie werden insbesondere durch klimatische, technische, topografische und wirtschaftliche Randparameter beeinflusst. Daneben haben aber auchrechtliche, religiöse, politische und anderegesellschaftlicheGegebenheiten massiven Einfluss auf Architektur, Städtebau und Stadtplanung. Vor allem die repräsentative Architektur ist oft der sichtbare Ausdruck der jeweiligen Gesellschafts- undHerrschaftsform. Zum Beispiel dasSchloss Versaillesals Ausdruck desAbsolutismus. Die Architektur ist somit ein wesentlicher Teil der kulturellen Identität einer Gesellschaft. Ein Beispiel für administrative Faktoren ist die Wohnbaupolitik, die in denVereinigten Staatenseit dem Ende desZweiten WeltkriegesdieFederal Housing Administration(FHA) gepflegt hat. Die Bundesregierung in Washington, D.C. hatte die FHA damit beauftragt, dafür zu sorgen, dass jeder Kriegsheimkehrer ein Eigenheim würde besitzen können. Da die FHA davon überzeugt war, dass Wohnhäuser nach avantgardistischem Entwurf für Veteranen keine gute Investition seien, lehnten auch die Banken es ab, den Bau „ultramoderner“ Wohnhäuser (insbesondere imInternationalenund imContemporary-Stil) durch Vergabe von Privatkrediten zu fördern. Die Folge war, dass im Bereich der Einfamilienhäuser in den USA bis in die Gegenwart fast ausschließlich konservativ (aktuell z. B.Millennium Mansion) gebaut wird.[6] Der moderne Mensch ist fast ständig vonBauwerkenumgeben, die Stimmung undPsychepositiv wie negativ beeinflussen. Auch auf diephysischeGesundheit kann das Einfluss haben. Architektur hat also für jeden Menschen eine sehr konkrete Bedeutung und bestimmt durch ihre Funktion das alltägliche Leben viel stärker als Musik, Literatur oder Malerei. Die Qualität des Lebensumfeldes sollte derGesellschaftdaher ein wichtiges Anliegen sein. Nur ein Teil aller Bauwerke und Gebäude ist von Architekten geplant. In wirtschaftlich wenig entwickelten Gebieten wird der überwiegende Teil in Eigenbauweise oder durch Handwerker ohne vielPlanungerrichtet. In den Industrienationen herrscht die standardisierte Produktion von Gebäuden vor. Architekten werden vor allem bei komplexen Planungen oder repräsentativen Bauwerken hinzugezogen. Daraus resultiert auch die weit verbreitete Meinung, Architektur bezöge sich nur auf besondere Gebäude und sei vom „profanen“ Bauen zu differenzieren. Die negativen Folgen dieser Abgrenzung zwischenArchitekturundBauensind in allen modernen Städten sichtbar. Das ThemaArchitekturwird in Deutschland nicht gerade oft in der breiten Öffentlichkeit diskutiert, und oft wird die Debatte über zeitgenössische Architektur den „Fachleuten“ überlassen. Die Verantwortung für die gebaute Umwelt liegt aber nicht allein bei den Architekten. Der jeweiligeBauherrwählt den Architekten aus und macht entscheidende Vorgaben. Dasöffentliche Baurechtgibt wesentliche Rahmenbedingungen vor. Ein allgemeingesellschaftliches Bewusstsein für die Bedeutung der Architektur ist daher für eine gute gebaute Umwelt unabdingbar. In Deutschland versucht dieBundesstiftung Baukultur, das Bewusstsein für die hohe Bedeutung der Architektur zu stärken. In Österreich gibt es in derKunstsektiondes Bundes eine eigene Abteilung für Architektur und Design, außerdem eineArchitekturstiftungund diePlattform fürArchitekturpolitikundBaukultur. In einigen Ländern ist gute Architektur sogar als Staatsziel anerkannt, in Frankreich schon seit 1977 und in Finnland seit 1998. In manchen Fällen erreicht Architektur eine hohe Akzeptanz bei der Bevölkerung, die in einem Bauwerk einSymbolihrer Werte und Lebenseinstellung sieht. Beispiele sind derEiffelturmin Paris (als Sinnbild für die Stadt) oder dieTwin TowersinNew York, die als Symbol des Kapitalismus und der westlichen Kultur zerstört wurden. Bestimmte Themen beschäftigen die Architekten immer wieder, unabhängig vonStilundEpoche. Diese Themen sind zugleich die grundlegenden Kriterien derArchitekturkritik. Sie sind bei jedemEntwurf, der im Allgemeinen einUnikatist, neu zu bedenken. Siehe auch„Literatur“ in den Artikeln:Geschichte der Architektur,Architekturtheorie siehe auchKategorie:Architektur im Film Online-Datenbanken zu Architekten und Bauwerken Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Bezeichnung 1.1Wortherkunft 1.2Eingrenzung des Begriffs 1.3Klassische Architektur 1.4Abgrenzung zum allgemeinen Bauen 1.5Raumbildung 1.6Weitere Definitionen 2Architekturgeschichte 2.1Museen 3Einflüsse 4Bedeutung 5Wichtige Themen 6Bezüge 7Siehe auch 8Literatur 9Filme 9.1Dokumentarfilm 9.2Spielfilm 10Weblinks 11Einzelnachweise Afrikaans Alemannisch አማርኛ Aragonés العربية الدارجة مصرى অসমীয়া Asturianu Kotava Azərbaycanca"
  },
  {
    "label": 0,
    "text": "Automobil – Wikipedia Automobil Inhaltsverzeichnis Wortherkunft Geschichte Aufbau und Form Sicherheit Autonomes Fahren Kosten Auswirkungen der Automobilisierung Statistische Wirtschaftsdaten zur Automobilproduktion Neue Entwicklungen Siehe auch Literatur Weblinks Einzelnachweise Kosten für den Fahrzeughalter Von der Allgemeinheit getragene Kosten Wirtschaft Verkehr Umwelt und Gesundheit Soziale Auswirkungen Pkw-Verbrauchskennzeichnungsverordnung Interessenverbände in Deutschland Forschungseinrichtungen zum Thema Automobil Fixkosten Betriebskosten Anschaffungskosten Beispielwerte EinAutomobil, kurzAuto(inDeutschlandamtlichKraftfahrzeug, in derSchweizamtlichMotorwagen), ist ein mehrspurigesmotorgetriebenesStraßenfahrzeugzur Beförderung von Personen oder Lasten. Umgangssprachlich nennt man „Auto“ vor allem Fahrzeuge, die zum Transport von Personen bestimmt sind; amtlich werden diese alsPersonenkraftwagen(kurz: Pkw) bezeichnet, oder – bei mehr Sitzplätzen – alsKraftomnibus. Soll ein Fahrzeug mehrheitlich Güter transportieren, heißt es amtlich „Lastkraftwagen“ (Lkw). Der weltweite Fahrzeugbestand lag im Jahr 2010 bei über 1,015 Milliarden Automobilen und stieg seitdem kontinuierlich an. 2011 wurden weltweit über 80 Millionen Automobile gebaut. In Deutschland waren im Jahr 2012 etwa 51,7 Millionen Kraftfahrzeuge zugelassen, davon sind knapp 43 Millionen Personenkraftwagen. Automobil(„Selbstbeweger“) ist ein substantiviertes Adjektiv, abgeleitet vongriechischαὐτόςautós, deutsch‚selbst‘, undlateinischmobilis‚beweglich‘. Die Wortschöpfung entstand in den 1860er-Jahren in Frankreich und diente damals – wielocomobile– zur Unterscheidung von den üblichen Landfahrzeugen, die damals von Pferden gezogen wurden. So bezeichnetefranzösischvoiture automobile‚selbstbewegender Wagen‘1875/1876 eine mit Pressluft betriebeneStraßenbahn.[1]Die Anwendung im heutigen Sinn setzte sich in den 1890er-Jahren durch: 1893 hieß es in der PublikationRadfahrergeschichtennoch mit französischem Plural: „In den Straßen von Paris wird es bald komisch aussehen. Viele Fiaker fahren bereits ohne Pferde als numerirte Automobiles umher.“[2]Anschließend wurde das Wort im Deutschen schnell heimisch, zunächst nach französischem Vorbild noch als Femininum, unter dem Einfluss der KurzformAutoaber bald schon als Neutrum.[1]Das ältere deutsche Wort war hingegenMotorwagen.[2]Die deutschen ÄquivalenteKraftfahrzeugundKraftwagenwurden 1917 amtlich eingeführt.[3] Die Definition „selbstbewegendes Fahrzeug“ würde auch motorisierteZweiräderundSchienenfahrzeugeeinschließen. In der Regel wird unter einem Automobil jedoch ein mehrspuriges und nicht schienengebundenes Kraftfahrzeug verstanden, also ein Pkw, Bus oder Lkw. In der Alltagssprache ist meist nur der Pkw gemeint. Der Darmstädter Dozent für Kraftwagen,Freiherr Löw von und zu Steinfurthversuchte, sich in seinem StandardwerkDas Automobil – sein Bau und sein Betriebüber alle Ausgaben ab 1909 hinweg an möglichst exakten Definitionen von „Automobil“. In der 5. Auflage von 1924 schreibt er: „Das Automobil ist ein Fahrzeug, das Um diese strenge Klassifizierung zu beleuchten, lässt er beispielsweise Forderung 2 weg und kommt damit „zu den sogenanntengleislosen Bahnen, die aus elektrischen Wagen bestehen, denen durch eine Oberleitung die Energie zugeführt wird.“ ImEnglischenist mit einemautomobilebzw.carnur ein Pkw gemeint. Eine Übersetzung im Sinne des zitierten von und zu Steinfurth gibt es im Englischen nicht. Das in diesem Zusammenhang oft erwähnte Wortmotor vehicleschließt auch Krafträder mit ein und ist demzufolge demdeutschenBegriff Kraftfahrzeug gleichzusetzen. Der FranzoseNicholas Cugnoterbaute 1769 einenDampfwagen– das erste bezeugte und tatsächlich erbaute Fahrzeug, das nicht auf Muskelkraft oder einer anderen äußeren Kraft (wie z. B. Wind) basierte (und kein Spielzeug war). Um 1832 sollRobert Andersonin Aberdeen einenElektrokarrengebaut haben.[5]Im Jahr 1863 machteÉtienne Lenoirmit seinem „Hippomobile“ eine 18 km lange Fahrt; es war das erste Fahrzeug mit einem Motor mit interner Verbrennung. DerTaylor Steam Buggyvon 1867 mitDampfmotorund zwei Zylindern erreichte eine Höchstgeschwindigkeit von 24 km/h und ist als das älteste kanadische Automobil. Jedoch gilt das Jahr 1886 mit demMotordreirad„Benz Patent-Motorwagen Nummer 1“ des deutschen ErfindersCarl Benzals das Geburtsjahr des „modernen“ Automobils mitVerbrennungsmotor, da es große mediale Aufmerksamkeit erregte und zu einer Serienproduktion führte. Zuvor bauten auch andere Erfinder motorisierte Gefährte mit ähnlichen oder gänzlich anderen Motorkonzepten. Z.B. wurde bereits 1881 mit demTrouvé Tricyclebereits 7 Jahre vorher das erste anerkannteElektroautogebaut. MotorisierteWagenlösten in nahezu allen Bereichen die von Zugtieren gezogenenFuhrwerkeab, da sie deutlich schneller und weiter fahren und eine höhereLeistungerbringen können. Durch diesen Vorteil steigerte sich seit dem Geburtsjahr des Automobils die Weite der zurückgelegten Strecken, u. a. deshalb wurde dem motorisierten Straßenverkehr immer mehr Raum zugestanden. Trotz der schon Ende des 19. und Anfang des 20. Jahrhunderts bekannten Vorteile der Elektromobilität setzte sich zunächst der Verbrennungsmotor als Antrieb durch.[6] Zu den wesentlichen Bestandteilen des Automobils gehören dasFahrwerkmitFahrgestellund anderen Teilen, fernerKarosserie,Motor,GetriebeundInnenraum. Europäische Pkw bestehen zu über 54 Prozent ausStahl, die Hälfte davon hochfesteStahlgüten.[7]Die Technik der Fahrzeuge müssen Ingenieure und Designer in eine funktionale, ergonomische und ästhetische Form bringen, die die Markenwerte des Herstellers vermittelt und Emotionen weckt.[8]Beim Kauf eines Autos ist dasFahrzeugdesignheute eines der wichtigsten Entscheidungskriterien.[9] Nach Zahlen derWHOsterben 1,25 Millionen Menschen jährlich[10]an den direkten Folgen vonVerkehrsunfällen. Die Sicherheit von Insassen und potenziellen Unfallgegnern von Kraftfahrzeugen ist unter anderem abhängig von organisatorischen und konstruktiven Maßnahmen sowie dem persönlichen Verhalten der Verkehrsteilnehmer. Zu den organisatorischen Maßnahmen zählen zum Beispiel Verkehrslenkung (Straßenverkehrsordnung mitVerkehrsschildernoder etwas moderner durch Verkehrsleitsysteme), gesetzliche Regelungen (Gurtpflicht, Telefonierverbot), Verkehrsüberwachung und straßenbauliche Maßnahmen. Die konstruktiven Sicherheitseinrichtungen moderner Automobile lassen sich grundsätzlich in zwei verschiedene Bereiche gliedern.Passive Sicherheitseinrichtungensollen die Folgen eines Unfalls mildern. Dazu zählen beispielsweise derSicherheitsgurt, dieSicherheitskopfstütze, derGurtstraffer, derAirbag, derÜberrollbügel, deformierbare Lenkräder mit ausklinkbaren Lenksäulen, dieKnautschzone, der Seitenaufprallschutz sowie konstruktive Maßnahmen zum Unfallgegnerschutz. Aktive Sicherheitseinrichtungen sollen einen Unfall verhindern oder in seiner Schwere herabsetzen. Beispiele hierfür sind dasAntiblockiersystem(ABS) sowie daselektronische Stabilitätsprogramm (ESP). Zu den persönlichen Maßnahmen zählen Verhaltensweisen wie eine defensive Fahrweise, das Einhalten der Verkehrsvorschriften oder Training der Fahrzeugbeherrschung, beispielsweise bei einemFahrsicherheitstraining. Diese sowie dieVerkehrserziehungspeziell für Kinder helfen das persönliche Unfallrisiko zu vermindern. Alle Maßnahmen zur Erhöhung der Verkehrssicherheit zusammen können dazu beitragen, dass die Zahl der bei einem Verkehrsunfall getöteten Personen reduziert wird. In den meisten Industrienationen sind dieOpferzahlenseit Jahren rückläufig. In Europa spielen Verkehrsunfälle als Todesursache heute eine geringere Rolle als vor einigen Jahrzehnten, die Zahl der Todesopfer liegt unter den Zahlen derDrogentotenoderSuizidenten. So fielen in Deutschland, Österreich, den Niederlanden oder der Schweiz die Opferzahlen seit den 1970er-Jahren, trotz kaum rückläufiger Zahlen derVerkehrsunfälle, auf ein Drittel. 2011 ist in Deutschland die Zahl der Verkehrstoten zum ersten Mal seit 20 Jahren wieder gestiegen,[11]in Österreich und der Schweiz allerdings auf dem historisch tiefsten Stand. Nach längerer freiwilliger Aktion wurde das Fahren mit eingeschaltetem Licht am Tag inÖsterreicham 15. November 2005 verpflichtend eingeführt und 2007 auch per Strafe eingefordert. Zum 1. Januar 2008 wurde die Lichtpflicht allerdings wieder abgeschafft.[12]Ziel dieser Kampagne war es, die menschlichen Sinneseindrücke auf die Gefahrenquellen zu fokussieren und damit die Zahl der Verkehrstoten zu verringern. Schätzungen des Bundesministeriums zufolge wurden jährlich 15 Verkehrstote weniger erwartet. Allerdings zeigte sich nicht der erwartete Effekt, da vermehrt die Aufmerksamkeit von unbeleuchteten Gefahrenquellen (Hindernisse oder andere Verkehrsteilnehmer z. B. Fußgänger) weg zu den bewegten und beleuchteten Fahrzeugen gelenkt wurde. Auch inNorwegenwurden in den Jahren nach der Einführung der Lichtpflicht 1985 deutlich mehr Verkehrstote gezählt als in den Jahren davor.[13]Trotzdem wird in einigen Ländern (etwa Deutschland) weiterhin die Einführung einer solchen Maßnahme in Erwägung gezogen. Sowohl Automobilbauer[14]und Zulieferbetriebe als auch Unternehmen aus der IT-Branche (insbesondereGoogle[15]undUber[16]) forschen und entwickeln am autonom fahrenden Kraftfahrzeug (meist Pkw). „Roboter-Autos sind feinfühligere und sicherere Autofahrer als Sie und ich“ (Chris Urmson, Googles Projektleiter undCarnegie-Mellon-Professor: heise.de:Rückenwind für autonome Autos). Erfahrungen amerikanischer Autoversicherungen würden nahelegen, dass bereits die Anzeigen der Assistenz-Sensorik das Unfallrisiko senken können.[17]Auch wird die Ansicht vertreten, dass ein gewisses Maß an Unsicherheit den Erfolg autonomer Automobile nicht verhindern wird.[18] Das „Wiener Übereinkommen über den Straßenverkehr“ von 1968 verbot lange Zeit autonome Automobile, wurde jedoch Mitte Mai 2014 von der UN geändert, so dass „Systeme, mit denen ein Pkw autonom fährt, zulässig [sind], wenn sie jederzeit vom Fahrer gestoppt werden können.“[19]Davor schrieb es unter anderem vor, dass jedes in Bewegung befindliche Fahrzeug einen Fahrer haben und dieser das Fahrzeug auch beherrschen muss. Zu klären sind insbesondere Fragen bezüglich des Haftungsrechts bei Unfällen, wenn technische Assistenzsysteme das Fahren übernehmen.[20]Im bisher dem Fortschritt zugeneigten Kalifornien, das lange Zeit liberale Regelungen für autonome Automobile hatte, wurde 2014 die gesetzliche Situation jedoch verschärft – jetzt muss immer ein Mensch am Steuer sitzen, der „jederzeit eingreifen kann“.[21]Einer Studie des Bundesministeriums für Wirtschaft und Energie zufolge rechnet man damit, dass zumindest die Automatisierung einiger Fahrfunktionen bis spätestens 2020 technisch realisierbar sein werden, während fahrerlose Fahrzeuge auf öffentlichen Straßen erst weit später zu erwarten seien.[22] Auch Fahrzeuge ohne Lenkrad, Bremse und Gaspedal werden erprobt. In diesem Zusammenhang werden Verkehrskonzepte wie ein erweitertesCarsharingdiskutiert: Man bucht das Auto übers Internet und steigt bei Bedarf zu. Keiner der Insassen benötigt eine Fahrerlaubnis. Die Gesamtbetriebskosten eines Autos setzen sich zusammen aus Fixkosten (auch „Unterhaltskosten“ genannt) und variablen Kosten (auch „Betriebskosten“ genannt), hinzu kommt der Wertverlust des Autos. Die Kosten werden von vielen Menschen unterschätzt.[23] Die Fixkosten fallen unabhängig von der jährlichen Kilometerleistung an. Sie setzen sich im Wesentlichen zusammen aus derKraftfahrzeugsteuer, den obligatorischenKraftfahrzeug-Haftpflichtversicherungen, in vielen Ländern eines zwangsweisen Mautbeitrags sowie sporadisch vorgeschriebenenTechnischen Prüfungen. Daneben können freiwillige Zusatzversicherungen abgeschlossen werden, wie eineKaskoversicherungsowie weitere Versicherungen oder zusätzliche versicherungsähnliche Leistungen, welche dieAutomobilclubsbei einer Mitgliedschaft anbieten. Die Betriebskosten hängen weitgehend von der jährlichen Kilometerleistung ab. Es entstehen Aufwände für denEnergieverbrauch(bei Verbrennungsmotoren ist das derKraftstoffverbrauch), den Ersatz von Verschleißteilen (insbesondereAutoreifen), sowie für weitere Wartung und ggf. außerplanmäßige Reparaturen. Die Wartung ist je nach Zeit und Kilometern erforderlich. Typische Zeitintervalle liegen bei 1 bis 2 Jahren, typische Kilometerintervalle bei 10.000 km bis 30.000 km.[24]Werden die Wartungsintervalle nicht eingehalten, kann das zu Schwierigkeiten mit Garantieansprüchen bei Defekten führen. Je nach individuellem Wunsch entstehen Kosten für die Fahrzeugreinigung. Nicht direkt kilometerabhängig sind Park- und Mautgebühren. Der Kaufpreis verringert sich sofort alsWertverlustauf den jeweiligen, zeitabhängigenVerkehrswert, während beimLeasingein ähnlicher Verlust durch Zinszahlungen entsteht. Statistisches BundesamtundADACveröffentlichen vierteljährlich einen Autokosten-Index. Dieser gibt an, um wie viel Prozent sich verschiedene Kostenbestandteile verteuert oder verbilligt haben.[25] Der ADAC veröffentlicht eine Voll-Kalkulation für Neuwagen, eingeteilt in 6 Klassen (Stand: 04/2018): Angeführt ist das jeweils günstigste Modell jeder Klasse.[26][27] Der Pkw-Verkehr bringtexterne Kosten, insbesondere im BereichUmweltverschmutzungund Unfallfolgekosten, mit sich. Viele der dabei betrachteten Größen sind kaum bzw. nur sehr ungefähr zu quantifizieren, weshalb verschiedene Publikationen zum Thema unterschiedlich hohe externe Kosten benennen. Eine Studie aus dem Jahr 2022 kommt zu dem Schluss, dass jeder Autofahrer im Jahr mit Rund 5.000 Euro von der Allgemeinheit subventioniert wird. Einbezogen wurden zehn verschiedene soziale oder externe Kostenarten, darunter Luftverschmutzung, Lärmbelastung, Schaffung und Erhalt der Straßeninfrastruktur, Parken im öffentlichen Straßenraum und Kosten des Klimawandels.[28][29] GemäßUmweltbundesamtbetrugen die externen Kosten im Straßenverkehr in Deutschland im Jahr 2005 insgesamt 76,946 Mrd. Euro, wovon 61,2 Mrd. auf den Personen- und 15,8 Mrd. auf den Güterverkehr entfielen. Die Unfallkosten machten dabei 52 % (entspricht 41,7 Mrd. Euro) der externen Kosten aus.[30]Das Umweltbundesamt berechnete 2007, dass Pkw in Deutschland durchschnittlich etwa 3 Cent pro Kilometer an Kosten für Umwelt und Gesundheit verursachen, die hauptsächlich durch Luftverschmutzung entstehen. Das ergibt rechnerisch Kosten von 3000 Euro für einen Pkw mit 100.000 Kilometern Laufleistung. Für Lkw betragen diese Kosten sogar 17 Cent pro Kilometer.[31]Diese externen Kosten werden nicht oder nur teilweise durch den Straßenverkehr getragen, sondern u. a. durchSteuernsowie Krankenkassen- und Sozialversicherungsbeiträge finanziert. Die Kostenunterdeckung des Straßenverkehrs (also alle durch den Straßenverkehr direkt und indirekt verursachten Kosten abzüglich aller im Zusammenhang mit dem Straßenverkehr geleisteten Steuern und Abgaben) beziffert das Umweltbundesamt für das Jahr 2005 auf rund 60 Mrd. Euro.[32] Der österreichische Pkw-Verkehr trug im Jahr 2000 nur einen Teil der von ihm verursachten Kosten: Ein großer Teil der Kosten für die Errichtung und Erhaltung der Straßen sowie derSekundärkostenwieUnfall-undUmweltkosten(Lärm, Luftschadstoffe) aller Verkehrsteilnehmer werden von der Allgemeinheit übernommen. Während der Pkw-Verkehr für 38 % der durch ihn verursachten Kosten aufkam, trugen Busse die eigenen Kosten zu 10 % und Lkw zu 21 %.[33] DasHandbuch über die externen Kosten des Verkehrsder Europäischen Union bezifferte 2019 die wirtschaftlichen Kosten einer 20.000 km langen Autofahrt wie folgt:[34] Der Pkw-Verkehr ist Forschungsgegenstand der Volkswirtschaft, namentlich derVerkehrswissenschaft. Das Automobil als industriellesMassenprodukthat den Alltag der Menschheit verändert. Seit dem Beginn des 20. Jahrhunderts hat es mehr als 2.500Unternehmengegeben, die Automobile produzierten. Viele Unternehmen, die im 19. JahrhundertEisenwarenoderStahlproduzierten, fingen Mitte des Jahrhunderts mit der Fertigung vonWaffenoderFahrrädernan und entwickelten so die Kenntnisse, die Jahrzehnte später im Automobilbau benötigt wurden. Heute gibt es neben den großen Herstellern viele kleineBetriebe, die als Automanufakturzumeist exklusive Fahrzeuge produzieren, beispielsweiseMorgan(GB). Die Bedeutung des Automobils basiert neben der vergleichsweise hohen physischen Leistungsfähigkeit des Systems auch auf der hohen Freizügigkeit in den Nutzungsmöglichkeiten bezüglich der Transportaufgaben und der Erschließung räumlicher bzw. geografischer Bereiche. Bis ins 19. Jahrhundert gab es nur wenigeFortbewegungsmittel, zum Beispiel dieKutscheoder dasPferd. Die Verbreitung derEisenbahnsteigerte zwar die Reisegeschwindigkeit, aber man war anFahrpläneund bestimmte Haltepunkte gebunden. Mit demFahrradstand ab Ende des 19. Jahrhunderts erstmals ein massentauglichesIndividualverkehrsmittelzur Verfügung, allerdings ermöglichte erst das Automobil individuelle motorisierte Fortbewegung sowie den flexiblen und schnellen Transport auch größerer Lasten. In den 1960er Jahren herrschte eine regelrechte Euphorie, woraus eine vorherrschende Meinung entstand, der gesamte Lebensraum müsse der Mobilität untergeordnet werden („Autogerechte Stadt“). Schon in den 1970er Jahren wurden einige solche Projekte jedoch gestoppt. Die Emissionen aus dem Verkehr steigen auch im Jahr 2011 immer noch und im Gegensatz zu den Brennstoffen können die vereinbarten Ziele zum Klimaschutz bei den Treibstoffen (in der Schweiz) nicht erfüllt werden.[35] Zum 1. Januar 2004 waren in Deutschland 49.648.043 Automobile zugelassen. Im Vergleich mitFußgängernund Fahrrädern, aber auch mitBussenundBahnenhat das Auto einen höheren Platzbedarf. Insbesondere in Ballungsgebieten führt dies zu Problemen durchStausund Bedarf an öffentlichen Flächen, wodurch sich einige der Vorteile des Automobils auflösen. DerGüterverkehrauf der Straße ist ein elementarer Bestandteil der heutigen Wirtschaft. So erlaubt es die Flexibilität derNutzfahrzeuge, leicht verderbliche Waren direkt zum Einzelhandel oder zum Endverbraucher zu bringen. MobileBaumaschinenübernehmen heute einen großen Teil der Bauleistungen. DieJust-in-time-Produktionermöglicht einen schnelleren Bauablauf. Beton wird inBetonwerkengemischt und anschließend mitFahrmischernzur Baustelle gebracht, mobileBetonpumpenersparen denGerüst-oder Kranbau. Der massenhafte Betrieb von Verbrennungsmotoren in Autos führt zuUmweltproblemen, einerseits lokal durchSchadstoffemissionen, die je nach Stand der Technik vielfach vermeidbar sind, andererseits global durch den systembedingtenCO2-Ausstoß, der zurKlimaerwärmungbeiträgt. Weiters argumentiert der PhilosophKilian Jörg, dass das Automobil ein katastrophales Umweltverhältnis zum Gemeinsinn macht. Natur wird demnach durch Prothesen wie das Auto zur Konsumware[36]. DieLuftverschmutzungdurch dieAbgaseder Verbrennungsmotoren nimmt, gerade in Ballungsräumen, oft gesundheitsschädigende Ausmaße an (Smog,Feinstaub). Die Kraftstoffe der Motoren beinhalten giftige Substanzen wieXylol,Toluol,BenzolsowieAldehyde. Noch giftigereBleizusätzesind zumindest in Europa und den USA nicht mehr üblich. Allein in Deutschland sterben jährlich 11.000 Menschen infolge von Luftverschmutzung durch den Straßenverkehr; Todesfälle, die potentiell vermieden werden könnten. Diese Zahl ist 3,5 Mal so hoch wie die Zahl der Todesopfer durch Unfälle.[37] Auch der überwiegend vom Automobil verursachteStraßenlärmschädigt die Gesundheit. Hinzu kommt, dass das Autofahren, besonders über längere Zeit, teilweise mitBewegungsmangelverbunden sein kann. Über die Folgen, welche vom massenhaftenReifenverschleißausgehen, ist bisher erst wenig bekannt. Ein großer Teil davon wird mit dem Regen in dieOberflächengewässergespült.[38][39]Durch das freigesetzte Ozonschutzmittel6PPDkönnenFischsterbenverursacht werden.[40]Über die Auswirkungen der chemischen Verbindungen, welche insGemüsegelangen, wurde noch nicht viel geforscht.[41] Der Verbrauch vonMineralöl, einemfossilen Energieträgerzum Betrieb konventioneller Automobile erzeugt einen CO2-Ausstoß und trägt damit zumTreibhauseffektbei. Nach Planungen derEU-Kommissionsollen bis zum Jahr 2050 Autos mit Verbrennungskraftmaschinenantrieb aus den Innenstädten Europas gänzlich verbannt werden.[42]Inzwischen will dasEU-Parlamentden Verkauf von Neuwagen mit Verbrennungsmotor ab 2035 verbieten.[43]Die EU-Umweltministerhingegen, wollen ab 2035 nur noch Neuwagen ohne CO2-Emissionen zuzulassen.[44] DerFlächenverbrauchfür Fahrzeuge und Verkehrswege verringert den Lebensraum für Menschen, Tiere und Pflanzen. Das Platz- und Parkplatzproblem derBallungsgebietezeigte sich bereits in den 1920er Jahren und schon 1929 verfolgte der deutsche Ingenieur und ErfinderEngelbert Zaschkain Berlin den Ansatz des zerlegbarenZaschka-Threewheelers(Faltauto). Dieses Stadtauto-Konzept hatte das Ziel, kostengünstig und raumsparend zu sein, indem sich das Fahrzeug nach Gebrauch zusammenklappen ließ.[45][46][47] Die Fertigung von Automobilen verbraucht darüber hinaus erhebliche Mengen an Rohstoffen, Wasser und Energie.Greenpeacegeht von einem Wasserverbrauch von 20.000 l für einen Mittelklassewagen aus.[48]Die ZeitschriftDer Spiegelberechnete 1998 für die Herstellung eines Pkw der oberen Mittelklasse (etwaMercedes E-Klasse) gar 226.000 l Wasser.[49]Die Wasserwirtschaft sieht branchenpositive 380.000 l für ein Fahrzeug als notwendig an. Das Automobil wird derzeit (2013) zu 85 Prozent recycelt und zu 95 Prozent verwertet. Bei metallischen Bestandteilen beträgt die Recyclingquote 97 Prozent.[50] Einen Überblick zur Umweltfreundlichkeit von jeweils aktuellen Pkw-Modellen veröffentlicht derVerkehrsclub Deutschland(VCD) jährlich in derAuto-Umweltliste. Zu den Gefahren des Kraftfahrzeugverkehrs beziehungsweise zu den durch dessen Umwelteinwirkungen verursachten Kosten siehe die KapitelSicherheitbzw.Externe Kosten. Die verbreitete Verwendung des Autos verändert die sozialen Räume – u. a. wurden folgende Auswirkungen in der Schweiz beklagt: Die gesamte kindliche Entwicklung wird beeinflusst.[53] Seit 1. Dezember 2011 müssen in Deutschland Neuwagen mit einer Energieverbrauchskennzeichnung versehen werden. Die Klassen reichen von A+ bis G. Der Verbrauch wird auf das Fahrzeuggewicht bezogen, womit Vergleiche nur innerhalb einer Gewichtsklasse möglich sind. Dass ein leichterer Wagen bei gleicher Benotung weniger Energie für einen Transport benötigt als ein schwererer Wagen, ist an demLabelnicht erkennbar. In Deutschland sind eine Reihe vonVerbändenentstanden, die anfangs Dienstleistungen für Autofahrer auf Gegenseitigkeit organisierten, vor allem Pannenhilfe. Heute arbeiten sie zunehmend auch alsLobby-Verbände und vertreten die Interessen der Autofahrer und der Automobilindustrie gegenüber Politik, Industrie und Medien. Bereits 1899 wurde derAutomobilclub von Deutschland(AvD) gegründet, der ein Jahr später die erste Internationale Automobilausstellung organisierte. 1911 war derAllgemeine Deutsche Automobil-Club, der ADAC, aus der 1903 gegründetenDeutschen Motorradfahrer-Vereinigungentstanden. Er ist heute mit 15 Millionen Mitgliedern Europas größter Club. Weitere Verbände in Deutschland sind derAuto Club Europa(ACE), der 1965 von Gewerkschaften gegründet wurde, sowie seit 1986 der ökologisch orientierteVerkehrsclub Deutschland(VCD), der zusätzlich auch die Interessen der anderen Verkehrsteilnehmer (Radfahrer, Fußgänger,ÖPNV-Benutzer) vertritt. Die Interessen der Automobilhersteller und deren Zulieferunternehmen vertritt derVerband der Automobilindustrie(VDA). Zu den neuen Entwicklungen gehören alternative Antriebe wie dasElektroauto(Elektrofahrzeug). Eine weitere Entwicklung ist das autonome Fahren (Autonomes Landfahrzeug). DurchCarsharingwechselt ein Auto vom Privatbesitz in einen Gemeinschaftsbesitz. Experimentell entwickelt werden zudem Prototypen vonFlugautos. In den letzten Jahren nahm dieÜberwachungder Fahrer durch ihre Autos zu. In einer Untersuchung der US-Datenschutzbestimmungen von Auto-Herstellern durch dieMozilla Foundation2023 fielen alle 25 untersuchten Automarken durch. Sechs der Hersteller erlauben sich explizit, genetische Informationen zu sammeln, Nissan und Kia gar Informationen zum Sexleben.[54] Nach Erscheinungsjahr geordnet Dieser Artikel ist als Audiodatei verfügbar: Mehr Informationen zur gesprochenen Wikipedia Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Wortherkunft 2Geschichte 3Aufbau und Form 4Sicherheit 5Autonomes Fahren 6Kosten 6.1Kosten für den Fahrzeughalter 6.1.1Fixkosten 6.1.2Betriebskosten 6.1.3Anschaffungskosten 6.1.4Beispielwerte 6.2Von der Allgemeinheit getragene Kosten 7Auswirkungen der Automobilisierung 7.1Wirtschaft 7.2Verkehr 7.3Umwelt und Gesundheit 7.4Soziale Auswirkungen 7.5Pkw-Verbrauchskennzeichnungsverordnung 7.6Interessenverbände in Deutschland 7.7Forschungseinrichtungen zum Thema Automobil 8Statistische Wirtschaftsdaten zur Automobilproduktion 9Neue Entwicklungen 10Siehe auch 11Literatur 12Weblinks 13Einzelnachweise Afrikaans Alemannisch አማርኛ Aragonés Ænglisc"
  },
  {
    "label": 0,
    "text": "Baseball – Wikipedia Baseball Inhaltsverzeichnis Spielprinzip und grundsätzliche Regeln Spielgeräte, Ausrüstung Softball Besonderheiten des Baseballsports Allgemeines zur Terminologie Spielbetrieb Historische Entwicklung Sonstiges Siehe auch Literatur Weblinks Einzelnachweise Spielfeld Innings und Spieldauer Pitcher vs. Batter Geschlagener Ball Base Stealing Positionen der Defensive Schiedsrichter Scorer Ball Schläger Handschuhe Schlaghelm Catcher-Ausrüstung Fastpitch Softball Slowpitch Softball Keine Begrenzung der Spieldauer Verbindung von Mannschafts- und Individualsportart Strategie, Wiederholung der Ausgangssituation Keine Strafen Terminologie in Deutschland Einfluss auf die amerikanische Sprache Spielbetrieb in Deutschland Spielbetrieb in Österreich Spielbetrieb in der Schweiz Spielbetrieb international Baseball in den USA Historische Entwicklung in Deutschland Entwicklung in der Schweiz Pitcherwechsel Intentional Walk Bunt Squeeze Play Weltweite Verbreitung Baseball in Nordamerika heute Baseball in Japan Baseball in Mexiko Baseball in Kuba Baseball bei Olympischen Spielen Baseball-Weltmeisterschaft Baseball-Europameisterschaft Baseballist einSchlagballspielmit zweiMannschaften. Die Verteidiger bringen einen Ball ins Spiel, den die Angreifer mit einem Schläger treffen müssen. Wurde der Ball erfolgreich getroffen, können die Angreifer durch das Ablaufen von vier Laufmalen(bases)Punkte erzielen. Die Verteidiger versuchen dies zu verhindern, indem sie den geschlagenen Ball vorher zum Laufmal werfen. Das amerikanische Baseball ist aus europäischenSchlagball-Varianten des 18. Jahrhunderts hervorgegangen. Auswanderer brachten das Spiel in dieVereinigten Staaten, wo ab Mitte des 19. Jahrhunderts die heutigen Regeln entwickelt wurden. Die wirtschaftlich stärksteProfi-Liga der Welt ist die nordamerikanischeMajor League Baseball(MLB) mit einem Umsatz von über 10 Milliarden US-Dollar.[1]Darüber hinaus genießt Baseball vor allem in TeilenLateinamerikasundOstasiensPopularität. Es folgen die wichtigsten Regeln und eine Zusammenfassung des Spielprinzips. Mehr Details enthält der ArtikelBaseballregeln. Das SpielBrennballist eine stark vereinfachte Variante von Baseball und kann als Ausgangspunkt für das Verständnis des Baseballs dienen. AuchCricketist, trotz vieler Abweichungen im Detail, relativ eng mit dem Baseball verwandt. Die Regeln des modernen Baseball-Spiels lassen sich auf ein Regelwerk zurückführen, dasAlexander Cartwrightunter dem TitelRules & Regulations of the Recently Invented Game of Base Ball as adopted by the Knickerbocker Base Ball Club on September 23, 1845[2]verfasste. Baseball wird von zwei Teams zu je neun Spielern gespielt. Mehrfach abwechselnd hat ein Team das Schlagrecht(Offense)und kannRuns(Punkte) erzielen, während das andere Team in der Verteidigung(Defense)das Feld verteidigt und den Ball schnell unter Kontrolle zu bringen versucht. Ziel des Spiels ist es, mehrRunszu erzielen als der Gegner. Die Spieler derOffenseversuchen, den von derDefensegeworfenen Ball zu schlagen und anschließend gegen den Uhrzeigersinn den nächsten sicheren Standpunkt(Base)zu erreichen. Wenn die Spieler derDefenseden Ball schneller unter Kontrolle bringen, können sie dies verhindern und der Spieler derOffensescheidet aus. Wenn ein Spieler derOffenseden Ball nicht trifft oder sich nicht auf einerBasebefindet, kann er durch Berührung mit dem Ball aus dem Spiel genommen werden(out). EinRunwird erzielt, wenn ein Spieler der Offense alle drei Bases passiert und seinen Ausgangsstandpunkt(Home Plate)wieder erreicht hat. Wenn vom Team der Offense drei Spieleroutsind, wechseln beide Mannschaften. Ein Durchgang (eine Mannschaft spielt Offense und Defense) wird alsInningbezeichnet. Nach neun Innings endet das Spiel. Das Spielfeld besteht aus zwei Teilen und wird in der Regel durch eine Umzäunung begrenzt. Das so genannteFair Territoryhat normalerweise etwa die Form eines Viertelkreises, dessen gerade Kanten als Seitenauslinien(Foul Lines)zwischen 90 und 120 Meter lang sind. Der Bereich außerhalb derFoul Lineswird alsFoul Territorybezeichnet (in der nebenstehenden Zeichnung blau dargestellt). Die meisten Aktionen finden imInfieldstatt (in der Zeichnung oliv), einem Quadrat in der Spitze des Viertelkreises von 90 Fuß (27,43 m) Kantenlänge, dessen Ecken durch die dreiBasesund dieHome Platemarkiert sind. Der Rest desFair TerritoryheißtOutfield(in der Zeichnung grün dargestellt). Die Amerikaner nennen das Spielfeld wegen der Rautenform desInfieldsauchDiamond(Diamant, Raute,Karobei Spielkarten). Ein Spielabschnitt heißt Inning und besteht aus zweiHalf Innings(Top & Bottom). Dabei ist im ersten Halbinning immer die AuswärtsmannschaftOffense. Im zweiten Halbinning ist dann die Heimmannschaft am Schlag. Beide Mannschaften dürfen dabei in ihrem offensiven Halbinning jeweils so lange schlagen, bis drei ihrer Spieler ’out’ sind. Die Spieler der Offense treten in einer vor dem Spiel von ihrer Mannschaft festgelegten und den Schiedsrichtern bekanntgegebenen Reihenfolge (Batting Order) einzeln gegen den Pitcher an. Die Batting Order ist als eine Rotation zu verstehen, das heißt nach dem neunten Spieler der Batting Order geht wieder der erste Spieler an den Schlag. Dabei wird die Batting Order am Beginn eines Halbinnings nicht von vorne begonnen, sondern es schlägt der Spieler, der auf der Liste unter dem zuletzt (im vorigen Inning) schlagenden Spieler seiner Mannschaft steht. (Ausnahme: Das dritteoutwar ein anderer Runner, z. B. durch Pick-off und der Batter selbst ist nichtout. In diesem Fall ist der Batter im nächsten Inning wieder am Schlag.) Ein Spiel besteht im Regelfall aus neun solcherInnings. Gemäß der 10-Run-Rule, welche in vielen Ligen Anwendung findet, wird das Spiel bei zehn oder mehr Punkten Führung am Ende eines Innings beendet, jedoch nicht früher als zweieinhalb Innings vor dem regulären Ende. In anderen Ligen kann es 15-Run- und 20-Run-Rules geben. Führt die Heimmannschaft am Ende des achten Innings und erzielen die Gäste in ihrem Teil des neunten nicht genügend Runs, um mindestens gleichzuziehen, so wird auf das zweite Halbinning – welches ohnehin nur noch Ergebniskosmetik liefern könnte – verzichtet, und das Spiel ist entschieden. Steht es nach der festgelegten Zahl von Innings unentschieden, so wird so lange jeweils um ein weiteres volles Inning verlängert(Extra Inning), bis eine Mannschaft gewinnt oder das Wetter, der Mangel an Pitchern oder auf unbeleuchteten Plätzen die Dunkelheit zum Abbruch bzw. zur Unterbrechung des Spiels führen. In Japan und imSpring Trainingwird ein Spiel nach einer festgelegten Anzahl von Extra Innings als Unentschieden gewertet. In einigen Wettbewerben mit relativ großen Unterschieden in der Spielstärke der beteiligten Mannschaften – darunter die Olympischen Spiele – wird ein Spiel vorzeitig beendet, wenn eine Mannschaft eine bestimmte Anzahl von Runs in Führung liegt. Diese Regelung verhindert, dass die bessere Mannschaft übermäßig lange am Schlag bleibt, weil es der schlechteren Mannschaft nicht gelingt, innerhalb eines vernünftigen Zeitraums die nötigen drei 'out’ zu erzielen. In deutschen Ligen werden teilweise auchDouble Headergespielt, also zwei Spiele hintereinander. Je nach Liga sind in Deutschland 2×5, 2×7, 1×7 oder 1×9 Innings üblich. Seit 2008 werden in der deutschen Bundesliga Double Header mit 2×9 Innings gespielt. In der US-amerikanischenMajor League Baseballwerden immer mindestens neun Innings gespielt (achteinhalb, wenn die Heimmannschaft in Führung liegt). Wenn ein Spiel abgesagt oder unterbrochen wird, etwa aufgrund schlechter Wetterverhältnisse, wird es an einem anderen Spieltag nachgeholt bzw. zu Ende gespielt, und zwar zusätzlich zu dem ursprünglich an diesem späteren Spieltag angesetzten Spiel (Double Header). Der Schiedsrichter entscheidet, ob ein Spiel wegen schlechter Wetterverhältnisse oder aus anderen Gründen abgesagt oder unterbrochen wird. In den Baseball-Ligen unterhalb der Major Leagues (die so genanntenMinor Leagues) werden Spiele hingegen unter bestimmten Umständen, etwa bei schlechten Wetterverhältnissen, vorzeitig beendet und die aktuell führende Mannschaft zum Sieger erklärt. Es müssen allerdings mindestens fünf Innings gespielt worden sein (viereinhalb, wenn die Heimmannschaft in Führung liegt), anderenfalls wird das Spiel auch hier unterbrochen und zu einem anderen Zeitpunkt fortgesetzt. Auch in den Minor Leagues werden abgesagte Spiele im Rahmen einesDouble Headersnachgeholt, allerdings werden beide Spiele auf sieben Innings verkürzt. Wenn ein Spiel dagegenunterbrochenwird (dies geschieht beispielsweise dann, wenn weniger als fünf Innings gespielt wurden, aber das Spiel aufgrund schlechter Wetterverhältnisse nicht fortgesetzt werden kann), dann wird das unterbrochene Spiel an einem der darauf folgenden Spieltage zu Ende gespielt, und danach findet das ursprünglich angesetzte Spiel in voller Länge statt. In den Minor Leagues werden im Gegensatz zu den Major Leagues auch heute noch recht häufig Spiele wegen schlechten Wetters abgesagt oder unterbrochen. Dies liegt zum Teil daran, dass die Stadien schlechter ausgestattet sind (keine Überdachung, schlechtere Beleuchtung, schlechterer Windschutz) oder die Spielfelder eine schlechtere Qualität besitzen (der Boden weicht bei Regen leichter auf). Die Eintrittskarten der Zuschauer verlieren bei abgesagten Spielen nicht ihre Gültigkeit, sondern können für den nächsten Spieltag eingelöst werden. Im Mittelpunkt des Spiels steht das Duell zwischen einemBatterderOffenseund demPitcherder Feldmannschaft. Die Spieler derOffensetreten in einer vorher festgelegten Reihenfolge (Batting Orderoder auchLineupgenannt) einzeln gegen den Pitcher an. Dieser versucht, den Ball aus gut 18 m so durch dieStrike Zonezu seinemCatcherzu werfen, dass derBatterihn mit seinem Schläger nicht oder nur schwach schlagen kann. DieStrike Zoneist der Bereich über der 43 cm breitenHome Plate, der oben und unten durch Brust- und Kniehöhe des Batters begrenzt ist. Gelingt es dem Pitcher, dreimal in die Strike Zone zu werfen, ohne dass der Batter den Ball trifft, ist der Batter out (sogenanntesStrike Out. Vom Schiedsrichter hört man in diesem Fall vielfach: „Strike three; he’s out.“). Für einenStrikemuss der Ball allerdings nicht völlig verfehlt werden. Der Pitcher bekommt auch einenStrikezugesprochen, falls der Batter einFoulschlägt. Wenn der Batter den Ball gerade noch eben – meist von unten – leicht trifft und dieser dann außerhalb des Spielfeldes aufkommt, etwa außerhalb der Seitenlinien, hinter derHome Plateoder auf der Tribüne, so ist dies einFoul(dieser Begriff ist dabei nicht im Sinne einer Unsportlichkeit oder Regelwidrigkeit zu verstehen). Ein aus dem Stadion geschlagener Ball ist ebenfalls ein Foul, wenn er das Stadion nicht im Bereich des Spielfeldes, sondern links oder rechts davon verlassen hat. Der geschlagene Ball ist allerdings erstFoul, wenn er außerhalb des Spielfelds den Boden berührt hat. Solange er sich in der Luft befindet, ist der Ball im Spiel und kann von einem Spieler der verteidigenden Mannschaft gefangen werden(Fly out). Dies kann zu spektakulären Situationen führen, wenn ein Spieler der verteidigenden Mannschaft bis zur Tribünenabsperrung läuft und hochspringt, um den Ball zu fangen, bevor er auf der Tribüne landet. Eine wichtige Ausnahme dieser Regelung ist, dass einFoulniemals als dritter Strike und somit alsStrikeoutzählen kann. Schlägt der Batter beim Stand von zweiStrikeseinFoul, bleibt es bei zweiStrikesund der Wurf wird wiederholt. Gesondert behandelt wird allerdings der so genannteFoul tip. EinFoul tipzählt im Gegensatz zumFoulimmer als Strike. Dieser wird gegeben, wenn der Batter den Ball nur hauchdünn trifft, sodass er in einer Linie in Richtung desCatchersweiterfliegt und dann direkt von diesem gefangen wird. Der Ball darf nur minimal abgelenkt werden und muss als erstes den Handschuh des Catchers berühren. Fliegt der Ball durch einen stärkeren Treffer dagegen in hohem Bogen in die Luft, bevor der Catcher ihn fängt, handelt es sich um einFly out. Trifft der Pitcher nicht in dieStrike Zone, so ist dies einBall(englische Aussprache). Hat allerdings der Batter bei einem solchenBallgeschlagen und diesen nicht getroffen, so zählt dies alsStrikezu seinem Nachteil, obwohl dieStrike Zoneverfehlt wurde. Sieht der Batter indessen noch rechtzeitig, dass der Ball dieStrike Zoneverfehlt, und hält mit dem Schwung ein(Checked Swing), so bleibt der Wurf einBall. Der Schläger darf hierbei nur soweit geschwungen werden, bis er eine gerade Linie vom Batter weg darstellt. Da dies vom Home Base Umpire (Schiedsrichter) nicht immer klar gesehen werden kann, werden die 1st oder 3rd Base Umpires um ihr Urteil gefragt, da sie oft einen besseren Einblick auf den Schwung des Schlägers haben. Die Kunst des Pitchers besteht darin, den Bällen beim Wurf einenEffetmitzugeben (Curveball,Slider,Sinking Balletc.), so dass es dem Batter erschwert wird, einzuschätzen, ob der Pitch regelgerecht ist. So erlebt man es häufig, dass der Batter seine Schlagbewegung anhält, doch vom Schiedsrichter belehrt wird, dass der Ball dieStrike Zonedurchflog und er einen Strike passieren ließ. Auch ein langsamer Wurf(Changeup)kann das Timing des Batters durcheinanderbringen, wenn er mit einem schnellen Pitch rechnet. Unterlaufen dem Pitcher gegen einenBattervierBalls, so darf dieser auf die erste Base vorrücken. Das nennt man gemeinhin einenWalkoder korrektBase on Balls; der Batter kann auf diesem Weg nämlichgehendzur ersten Base kommen. Sollte dort schon einRunnerstehen, so darf dieser auf die zweite Base vorrücken, da auf jeder Base jeweils nur ein Spieler stehen darf. Sind alle Bases besetzt(Bases loaded), dann bringt einWalkderOffensezugleich einen Punkt, da alleRunnereine Base weiterrücken, der Spieler auf der dritten Base also dieHome Plateerreicht und somit einenRunerzielt. Nicht selten wird einWalkauch absichtlich herbeigeführt und solch einIntentional Walkeingesetzt, um einem als hochklassig bekannten Batter – mit hohem Trefferdurchschnitt(Batting Average)und vielenHome Runs– lieber keinen Schlag zu ermöglichen. Auf Zeichen seines Trainers(Manager)bewegt sich der Catcher, nachdem der Ball geworfen wurde, einen Meter neben dieHome Plateund fängt dort vier vom Pitcher bewusst an der Homeplate vorbeigeworfene Bälle. Die interessantesten Situationen entstehen dann, wenn der Batter den Ball trifft und zurück ins Feld schlägt. Dadurch wird er zumRunner(Läufer) und muss zur ersten Base laufen. Wird sein geschlagener Ball von einem Feldspieler direkt aus der Luft gefangen(Fly Ball), ist der Schlagmann selbst sofortout(Fly Out). Dabei ist es unerheblich, ob der Ball im Fair- oder Foul Territory gefangen wurde. Andere Runner auf dem Feld, die bereits vor dem Fly Out ihre Base verlassen hatten, müssen zu dieser zurückkehren und dürfen diese erst verlassen und zur nächstenBaselaufen, nachdem der Ball im Handschuh des Feldspielers gelandet ist („Tag up“). Erzielt ein Läufer daraufhin einen Punkt, nennt man den (gefangenen) FlugballSacrifice Fly(wörtlich „Opferflugball“), weil durch das „Opfer“ des Schlagmanns (outzu sein) der Mitspieler zur Home Plate vorrücken konnte. Dasoutkann auch erzielt werden, wenn der mit dem Schläger nur gestreifte Ball steil hoch und dann hinter die Auslinie fliegt(foul pop). Der Ball ist dann imFoul Territoryund etwaige Runner dürfen nicht weiterlaufen, die Verteidigung – meist derCatcher– darf den Ball jedoch dort fangen und damit dasFly Outmachen. Dabei ist schon mancher Spieler, der sich ganz lang machen wollte, über die Barriere am Spielfeldrand den Zuschauern vor die Füße gefallen. Nach einem erfolgreichen Fang dürfen dieRunnerjedoch, wie bei jedemFly Out, laufen, sofern sie einTag upgemacht haben. Der neue Runner ist ebenfalls out, wenn ein Feldspieler den Ball vom Boden („Ground Ball“) aufnimmt und zum ersten Baseman wirft und dieser den Ball fängt, während er das erste Base berührt und bevor der Batter/Runner selbst dort ankommt(Ground Out). Der Feldschiedsrichter(Field Umpire)entscheidet zwischensafeoderout. Jeder Runner, deroutist, muss das Spielfeld verlassen und wieder auf der Spielerbank „(Dugout)“ Platz nehmen, bis er wieder neu als Batter an die Reihe kommt. Jeder Runner, der gerade keine Base berührt, ist auchout, wenn er von einem Feldspieler mit dem Ball selbst oder mit dem Handschuh berührt wird, in dem sich der Ball befindet(Tag Out). Das trifft nicht in den Fällen zu, in denen Läufer eine oder mehrere Bases vorrücken dürfen, ohne Gefahr zu laufen,outgemacht zu werden. Ein häufiges Beispiel hierfür ist ein Base on Balls mit Läufern auf den Bases. Hier dürfen die Läufer zur nächsten Base vorrücken und können dabei nichtoutgemacht werden. Wird ein Runner von einem geschlagenen Ball im Fair Territory getroffen, ist erout. Dies trifft nicht zu, wenn der Ball schon an einem verteidigenden Spieler vorbeigegangen ist, der diesen Ball hätte spielen können. Der Batter bekommt in diesem Fall die erste Base zugesprochen. Ein Runner istsafe, wenn er eine Base erreicht, bevor die Feldmannschaft den Ball dorthin bringen kann. Er kann jederzeit versuchen, auch zwei oder drei Bases auf einmal weiter zu laufen, es darf sich allerdings höchstens ein Runner auf jeder Base befinden. Ein Runner ist auch automatischout, wenn er einen vor ihm laufenden Runner überholt. Ein Schlag, der gut genug ist (fest oder locker geschlagen), um den Batter aus eigener Kraft eine Base erreichen zu lassen, wirdHitgenannt. Schafft es der Batter durch seinen eigenen Schlag auf die erste Base, hat er einSingleerzielt. Schafft er es zur zweiten oder dritten Base, erzielt er entsprechend einDoublebeziehungsweiseTriple. Ein Runner bleibt an einer Base, die ersafeerreicht hat, bis ein neuer Batter zum Duell gegen den Pitcher antritt. Durch dessen Schlag können alle Runner dann weiter vorrücken oder sogar einen Run erzielen, in dem sie wieder sicher an derHome Plateankommen. Schlägt einBatterden Ball über den Außenzaun hinweg, so nennt man das einenHome Run. Der Batter und alle eventuell sich gerade auf den Bases befindenden Runner dürfen die Bases in aller Ruhe ablaufen und je einen Run erzielen. Ein Schlagmann kann folglich mit einemHome Runmaximal vier Punkte für seine Mannschaft verbuchen, nämlich für dieRunnerauf der ersten, zweiten und drittenBaseund für sich selbst. Diese Maximalausbeute hat den NamenGrand Slam Homerun. Ein Home Run ist auch dann möglich, wenn der Ball das Feld nicht verlässt (ein sog.inside the park homerun). Dieser Fall ist jedoch sehr selten, weil dazu einerseits ein Batter der läuferischen Spitzenklasse (der auch ein exzellenter Kurzstreckler wäre) und zudem einHitnötig wären, bei dem sich der Ball extrem schwer unter Kontrolle bringen lässt. Meist sind dies Bälle in die wirklich hinterste Ecke des Feldes, nicht zuletzt solche, die erst nach dem Auftreffen im „Fair Territory“ insFoul Territoryrollen und damit der Regel nachlive, also noch im Spiel und nichtfoulsind. Oft wirken dabei auch besondere, für die Verteidigung unglückliche Umstände mit, etwa ein Verspringen des Balles in eine nicht zu erwartende Richtung (sogenannter „bad hop“). Allerdings ist nicht jeder Ball, der im Feld landet, damit auch schon einHit, auch wenn der Batter die erste Base erreicht. Wäre der Schlag für die Verteidigung leicht abzufangen gewesen, so spricht man von einemerror(leichter Fehler), etwa wenn ein nicht sonderlich hart geschlagener Ball direkt auf einen Verteidiger(Fielder)zufliegt und dieser ihn dennoch nicht fängt. Ob ein Error – also kein Hit – vorliegt, entscheidet übrigens keiner der Schiedsrichter(Umpire), sondern ein dafür bestellter Spielschreiber(Official Scorer). Es sollen nicht selten schon Spieler recht unglücklich mit dessen Entscheidung gewesen sein, gehen Errors doch zu Lasten ihrer Schlagstatistik(Batting Average). Auch wird kein Hit vergeben, wenn die Verteidigung den geschlagenen Ball unter Kontrolle gebracht, dann aber nicht den Batter ausmachen wollte, sondern das Out an einem vorauslaufenden Runner versucht. In diesem Fall spricht man vonFielder’s Choice(Wahl des Feldspielers), da ja im Regelfall der Batter das leichtere Out ist. Anders als die Runner kann er sich nicht schon vor dem Schlag von der Base lösen und hat somit einen längeren Weg zurückzulegen. Für den Spielstand selbst ist die Frage nach Hit oder nicht jedoch nicht von Bedeutung. Ein Runner kann jederzeit versuchen, die nächste Base zu „stehlen“, also sie zu erlaufen, auch wenn der Ball vom Batter gar nicht geschlagen wurde. Eine typische Gelegenheit ist, wenn der Pitcher seine Wurfbewegung begonnen hat. Diese darf nicht unterbrochen werden. Der Runner versucht, eher an der nächsten Base anzukommen als der Ball, der vomPitcherzumCatcherund von da aus zu der entsprechend angelaufenenBasegeworfen wird. Nicht zuletzt deshalb ist ein guterCatcherfür eine erfolgreicheDefensevon großer Bedeutung, denn er muss mögliche Spielzüge im Voraus erkennen und entscheiden, wohin der Ball am besten gespielt werden sollte, falls mehrere Gegner auf den Bases sind. Er muss erkennen, wo am leichtesten einOutzu machen ist oder aber wo dies gerade am dringendsten benötigt wird. Natürlich muss er den Ball dann auch schnell und präzise dorthin werfen. Erfüllt er seine Aufgabe, so ist es durchaus möglich, mehrere Runner in einem Spielzugoutzu machen (Double Playoder, sehr selten,Triple Play). Als taktisches Mittel istBase Stealingvor allem bei einem knappen Spielstand, besonders bei Gleichstand, interessant, dies zumal in den späten Innings, weil so durchaus ein Spiel gewonnen werden kann, etwa wenn der stärkere Teil des eigenenBatting Ordernoch folgt und man hoffen darf, ein Batter werde für einen guten Hit sorgen können. Schafft es der Runner auf diese Weise etwa von der ersten auf die zweite Base, dann kann durchaus ein folgender langer Single genügen, um das Spiel zu gewinnen. Die verteidigende Mannschaft besteht aus 9 Spielern, jeder mit einer eigenen Position auf dem Spielfeld und teilweise verschiedenen Aufgaben. Die Position ist dabei – abgesehen vonPitcher(Werfer) undCatcher(Fänger) – nicht fest, so steht beispielsweise der3rd Basemanin der Nähe der dritten Base, es kommt aber auf die aktuelle Spielsituation und den derzeitigenBatter(Schlagmann) an, ob er nun weiter imInfieldoder RichtungOutfieldorientiert ist bzw. eher rechts von der Base steht. Die Positionen und deren Aufgaben im Einzelnen: Die Nummer, die jeder Position zugeordnet ist, dient hauptsächlich der statistischen Erfassung der Spielzüge. Hierzu zwei Beispiele: Schiedsrichterheißen beim BaseballUmpire. Umgangssprachlich werden sie auch mitUmpoderBlueangesprochen. Letzteres ist auf die traditionell blauen Hemden der Umpire zurückzuführen. Ein Spiel wird in der Regel von zwei Schiedsrichtern geleitet. In den amerikanischen Profiligen sind vierUmpire(für jedeBaseeinen) an der Tagesordnung. Während derPlay-offs, also der Meisterschaftsrunde der MLB, werden sogar sechs Schiedsrichter eingesetzt, das heißt ein Schiedsrichter zusätzlich an den Foullines, um auch imOutfieldauf oder um dieFoullineauftreffende Bälle sicher zu bewerten und die EntscheidungFair BalloderFoul Ballfällen zu können. Der Hauptschiedsrichter(Plate Umpire)steht immer hinter demHome Plate, wo er entscheidet, ob derPitchereinenStrikeoder einenBallgeworfen hat. Obwohl derPlate Umpireim Zweifelsfall das letzte Wort hat, vergewissert er sich mitunter bei seinen Kollegen, bevor er seine Entscheidung fällt. Dies gilt beispielsweise bei der Frage, ob der Batter nur zum Schlag angesetzt hat(Checked Swing)oder ob er versucht hat, den Ball zu schlagen. Er muss außerdem entscheiden, ob ein Runner dieHome Platesicher erreicht, also einen Run erzielt hat. Der oder die anderen Schiedsrichter arbeiten im Feld. Wenn es nur einen oder zwei Feldschiedsrichter gibt, müssen diese jeweils in die Nähe derjenigen Base laufen, an der sie die nächste Aktion erwarten und somit eine gute Position und Entfernung zumBasewichtig ist. Auf Grund der vielen zu treffenden Tatsachenentscheidungen einerseits und des sehr umfangreichen Regelwerkes mit unzähligen Sonderregelungen andererseits werden unerfahreneUmpire(vor allem in niederen deutschen Ligen) in Diskussionen verwickelt. Während Proteste der Spieler und Trainer beispielsweise beim Fußball so gut wie nie zu einer nachträglichen Änderung der Entscheidung führen, kann es beim Baseball (wie auch beim Football) zu einer Entscheidungsänderung kommen. Bei Protesten kann es passieren, dass die Trainer der Teams zusammen mit denUmpireund eventuell auch demScorer, möglicherweise auch unter Zuhilfenahme des Regelbuches, die vergangene Spielsituation rekapitulieren und im Falle der Feststellung einer offensichtlichen Fehlentscheidung am Ende vomUmpire in Chief(oder auchCrew Chief) eine andere als die ursprüngliche Entscheidung getroffen wird. Ab der Saison 2014 wurde in der amerikanischen Profiliga der Videobeweis im Baseball als Letzte der vier großen amerikanischen Sportarten (Baseball,Basketball,FootballundEishockey) eingeführt. Hierfür wurde inNew Yorkein Kontrollzentrum eingerichtet, in dem ein Schiedsrichter jeweils maximal vier Spiele des Ligabetriebs gleichzeitig beobachtet und im Falle eines Einspruchs durch einen Manager eines Teams mittels Videoaufnahmen über die Spielsituation entscheidet. Die Manager der Teams dürfen bis zum 6. Inning maximal zweimal Einspruch gegen eine Tatsachenentscheidung aus einem von der Ligastelle definierten Katalog von Entscheidungen erheben (das zweite Mal nur bei erfolgreichem ersten Einspruch). In diesem Fall geht die Entscheidungsgewalt auf den Schiedsrichter im New Yorker Kontrollzentrum über, der die Entscheidung der Feldschiedsrichter bestätigen, widerrufen oder als nicht bewertbar (nonconclusive) einordnen kann. Diese Entscheidung ist bindend und kann nicht von einem Feldschiedsrichter geändert werden. Ab dem 7. Inning obliegt die Entscheidung, ob ein Videobeweis zur Hilfe genommen wird, den Feldschiedsrichtern. EinScoreram Spielfeldrand protokolliert alle Aktionen und Spielzüge auf einem vorgefertigten Formular, demScoresheet. Das ausgefüllte Scoresheet dient nicht nur als Spielbericht. Auf der Basis derScoring-Aufzeichnungen werden zudem umfangreicheStatistikenerstellt, die Auskunft über Spielstärke vonMannschaftenund Einzelspielern geben. Der Ball hat einen Durchmesser von etwa 7,4 Zentimetern, der Umfang muss mindestens 22,8 Zentimeter (9 inches) und darf höchstens 23,5 Zentimeter (9 1/4 inches) betragen. Das Gewicht soll nicht weniger als 141,7 Gramm (5 ounces) und nicht mehr als 148,8 Gramm (5 1/4 ounces) sein. Er ist also etwas größer als ein Tennisball. Der Baseball ist von zwei Stücken weißem Leder umhüllt, die mit roten Fäden zusammengenäht sind. Er ist nicht mit Luft aufgepumpt, sein Inneres besteht aus einem Korkkern und äußerst dicht darum gewickeltem Faden. Dadurch wird der Ball sehr hart, weshalb die Schlagmänner(Batter)schon lange Schutzhelme und dieCatcherundPlate Umpiresstabile Schutzmasken tragen. Nicht zuletzt wegen dieser Härte des Balls kommt es manchmal zu – oft handgreiflichen – Kontroversen(Charging the Mound), wenn ein vomPitchergeworfener Ball denBatterzum Ausweichen zwingt(Brushback Pitch)oder am Körper oder gar an Hals oder Kopf trifft(Beanball/Hit by Pitch). Da man jedem Pitcher so viel Präzision zutraut, dass er dies vermeiden kann, unterstellen die Gegner in solchen Fällen schnell Absicht (etwa Revanche wegen eines vom Pitcher gesehenen unfairen Verhaltens des Batters). Die in Profispielen verwendeten Bälle – insbesondere solche, mit denen herausragende Schläge erreicht wurden – gelten als begehrte Sammelstücke. Anders als etwa im Fußball dürfen in den höheren Ligen auf die Tribüne geschlagene Bälle (z. B. Homeruns oder auch Foul Balls) von den Zuschauern als Souvenir behalten werden. In manchen Stadien werfen die einheimischen Fans jedoch einen Homerun-Ball des Gegners wieder auf das Feld zurück, um ihren Unmut auszudrücken. Ein Baseballschläger besteht ausHolzoder einer Aluminiumlegierung. Selten sind auch Schläger ausCarbonanzutreffen. In Profiligen und in der deutschen ersten und zweiten Bundesliga dürfen ausschließlich Holzschläger verwendet werden, in den meisten anderen Amateurligen sind auch Schläger aus anderen Materialien erlaubt. In den US-amerikanischen Colleges sind Aluminiumschläger vorgeschrieben. Zum Warmschwingen wird ein Gewichtsring (Bat Weight, inoffiziell meistDoughnutgenannt) oder ein „Batsock“ (ein kurzer, schwerer Schlauch, der die gleiche Funktion wie derDoughnuthat) verwendet. Dieser wird auf den Schläger geschoben, um beim Warm-up direkt vor dem Schlagversuch ein höheres Gewicht am Schläger zu bekommen. Er bewirkt, dass der Schläger sich ohne denDoughnutleichter anfühlt und die Schlaggeschwindigkeit zunimmt. Jeder Spieler der Feldmannschaft trägt zum Fielden (Aufnehmen) oder zum Fangen einen Lederhandschuh, welcher das leichte und schmerzfreie Fangen des Balles ermöglicht. Die im Infield stehenden Spieler tragen, auf Grund der Schnelligkeit im Infield, einen etwas kleineren Handschuh als die drei Outfielder. Nur der Fanghandschuh desFirst Basemanist etwas größer als jene der Anderen. DerCatcherträgt einen besonders gepolsterten Handschuh(Mitt), um die vomPitcherteilweise sehr hart geworfenen Bälle zu fangen. Der Schlagmann trägt gewöhnlich ein Paar dünne Lederhandschuhe(Batting Gloves), um Blasen an den Fingern zu vermeiden, einen besseren Griff zu haben und die Vibration beim Auftreffen des Baseballs auf den Schläger zu mindern, was wiederum Schmerzen erspart. Manche Schlagmänner benutzen auch nur einen oder gar keinen Schlaghandschuh. Batter und Runner tragen Kunststoffhelme oder auch Glasfaserhelme, um vor Kopftreffern mit dem Ball geschützt zu sein. Auf der dem Pitcher zugewandten Seite bedecken diese auch das Ohr, im Jugendbaseball sogar beide Ohren. Der Catcher trägt zusätzliche Schutzausrüstung, da er hinter dem Batter in der Hocke sitzt und vor nicht getroffenen oder abgefälschten Bällen geschützt sein muss. Seine Ausrüstung besteht aus einem noch größeren und stark gepolsterten, fingerlosen Fanghandschuh, einer Gesichtsmaske, einem Helm, einem Brustschutz, einem Genitalschutz und Knie- und Schienbeinschützern. Helm und Gesichtsmaske können leicht abgeworfen werden, etwa wenn der Catcher einenFly Ballfangen muss und dafür freie Sicht braucht. Der hinter ihm leicht in der Hocke stehende Plate Umpire ist ähnlich geschützt, trägt aber keinen Handschuh. Außerdem wird die Schutzbekleidung des Plate Umpire unter der Kleidung getragen. Softball ist eine Variante von Baseball. Dabei wird der Ball vom Pitcher nicht von oben geworfen, sondern mit einer Kreisbewegung von unten. Einige Regeln unterscheiden sich vom Baseball, das Spielprinzip ist aber identisch. Baseball wird zur Unterscheidung vom Softball manchmal auchHardballgenannt. Das Spielfeld beim Softball ist etwa ein Drittel kleiner als beim Baseball. Ebenso sind die Bases nur 60 Fuß (18,29 Meter) voneinander entfernt. Die Schläger sind meistens etwas dünner und leichter. Der Name des Spiels ist irreführend: Der Ball selbst ist größer als ein Baseball, aber genauso hart. Er kann nur wegen seiner Größe nicht ganz so hart geworfen und geschlagen werden. In denUSAwird freizeitmäßig von Erwachsenen vorwiegend Softball gespielt, Baseball dagegen von Profis und männlichen Schülern und Studenten; Frauen spielen meist Softball. Es gibt zwei Versionen von Softball,Fastpitch SoftballundSlowpitch Softball. BeimFastpitch Softballkann der Ball beliebig hart geworfen werden. Diese Variante wird üblicherweise von den Damen in Schulen und Universitäten, sowie von Damen und Herren in ambitionierten Amateurligen gespielt, auch in allen deutschen Damenligen. Damen-Fastpitch-Softball ist seit 1996olympischeDisziplin. Neben den USA (Olympiasieger1996,2000,2004) gehören Japan (Olympiasieger2008) und Australien zu den besten Nationalteams der Welt. Slowpitch Softballist die „Freizeitvariante“ des Softball, bei dem der vomPitchergeworfene Ball einen deutlichen Bogen beschreiben muss. Der Ball muss unterhalb der Gürtellinie des Werfers losgelassen werden und auf dem Weg zum Schlagmann über Kopfhöhe fliegen, er beschreibt im Flug also einen „hohen Bogen“ – was ihn unter Umständen schwer zu treffen macht, denn er kann wegen des Bogens sehr steil in die Schlagzone fliegen. Base Stealing und einige andere Variationen sind beim Slowpitch Softball nicht erlaubt. Slowpitch Softball wird in Deutschland meistens in Turnierform gespielt oder als organisierter Ligabetrieb in der Rhein-Main- und Rhein-Neckar-Liga. Baseballspiele haben keine Zeitbegrenzung. Es wird eine festgelegte Zahl von Spielabschnitten (Innings) gespielt. Ein Unentschieden ist nicht möglich, da bei Gleichstand nach Absolvieren der neun bzw. sieben Innings so lange einzelneExtra Inningsgespielt werden, bis ein Sieger feststeht. In der japanischen Profiliga wurde dies geändert; hier kann ein Spiel nach dreiExtra Inningsunentschieden enden. Es ist im Baseball nicht möglich, durch besondere Spielweise eine Führung „über die Zeit zu retten“. In der Major League Baseball (MLB) wird für die Tabelle nur die reine Zahl der Spiele (gewonnen/verloren) gezählt. Es ist dennoch schon mehrfach vorgekommen, dass es zum Saisonende Gleichstand zwischen zwei Clubs gab und dann noch einOne Game Playoff(Entscheidungsspiel), bzw. eine Best-of-three-Serie in der National League vor 1969, um die Teilnahme an den Play-offs ausgetragen werden musste. Bei der in der Major League Baseball aufwendig geführten Statistik (Hits, Runs, Home Runs, Stolen Bases bzw. beim Pitching Strikeouts und Walks, um nur einen Teil zu nennen) wäre ein Gleichstand in der Abschlusstabelle unwahrscheinlich, würde man all dies werten, zumal bei der hohen Zahl von in der Regel 162 Spielen pro Saison. In Deutschland gibt es „Gnadenregeln“(Mercy Rules), nach denen bei deutlichem Vorsprung einer Mannschaft das Spiel vorzeitig beendet werden kann. Auch können die Landesverbände in den Ligen unterhalb der Verbandsliga die Spieldauer begrenzen – aber auch in diesem Fall muss bei Spielende ein Sieger feststehen. Baseball gilt gemeinhin als Mannschaftssport. Beim Duell gegen denPitchertritt einBattereinzeln an. Seine Mitspieler können seine Leistung oder den Erfolg seines Einsatzes nicht beeinflussen. Aus dieser Perspektive lässt sich Baseball auch als Kombination aus Mannschafts- und Individualsport verstehen. Das Spielprinzip von Baseball baut auf dem Duell zwischen Pitcher und Batter auf. Die Ausgangs- und Randbedingungen für diesen Zweikampf (z. B. Links- oder Rechtshänder, welche Bases sind besetzt, wie viele Strikes, wie viele Balls, wie viele Outs) wiederholen sich im Laufe eines Spieles viele Male. Das erhöht die Bedeutung vonstrategischenMitteln im Vergleich zu Sportarten, in denen aufgrund variabler Situationen eherintuitivbis taktisch gehandelt wird. Daraus resultiert auch die Möglichkeit, statistische Mittel auf die Bewertung der Leistungsfähigkeit eines Spielers oder einer Mannschaft anzuwenden. Beispielsweise entspricht derSchlagdurchschnitt(Batting Average)der Zahl der geglückten Schläge(Base Hits)dividiert durch die Zahl der Schlagversuche(At Bats). Er gilt bei ausreichend großer Anzahl von Messungen als halbwegs aussagekräftiger Parameter zur Fähigkeit eines Batters. Neben diesen traditionellen, eher intuitiven Statistiken versucht die moderne Denkschule derSabermetricsstärker für Sieg oder Niederlage aussagekräftige Statistiken durch mathematische Analysen zu begründen. Typische taktische Mittel sind etwa das Einwechseln eines anderen Pitchers, wenn ein bestimmter Batter an den Schlag kommt – z. B. die Einwechslung eines linkshändigen Pitchers gegen einen linkshändigen Batter –, oder in der Offensive das Einwechseln eines Ersatzmannes als Batter(Pinch Hitter). Auch ein anderer (schnellerer) Läufer(Pinch Runner)ist oft erwünscht, etwa bei einem knappen Spielstand, um den entscheidenden Run zu machen oder eine Base zu stehlen.Herb Washington, der ausschließlich als Pinch Runner eingesetzt wurde, bestritt auf diese Weise 131 MLB-Spiele. Hiermit bezeichnet man das bewusste Werfen von vierBallsdurch den Pitcher, um nicht gegen einen Batter pitchen zu müssen, den die verteidigende Mannschaft als besonders gefährlich einschätzt. Der Pitcher lässt damit den Batter mit Absicht zur ersten Base vorrücken, ohne ihm einen Hit zu ermöglichen. In der MLB geht der Batter direkt zur ersten Base, ohne dass ein Ball geworfen werden muss, wenn dies der Trainer der gegnerischen Mannschaft wünscht.[3]Diese Taktik wird oft gegen Spielende und bei Punktegleichstand oder einer knappen Führung eingesetzt, insbesondere dann, wenn nur noch ein oder zwei Outs nötig sind, um das Inning für die verteidigende Mannschaft zu beenden. In diesen Situationen ist es besonders wichtig, einen Punkt der gegnerischen Mannschaft zu verhindern. Mit dem Intentional Walk vermeidet der Pitcher die Konfrontation mit einem besonders starken Batter, gibt der gegnerischen Mannschaft aber durch den zusätzlichen Baserunner die Möglichkeit, bei einem nachfolgenden Hit noch mehr Runs zu erzielen. Ein Intentional Walk wird nur sehr selten eingesetzt, wenn alle Bases leer sind oder der Baserunner, der der Home Plate am nächsten ist, durch den Walk eine weitere Base vorrücken würde, denn dadurch erhöht sich die Wahrscheinlichkeit, dass die gegnerische Mannschaft bei einem darauf folgenden Hit einen Punkt erzielt. Bevorzugt wird er dagegen eingesetzt, wenn sich bereits Baserunner auf der zweiten oder dritten Base befinden, die erste Base aber unbesetzt ist. Dadurch ergibt sich für die verteidigende Mannschaft die gute Möglichkeit für ein Double Play, da dieses bei einer besetzten ersten Base am wahrscheinlichsten ist. AlsBuntbezeichnet man eine spezielle Schlagtechnik. Dies ist – fast nur – ein „Wegschieben“ des Balls, wobei der Batter blitzschnell die typische Schlaghaltung aufgibt und den Schläger in eine Schräghaltung bringt, um den Ball fast nur „abtropfen“ zu lassen. Der Ball bleibt dabei meist imInfield. Andererseits gehört es zur Taktik der Defensive, in Situationen, die nach einemBunt-Versuch „riechen“, sehr eng zum Infield hin aufzurücken, um den Ball möglichst schnell aufzunehmen und um die Laufwege zu verkürzen. Für den Batter ist der Versuch eines Bunts ein Risiko, wenn er bereits zwei Strikes hinnehmen musste. Ein Bunt, der imFoul territorylandet, zählt im Gegensatz zu einem normal geschlagenen Foul immer als Strike. Ziel desBuntals Spielzug ist es, in knappen Situationen (etwa Gleichstand im achten oder neunten Inning) mit allen Mitteln einenRunzu erzielen oder wenigstens wesentlich vorzubereiten. Mit demBuntgeht es vor allem darum, den Ball an eine Stelle zu befördern, wo es für die Gegner gerade lange genug dauert, ihn aufzunehmen und gegebenenfalls weiterzugeben, damit der/die eigenen Runner eine Base aufrücken können. Sehr häufig ist dabei derSacrifice Bunt, weil der Batter zwar wie üblich versucht, die erste Base zu erreichen, was ihm bei dem eben typischerweise kurz angelegten Ball oft nicht gelingt, da dieser doch recht schnell dorthin kommt und so dort ein out gemacht werden kann. Hat einer seiner Mitspieler unterdessen als Runner die nächste Base besetzt, ist der Zweck desSacrificeerreicht. Der Batter hat sich also für die bessere Position seiner Mannschaftskameraden „geopfert“. In manchen Fällen wird derSuicide Squeezeangewandt. Dies bedeutet, dass der Läufer aus dem Lead der dritten Base mit der Pitchbewegung in Richtung Homeplate losläuft – also noch bevor der Ball die Wurfhand des Pitchers verlassen hat. Der Batter muss nun mit aller Gewalt versuchen, den Ball ins Feld zu befördern, damit der Run zählt – auch wenn der Pitch weit an derStrike Zonevorbeigeht, wird er versuchen, diesen noch irgendwie zu treffen. In den meisten Fällen versucht der Batter den Ball zu bunten. Gelingt es dem Batter nicht, den Ball ins Feld zu befördern, hat man in einer „Selbstmordaktion“ gewissermaßen seinen Runner geopfert, da der Catcher nun einfach den Runner mit dem Ball berühren muss, bevor dieser die Homeplate erreicht. Bei einemSafety Squeezebeginnt der Runner aus dem Lead der dritten Base erst nach dem Bunt in Richtung Homeplate loszulaufen. Meistens wartet er, bis er Gewissheit hat, dass der Ball in einer Region ist, die es der Defense schwierig macht, dasoutan der Home Plate zu erreichen. Mit wenigen Ausnahmen gibt es im Baseball keine Strafen. Im Falle regelwidriger Handlungen eines Spielers lassen die Schiedsrichter meist das Spiel von der Situation aus wieder aufnehmen, die ohne regelwidrige Handlungen herbeigeführt worden wäre. Fouls (nichtFoul Balls, sondern Fouls im Sinne von gezielt eingesetzten, regelwidrigen Unsportlichkeiten) sind im Baseball recht selten. Dennoch kommt es in Begegnungen nicht selten zu Diskussionen, bei denen einzelnen Spielern vorgeworfen wird, sich unsportlich verhalten zu haben. Obwohl Baseball prinzipiell ein körperkontaktloser Sport ist, geben Situationen, bei denen sich Gegenspieler rempeln oder behindern, Anlass zu Auseinandersetzungen. Hierzu zwei typische Szenarien: Man kann sich leicht vorstellen, dass bei beiden Situationen der Unterschied zwischen einem fairen und einem übertriebenen Einsatz – oder sogar bewussten Attackieren – nicht immer einfach ist. Bei grob regelwidrigem Verhalten kann ein Spieler vom Feld gestellt werden(Ejection). Verweise vom Feld bzw. aus dem Spiel können auch gegen Spieler auf der Bank oder gegen Trainer bzw. Manager ausgesprochen werden, die sich unsportlich verhalten (z. B. Beleidigungen oder Pöbeln). Die Umpire können auch Pitcher des Feldes verweisen, die entweder ihrer Ansicht nach absichtlich oder im Laufe des Spiels häufiger Batter abgeworfen haben. Das Treffen des Batters mit dem Ball durch den Pitcher wird alsHit by Pitchbezeichnet. Baseball hat, wie jede andere Sportart, eine eigeneTerminologie. Praktisch alle Fachbegriffe stammen aus dem Ursprungsland USA und werden in deutschsprachigen Ländern unverändert verwendet. Einzelne Versuche, Fachbegriffe einzudeutschen, schlugen fehl. Heute ist die starke Anglifizierung der Baseball-Sprache in Deutschland grundsätzlich akzeptiert. In anderen Ländern wie denNiederlandenoderFrankreichhaben Begriffe in der Landessprache die englischen weitgehend ersetzt. In seinem Herkunftsland USA hat Baseball durch seine Historie die englische Alltagssprache beeinflusst, so dass heute einige Fachbegriffe in anderem Kontext verwendet werden.to touch basebedeutet so viel wiekurzen Kontakt aufnehmen,to throw someone a curve ballbedeutetjemanden auf dem falschen Fuß erwischen, wohingegen ein „soft ball“ eine (beabsichtigt) einfache Frage ist undto go to batheißt (in entsprechendem Zusammenhang)sich einsetzen, etwas bewegen, auchto go to bat for someone:jemandem helfen; sich für ihn einsetzen. Auch diverse Lebensweisheiten werden oft mit Baseballausdrücken formuliert;keep your eye on the ballbedeutet beispielsweiselass dich nicht ablenken. InKalifornienund einigen anderen US-Bundesstaaten gibt es seit Mitte der 1990er Jahre eine Gesetzgebung unter dem Motto „three strikes and you’re out“, die den Richtern bei jedem zum dritten Mal straffällig gewordenen Täter die Verhängung einer lebenslangen Freiheitsstrafe zwingend vorschreibt, auch wenn es sich um kleinere Delikte handelt. Baseball hatte in den USA bis in die 1970er Jahre eine so hohe Popularität gegenüber American Football undBasketball, dass mit den generischen BegriffenBallplayer,BallgameundBallparkauch heute noch einBaseballspieler, einBaseballspielbzw. einBaseballstadionbezeichnet wird. Baseball kommt in der amerikanischen Jugendsprache eine ganz besondere Rolle als Grundlage von Metaphern für romantische und sexuelle Aktivität zu. Dabei wird der Grad der Aktivität durch Bezüge auf die Bases im Baseball zum Ausdruck gebracht. So redet man zum Beispiel davon, mit dem Partner auf der „First Base“ gewesen zu sein, wenn man sich geküsst hat, „Second Base“ steht in der Regel für intensives Berühren des Körpers und „Third Base“ fürOralverkehr, während das Erreichen der „Fourth Base“ oder ein „Homerun“ für vollzogenenGeschlechtsverkehrsteht. Diese Metaphern sind in Amerika so weit verbreitet, dass sie allgemein verständlich sind, durch ihre harmlose, quasi-euphemistische Natur dabei aber problemlos ‚aussprechbar‘ bleiben. Die Anspielungen werden deswegen auch in Schulen zurSexualerziehungeingesetzt.[4] Heute sind knapp 30.000 Spieler in Deutschland aktiv. DerDeutsche Baseball und Softball Verbandorganisiert mit seinenLandesverbändenden Spielbetrieb in verschiedenenLigen: In Europa wird mit dem hierzulande im Sportbetrieb allgemein üblichen Auf- und Abstieg gespielt, während dieses in den USA praktisch unbekannt ist. Hinzu kommen Nachwuchsligen in denAltersklassen(Klein-)Kinder (B-Ball) (3–6 Jahre), Kinder (T-Ball) (4–8 Jahre), Schüler (9–12 Jahre), Jugend (13–15 Jahre) und Junioren (16–18 Jahre). Im Softball gibt es die Altersklassen der Kinder (5–9 Jahre), Schülerinnen (10–13 Jahre), Jugend (14–16 Jahre) und Juniorinnen (17–19 Jahre). Parallel zum Ligabetrieb wurde von 1993 bis 2006 derDBV-Pokalausgespielt. In diesem imK.-o.-Systemausgetragenen Wettbewerb wurde der Pokalsieger ermittelt. Qualifiziert waren jeweils die Gewinner der Pokalwettbewerbe der Landesverbände. Nach einem massiven Wachstum während der 1990er Jahre ist die quantitative Entwicklung des Baseball in Deutschland seit etwa 2004 rückläufig. Die Mehrzahl der deutschen Baseball-Vereine besitzen keine stabilen Führungs- und Mitgliederstrukturen. Sie sind von einem oder wenigen engagiertenEhrenamtlichenabhängig und nach deren Weggang häufig in ihrer Existenz gefährdet. Ebenso kann der Abstieg einer Mannschaft die Existenz eines gesamten Vereins gefährden. In den landesweiten Massenmedien ist deutscher Baseball praktisch nicht präsent. Der Versuch, eine deutsche Baseball-Liga durchgängig im Fernsehen zu präsentieren (1990 durch denDSF-VorgängerTele 5), schlug fehl. Dies lag unter anderem auch daran, dass Baseball grundsätzlich nur mit hohem Aufwand fernsehgerecht einzufangen ist (Zahl der Kameras etc.). Spiele der amerikanischen Profiligen (National und American League bilden zusammen die Major League Baseball) wurden aber von einigenBezahlfernsehsendernangeboten. Eine sehr ausführliche Berichterstattung, auch mit Live-Spielen, bot in Deutschland der SenderESPN America. Seit dessen Einstellung berichtet der BezahlsenderSport1 USüber die Major League Baseball und zeigt ein bis zwei Spiele pro Woche live. Zusätzlich kann man mit einemAbonnementdes Streamingdienstes mlb.tv sämtliche Spiele der amerikanischen Profivereine live empfangen. Aufgrund der mangelnden Medienpräsenz ist das Interesse von möglichen Sponsoren in der Regel gering und meist regional beschränkt. In vielen Fällen von finanzieller Unterstützung rechnen Unternehmen nicht mit einer Werbewirkung, es handelt sich daher eher umMäzenentum. Qualitativ entwickelt sich der Baseball-Sport in Deutschland stetig weiter. Die jüngsten Erfolge bei europäischen Meisterschaften (insbesondere in der Jugend) zeigen die steigende Leistungsfähigkeit des deutschen Baseball. 2019 konnte mit denHeidenheim Heideköpfeerstmals eine deutsche Mannschaft denCEB Cup(vergleichbar mit derEuropaleagueim Fußball) gewinnen. Im selben Jahr gewann die U 15-Nationalmannschaft zum dritten Mal in Folge die Junioren-EM, während die U 23-Nationalmannschaft Vizeeuropameister wurde. Heute sind knapp 3000 Spieler in Österreich aktiv. DieAustrian Baseball Federation(ABF) organisiert mit seinen Landesverbänden den Spielbetrieb in verschiedenen Ligen: Hinzu kommen noch diverse Nachwuchsligen in den Altersklassen Junioren (16–18 Jahre), Jugend (14–16 Jahre), Pony (12–14), Schüler (10–12 Jahre) und Kinder (6–10 Jahre), sowie die Österreichischen Nachwuchsmeisterschaften (Junioren, Jugend, Schüler, Kinder). Einige Teams aus derSlowakeiundUngarnnehmen an den Nachwuchsmeisterschaften der Ostliga teil. DerSchweizerische Baseball- und Softball-Verband(SBSV) wurde am 26. Juli 1981 gegründet, seit dem 19. Januar 2008 nennt er sichSwiss Baseball and Softball Federation(SBSF) mit Sitz inTherwilBL. Der Verband organisiert die folgenden Ligen: Baseball gilt nicht nur in den USA alsNationalsport, sondern auch in vielen lateinamerikanischen und ostasiatischen Ländern wieMexiko,Kuba, derDominikanischen Republik,Venezuela,Puerto Rico,Nicaragua,Panama,Japan,Südkorea, denPhilippinenundTaiwan. In Europa gab es in den letzten 20 Jahren eine beachtliche Entwicklung. Professionelle Ligen gibt es inItalienseit 1948 und den Niederlanden seit 1922. In Finnland gibt es seit 1922 eine Variante des Baseball „Pesäpallo“ genannt. Pesäpallo wird auch von Frauen gespielt. ImSport der Vereinigten Staatenhat der einst alles beherrschende Baseball nach dem Zweiten Weltkrieg allmählich immer mehr Zuschauer an American Football und in den letzten Jahrzehnten auch an Basketball verloren und wurde in den 1970er Jahren durch American Football von der Spitzenposition auf den zweiten Platz verdrängt.[5]Die durch wiederholte Streiks erkämpften extremen Spielergehälter sowie undurchsichtige Deals und Ligenumstrukturierungen der Klubbesitzer haben dem Ansehen des Baseballs in den 1990er Jahren sehr geschadet. Der Sport hat aber immer noch eine große und treue Fanbasis und eine tiefe Verankerung in der US-amerikanischen Kultur. Das gemeinsame Baseballspielen oder -schauen gilt weithin immer noch alsdasVater-Sohn-Erlebnis schlechthin und alsAmerica’s Favorite Pastime, Amerikas liebste Freizeitbeschäftigung. In der Saison 2019 besuchten knapp 68,5 Millionen Zuschauer die Spiele der Major League Baseball, kurz MLB,[6]so viele wie in keiner anderen Sportliga der Welt. Dies liegt vor allem der hohen Anzahl der Spiele (162 Spiele in der regulären Saison) und dem günstigen Terminplan zugrunde. Die Saison wird hauptsächlich im Sommer ausgetragen und somit steht Baseball nicht mit den anderen großen SportligenNFL,NBA, undNHLin Konkurrenz. Dennoch hat die Liga mit einem stetigen Zuschauerschwund zu kämpfen. Seit 2015 ist der Zuschauerschnitt um 7,14 Prozent bzw. 5,2 Millionen Fans gesunken.[7]Nach Gesamtzuschauerzahlen ist die Major League Baseball immer noch die meistbesuchte Sportliga der Welt, auch wenn die Zuschauerzahlen pro Spiel inzwischen von mehreren anderen Sportarten (so der NFL, aber auch der deutschenFußball-Bundesliga) überboten werden. Die bekanntesten Profiligen in den USA werden von der MLB organisiert. Diese teilt sich auf in dieAmerican Leagueund dieNational League. Unterhalb dieser Ligen befinden sich dieMinor Leagues, ebenfalls Profiligen, deren Teams mit je einem Major-League-Team eng assoziiert sind und ihnen als Nachwuchsligen für ihre Talente dienen. Daneben gibt es noch unabhängige Ligen, in denen professionell Baseball gespielt wird. Die Teams bestehen aus Profis, die in einem MLB-Team keinen Vertrag mehr bekommen haben, entlassenen Minor-League-Spieler, oder ehemaligen College-Spielern, die imMLB Draftnicht ausgewählt wurden. Ähnlich wie bei anderen Sportarten in den USA wird Amateur-Baseball hauptsächlich von Schulen und Universitäten betrieben. Im Vergleich zuAmerican FootballoderBasketballhat der College-Baseball eine geringe Popularität. Oft werden die besten High-School-Spieler direkt von den professionellen Teams verpflichtet, was die Qualität der College-Ligen beeinträchtigt. Ein weiterer Grund ist, dass selbst die besten College-Talente es nicht direkt in die MLB schaffen, sondern zumeist mehrere Jahre in den Minor Leagues entwickelt werden. Der Jugendbereich wird von mehreren Organisationen abgedeckt. Die bekannteste ist dieLittle League. Hier spielen Jungen und Mädchen zwischen 5 und 18 Jahren in sechs Altersklassen Softball und Baseball. Dabei wird häufig mit angepassten Regeln gespielt, so etwa auf kleineren Feldern und mit weniger Körperkontakt. In Japan wurde der Baseball 1872 durch den EnglischprofessorHorace Wilsoneingeführt. Um die Jahrhundertwende begann mit der Verbreitung an den Universitäten der Aufstieg des Baseball zum Nationalsport – nebenSumōund, in jüngerer Zeit,Fußball. In den 1920er Jahren begann der professionelle Ligabetrieb, daneben werden seit 1915 zweimal im Jahr Oberschulturniere imKōshienausgetragen, die nationale Aufmerksamkeit erhalten. Baseball ist eine sehr beliebte Sportart in Mexiko. Obwohl die Ursprünge des Sports in Mexiko zwischen den 1870er und 1890er Jahren liegen, wurde die erste Profiliga erst 1925 gegründet, dieLiga Mexicana de Béisbol(LMB). 2009 spielte die Liga mit 16 Mannschaften in zwei Divisionen. Eine weitere Profiliga, dieLiga Mexicana del Pacífico(LMP) wurde 1945 gegründet. Darüber hinaus gibt es viele regionale Ligen, die als Talentsichtung und -förderung für die Profiligen dienen. Das nationale kubanische System des Baseball besteht nicht aus einer einzelnen Liga, sondern es ist ein Überbau verschiedener Ligen und Serien unter dem Dach derKubanischen Baseballföderation. Die Föderation organisiert die nationalen Meisterschaften und die Auswahl für diekubanische Baseballnationalmannschaft. Baseball ist eine olympische Sportart und war von1992bis2008– jeweils gemeinsam mit Softball – im Programm der Olympischen Spiele. Die weltbesten Spieler nehmen jedoch nicht an den olympischen Turnieren teil, da die Profiligen bisher nicht bereit sind, dafür ihren Spielbetrieb zu unterbrechen. Beim Baseball ist, im Gegensatz zu den meisten anderen Teamsportarten, eine im Frühling beginnende und im Herbst endende Saison ohne Sommerpause üblich, sodass sich Olympia nur schlecht einfügt. Dopingskandale beim US-Profibaseball und eine als allzu lax empfundene Haltung der Ligabosse zu diesem Problem schaden zudem dem internationalen Ruf der Sportart. So entschied dasIOCam 8. Juli 2005, dass Baseball und auch Softball weiterhin gemäß Artikel 46 derOlympischen Charta„olympische Sportart“ bleiben, aber2012inLondonnicht ausgetragen werden. Damit sind Baseball und Softball die ersten aus dem Programm gestrichenen Sportarten seit 1936, alsPologestrichen wurde. Diese Entscheidung war vor allem gedacht, um Platz zu schaffen für neue Sportarten (u. a.Rugby,Golf,KarateundSquash), allerdings fand dann keine dieser Sportarten die nötige Mehrheit.2008inPekinggab es noch ein olympisches Baseball- bzw. Softballturnier, bei der IOC-Sitzung 2009 wurde der Antrag auf Wiederaufnahme ins Programm für dieOlympischen Sommerspiele 2016jedoch abgelehnt. Am 3. August 2016 beschloss das IOC, Baseball für dieOlympischen Sommerspiele 2020in Tokio wieder ins Programm zu nehmen. DieOlympischen Sommerspiele 2024in Paris finden wieder ohne Baseball statt. Bei denOlympischen Spielen 2028in Los Angeles steht Baseball dagegen wieder auf dem Programm.[8] DieInternational Baseball Federation(IBAF) arbeitete seit 2003 mit der MLB daran, zukünftig eine echte Weltmeisterschaft für Nationalmannschaften nach dem Vorbild der Fußball-Weltmeisterschaft zu veranstalten, zu der dann wirklich die besten Spieler jedes Landes kommen sollen. Diese sollte zum ersten Mal 2006 oder 2007 und danach mindestens alle vier Jahre stattfinden, jeweils etwa zwei Wochen dauern und Nationalmannschaften von allen Kontinenten umfassen. Um den Ligabetrieb nicht unterbrechen zu müssen, soll sie jeweils im März in einer zu dieser Jahreszeit ausreichend warmen Region stattfinden. Das erste Turnier sollte in den USA abgehalten werden, wobei dann nur in Stadien gespielt würde, die entweder im südlichen Teil der USA liegen oder überdacht sind. Aus finanziellen und organisatorischen Gründen musste man aber von einer in einem einzigen Land stattfindenden Meisterschaft abrücken. Von 3. bis 20. März 2006 fand in Tokio (Japan), San Juan (Puerto Rico), Orlando, Phoenix, Anaheim und San Diego (Vereinigte Staaten) zum ersten Mal die von der MLB ausgerichteteWorld Baseball Classicstatt, aus dem die Mannschaft Japans als Sieger hervorging. Auch die zweite Auflage dieser Veranstaltung im Jahre 2009 konnte die Mannschaft aus Japan für sich entscheiden. Die dritte „World Baseball Classic“-Weltmeisterschaft fand im März 2013 statt. Europameisterschaften im Baseball werden seit 1954 ausgetragen und vom europäischen Baseball-DachverbandCEBausgerichtet.2007diente die EM gleichzeitig alsOlympiaqualifikation der europäischen Teams für Peking 2008. Die letzte Europameisterschaft wurde2019inBonnundSolingenausgetragen. DieNiederlandekonnte ihren Titel verteidigen. Mit zwei Ausnahmen wurden alle bisherigen Europameisterschaften von den Teams aus den Niederlanden und aus Italien gewonnen. In England ist erstmals 1744 ein Spiel unter dem Namenbase ball, das sich wahrscheinlich aus Vorläufern des Cricket entwickelt hat, belegt. Die weitverbreitete These, es habe sich aus dem englischen SpielRoundersentwickelt, ist mittlerweile widerlegt. Der erste dokumentiert gegründete Verein in den USA waren dieNew York Knickerbockers1845. Das erste Profi-Team, dieCincinnati Red Stockings, wurde am 1. Juni 1869 gegründet. In New York wurde 1876 dieNational Leaguevon Teams aus Cincinnati, Chicago, Boston, St. Louis, Hartford, Louisville, New York und Philadelphia gegründet. In den ersten Jahren des Profibetriebs gab es noch eine ganze Reihe anderer, kurzlebiger Ligen. 1901 wurde dann dieAmerican Leaguegegründet, zunächst als Konkurrenz. Beide Ligen gelten bis heute als dieMajor Leagues. Die größten Helden der Major Leagues werden in derBaseball Hall of Famegeehrt, die sich inCooperstownim BundesstaatNew Yorkbefindet. Seit 1903 kooperierten die beiden Ligen und tragen jährlich als Finale dieWorld Seriesaus. Die älteste Beschreibung des Spiels findet sich beiJohann Christoph Friedrich GutsMuthsaus dem Jahr 1796. Das erste offizielle Baseballspiel auf deutschem Boden fand bei denOlympischen Spielen 1936inBerlinstatt. Damals verfolgte die größte und bis jetzt nicht einmal mehr annähernd erreichte Rekordkulisse von mehr als 90.000 Zuschauern ein Demonstrationsspiel zwischen zwei US-Teams imBerliner Olympiastadion. Begünstigt durch die Anwesenheit US-amerikanischer Truppen in Deutschland entwickelte sich in den 1950er Jahren eine deutsche Baseball-Gemeinde. In den Jahren nach 1968 kam der Baseballsport in Deutschland praktisch zum Erliegen und fand nur in einer deutsch-amerikanischen Liga statt, in der High-School- und Armeemannschaften spielten. Erst in den frühen 1980ern entwickelte sich der Sport wieder. 1982 wurde wieder eine deutsche Meisterschaft eingeführt. Der deutschen Nationalmannschaft gelang es zwischen 1989 und 2005 nicht, sich dauerhaft in der europäischen Elite festzusetzen: Sie pendelte mehrmals zwischen A- und B-Pool hin und her. Im Jahr 2005 erreichte das Team allerdings den vierten Platz bei der EM und konnte sich damit erstmals nach dreißig Jahren für eine Weltmeisterschaft (im Jahre 2007) qualifizieren, wo mit einem Erfolg gegen Thailand (2:0) auch der erste Sieg bei einer WM gelang. 2007 konnte der 4. Platz bei der EM wiederholt werden, wodurch auch die erneute Qualifikation für die WM (2009) erreicht wurde. 2010 wurde bei der EM im eigenen Land sogar der dritte Platz erreicht. Baseball wird in derSchweizseit 1980 gespielt. Im Sommer 1980 begann sich eine Gruppe von Gleichgesinnten regelmäßig zum Baseball auf der Zürcher Allmend zu treffen. Trainiert wurde auf der Anlage der Hammerwerfer, deren Netz diente gleich als Backstop. Ein Dodge Challenger, fahrbarer Untersatz eines Mitglieds, verlieh der Gruppe den Namen Challengers. Im November verabredeten sich die Challengers mit einer Gruppe, die sich White Sox nannte, in Reussbühl LU zum ersten Baseballspiel in der Schweiz. Das Spiel musste jedoch vorzeitig abgebrochen werden. Nicht etwa wegen einsetzenden Schneefalls: Die Bälle gingen aus. Am 5. Dezember 1980 wurde mit demChallengers Baseball Cluboffiziell der erste Baseballverein inZürichgegründet. Dessen Exponenten waren im folgenden Jahr auch an der Gründung des Schweizerischen Baseball und Softball Verbandes (SBSV) beteiligt. Die Challengers gewannen anlässlich der Premiere des nationalen Championats 1982 denSchweizer Meistertitel. Als angeblich genuin US-amerikanische Sportart hat Baseball, das auch mit Begriffen wie Unschuld und Idylle verbunden wurde,[9]auch Eingang in das Werk zahlreicher US-amerikanischer Schriftsteller gefunden, wie z. B. um die Wende vom 19. zum 20. Jahrhundert und in der ersten Hälfte des 20. JahrhundertsRing Lardner,Charles E. Van Loan,Gerald BeaumontoderDamon Runyon, die auch für Blätter wie dieSaturday Evening PostentsprechendeKurzgeschichtenverfassten. In neuerer Zeit habenBernard Malamud,Robert Coover,John Grisham,Chad HarbachoderPhilip Rothdarüber geschrieben. Teilweise werden in diesen Werken auch Auswüchse wie Bestechungsskandale oder exzessiver Starkult thematisiert. Stephen King bezeichnet in seiner Kurzgeschichte Pin Up (unter dem NamenShawshank Redemptionverfilmt) Baseball als den einzigen Sport, an dem auch Gott gefallen finden würde. In vielen Baseballspielen findet in der Mitte des siebten Innings einSeventh-inning stretchstatt, in dem die Zuschauer sich für ca. zehn Minuten strecken können. Traditionell wird hierbei das LiedTake Me Out to the Ball Gamegesungen. Infield:Pitcher (1 / P)|Catcher (2 / C)|First Baseman (3 / 1B)|Second Baseman (4 / 2B)|Third Baseman (5 / 3B)|Shortstop (6 / SS) Outfield:Left Fielder (7 / LF)|Center Fielder (8 / CF)|Right Fielder (9 / RF) Besonderes:Designated Hitter (DH)|Pinch Hitter (PH)|Pinch Runner (PR) Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Spielprinzip und grundsätzliche Regeln 1.1Spielfeld 1.2Innings und Spieldauer 1.3Pitcher vs. Batter 1.4Geschlagener Ball 1.5Base Stealing 1.6Positionen der Defensive 1.7Schiedsrichter 1.8Scorer 2Spielgeräte, Ausrüstung 2.1Ball 2.2Schläger 2.3Handschuhe 2.4Schlaghelm 2.5Catcher-Ausrüstung 3Softball 3.1Fastpitch Softball 3.2Slowpitch Softball 4Besonderheiten des Baseballsports 4.1Keine Begrenzung der Spieldauer 4.2Verbindung von Mannschafts- und Individualsportart 4.3Strategie, Wiederholung der Ausgangssituation 4.3.1Pitcherwechsel 4.3.2Intentional Walk 4.3.3Bunt 4.3.4Squeeze Play 4.4Keine Strafen 5Allgemeines zur Terminologie 5.1Terminologie in Deutschland 5.2Einfluss auf die amerikanische Sprache 6Spielbetrieb"
  },
  {
    "label": 0,
    "text": "Basketball – Wikipedia Basketball Inhaltsverzeichnis Geschichte Ausrüstung Spielprinzip Spielregeln Verbände und Ligen Varianten Medien Popkultur Siehe auch Literatur Weblinks Einzelnachweise Die Erfindung des Basketballspiels Entstehung der ersten Mannschaften College-Basketball Die Entstehung der NBA Basketball heute Basketball international Spielball Bekleidung Spielfeld Mannschaften Positionen Schiedsrichter Zeiteinteilung Punktgebung Angriff Verteidigung Begriffe Sprungball Fouls Zeitübertretungen Aus Rückspiel Schrittfehler Doppeldribbling Goaltending Fußspiel Entwicklung der Basketballregeln Spielregeln der FIBA und anderer Ligen FIBA US-amerikanische Profiligen Deutsche Verbände Internationale Verbände Streetball Einradbasketball Rollstuhlbasketball Basketball für Gehörlose Basketball für Menschen mit geistiger und mehrfacher Behinderung Beachbasketball Film Videospiel Basketballist eine meist in derHallebetriebeneBallsportart, bei der zweiMannschaftenversuchen, denBallin den jeweils gegnerischenKorbzu werfen. Die Körbe sind 3,05 Meter hoch und an den beiden Schmalseiten des rechteckigen Spielfelds angebracht. Eine Mannschaft besteht in der Regel aus fünf Feldspielern (wobei es auch andere Variationen wie die zunehmend populäre3-gegen-3-Variantegibt) und bis zu sieben Auswechselspielern, die beliebig oft wechseln können. Jeder Treffer in den Korb aus dem Spiel heraus zählt je nach Entfernung zwei oder drei Punkte. Ein getroffener Freiwurf zählt einen Punkt. Es gewinnt die Mannschaft mit der höheren Punktzahl. Basketball wurde im Jahr 1891 vom kanadischen Arzt und PädagogenJames Naismithals Hallensport ursprünglich fürYMCA-Studenten erfunden und erfuhr über den YMCA schnell weltweite Verbreitung. Seit 1936 ist die Sportart für Männer, seit 1976 auch für Frauenolympisch. Heute hat der Basketballsport global, insbesondere in denVereinigten Staaten,China, denPhilippinen,LitauenundSüdeuropaeinen hohen Stellenwert und ist auch in einigen weiteren Ländern wie z. B.Kanada,Australienund weiten Teilen Lateinamerikas sowie Mittel- und Osteuropas populär. Die weltweit mit Abstand populärste und umsatzstärkste Liga ist die nordamerikanischeNBA. Alle vier Jahre findet in einem jeweils anderen Land eineBasketball-Weltmeisterschaftstatt, die vom WeltbasketballverbandFIBAveranstaltet wird. LautFIBAspielen etwa 450 Millionen Menschen weltweit Basketball.[1]Dieerfolgreichsten Athletenzählen international zu den höchstbezahlten Profisportlern. Aufgrund der nordamerikanischen Prägung sind im Basketball viele Begriffe englisch. Ein dem Basketball ähnliches Spiel wird in einem in Frankfurt am Main erschienenen Werk \"Wahrhafftige Abconterfaytung der Wilden\" von 1591 erwähnt und wurde jahrhundertelang in Nordamerika praktiziert, wo es zum Vorbild für James A. Naismith wurde.[2][3]Basketball zählt zu den wenigen Sportarten, die von einer Einzelperson erfunden wurden. Der kanadische Arzt und PädagogeJames Naismithentwickelte das Ballspiel im Jahr 1891 inSpringfield (Massachusetts)als Hallensport für seine Studenten. Naismith hatte erkannt, dass die Kampfbetontheit in anderen Ballsportarten daher kommt, dass sich das ganze Geschehen in derselben Ebene abspielt (so z. B. imAmerican Football). Er suchte eine weniger kämpferische Sportart mit einem geringen Verletzungsrisiko, um die 18 Studenten der Klasse im Winter abzulenken. Deshalb verlagerte er die Körbe (engl.baskets) in eine andere Ebene, 1½ Meter über den Spielern. Der HausmeisterPop Stabbinsbefestigte damalsPfirsichkörbean den 10 Fuß hohen Balkonen (Emporen) derYMCATraining School in Springfield. Die damals mehr zufällig bestimmte Aufhängehöhe entspricht 3,05 Metern und ist bis heute international gültig. Die Bälle wurden mittels eines Stocks herausgeholt; erst 1906 wurde das heute noch übliche unten offene Netz eingeführt. Um zu verhindern, dass Zuschauer von der Galerie aus Korbwürfe beeinflussen konnten, wurde hinter jedem Korb ein Brett montiert. Die Schulsekretärin Lyons half Naismith bei der Erstellung der 13Grundregeln, die am 15. Januar 1892 in der Schulzeitung des Springfield College veröffentlicht wurden[4]und bis heute fast unverändert geblieben sind. Das erste offizielle Basketballspiel fand am 20. Januar 1892 in Springfield statt. In den beiden Spielhälften, mit einer Halbzeitpause von fünf Minuten, wurde meist nur ein einziger Treffer erzielt. Trotz dieser niedrigen Trefferquote setzte sich Basketball in den Vereinigten Staaten durch. Bereits im folgenden Jahr wurde Frauen-Basketball amSmith Collegeeingeführt.Senda Berenson Abbotthat den Frauen-Basketball zu dieser Zeit sehr geprägt, indem sie die von James Naismith entwickelten Grundregeln veränderte und den Frauen anpasste.[5][6]Am 22. März 1893 fand das erste Basketballspiel der Frauen am Smith College statt. Senda Berenson veröffentlichte daraufhin ein auf Frauen-Basketball spezialisiertes Magazin. In den darauffolgenden Jahren wurde zunächst mitPaneel-Bällen, die heutigen Volleybällen ähnelten, gespielt. Da damals noch die Regel galt, dass ein außerhalb des Spielfeldes gelandeter Ball in den Besitz desjenigen Teams gelangt, das ihn zuerst erreicht, sprangen die Spieler oft ohne Rücksicht auf die Zuschauer in die Ränge. Damit der Ball gar nicht erst ins Aus gelangen konnte, ging man dazu über, das Basketballfeld mit einem Käfig aus Hühnerdraht zu umzäunen. Diese „Basketball-Käfige“ gaben der Sportart ihren Spitznamen „cage game“ (englischfürKäfigspiel). Wie unangenehm das Spielen in den Käfigen war, beschrieb Barney Sedran, ein Spieler der New York Whirlwinds: „Die meisten von uns hatten ständig Schnittwunden, und der Court war mit Blut bedeckt.“[7] Zu Beginn des 20. Jahrhunderts bildeten sich erste berühmte Mannschaften, die später in dieNaismith Memorial Basketball Hall of Fameaufgenommen wurden. DieBuffalo Germanszählten zu den stärksten Mannschaften am YMCA. Im Jahr 1904 demonstrierten die Germans Basketball in Exhibition Games bei denIII. Olympischen SpieleninSt. Louis. Es ist zweifelhaft, ob Basketball überhaupt ein offizieller olympischer Wettkampf war, immerhin traten bei den über mehrere Monate laufenden Olympischen Spielen am Rande derWeltausstellungkeine Nationalmannschaften an, dies sollte erst 1936 in Berlin geschehen. Neben den Germans waren ab der zweiten Dekade des 20. Jahrhunderts dieOriginal Celticseine der einflussreichsten Mannschaften. In den 1920er Jahren wurden die Celtics durch andere Mannschaften wie dieNew York Renaissanceoder dieCleveland Rosenblumsergänzt. Im Jahr 1925 wurde dieAmerican Basketball League(kurz: ABL) gegründet, die in manchen Bereichen das Basketballspiel veränderte. Der Hühner- bzw. Metalldraht wurde abgeschafft und durch Seile ersetzt. Des Weiteren wurde das Rückbrett (engl.back-board) hinter den Körben offiziell eingeführt. Schon bald nach der Erfindung des Basketballs im Jahr 1891 konnte sich diese Sportart anCollegesund Universitäten in den Vereinigten Staaten durchsetzen. Das erste Basketballspiel zwischen zwei College-Mannschaften fand am 8. April 1893 in Beaver Falls inPennsylvaniastatt. DasGeneva Collegekonnte an diesem Tag gegen denNew Brighton YMCAgewinnen.[8]Die ersten Basketballspiele wurden zunächst mit sieben oder neun Spielern je Mannschaft ausgetragen. Am 18. Januar 1896 fand das erste Spiel mit dem aktuellen Spielsystem von lediglich fünf Spielern inIowa Citystatt, dieUniversity of Chicagogewann dieses Spiel mit 15–12 Punkten gegen dieUniversity of Iowa. In den nächsten Jahren wurde der College-Basketball innerhalb der Vereinigten Staaten immer populärer. Die anerkanntesten Universitäten (so z. B. dieColumbia University) und Colleges sponserten und unterstützten ihre Mannschaften. Aufgrund der zahlreichen College-Mannschaften wurde im Jahr 1906 der HochschulsportverbandNational Collegiate Athletic Association(kurz: NCAA) in Chicago gegründet. Nach dem Wegzug desAmateur-Athletic-Union-Turniers nach Denver gründete Naismith deswegen 1937 mitEmil Listonund anderen Bürgern der Schwesterstädte am Missouri den Vorläufer des NAIA-Turniers derNational Association of Intercollegiate Athleticsin Kansas City, Missouri. Seit Beginn der 1930er Jahre warCollege-Basketballein lukrativer Dauerbrenner imMadison Square Gardenin New York, und so entstand 1938 dasNational Invitation Tournament(NIT), das dortselbst ausgetragen wurde und das bis in die 1970er Jahre das wichtigste College-Einladungsturnier bleiben sollte. Das ersteNCAA-Division-I-Basketball-Championship-Turnier der Männer wurde erst 1939 vor 5500 Zuschauern inEvanston (Illinois)ausgetragen. DieUniversity of Oregonbesiegte im Finale die gegnerische Mannschaft derOhio State Universitymit 46–33. Eines der denkwürdigsten College-Basketballspiele, dasSecret Game, fand am 12. März 1944, an einem Sonntagvormittag, zwischen dem weißen Team der militärmedizinischen Fakultät derDuke Universityund dem schwarzen Team derNorth Carolina Central University(NCCU), damals noch North Carolina College for Negroes (NCC), inDurham (North Carolina)statt. Erst am 31. März 1996 wurde dieses Spiel einer breiten Öffentlichkeit bekannt, als Scott Ellsworth, ein Historiker und Duke-Absolvent,[9]einen Artikel in der New York Times veröffentlichte, in dem er u. a. anmerkte, das Spiel sei „symbolisch dafür, wie Widerstand gegen die Jim-Crow-Gesetze außerhalb der traditionellen Bürgerrechtsbewegung aufgetreten ist (has become symbolic of how resistance to Jim Crow occurred outside the traditional civil rights movement)“.[10]Im Jahre 2015 brachte er die Geschichte dieses Spiels unter dem TitelThe Secret Game. A Wartime Story of Courage, Change, and Basketball’s Lost Triumphals Buch heraus. Der College-Basketball verlor in den Jahren 1948 bis 1951 stark an Glaubwürdigkeit und Popularität. Dies lag vor allem an zahlreichen Skandalen und Regelverletzungen. Am schwersten wog der sogenanntePoint-shaving-Skandal, in den die Spieler zahlreicher Mannschaften verwickelt waren, die imMadison Square Gardengespielt hatten. Die Spieler hatten Bestechungsgelder angenommen, um die Punkteverteilung, auf die gewettet wurde, den Wetten entsprechend zu manipulieren. Zu den Hochschulen gehörten unter anderen das Manhattan College, dasCity College, dieBradley Universityund dieUniversity of Kentucky. Insgesamt 32 Personen wurden verurteilt und alle Spieler auf Lebenszeit gesperrt. Nicht einmal zehn Jahre später gab es einenMatch-fixing-Skandal, in den zwanzig Colleges verwickelt waren. 1981 gabHenry Hill, die Vorlage für den Mafia-FilmGoodFellasund Beteiligter amLufthansa-Raubim Dezember 1978, zu, in den 1970ern Spieler desBoston Collegedafür bezahlt zu haben, die Punkteabstände der Ergebnisse zahlreicher Spiele zu manipulieren. Der nächste Point-shaving-Skandal, in den sogar Coaches verwickelt gewesen sein sollen, ereignete sich nur kurz danach an derTulane University. Drei Studenten wurden zu Haftstrafen verurteilt und der Universitätspräsident löste die Basketballmannschaft auf – für drei Jahre.[11] Immer wieder ist auch die (verbotene) Kompensation von College-Spielern im Millionengeschäft College-Basketball ein Thema,[12]zuletzt 2017 in der Ausbeutung dieses Verbots im Skandal um den suspendierten Louisville-Head-CoachRick Pitino, den Adidas-Manager Jim Gatto und Bestechungszahlungen an und Escort Services für High-School-Spieler[13], ganz abgesehen von akademischem Betrug und gefälschten Noten. DerDamenbasketballhat sich erst spät an Colleges und Universitäten weiterentwickelt. Im Jahr 1926 hat noch die Amateur Athletic Union die erste Basketballmeisterschaft für Frauen organisiert. Und ab den 1930er Jahren wurden durch tingelnde Profi-Teams wie den afrikanisch-amerikanischen Philadelphia Tribune Girls oder den bis 1986 existierenden All American Red Heads derHall of Fameauch immer mehr Basketball-Challenge-Games zwischen Frauen und Männern veranstaltet, bei denen jedoch immer die für den Männer-Basketball vorgeschriebenen Regeln verwendet wurden. 1971, ein Jahr vorTitle IXdesEducation Amendmentsvon 1972 gegen sexuelle Diskriminierung, wurde auf College-Ebene dieAssociation for Intercollegiate Athletics for Womengegründet, in der etwa die Delta State University oder dieMighty Macsdes Immaculata College (heute Universität) große Erfolge feierten und von der die Finalspiele ab Mitte der 1970er Jahre im Fernsehen gezeigt wurden. In der letzten AIAW-Saison 1981/82 boten sowohl NCAA als auch NAIA Division-1-Konkurrenz-Meisterschaften an, die bis heute existieren, wodurch die Organisation massiv an Einfluss verlor, bis sie sich 1983 auflöste. Die letzte Profi-Liga der Damen war die von der NBA betriebeneWomen’s National Basketball Association(kurz: WNBA), die 1996 gegründet wurde und die während der Sommerpause der NBA bis heute existiert. Am 6. Juni 1946 wurde dieBasketball Association of America(kurz: BAA) gegründet.Walter Brown, der damalige Präsident derBoston Bruins, undEddie Gottlieb, Boxpromoter und Besitzer derPhiladelphia SPHAs, zählen zu den Gründungsmitgliedern dieser Liga. DiePhiladelphia Warriors(später:Golden State Warriors) gewannen die erste Finalserie der Liga mit 4–1 Siegen gegen dieChicago Stags. Im Jahr 1949 fusionierte diese Liga mit derNational Basketball League(kurz: NBL) und wurde inNational Basketball Association(kurz: NBA) umbenannt. Die bekanntesten Spieler der 1940er Jahre warenBob DaviesundGeorge Mikan. Der amerikanische UnternehmerFred Zollnerhat als Eigentümer derFort Wayne Pistons(später:Detroit Pistons) viele Änderungen im Basketball-Bereich eingeführt. Seit dem Jahr 1952 wurde seine Mannschaft mit einem Team-Flugzeug zu den Basketball-Spielen transportiert. Er half der BAA und der NBA finanziell und war an wichtigen Regeländerungen (z. B.Wurfuhr) beteiligt. Am 1. Oktober 1999 wurde er als Förderer in dieNaismith Memorial Basketball Hall of Fameaufgenommen. Einen großen Fortschritt in der weltweiten Wahrnehmung machte der Basketball im Jahr 1992, als bei den Olympischen Spielen inBarcelonaerstmals Profis zugelassen waren und die amerikanische Nationalmannschaft (auchDream Teamgenannt) ihren legendären Siegeszug antrat. In den nachfolgenden Jahren wurde die Präsenz von Basketball in den Medien immer mehr verstärkt. Namhafte Basketballspieler wieMichael Jordanrepräsentierten diese Sportart in bekannten Werbekampagnen oder auf den Titelseiten verschiedener Magazine. Der Basketballsport ist in vielen Ländern der Welt verbreitet. Die meisten europäischen Länder besitzen eine eigene Basketball-Liga und in vielen Ländern finden immer mehr Training-Camps statt, die dem normalen Spieler das Grundprinzip und die Spielweise beibringen. Neben den Meisterschaften und Spielen gibt es auch zahlreiche Events (z. B. die „And1 Mixtape“-Tour), die von Firmen gesponsert werden und der Unterhaltung dienen. Zu Beginn des 20. Jahrhunderts wurde Basketball in verschiedenen Ländern der Welt vorgestellt. Bereits 1893 fand in Paris das erste Spiel auf französischem Boden statt. Im Jahr 1902 wurden erstmals die von James Naismith verfassten Regeln in die deutsche Sprache übersetzt. Vier Jahre später wurde Basketball in Italien eingeführt. Im Jahr 1913 wurde die Ballsportart inPuerto Ricofreundlich empfangen und gefeiert. In den darauffolgenden Jahren wurde Basketball in vielen weiteren Ländern präsentiert (u. a. 1916 inBulgarienund 1917 inAlbaniensowie inGriechenland). Im Jahr 1923 fand in der Sowjetunion die erste nationale Meisterschaft der Herren statt. Bis zum Ende der 1920er Jahre erfreute sich Basketball einer steigenden Beliebtheit. Im Jahr 1930 fand vom 6. bis 14. Dezember die erste Kontinental-Meisterschaft der Herren in Südamerika statt, bei der sichUruguaygegenArgentiniendurchsetzen konnte. Der erste Schritt für die internationale Akzeptanz dieser Sportart wurde im Jahr 1930 gelegt, als dasInternationale Olympische Komitee(kurz: IOC) Basketball als olympische Sportart aufnahm. 1932 gründeten acht Nationalverbände in Genf den Basket-WeltballverbandFIBB, ab 1935 bis 1986 Fédération Internationale de Basketball Amateur [FIBA] (sieheVerbände und Ligen). Der Weltbasketballverband kontrolliert die internationalen Meisterschaften und das olympische Turnier und legt die internationalen Regeln fest. 1958 führte die FIBA den Europapokal der Landesmeister (Männer) und 1959 der Landesmeister (Frauen) ein. Ab 1932 hielt Basketball auch in Deutschland Einzug, zuerst durch Hugo Murero an derHeeressportschule Wünsdorf, danach in Breslau und Gera sowie ab 1933 in Bad Kreuznach durchHermann Niebuhr. Er hatte als Lehrer an der Deutschen Schule in Istanbul Basketball kennengelernt.[14]Er gründete 1935 die erste Basketball-Abteilung beimVfl 1848 Bad Kreuznach. Ebenfalls 1935 beteiligte sich eine deutsche Hochschulauswahl am Basketballturnier der Akademischen Weltspiele in Budapest. 1936 nahm Deutschland erst in letzter Minute am ersten olympischen Basketballturnier in Berlin teil; alle drei Spiele gingen verloren. 1939 fand die erste deutsche Meisterschaft der Männer in Hamburg statt. Den Titel gewann der Luftwaffen-Sport-Verein (LSV) Spandau. Deutschland bestritt von 1936 bis 1942 19 Länderspiele, 4 Begegnungen wurden gewonnen. Basketball unterstand imDritten Reichdem Fachamt 4 Handball/Basketball des Deutschen, später Nationalsozialistischen Reichsbundes für Leibesübungen. Nach dem Ende des Zweiten Weltkrieges fanden bereits im Herbst 1945 erste Basketballspiele statt. Nach zwei Vorgängerorganisationen wurde am 1. Oktober 1949 in Düsseldorf der „Deutsche Basketball Bund“ (DBB) als eigenständige Organisation in der Bundesrepublik gegründet. Dieser nationale Verband ist seitdem für die Ausrichtung der deutschen Basketballmeisterschaft verantwortlich. In der DDR bestand zuerst die Sektion Basketball, ab 1958 der Deutsche Basketball-Verband. 1953 entsandten beide Verbände eine gesamtdeutsche Mannschaft zur Männer-Europameisterschaft in Moskau. Mit der Gründung derBasketball-Bundesliga(kurz BBL) im Jahr 1966 wurde erstmals eine professionelle Basketball-Liga in Deutschland eingeführt. Deutschland gewann 1993 mit 71:70 in München gegen Russland die Europameisterschaft. Basketball wurde im Jahr 1936 bei denOlympischen Spielen 1936inBerlinoffiziell gespielt. Das Spiel wurde in zwei Spielhälften mit je 20 Minuten aufgeteilt und auf Tennisplätzen desReichssportfeldespraktiziert. Im Finale konnte sich die amerikanische Nationalmannschaft mit 19:8 gegen Kanada durchsetzen. Im Jahr 1950 fand die erste offizielle „Basketball-Weltmeisterschaftder Herren“ inBuenos Aires,Argentinienstatt. Die argentinische Basketballmannschaft konnte sich im Finale gegen die USA durchsetzen. Drei Jahre später fand inSantiago de Chileauch die erste offizielle „Basketball-Weltmeisterschaftder Frauen“ statt. Hier konnte sich das Team aus den USA gegen die Gastgeber behaupten. Seit1976(Montreal) spielen auch die Basketball-Frauen um olympische Medaillen. Der Basketballsport ist heute eine der meistverbreiteten Sportarten der Welt und hat in zahlreichen Regionen der Welt, darunter auch europäischen Ländern wie z. B.Spanien, den baltischen Staaten, den Staaten des ehemaligenJugoslawien,Griechenland, derTürkeiundIsrael, aber auch in einigen südamerikanischen Ländern (z. B.Mexiko,Brasilien,Venezuela,Puerto RicoundArgentinien) und einigen Ländern Südostasiens (insbesondereChina, denPhilippinenundTaiwan) sowie inAustralienundNeuseelandeinen hohen Stellenwert. Bei derBasketball-Weltmeisterschaft 2006der Herren in Japan gewann die spanische Basketballmannschaft mit 70:47 gegen die Mannschaft aus Griechenland. Bei den vorherigen Basketball-Weltmeisterschaften waren Länder wieSerbienundKroatien, Jugoslawien oder bei Frauen und MännernRusslandbzw. die Sowjetunion sehr erfolgreich.2010und2014wurde dieUS-NationalmannschaftWeltmeister,2019zum zweiten Mal Spanien. Der Spielball ist im Laufe der letzten hundert Jahre entwickelt und verbessert worden. In den ersten beiden Jahren des Basketballspiels wurde mitPaneel-Bällengespielt. Diese Paneel-Bälle waren mit aktuellen Volleybällen vergleichbar. Von 1894 bis in die 1940er Jahre wurden bei Wettkämpfen und Spielen geschnürte Basketbälle verwendet. Hierbei war bereits die „typische“ Form der Linien des Basketballs zu erkennen. Der aktuelle Basketball besteht aus synthetischem Material oder Leder sowie Nylon-Fäden. Bei Wettkämpfen für Männer hat der offizielle Spielball einen Umfang von 749 bis 780 Millimeter (Größe 7) und ein Gewicht von 567 bis 650 Gramm. In den deutschen Damen-Basketball-Ligen wird seit der Saison 2004/05 mit einem Ball gespielt, der einen Umfang von 724 bis 737 Millimeter (Größe 6) hat und 510 bis 567 Gramm wiegt. In der ersten Hälfte des 20. Jahrhunderts trugen die Spieler Trikots aus Wolle und Stoffhosen. Außerdem war das Tragen von Knie-, Ellenbogen- und Schienbeinschonern während des Spiels Pflicht. Dies lag vor allem an der harten Spielweise und dem schlechten Zustand der Arenen. In den 1960er Jahren hat sich die Bekleidung der Spieler erheblich verändert. Die Trikots wurden komfortabler und freier, und die Stoffhosen wurden durch bequeme kurze Shorts ersetzt. Auch dieBasketballschuhehaben sich seither verändert. Zunächst trugen die Spieler unpraktische Lederschuhe. In den 1980er Jahren wurden die ersten Stars von Sportartikel-Herstellern wieNike,Adidas,PumaoderConversevertraglich verpflichtet. Das Ziel des Spiels besteht darin, denSpielballdribbelnd (den Ball auf den Boden tippen) oder per Passspiel in die gegnerische Spielhälfte zu bewegen und dort in den gegnerischen Korb zu werfen. Der Basketball muss von oben in den Korb fallen, der in einer Höhe von 3,05 Metern hängt. Dabei kann der Ball auch über das Brett gespielt werden. Ein erfolgreicher Korbwurf,KorblegeroderDunkwird im Normalfall mit zwei Punkten gewertet. Ein Korbwurf von jenseits der Drei-Punkte-Linie zählt drei Punkte, ein Freiwurf einen Punkt. Sieger des Wettkampfes ist diejenige Mannschaft, die nach Ablauf der Spielzeit mehr Punkte erzielt hat als der Gegner. Bei einem Gleichstand wird eine Verlängerung von fünf Minuten gespielt. Das wird solange wiederholt, bis ein Sieger feststeht. Basketballspiele werden immer auf einem rechteckigen Spielfeld mit einer harten Oberfläche ausgetragen. Für die offiziellen Hauptwettbewerbe derFIBAmuss die Abmessung des Spielfeldes 28 Meter mal 15 Meter betragen[15]Es wird durch Kreise, Linien und Zonen, die eine eigene Funktion besitzen, unterteilt. Hier die wichtigsten davon: Auf dem Feld spielen zwei Mannschaften mit je fünf Feldspielern. Die Anfangsformation einer Mannschaft wird auch alsStarting Five(deutsch:Startende Fünfoder kürzerErste Fünf) bezeichnet. Das müssen nicht immer die fünf leistungsstärksten und besten Spieler des Teams sein. Allerdings gilt es als unbedingt notwendig, über eine starke und ausgeglichen besetzte Bank zu verfügen. Häufig sind Bankspieler Routiniers oder Rollenspieler, die in kritischen Situationen für die notwendigen – und in diesem Moment benötigten – Impulse im Spiel sorgen sollen. Dies kann z. B. durch Dreipunktwürfe, schnelles Spiel, Ballsicherung, spezielle Verteidigungsaufgaben oder einfach eine starkeReboundpräsenz erreicht werden. Spieler können unbegrenzt oft gewechselt werden. Ein Wechsel ist während jeder Spielunterbrechung möglich. In der Entstehungsgeschichte des Basketballs wurden die Spieler inAngreifer(Forwards) undVerteidiger(Guards) eingeteilt. Mit der steigenden Popularität der Sportart und dem Einführen von neuen Regeln haben sich für die fünf Spieler spezielle Aufgabenbereiche entwickelt. Die Startaufstellung der fünf aktiven Feldspieler besteht meistens aus einem körperlich großen Akteur, demCenter, zwei Forwards und zweiGuards. Es sind aber auch andere Variationen möglich, z. B. der Einsatz von drei Guards (das sogenannte „small ball“, da Guards üblicherweise die kleineren Spieler sind), wie häufig von denDetroit PistonsunterChuck Dalypraktiziert, oder aber das Spielen mit zwei Centern, wie es dieHouston RocketsMitte der 1980er taten und dieSan Antonio Spursnoch immer tun. Im deutschen Basketball kommt es jedoch des Öfteren vor, dass man dieses System etwas abwandelt, es wird meist mit zwei Centern, zwei Flügelspielern (forwards) und einem Aufbauspieler (guard) gespielt. In der NBA dominiert seit Mitte der 2010er Jahre auch durch die Dominanz des 3-Punkte-Spiels eine Spielphilosophie, die die klassischen fünf Positionen zugunsten des sog. „positionless basketball“ teilweise auflöst.[17] Der Center ist meist der größte und körperlich stärkste Spieler einer Mannschaft. Er agiert meistens in der Zone und muss möglichst viele Rebounds holen. Die Guards(Aufbauspieler)werden in „Shooting Guard“ und „Point Guard“ unterteilt. Der Shooting Guard ist auf Distanzwürfe(Drei-Punkte-Wurf)spezialisiert, während der Point Guard als Spielmacher (Playmaker)(sieheBegriffe)über den Spielzug seiner Mannschaft entscheidet. Die Forwards(Flügelspieler)werden auch in zwei weitere Positionen unterteilt: „Small Forward“ und „Power Forward“. Hierbei liegt der größte Unterschied zwischen den beiden Positionen in der Größe der Spieler. Beide Forwards sind Angriffsspieler, die wie der Center in der Zone agieren und versuchen, möglichst viele Treffer im Angriffsraum zu erzielen. Die Positionen werden vom kleinsten Spieler bis zum größten Spieler durchnummeriert, von #1 (Point Guard) bis #5 (Center). Ein Spiel wird grundsätzlich von zwei Schiedsrichtern geleitet. In derNBA, der NCAA und vielen „höheren“ nationalen und internationalen Ligen bzw. Wettbewerben kommen jedoch drei Schiedsrichter zum Einsatz. Alle Schiedsrichter sind gleichberechtigt und haben nur unterschiedliche Beobachtungsbereiche und Verantwortungen, die aber permanent wechseln. Maßgeblich ist hierbei die Position des Balls auf dem Spielfeld. Bei derZwei-Schiedsrichter-Technikist ein Unparteiischervorderer Schiedsrichter. Seine Position ist hinter der Grundlinie der verteidigenden Mannschaft. Sein Kollege nimmt alsfolgender Schiedsrichtereine Position hinter dem Angriff etwa drei Meter vom Ball entfernt ein, wobei er sich im Bereich von der linken Auslinie bis etwa zur Spielfeldmitte aufhalten muss. Außer den Schiedsrichtern gibt es noch ein Kampfgericht am so genannten Anschreibetisch. Hier sitzen Zeitnehmer (bei jedem Pfiff wird die Zeit gestoppt), 24-Sekunden-Zeitnehmer (man hat nur 24 Sekunden für einen Angriff) und Anschreiber (alle Punkte und Fouls werden im Anschreibebogen protokolliert). In Österreich lauten die Begriffe abweichendSchreiber,SchreibertischundSpielbericht. In der Schweiz schreiben die Anschreiber amSchreibertischdasMatchblatt.[18] Ein Basketballspiel besteht regulär aus Vierteln. In den Verbänden, die zur FIBA gehören, hat jedes Viertel eine Dauer von zehn Minuten. In der NBA werden pro Viertel zwölf Minuten gespielt. Steht es am Ende des vierten Viertels unentschieden, gibt es Verlängerungen zu je fünf Minuten (engl.overtime), bis eine Mannschaft als Sieger feststeht. Ursprünglich spielte man zwei Halbzeiten mit je zwanzig Minuten. Die neue Zeiteinteilung ist eine Übernahme aus derNBA, in der schon länger vier Viertel gespielt werden. Eine Ausnahme bildet die US-amerikanische Collegeliga derNCAA, bei der nach wie vor zwei Halbzeiten gespielt werden. Anders als z. B. beim Fußball wird hier nur die reine Spielzeit gezählt, die Zeit wird bei Spielunterbrechungen gestoppt, sobald der Schiedsrichter das Spiel unterbricht (z. B. bei Fouls oder Ausbällen). Die tatsächliche Dauer eines Spieles beträgt in der Regel 80 bis 100 Minuten. Nach jedem Viertel und jeder Verlängerung gibt es eine Pause von zwei Minuten, die Halbzeitpause nach dem zweiten Viertel dauert fünfzehn Minuten (FIBA). Für einen erfolgreichen Wurf werden im Normalfall zwei Punkte berechnet. Ein Wurf, bei dem sich der werfende Spieler zwingend hinter (und nicht auf) der so genanntenDrei-Punkte-Liniebefindet, bringt seiner Mannschaft drei Punkte. Die Drei-Punkte-Linie ist 6,75 Meter (seit Saison 2010/11) vom Mittelpunkt des Korbes (NBA: 7,24 m) entfernt. Bei einemFoulwährend eines Korbwurfversuches bekommt der gefoulte Spieler die gleiche Anzahl an Freiwürfen, wie Punkte mit einem erfolgreichen Wurf möglich gewesen wären. Der Freiwurf erfolgt von der Freiwurflinie (englischfree-throw line) aus, die in 5,80 Meter Entfernung parallel zur Endlinie verläuft. Ein erfolgreicher Freiwurf zählt immer einen Punkt. Wird ein Spieler unmittelbar während eines Wurfversuchs gefoult und ist der Versuch trotz des Fouls erfolgreich, werden diese Punkte regulär gezählt und der Spieler erhält zusätzlich einen Bonusfreiwurf. Somit hat er die Möglichkeit, 3 (bzw. 4) Punkte zu erzielen. Im Angriff (engl.offense) gibt es zahlreiche Varianten. Oft werden sogenannte Systeme gespielt. Dabei handelt es sich um Varianten eines eingespielten Spielzugs, in dem jeder Angreifer einen bestimmten Laufweg hat. Ziel ist es durch das Stellen von Blocks usw. einem Spieler einen freien Wurf zu ermöglichen. Gegen eine Zonenverteidigung wird die angreifende Mannschaft versuchen, eine Überzahlsituation auf einer Seite zu schaffen. Oder sie versucht viele Verteidiger auf eine Seite zu locken, um einen freien Spieler auf der anderen Seite zu erhalten. Gegen eine Mannverteidigung kann der Angreifer versuchen, sich möglichst von dem ballführenden Spieler fernzuhalten. So bindet er seinen eigenen Verteidiger und ermöglicht dem Ballführenden eine 1-gegen-1-Situation. Ein Beispiel ist die „Isolation“, wobei der Ballführer den Spielzug ansagt und alle Mitspieler ihren Mann nach außen drängen, sodass der ballführende Spieler nur gegen seinen direkten Gegenspieler („eins gegen eins“) zum Korb ziehen kann. Auf dasSystemder Verteidigung (engl.defense) bezogen unterscheidet man grundsätzlich die Zonen- von der Mannverteidigung. Bei der Zonenverteidigung handelt es sich um eineRaumdeckung. Vereinfacht gesagt hat dabei jeder Verteidiger ein bestimmtes Gebiet (z. B. vorne rechts) zu verteidigen. Vorteil ist ein sehr kompaktes Zentrum. Es wird deshalb für den Gegner schwieriger, Punkte in der Nähe des Korbes zu erzielen. Nachteilig ist die Verteidigung von Fernwürfen (z. B. Dreier). Außerdem kann der Gegner durch geschicktes Überlagern versuchen, auf einer Seite gezielt eine Überzahlsituation herbeizuführen. Bei der Mannverteidigung ist jedem Angreifer ein Verteidiger zugeordnet. Demgemäß wird ein freier Wurf von draußen schwieriger. Allerdings ist das Zentrum nicht voller Verteidiger, was den Zug zum Korb einfacher macht. Schließlich gibt es noch Mischformen, die allerdings eher selten praktiziert werden. So könnte man bspw. mit vier Mann eine Zone spielen, während einer den gegnerischen Aufbauspieler Mann zu Mann verteidigt. Das bietet sich an, wenn der Gegner einen überragenden Spieler besitzt. Die im folgenden Abschnitt beschriebenen Spielregeln beziehen sich auf die offiziellen FIBA-Regeln. Unterschiede zu Regeln anderer Ligen, wie die der NBA oder NCAA, werden hier nicht immer berücksichtigt. Nach den FIBA-Regeln beginnt jedes Spiel mit einem Sprungball, um so den ersten Ballbesitz zu entscheiden. Dabei wirft einer der Schiedsrichter den Spielball im Mittelkreis zwischen zwei gegnerischen Spielern in die Höhe, die Spieler versuchen anschließend den fallenden Ball einem Mitspieler zuzuspielen. In den nachfolgenden Vierteln wechselt der Ballbesitz und wird mit einem Richtungspfeil am Kampfgericht angezeigt. In der NBA wird der Sprungball auch bei anderen Spielsituationen eingesetzt, beispielsweise nach einem Doppelfoul mit Freiwürfen, allgemein wenn der Ballbesitz unklar ist. Der Sprungball wird dann nicht im Mittelkreis, sondern in dem Kreis (sieheSpielfeld), welcher der letzten Spielsituation am nächsten ist, ausgeführt. Man unterscheidet zwischen persönlichen, technischen, unsportlichen (früher absichtlichen) und disqualifizierenden Fouls. Technische Fouls gibt es für technische Fehler, administrative Vergehen und Disziplinlosigkeit von Spielern und Trainern. Vergehen dieser Art sind beispielsweise Meckern, zu viele Spieler auf dem Feld, Hängen am Ring, Stören des Gegners durch Gestik und Mimik (z. B. Klatschen oder Schreien während des Wurfversuchs) oder heftiges Schwingen mit den Ellbogen, auch wenn kein Kontakt entsteht.[19] Unsportliche Fouls werden verhängt, wenn der Kontakt sehr hart ist oder der Spieler keine Aussicht hat, den Ball zu spielen und es zum Kontakt kommt (z. B. Stoß mit beiden Händen in den Rücken). Seit 2008 wird bei einem Schnellangriff (englisch:Fast break) ein Kontakt von der Seite und von hinten ebenfalls als unsportliches Foul gewertet.[19] Disqualifizierende Fouls werden wegen grober Unsportlichkeit (Tätlichkeit, Beleidigung etc.) ausgesprochen. Ein Foul, bei dem die Verletzung des Gegners in Kauf genommen wird, führt je nach Schwere zu einem unsportlichen oder disqualifizierenden Foul. 1998 ist das „Vorteil-Nachteil-Prinzip“ in die Basketballregeln mit aufgenommen worden, z. B. muss das Berühren des Gegners mit den Händen kein Foul sein. Die Schiedsrichter müssen entscheiden, ob der Spieler, der den Kontakt verursacht hat, einen unfairen Vorteil davon hat (siehe „Fouls“ bzw. Artikel 33.10 der offiziellen Basketballregeln von 2008[20]). Fouls des Verteidigers Der Verteidiger begeht ein Foul durch Halten, Blockieren, Stoßen, Rempeln, Beinstellen und indem er die Bewegung eines Gegenspielers durch Ausstrecken von Hand, Arm, Ellbogen, Schulter, Hüfte, Bein, Knie oder Fuß behindert. Hat der verteidigte Angreifer gerade keinen Ball, ist durchaus ein gewisses Schieben und Zerren erlaubt. Hat der verteidigte Angreifer den Ball, sind die Möglichkeiten des Verteidigers eingeschränkt. Der angreifende Spieler darf nicht gestoßen werden, es sei denn, dieser sucht gezielt den Körperkontakt. In diesem Fall darf der Angreifer nicht mit Beinen oder Armen behindert werden, sondern nur mit dem Körper. Gute Verteidiger sind so schnell, dass sie den Angreifer ohne den Einsatz der Arme abdrängen, vielleicht sogar zum Rückwärtslaufen bringen können. Ein Angreifer mit Ball begeht ein Foul, wenn es mit einem in legaler Verteidigungsposition stehenden oder sich rückwärts bewegenden Verteidigungsspieler zu einem Kontakt kommt (offensives Foul) und der Angreifer dadurch einen unfairen Vorteil bekommt. Typische Offensivfouls sind illegale Kontakte mit dem Ellenbogen, Wegstoßen des Gegners mit dem Unterarm oder wenn der Angreifer mit der Schulter voran in den Gegenspieler läuft. Ein Angreifer ohne Ball begeht ein Foul, wenn er einen „bewegten Block“ (englisch:illegal screenodermoving pick) setzt. Stehende Blocks hingegen sind im Basketball erlaubt (im Gegensatz zum Fußball, wo das sogenannte „Auflaufenlassen“ als Foul gewertet wird). Ein weiterer Unterschied zum Foul eines Verteidigers ist, dass bei Offensivfouls (Foul der ballführenden Mannschaft) keine Freiwurfstrafen verhängt werden (Ausnahme: unsportliches Offensivfoul). Sie zählen allerdings zu den Teamfouls. Ein disqualifizierendes Foul oder zwei unsportliche Fouls führen zum Spielausschluss. Der Disqualifizierte muss die Halle sofort verlassen oder in der Mannschaftskabine das Ende des Spiels abwarten. Fünf persönliche oder technische Fouls führen zum Verlust der Spielberechtigung für das laufende Spiel (NBA: sechs Fouls). Zwei technische Fouls gegen einen Trainer, ein technisches Foul gegen einen Trainer und zwei technische Fouls gegen die Bank oder drei technische Fouls gegen die Bank führen zur Disqualifikation des Trainers.[19] Zu beachten ist, dass es sich um persönliche Strafen handelt. An der Anzahl der Spieler auf dem Feld ändert sich nichts. Grundsätzlich führt ein Foul beim erfolglosen Korbversuch je nach Position des Gefoulten zu zwei oder drei Freiwürfen. Bei einem Korberfolg mit Foul – sprich der Angreifer wird bei der Korbwurfaktion gefoult – erhält der Gefoulte die Punkte und zusätzlich einen Bonusfreiwurf. Ein Foul ohne Korbversuch führt grundsätzlich nicht zu Freiwürfen. Ausnahme: Ab dem 5. Mannschaftsfoul (alle persönlichen und technischen Fouls aller Spieler eines Teams pro Viertel, Foulgrenze) gibt es pro Verteidiger-Foul grundsätzlich zwei Freiwürfe (früher gab es in dieser Situation 1 + 1 Freiwürfe, d. h., nur wenn der erste ein Treffer war, gab es einen zweiten). Jeder Angriff darf maximal 24 Sekunden dauern (u. a. inDeutschland,USA; 30 oder 45 Sekunden sind nur in wenigen Ländern erlaubt), die auf einerWurfuhrheruntergezählt werden. Die Zeit wird dabei neu gestartet, wenn der Schiedsrichter „absichtliches Spielen des Balles mit dem Fuß“ pfeift.[21]Außerdem startet die Zeit nach jeder Ringberührung des Balles mit nun 14 Sekunden von neuem. Schließlich führt auch ein Ballwechsel, (Verteidiger erobert den Ball und wird zum Angreifer, sogenannter „Steal“) sowie ein Foul der verteidigenden Mannschaft zum Neustart der 24-Sekunden-Uhr. Hingegen führt eine Ausball-Entscheidung ohne Wechsel des Ballbesitzes nicht zum Neustart. Zu spektakulären Szenen führt folgende Besonderheit: Ein Korb zählt, wenn ein Spieler den Ball vor Ablauf der 24-Sekunden-Uhr abwirft. Das Signal ertönt dann, während der Ball sich in der Luft befindet (auch ein in der letzten Sekunde des Spieles abgeworfener Ball zählt, obwohl er den Korb erst nach Ablauf der Spielzeit erreicht). Im amerikanischen College-Basketball hat man für einen Angriff 35 Sekunden Zeit, was zu weniger Punkten als im Profi-Basketball führt. 8-Sekunden-RegelBekommt eine Mannschaft den Ball oder gibt es einen Einwurf, so muss sie innerhalb von acht (bei 24 Sekunden) Sekunden den Ball in die gegnerische Hälfte bringen. Gelingt ihr das nicht, gibt es einen Einwurf für den Gegner an der Mittellinie. 3-Sekunden-RegelWährend eines Angriffs dürfen sich die Spieler der angreifenden Mannschaft nicht länger als drei Sekunden ununterbrochen in der gegnerischen Zone (im Freiwurfraum) aufhalten, unabhängig davon, ob der jeweilige Spieler im Ballbesitz ist oder nicht. Hier ist aber anzumerken, dass kein Schiedsrichter die drei Sekunden mit der Uhr stoppt. Sie werden (wie die 8-Sekunden) „im Kopf“ gezählt oder nach Gefühl entschieden. Auf hochklassigem Niveau wird selten aufgrund dieser Regel gemaßregelt. Mittlerweile sind die Schiedsrichter angewiesen, den 3-Sekunden-Verstoß nicht zu ahnden, wenn ein Angreifer zwar mehr als drei Sekunden in der Zone steht, aber während dieser Zeit nicht aktiv ins Spiel eingreift. Wenn ein Spieler den Ball erhält, nachdem er bereits drei Sekunden oder länger in der Zone gestanden hat, ist dies eine Regelübertretung. Nachsicht wird mit einem Spieler geübt, wenn dieser zwei Sekunden in der Zone ist, aber er sofort auf den Korb wirft oder zum Korbleger ansetzt.[22] 5-Sekunden-RegelEin Spieler darf beim Einwurf den Ball nur maximal fünf Sekunden festhalten, bis er den Einwurf ausführt. Im Spiel muss er nach fünf Sekunden einen Korbwurf machen, anfangen zu dribbeln oder den Ball abgeben, wenn er nah bewacht wird. Sollte eine dieser Regeln verletzt werden, so erhält die gegnerische Mannschaft den Ball durch Einwurf an der nächstgelegenen Auslinie. Auf Aus wird entschieden, wenn Ball oder ballführender Spieler auf oder außerhalb der Auslinie den Boden berühren. Der Ball ist hingegen nicht im Aus, wenn er sich außerhalb der Auslinie in der Luft befindet. Ein Spieler, der innerhalb des Spielfelds abspringt, kann ihn ins Spiel zurückpassen, solange Spieler oder Ball nicht den Boden berühren. Bei einem Angriff darf der Spielball von keinem Spieler der ballführenden Mannschaft von der gegnerischen Hälfte (Vorfeld) in die eigene Spielfeldhälfte (Rückfeld) zurückgespielt werden. Geschieht dies doch, ist das ein Regelverstoß, das sogenannte „Rückspiel“ (engl.backcourt violation). Als Rückspiel wird dabei jede Ballbewegung über die Mittellinie gewertet, es ist also egal, ob gepasst oder gedribbelt wurde. Der Ball ist bei einem Dribbling erst dann im Vorfeld, wenn sowohl Ball als auch beide Füße des Dribbelnden Kontakt mit dem Vorfeld haben. Ein Verstoß dagegen wird mit einem Einwurf der gegnerischen Mannschaft von der Seitenlinie her bestraft; dies nächst der Stelle, an der der Spieler den Ball im Rückfeld berührt. Ausnahmen: Es ist kein Rückspiel, wenn ein Verteidiger den Ball ins Rückfeld der angreifenden Mannschaft zurücktippt (beim „Tippen“ des Balles wechselt nicht der Ballbesitz) oder der Ball von einem angreifenden Spieler ins Rückfeld gepasst und von einem gegnerischen Spieler abgefangen wird (der Einwurf entfällt hier logischerweise, da der Ballbesitz sowieso wechselt). Es gilt nicht als Rückspiel, wenn ein Spieler im Vorfeld abspringt und den Ball im Flug zurück ins Vorfeld spielt. Des Weiteren darf ein Verteidiger im Vorfeld abspringen, den Ball fangen und in seinem Rückfeld landen. Dagegen darf er nicht in dieser Situation den Ball zu einem Mitspieler tippen bzw. passen. Der ballführende Spieler muss dribbeln (den Ball auf den Boden tippen), wenn er sich fortbewegen will. Tut er dies nicht, wird auf Schrittfehler (travelling) entschieden und der Gegner bekommt Einwurf an der Seitenlinie. Nach den FIBA-Regeln muss zuerst gedribbelt werden, die NBA-Regeln erlauben, zuerst den Schritt und dann das Dribbling zu machen. Nachdem er aufhört zu dribbeln und noch in der Bewegung, d. h. beim Laufen ist, darf er noch zwei Bodenkontakte mit den Füßen haben, bevor er passt oder auf den Korb wirft. Dabei darf das Standbein zum Zwecke des Passes oder Wurfes angehoben, aber nicht wieder aufgesetzt werden (z. B. beim aufgelöstenSternschritt). Sobald ein Angreifer den Ball nach einem Dribbling (Tippen des Balles auf den Boden) aufnimmt, darf er nicht erneut zum Dribbling ansetzen. Ein Verstoß gibt Einwurf für den Gegner von der Seitenlinie. Eine andere Version hiervon istCarrying: In diesem Fall dreht der Spieler während des Dribbelns seine Hand um, sodass die Handunterdem Ball ist. Die Folgen sind dieselben wie beim normalen Doppeldribbling. Das sogenannte „Fumbling“ zählt nicht als Dribbling. Dabei tippt der Ball zwar auf dem Boden auf, aufgrund der fehlenden Ballkontrolle ergibt sich daher aber keine Regelübertretung. Es ist nur erlaubt, einen vom Gegner gezielt auf den Korb geworfenen Ball aus der Luft zu fangen oder zu blocken, solange er sich in der Aufwärtsbewegung befindet. Hat er den Scheitelpunkt seines Fluges erreicht oder befindet sich bereits im Sinkflug und vollständig über Ringniveau, muss der Ball erst den Korb berühren, bevor er wieder frei spielbar ist. Etwas anderes gilt nur, wenn der Ball offensichtlich danebengeht. Anfangs gab es diese Regel nicht und so gingen sehr groß gewachsene Spieler dazu über, sich unter den eigenen Korb zu stellen und alle Würfe abzufangen. Eine weitere Form desGoaltendingliegt darin, den Ball zu blocken, nachdem er das Brett berührt hat und er sich vollständig über Ringniveau befindet. Berührt ein Ball bei einem Korbwurf das Brett, ist er nicht frei, außer es ist offensichtlich, dass er danebengeht. Dabei ist es im Gegensatz zum Wurf ohne Brett egal, ob er sich noch in der Aufwärts- oder schon in der Abwärtsbewegung befindet. Ein Spieler begehtGoaltendingbei einem Freiwurf, wenn er den Ball auf dem Flug zum Korb berührt, bevor dieser den Ring berührt. Auch das Greifen ins Netz oder Schlagen ans Brett durch einen Verteidiger, der dadurch einen Korb verhindert, kann man im weiteren Sinne alsGoaltendingbezeichnen. Folge vonGoaltendingist, dass der angreifenden Mannschaft der Korbversuch als Korberfolg gewertet wird. Die angreifende Mannschaft kann auchGoaltendinggepfiffen bekommen, die Voraussetzungen sind die gleichen. Das nennt man dannOffensive Interfence. Als Fußspiel bezeichnet man das absichtliche Berühren des Balles mit dem Fuß, Knie oder Bein. Wird das Fußspiel von einem Defensivspieler begangen, wird dieShot Clock, sofern mehr als 10 Sekunden vergangen sind, auf 14/24 Sekunden gesetzt (14 im Vorfeld, 24 im Rückfeld). Sind nicht mehr als 10 Sekunden vergangen, spielt man mit der bestehenden Zeit weiter. Sofern dagegen ein Offensivspieler das Fußspiel begeht, bekommt die gegnerische Mannschaft den Ball und die vollen 24 Sekunden eines neuen Angriffs. Am 15. Januar 1892 veröffentlichteJames Naismithseine Regeln für das von ihm erfundene Spiel „Basket Ball“:[23]Das ursprüngliche Spiel, das nach diesen Regeln gespielt wurde, unterschied sich stark von dem heutigen, da es kein Dribbling, Dunking, Drei-Punkte-Würfe oder eineWurfuhrgab, undGoaltendingwar legal. Grundsätzlich wird weltweit nach den jeweils gültigen FIBA-Regeln gespielt. In der NBA sind eigene Regeln gültig, die sich ebenfalls historisch entwickelt haben und die auf die besonderen US-amerikanischen Anforderungen des Profi-Sports (z. B. Unterbrechungen des Spiels für TV-Werbeeinblendungen) ausgerichtet sind. Bei internationalen Turnieren (z. B. Olympischen Spielen), die unter der Kontrolle der FIBA ausgerichtet werden, müssen sich alle NBA-Profis auf die FIBA-Regeln umstellen. Neben den Ligen, die nach den internationalen FIBA-Regeln[20]spielen, gibt es vor allem in der nordamerikanischen Liga leicht abweichende Regeln. Diese Abweichungen resultierten meist aus dem Grund, das Spiel attraktiver für die Zuschauer zu gestalten oder sind historisch gewachsen. Die folgende Tabelle zeigt die wesentlichen Regelabweichungen der wichtigsten nordamerikanischen Ligen zu den FIBA-Regeln: Die FIBA strebt weltweit einheitliche Regeln an. Dies soll schrittweise geschehen. Dazu hat sie am 25. Mai 2008 unter anderem folgende Regeländerungen beschlossen: Gültig ab 1. Oktober 2008: Die folgenden Regeln sind bei internationalen Wettbewerben der FIBA ab 1. Oktober 2010 und bei den höchsten Wettbewerben der nationalen FIBA-Verbände ab 1. Oktober 2012 gültig:[27] Der Weltbasketballverband wurde am 18. Juni 1932 in Genf unter dem Namen „FIBA“(Fédération Internationale de Basketball Amateur)gegründet. Die Gründungsmitglieder warenArgentinien,Griechenland,Italien,Lettland,Portugal,Rumänien,SchweizundTschechoslowakei; seit 2002 hatte die FIBA ihren Sitz inGenf. Zwei Jahre zuvor wurde die Sportart „Basketball“ offiziell von demInternationalen Olympischen Komiteeanerkannt. Seit dem Jahr 1950 findet in einem zeitlichen Abstand von vier Jahren ein FIBA-Turnier für Männer statt. Im Jahr 1953 wurde dieses Event auch für Frauen eingeführt. Im Jahr 1989 hat der Weltbasketballverband die Freigabe für professionelle Spieler erteilt. Seither vertreten internationale Basketballspieler wieDwyane WadeoderTim Duncanihre Nationalmannschaft bei denOlympischen Spielen. Eine der bekannteren späten Basketball-Profiligen vor der Entstehung der NBA wurde von drei Konzernen (General Electric,Firestone,Goodyear) im Jahr 1937 unter dem Namen „National Basketball League“ (kurz: NBL) gegründet. Diese Liga wurde im Jahr 1949 aufgelöst und mit der „Basketball Association of America“ (kurz: BAA) zur „National Basketball Association“ (kurz: NBA) vereint. Diese BAA, in der bekannte Spieler wie Bob Davies spielten, hatte im Gründungsjahr 1946 neue und größere Arenen für den Basketball erschlossen. Die NBA wird meist ohne besondere Erwähnung als Kontinuum der BAA angesehen. Die NBA ist zurzeit die populärste Basketball-Profiliga der Welt. Dreißig Mannschaften aus insgesamt sechs Divisions(Atlantic, Central, Southeast, Northwest, Pacific, Southwest)spielen in der Hauptrunde (Regular Season) um den Einzug in die Playoffs. In den Playoffs treten die jeweils acht besten Mannschaften der Western und Eastern Conference in einem K.-o.-System gegeneinander an. Im Zuge der zunehmenden Gleichberechtigung wurde am 24. April 1996 dieWomen’s National Basketball Association(WNBA) gegründet. DerBasketball in Deutschlandwird vomDeutschen Basketball Bund(DBB) mit Sitz inHagenorganisiert. Der Präsident des DBB ist seit dem Jahr 2006Ingo Weiss. Die oberste Spielklassen bilden bei den Männern dieBasketball-Bundesliga,ProAundProBsowie bei den Frauen die1. Damen-Basketball-Bundesligaund die2. Damen-Basketball-Bundesliga. Im Leistungsjugendbereich kommen noch dieNachwuchs-undJugend-Basketball-Bundesligahinzu. Der Breiten- und Amateursport sowie die Regionalligen werden von den folgenden Landesverbänden organisiert:[28] Weitere Varianten und Abwandlungen des Basketballs sind unter anderem bekannt:Korbball,Korfball,Mini-Basketball,Netball, Show-Basketball, Wasser-Basketball,Slamball Das so genannte Streetball ist eine Abwandlung des Basketballs. Es erfreut sich seit den 1990er Jahren als Freizeitsportart immer größerer Beliebtheit. Im Unterschied zum klassischen Basketball wird hier meistens drei gegen drei auf nur einen Korb gespielt und findet im Freien statt, wobei sich die Regeln zusätzlich noch vom „normalen“ Basketball unterscheiden.[29]Aufgrund der geringeren Zahl an Spielern pro Mannschaft liegt ein höheres Gewicht auf den direkten Zweikämpfen und damit den Fähigkeiten in der Ballbehandlung. Der Einradbasketball wird vor allem bei Wettbewerben derInternational Unicycling Federation(kurz IUF) gespielt. Die Teilnehmer müssen den Basketball auf ihrem Einrad so oft wie möglich in den gegnerischen Korb werfen. In dieser Variante werden die gleichen Regeln verwendet wie beim normalen Basketball. Der Rollstuhlbasketball wurde im Jahr 1946 in den USA erfunden, da einige Basketballspieler ihren Sport trotz Kriegsverletzungen betreiben wollten. DieInternational Wheelchair Basketball Federationist der internationale Dachverband. Rollstuhlbasketball zählt seit denParalympics1960 in Rom zu den paralympischen Sportarten. Die deutschen Rollstuhlbasketballer sind imDeutschen Rollstuhl-Sportverband(DRS) organisiert.[30] Basketball für Gehörlose ist für Gehörlose und Menschen mit Hörbehinderung ab 55 dB Hörverlust zugänglich. Die gleichen Regeln wie beim normalen Basketball werden verwendet, unerlaubte Hilfsmittel sind zum Beispiel Hörgeräte und Cochlea-Implantat-Geräte. DieDeaf International Basketball Federation(DIBF) ist der Weltbasketballverband für Gehörlose und organisiert die Weltmeisterschaften sowie diverse internationale Meisterschaften. Basketball für Gehörlose zählt seit denDeaflympics1949 in Brüssel zu den deaflympischen Sportarten. Die deutschen gehörlosen Basketballer sind imDeutschen Gehörlosen-Sportverband(DGSV) organisiert. Basketball (Special Olympics) ist eine Sportart, die auf den Regeln von Basketball beruht und in Wettbewerben und Trainingseinheiten der OrganisationSpecial Olympicsweltweit für geistig und mehrfach behinderte Menschen angeboten wird. Basketball ist seit 1968 beiSpecial Olympics World Gamesvertreten. Das zunächst in denVereinigten Staatenals Trainingsvariante des Basketballs entwickelte Beachbasketball erfreut sich seit einigen Jahren einer wachsenden Popularität. Dabei wird in Deutschland anders als in den Vereinigten Staaten auf zwei Körbe in einem verkleinerten Spielfeld gespielt.[31]Die deutsche Meisterschaft im Beachbasketball wird jährlich inCuxhavenausgetragen. Viele Spiele derNBAwerden in Deutschland in der Saison 2023/2024 beiProSiebengezeigt. 50 Partien der Regular Season, weitere Begegnungen in den Playoffs und den Finals sindfrei empfangbar.[32]DieBasketball-Bundesliga, derBBL-Pokalund wichtige Spiele derBasketball Champions Leagueüberträgt seit der Saison 2023/2024 der SportsenderDynfürAbonnenten. 40 Saison-Partien derBBLsind kostenlos aufBild TVund anderen Plattformen desSpringer-Verlagszu sehen.[33][34] Der erste Film mit einer Basketball-Thematik erschien im Jahr 1979 unter dem TitelThe Fish That Saved Pittsburgh. In dieser Komödie spielten sehr bekannte Basketballspieler wieJulius ErvingoderKareem Abdul-Jabbardie Hauptrollen. Im Jahr 1987 wurde der FilmFreiwurf(eng.hoosiers) in den Kinos veröffentlicht. In diesem, der Geschichte derHickory High Schoolnachempfundenem Sportdrama spielten bekannte Schauspieler wieGene Hackman,Barbara HersheyoderDennis Hopperdie Hauptrolle. Der Film handelt von einer hoffnungslosen High School Mannschaft, die sich durch die Hilfe von Coach Norman Dale (gespielt von Gene Hackman) zu einem Titelfavoriten entwickeln. Der Film wurde von der Presse positiv aufgenommen und war in zwei Kategorien für den Oscar nominiert. Durch den großen Erfolg der amerikanischen Basketballmannschaft bei denOlympischen Spielen 1992in Barcelona und die Siegesserie von BasketballlegendeMichael Jordanmit denChicago Bullsentstand in den 1990er Jahren ein Boom an Filmen mit Basketball-Thematik. Weiterhin ist Basketball zwar im FilmAir: Der große Wurfnicht das direkte Hauptthema. Allerdings wird der Beginn der Zusammenarbeit zwischen Michael Jordan undNikeim Jahr 1984 dargestellt. Im Jahr 1992 erschien die FilmkomödieWeiße Jungs bringen’s nicht, mit den SchauspielernWesley SnipesundWoody Harrelsonin der Hauptrolle, in den Kinos. Zwei Jahre später werden zwei weitere Filme und eine Dokumentation mit einer Basketballthematik in den Kinos veröffentlicht. In dem FilmdramaAbove the Rim – Nahe dem AbgrundspieltTupac Shakurdie Rolle des skrupellosen Ganganführers Birdie, der den hoffnungsvollen jungen Spieler Kyle in seiner Mannschaft haben will. In dem FilmBlue Chipsgeht es um den Coach Pete Bell (gespielt vonNick Nolte) und seine Basketball-Mannschaft, die durch das Brechen der Regeln zur Siegermannschaft aufsteigen wollen. Die BasketballspielerBob CousyundShaquille O’Nealspielten in diesem Film eine Nebenrolle. Der DokumentarfilmHoop Dreamsbefasst sich mit zwei afroamerikanischen Jungen, die aus ihrem schlechten Leben in dem Ghetto fliehen wollen, um professionelle Basketballspieler zu werden. Diese Dokumentation wurde mit zahlreichen Preisen ausgezeichnet und war im Jahr 1995 für den Oscar nominiert. Mitte der neunziger Jahre erschienen weitere bekannte Basketball-Filme wie die KomödieEddieoder der AnimationsfilmSpace Jam. Im Jahr 1998 veröffentlichte der Regisseur David Zucker die FilmkomödieDie Sportskanonen. In diesem Film erfinden die Freunde Joe Cooper und Doug Remer die Sportart „Baseketball“, eine Mischung aus Baseball und Basketball. Eine große Popularität unter Basketball-Fans erzielte der FilmSpiel des Lebens. Der Film wurde von dem RegisseurSpike Leegedreht und mit namhaften Schauspielern wieDenzel WashingtonoderMilla Jovovichbelegt. Im Jahr 2005 kam der FilmCoach Carterin die deutschen Kinos, der auf einer wahren Begebenheit beruht. Im Jahr 1979 wurde das erste Basketballvideospiel für denAtariveröffentlicht. Das Spiel konnte man damals mit einem sogenanntenTrackballgegen den Computer oder einen anderen Mitspieler spielen. Im Jahr 1989 wurde von dem SpieleentwicklerElectronic Artseine Serie gegründet, die zwei Mannschaften gegenüberstellt. Der erste Teil dieser Serie erschien unter dem TitelLakers versus Celticsfür den PC und Sega Mega Drive. In diesem Spiel konnte man erstmals NBA-Stars wieLarry Bird, Kareem Abdul-Jabbar oderMagic Johnsonspielen. Durch den großen Erfolg ermutigt, veröffentlichte Electronic Arts im Jahr 1991 den NachfolgerBulls versus Lakers and the NBA Playoffs. Das Sportspiel besaß 16 Originalmannschaften der Playoffs aus der vorherigen Saison und konnte mit einem weiteren Mitspieler gespielt werden. Ein Jahr später kam das SpielTeam USA Basketballweltweit auf den Markt. Der Spieler konnte aus insgesamt vierzehn Internationalen Mannschaften (u. a. Australien, Angola, China) eine aussuchen und mit dieser gegen andere Mannschaften antreten. Im folgenden Jahr veröffentlichte der SpieleentwicklerMidwaydas sehr beliebte und erfolgreiche BasketballspielNBA Jam. Das Spielprinzip unterschied sich sehr von den anderen Basketballspielen, die auf eine realistische Spielweise setzten. Im Jahr 1995 veröffentlichte der Spieleentwickler Electronic Arts den ersten Teil derNBA-Live-Serie für dasSuper Nintendo,Sega Mega Driveund den PC. Die Serie bot Originallizenzen der NBA und eine auf die Systeme angepassteGrafikengine. Innerhalb der nächsten Jahre entwickelte sichNBA Livezu einer der beliebtesten Basketball-Videospielreihe der Welt. Neben der NBA-Live-Reihe veröffentlicht EA Sports auchNBA Street. Diese Reihe bietet neben den Originallizenzen eine unrealistische Spielweise, die sehr an die NBA-Jam-Serie erinnert. Nach der 2009 erschienenen Version NBA Live 10 wurde die Serie eingestellt und EA begann die Entwicklung einer neuen Basketballsimulation, dieNBA Eliteheißen sollte. Nach mehreren Verschiebungen des Veröffentlichungstermins wurde die Entwicklung komplett eingestellt, so dass es für die Saison 2010/11 keine Version von EA gab. Seit 2005 gibt es eine weitere Spielserie, welche von2K Gamesproduziert wird. Sie nennt sichNBA2Kund entwickelte sich seit dem Start zum größten Konkurrenten der EA-Sports-Serie. Neben einer realistischen Simulation bietet das Spiel auch alle Lizenzen der NBA sowie einen Modus, der es den Benutzern ermöglicht, einen eigenen Profispieler zu erstellen. 1896•1900•1904•1908•1912•1920•1924•1928•1932•1936•1948•1952•1956•1960•1964•1968•1972•1976•1980•1984•1988•1992•1996•2000•2004•2008•2012•2016•2020•2024 1896•1900•1904•1908•1912•1920•1924•1928•1932•1936•1948•1952•1956•1960•1964•1968•1972•1976•1980•1984•1988•1992•1996•2000•2004•2008•2012•2016•2020•2024 Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 1.1Die Erfindung des Basketballspiels 1.2Entstehung der ersten Mannschaften 1.3College-Basketball 1.4Die Entstehung der NBA 1.5Basketball heute 1.6Basketball international 2Ausrüstung 2.1Spielball 2.2Bekleidung 3Spielprinzip 3.1Spielfeld 3.2Mannschaften 3.3Positionen 3.4Schiedsrichter 3.5Zeiteinteilung 3.6Punktgebung 3.7Angriff 3.8Verteidigung 3.9Begriffe 4Spielregeln 4.1Sprungball 4.2Fouls 4.3Zeitübertretungen 4.4Aus 4.5Rückspiel 4.6Schrittfehler 4.7Doppeldribbling 4.8Goaltending 4.9Fußspiel 4.10Entwicklung der Basketballregeln"
  },
  {
    "label": 0,
    "text": "Baum – Wikipedia Baum Inhaltsverzeichnis Entwicklung in der Erdgeschichte Systematik Morphologie Physiologie Ökologie Bäume und Menschen Superlative Siehe auch Filmografie Literatur Weblinks Einzelnachweise Wachstum Alter Schädigungen Stamm Wurzel Blätter Blüten Frucht- und Samenbildung Wuchs Wasserleitung Wald Verbreitungszentren, Diversität Nutzung Gesellschaftliches Mythologie und Religion In der Geschichte Einführungen/Übersichten Bestimmungsbücher Kulturgeschichte Definition Bestandteile AlsBaum(vonwestgermanischmittelhochdeutsch,althochdeutschboum, Herkunft ungeklärt,[1]Teil derSwadesh-Liste) wird im allgemeinen Sprachgebrauch eine verholztePflanzeverstanden, die aus einer Wurzel, einem daraus emporsteigenden, hochgewachsenen Stamm und einer belaubten oder benadelten Krone besteht. DieBotanikdefiniert Bäume alsausdauerndeundverholzendeSamenpflanzen, die eine dominierendeSprossachseaufweisen, die durchsekundäres Dickenwachstuman Umfang zunimmt. Diese Merkmale unterscheiden einen Baum vonSträuchern,Farnen,Palmenund anderenverholzenden Pflanzen. Im Gegensatz zu ihrenentwicklungsgeschichtlichenVorläufern verfügen die meisten Bäume zudem über wesentlich differenziertereBlattorgane, die mehrfach verzweigten Seitentrieben (Lang- und Kurztrieben) entspringen. Stamm, Äste und Zweige verlängern sich jedes Jahr durch Austreiben von End- und Seitenknospen, verholzen dabei und nehmen kontinuierlich an Umfang zu. Im Gegensatz zumStrauchist es besonderes Merkmal der Bäume, dass die Endknospen über die Seitenknospen dominieren (Apikaldominanz) und sich dadurch ein vorherrschender Haupttrieb herausbildet (Akrotonie). Die Voraussetzungen für die Entstehung und Verbreitung der Bäume waren: Die Vorläufer der Bäume kennt man aus demKarbon. Sie gehörten zu denSchachtelhalmgewächsen, denBärlappgewächsenund denFarnen. Sie besaßen verholzte Stämme, die auch ein sekundäres Dickenwachstum aufwiesen. Fossile Gattungen sind beispielsweiseLepidodendronundSigillaria. Die verdichteten Sedimente dieser Wälder bilden dieSteinkohle. Die weitereEvolutionder Pflanzen brachte imPermdieSamenpflanzenhervor. DieNacktsamerbreiteten sich als erste Bäume rasch aus, erreichten wohl in derTrias(vor etwa 200 Millionen Jahren) ihre größte Artenvielfalt, bis sie imPaläogen(vor etwa 60 Millionen Jahren) von denAngiospermenin ihrer Bedeutung abgelöst wurden.[2]Von den bekannten 220.000 Blütenpflanzen sind etwa 30.000 Holzarten, so dass etwa jede achte Blütenpflanze ein Baum oder Strauch ist. Die meisten Baumarten zählen zu denBedecktsamern(Angiospermen). Die Gymnospermen (Nacktsamer) umfassen nur ungefähr 800 Arten, bedecken aber immerhin ein Drittel der Waldfläche der Erde. Die globale Verteilung der Baumarten wurde vor allem durch die klimatischen Verhältnisse und durch dieKontinentalverschiebunggeprägt. Während zum Beispiel die Buchengewächse (Fagaceae) eine typische Familie derNordhemisphäresind, ist beispielsweise die FamiliePodocarpaceaevorwiegend in der Südhemisphäre verbreitet. Die heutige natürliche Artenverteilung wurde stark von denquartären Eiszeitenbeeinflusst. Das gleichzeitige Vordringen der skandinavischen und alpinen Gletschermassen Europas hat zu einer Verdrängung zahlreicher Spezies geführt und die im Vergleich zu Nordamerika auffällige Artenarmut in Zentraleuropa verursacht. So stehen etwa der einzigen in den montanen Regionen Mitteleuropas heimischen Fichtenart, derGemeinen Fichte(Picea abies), zahlreiche Fichtenarten auf dem nordamerikanischen Kontinent gegenüber. Baumförmige Lebensformen kommen in verschiedenen Pflanzengruppen vor: „Echte“ Bäume sind dieLaubbäumeunter denBedecktsamernund die baumförmigenNacktsamer, zu denen Nadelholzgewächse wie dieKoniferengehören, aber auchGinkgobiloba(als einziger noch existierender Vertreter der Ginkgogewächse) sowie zahlreiche Vertreter der fiederblättrigen Nacktsamer (Cycadophytina). Eigentümlichster Baum ist wohl die inNamibiavorkommendeWelwitschia mirabilis, deren Stamm im Boden verbleibt. Daneben können auch diePalmenund dieBaumfarneeine baumähnliche Form ausbilden. Diese Gruppen besitzen aber kein echtes Holz (sekundäresXylem) und gelten daher nicht als Bäume. Eine Sonderstellung nimmt derDrachenbaum(Dracaena) ein. Dieser gehört zwar zu den Einkeimblättrigen, hat aber einatypisches sekundäres Dickenwachstum. Baumähnliche Formen finden sich hauptsächlich in rund 50 höherenPflanzenfamilien. Dagegen fehlt die Baumform beiAlgen,Moosen,Liliengewächsen,Iridaceae,Hydrocharitaceae,Orchideen,Chenopodiaceae,Primelgewächsenund meist auch bei denConvolvulaceae,Glockenblumengewächsen,Cucurbitaceae,Doldengewächsen,Saxifragaceae,Papaveraceae,RanunculaceaeoderCaryophyllaceae. Bäume kommen heute innerhalb der Nacktsamer (Gymnospermae) einerseits in Form derGinkgoopsidamit der ArtGinkgo, andererseits der nadelblättrigen Nacktsamer (Coniferopsida, „Nadelbäume“) vor. Dominiert werden die Arten vor allem von der OrdnungPinalesmit den FamilienPinaceae(Fichten,Kiefern,Tannen,Douglasien,Zedern,Lärchen,Goldlärche),Cupressaceae(Zypressen,Scheinzypressen,Sumpfzypressen,Lebensbäume,Wacholder,Mammutbäume),Podocarpaceae(Steineiben,Harzeiben),Araucariaceae(Araukarien,Kauri-Bäume),Taxaceae(Eiben) undCephalotaxaceae(Kopfeiben). Viele Baumarten kommen aber auch innerhalb der Bedecktsamer (Angiospermen) vor. Die verschiedenen Unterklassen haben hier unterschiedliche Laubbaumtypen hervorgebracht. Zu den bedeutendsten gehören die Buchengewächse (Fagaceae), zu denen neben denBuchen(Fagusspp.) auch dieEichen(Quercusspp.) und dieKastanien(Castanea) gezählt werden. Ebenfalls bedeutend sind die Birkengewächse (Betulaceae) mit denBirkenundErlensowie die Nussbäume (Juglandaceae), dieUlmen(Ulmaceae) und die Maulbeergewächse (Moraceae). Zu denRosidenzählen dieLindenaus der Familie derMalvengewächse, die Obstgehölze aus der Familie derRosengewächse(Rosaceae) sowie die Leguminosen (Fabales) mit sehr zahlreichen, vor allem tropischen Arten. Neben der GattungDalbergia(Palisanderbäume) gehört auch die GattungRobiniain diese Gruppe. Wirtschaftlich bedeutsam sind die Zedrachgewächse (Meliaceae) mit den GattungenEntandrophragma(Mahagonibäume) undCedrelasowie die Familie derDipterocarpaceaemit der GattungShorea(Meranti, Bangkirai). Baumartige Lebensformen zeigen eine große Variationsbreite in ihrem Aufbau (Morphologie). Assoziiert wird mit dem BegriffBaum(bzw.Bäumlein) der Aufbau aus Baumkrone, Baumstamm und Baumwurzeln. Bei den baumartigenFarnenund den meistenPalmenfinden sich einfache Stämme, die keine Äste ausbilden, sondern schopfartig angeordnete, häufig gefiederte Blätter. Vor allem zeigen sie kein sekundäres Dickenwachstum und sind damit keine echten Bäume. Bei den echten Bäumen wächst aus demSprossder Keimpflanze durch Längen- und sekundäres Dickenwachstum der künftige Baumstamm heran: Es bildet sich der Spross an der Spitze durch die sich ständig erneuernde Gipfelknospeaufrecht weiter und wird zum geraden, bis zur höchsten Kronenspitze durchgehenden Baumstamm (Monopodium). In der Spitzenknospe gebildete Wuchsstoffe (Auxine) unterdrücken die Aktivität der Seitenknospen. Bei vielen Baumarten lässt diese Dominanz des Haupttriebs mit dem Alter nach und es bildet sich eine typische, verzweigte Laubbaumkrone. Bei anderen Gehölzen wie derBucheoder derHainbucheübernimmt eine subterminale Seitenknospe die Führung (Sympodium). Bei Bäumen entsteht so eine aufrechte „Scheinachse“ (Monochasium). Im späteren Verlauf lässt die Dominanz der führenden Knospe nach und aus weiteren Seitenknospen entwickeln sich stärkere Äste, die schließlich eine Krone bilden. Dies geschieht meist früher als bei Bäumen mit monopodialem Wuchs. Sträucherhingegen sind durch das völlige Fehlen der apikalen Dominanz gekennzeichnet. Zahlreiche bodenbürtige Seitentriebe bilden hier eine weit verzweigte Wuchsform. Bei Gehölzen bildet sich an den Wuchsachsen während der Vegetationsperiode je ein Triebabschnitt (Jahrestrieb), dessen Beginn lange an den schmalen ringförmigenBlattnarbender ehemaligen Knospenschuppen erkennbar ist. Ein weiterer Austrieb nach der Vegetationsperiode wird alsJohannistrieb(Prolepsis) bezeichnet. Tropische Arten neigen zu mehrfachem Austrieb. Aus der Zahl der Jahrestriebe und dem Grad der Verzweigung lässt sich das Alter eines Astes ermitteln. Diese Altersbestimmung wird jedoch bei zahlreichen Arten (zum Beispiel Fichte oder Tanne) und regelmäßig bei älteren Bäumen durch die Ausbildung von sogenanntenProventivtriebenerschwert, die aus „schlafenden“ Knospen austreiben. Die regelmäßige Bildung von Proventivtrieben wird alsReiteration(sprich: Re-Iteration) bezeichnet. Diese Wiederholungstriebe dienen der Erneuerung der Krone und verschaffen Bäumen die Möglichkeit, alternde Äste zu ersetzen sowie auf Stress (Schneebruch, Insektenkalamitäten) zu reagieren. Bäume können ein Alter von mehreren hundert Jahren, an bestimmten Standorten sogar von mehreren tausend Jahren erreichen. Die ältesten bekannten Individuen sind über 5000 Jahre alt. Die nachweislichältesten Bäume Deutschlandswerden auf etwa 600 Jahre datiert. Die ältesten bekanntenKlonkoloniender Welt hingegen sind 60–70 Mio. Jahre alt.[3][4][5] Wächst der Baum unter imJahresrhythmusschwankendenklimatischenBedingungen, wird während der Vegetationsperiode einJahresringangelegt. Mit Hilfe dieser Ringe lassen sich das Alter eines Baumes und dessen Wuchsbedingungen in den einzelnen Jahren ablesen. DieDendrochronologienutzt dies, um altes Holz zu datieren und das Klima einer Region bis zu mehreren 1000 Jahren zu rekonstruieren. Seine Entwicklung bringt für den Baum zahlreiche Probleme und Schädigungen mit sich. Hierunter fallen vor allem: Bei Jungbäumen kommt es insbesondere zu: Einige wichtige Krankheiten, von denen Bäume befallen werden können, sind Brand, Krebs, Rost,Mehltau, Rotfäule, Weißfäule, Braunfäule und Harzfluss. Zu Missbildungen an Bäumen zählen dieMaserkröpfe, dieHexenbesenoder Wetterbüsche sowie dieGallen. DerBaumstamm, in derDendrologieSchaftgenannt, ist die verholzende Hauptachse (Caulom) der Baumpflanze. Ein Querschnitt durch einen Baumstamm zeigt verschiedene Zonen. Ganz innen befinden sich das aus Primärgewebe bestehendeMarkund das toteKernholz. Bestimmte Baumarten (z. B. Buche, Esche) bilden fakultativ einenFalschkernaus, der sich in den Eigenschaften vom echten Kernholz unterscheidet. Weiter außen befindet sich dasSplintholz, das der Leitung und Speicherung dient und sich bei sogenannten Kernholzbäumen farblich meist deutlich vom Kernholz abhebt. Bei der Eiche, der Eibe und der Robinie ist dies sehr gut sichtbar. Die Fichte hat einen farblosen Kern (Reifholz). Die äußerste Schicht bildet die Baumrinde. Sie besteht aus derBastschicht, die in Wasser gelöste Nährstoffe transportiert, und derBorke, die den Stamm vor Umwelteinflüssen (UV-Einstrahlung, Hitze, mechanische und biotische Schäden) schützt. Zwischen der Bastschicht und demHolzbefindet sich bei Gymnospermen und Dikotyledonen dasKambium. Diese Wachstumsschicht bildet durch sekundäres Dickenwachstum nach innen Holz (Xylem) und nach außen Bast (Phloem). Das Holz zeichnet sich durch die Einlagerung vonLigninin die Zellwand aus. Dadurch werden die Zellen versteift und bilden ein festes Dauergewebe. Das sekundäre Dickenwachstum, die Lignifizierung der hölzernen Zellwand und die Vermehrung durch Samen verschafften den Bäumen in den meisten Biomen der Erde einen Vorteil gegenüber anderen Pflanzen und haben dort zur Entwicklung großflächiger Waldbestände geführt. Ausnahmen bilden die Wüsten, die arktischen Tundren und die zentralkontinentalen Steppen. Hinsichtlich des inneren Baus des Baumstamms weichen die zu denEinkeimblättrigengehörendenPalmenvon den echten Bäumen erheblich ab. Bei ersteren stehen die Gefäßbündel im Grundgewebe zerstreut, weshalb es keinenKambium­ring, keinen Holzzylinder und somit kein fortdauerndes sekundäres Dickenwachstum des Stammes gibt. Bei den zu den Dikotyledonen oder Gymnospermen gehörenden Bäumen besitzt der Stamm schon in der frühesten Jugend als dünner Stängel einen unter der Rinde gelegenen Kreis vonLeitbündeln, der den Rindenbereich vom innen liegenden Mark scheidet. Dieser Leitbündelring stellt in seiner inneren, dem Mark anliegenden Hälfte das Holz und im äußeren, an die Rinde angrenzenden Teil den Bast dar; zwischen beiden zieht sich der Kambiumring hindurch. Dieser wird aus zarten, saftreichen, sich ständig teilenden Zellen gebildet und vergrößert durch seinen laufenden Zellvermehrungsprozess die beiderseits ihm anliegenden Gewebe. So wird alljährlich an der Außenseite des Holzringes eine neue Zone Holzgewebe angelegt, wodurch die Jahresringe des auf diese Weise erstarkenden Holzkörpers entstehen, die als konzentrische Linien am Stammquerschnitt wahrnehmbar sind. Andererseits erhält aber auch der weiter außen liegende Bast an seiner Innenseite einen jährlichen, wenn auch weit geringeren Zuwachs. Auf diese Weise kommt die dauernde Verdickung des Stammes und aller Äste sowie auch der Wurzeln zustande. Auch in der Wurzelbildung unterscheiden sich die Bäume untereinander. Neben der genetischen Festlegung steuern die Erfordernisse der Verankerung des Baumes im Boden ebenso wie die Notwendigkeit der Versorgung der Pflanze mit Wasser undNährstoffendie Intensität und Art des Wurzelwachstums. Man spricht entsprechend der Form des Wurzelstocks vonPfahlwurzel,FlachwurzeloderHerzwurzel. Bei der Pfahlwurzel wächst die Hauptwurzel senkrecht in den Boden hinab, was besonders für dieEichecharakteristisch ist. Flachgründige Böden und hoch anstehendes Grundgestein oder Grundwasser begünstigen z. B. die Bildung von Flachwurzeln. Trockene Böden begünstigen eine Bildung von Pfahlwurzeln. Die überwiegende Masse des Wurzelstocks machen bei den Bäumen nicht die verholzten Wurzelteile, sondern die mit einerMykorrhizavergesellschafteten Feinwurzeln aus.[6]Im Boden verbinden sich viele Wurzeln symbiotisch mit Pilzmycelen. Bäume erhalten Mineralien wie Phosphor von den Pilzen, während Pilze von den Bäumen die Kohlenhydratprodukte der Photosynthese gewinnen.[7]Die Pilze können verschiedene Bäume miteinander verbinden, und es bildet sich ein Netzwerk, das Nährstoffe und Signale überträgt.[8][9]Die Gesamtwurzelmasse reicht oft an die Masse der oberirdischen Pflanzenteile heran. Beieinkeimblättrigenbaumähnlichen Lebensformen endet der Stamm nahe unter der Bodenfläche und es entwickelt sich ein sprossbürtiges Wurzelsystem (Homorhizie). An alten Bäumen finden sich meist jungeAdventivwurzeln, die alte, ineffektive Wurzeln ersetzen. Bei einigen Baumarten bilden oberflächennahe Wurzeln eine sogenannteWurzelbrut, eine Form dervegetativen Vermehrung. Wurzelkappungen infolge von Baumaßnahmen können das Absterben von Wurzelteilen bewirken und führen zum Eindringen von holzzerstörenden Pilzen in den Baum. Dies ist die häufigste Ursache von irreparablen Baumschäden im städtischen Bereich. Bäume tragenLaubblätteroderNadelblätter, die entweder mehrjährig am Baum verbleiben (immergrüne Arten) oder am Ende einer Vegetationsperiode abgeworfen werden (laubabwerfende Arten). Dazwischen liegen noch die halbimmergrünen Arten, die am Ende einer Vegetationsperiode nur einen Teil ihrer Blätter verlieren, bei Neuaustrieb dann aber die vorjährigen ersetzen. Die Nadelgehölze sind mit Ausnahme der GattungenLärchen(Larix) undGoldlärchen(Pseudolarix) immergrüne Arten. In den borealen und hochmontanenBiomender Nordhalbkugel haben sich die immergrünen Nadelgehölze durchgesetzt, da sie zu Beginn der Vegetationsperiode bei ausreichender Temperatur sofort mit der Assimilation beginnen können, ohne zunächst Assimilationsorgane bilden zu müssen wie die laubabwerfenden Baumarten. Die Gestalt derBlätter(Laub) ist ein wichtiges Bestimmungsmerkmal. Anordnung, Form, Größe, Farbe, Nervatur und Zähnung sowie haptische Eigenschaften können zur Differenzierung herangezogen werden. Nicht minder brauchbar zur Unterscheidung imwinterlichenZustand sind die (Blatt-)Knospendes Baumes. Eine eindeutige taxonomische Identifizierung der Arten ist allerdings nur anhand der Blüten oder Früchte möglich. Manche Bäume sind mitDornenausgestattet. Dies sind entweder kurze Zweige, die mit dorniger Spitze enden (Weißdorne, Wildformen vonObstbäumen) oder es sind stachelartig ausgebildeteNebenblätterwie etwa bei derGewöhnlichen Robinie. Ein europäischer Laubbaum trägt durchschnittlich 30.000 Blätter, die zusammen eine enorme Transpirationskapazität haben. An warmen Sommertagen kann der Baum mehrere hundert Liter Wasser verdunsten. Beispiel einer 80-jährigen, alleinstehendenRotbuche:[10]In diesem Lebensalter ist der Baum 25 Meter hoch, und seine Baumkrone mit einem Durchmesser von 15 Meter bedeckt eine Standfläche von 160 m². In ihren 2700 m³ Rauminhalt finden sich 800.000 Blätter mit einer gesamten Blattoberfläche von 1600 m², deren Zellwände zusammen eine Fläche von 160.000 m² ergibt. Pro Stunde verbraucht diese Buche 2,352 kgKohlenstoffdioxid, setzt 0,96 kgWasserum und speichert 25.435 KilojouleEnergie in Form von 1,6 kgTraubenzucker(das entspricht ca. 7 kWh, die eingestrahlteSonnenenergieist etwa siebenmal größer); dabei produziert sie 1,712 kgSauerstoffund deckt damit den Verbrauch von zehn Menschen. Die 15 m³ Holz des Baumes wiegen trocken 12.000 kg, allein 6000 kg davon sindKohlenstoff. DieBlütender Bäume aus gemäßigten Breiten sind manchmal verhältnismäßig unscheinbar; bei einigenTaxasind einzelne Blütenblattkreise reduziert. Einige Baumarten gemäßigter Breiten haben eingeschlechtige Blüten. Dabei sitzen die Blüten beider Geschlechter entweder auf demselben Baum (einhäusig getrenntgeschlechtig, zum BeispielEiche,Buche,Hainbuche,Birke,ErleundNussbaum) oder auf verschiedenen (zweihäusig getrenntgeschlechtig), so dass man männliche und weibliche Bäume zu unterscheiden hat (unter anderem beiWeidenundPappeln). Andere Bäume wie Obstbäume,Rosskastanieund viele Bäume der wärmeren Klimate haben Zwitterblüten, die sowohlStaub-als auchFruchtblätterausbilden. DieFrucht- undSamenbildungzeigt weniger Eigentümlichkeiten. Bei den meisten Bäumen fällt die Reife in denSommeroderHerbstdesselben Jahres; nur bei den Kiefernarten erlangen die Samen und die sie enthaltendenZapfenerst im zweiten Herbst nach der Blüte vollständige Ausbildung. Die Früchte sind meistensnussartigmit einem einzigen ausgebildeten Samen, oder sie bestehen aus mehreren einsamigen, nussartigen Teilen, wie bei denAhornen. SaftigeSteinfrüchte, ebenfalls mit einem oder wenigen Samen, finden sich bei den Obstbäumen, Kapseln mit zahlreichen Samen bei den Weiden und Pappeln. Wie bei allen Pflanzen unterliegen auch bei Bäumen der Stoffwechsel und das Wachstum sowohl endogenen (genetisch festgelegten) als auch äußeren Einflüssen. Zu letzteren zählen vor allem die Standortverhältnisse, das Klima und die Konkurrenz mit anderen Organismen beziehungsweise deren schädigende Wirkung. Während derVegetationsperiodesorgen die Spitzenmeristemeund dasKambiumfür stetigen Längen- und Dickenzuwachs. Beginn und Ende der Vegetationsperiode sind je nach Baumart durch die Witterung und die Wasserverfügbarkeit beziehungsweise durch die Tageslänge bestimmt. Das Wachstum wird dabei durchPhytohormonegesteuert und die Akkumulation vonBiomassegezielt optimiert. Bäume sind so in der Lage, sich an ändernde Wuchsbedingungen anzupassen und gerichtete Festigungs-, Leit-, Speicher- oderAssimilationsgewebeanzulegen. Die Produktion neuen Gewebes mit dem sekundären Dickenwachstum und die Anlage neuer Jahrestriebe bewirkt, dass sich ein Baum ständig von innen nach außen erneuert. Der amerikanische BaumbiologeAlex Shigohat daraus das Konzept der Kompartimentierung entwickelt, das den Baum als ein Ensemble zusammenwirkender Kompartimente sieht. Auf Verletzungen reagiert der Baum, anders als Tiere und Menschen, durch Abschottungsreaktionen und Aufgabe der eingekapselten Kompartimente (CODIT-Modell). Durch adaptives Wachstum optimiert er zudem seine Gestalt. Computermodellierungen des Karlsruher Physikers und BiomechanikersClaus Mattheckkonnten zeigen, dass Bäume durch adaptives Wachstum eine mechanisch optimale Gestalt anstreben und zum BeispielKerbspannungenin Verzweigungen vermeiden, so dass die Gefahr von Brüchen minimiert wird. Diese Erkenntnisse haben zu Optimierungen unter anderem im Maschinenbau geführt. Der Wassertransport wird in den Nadelgehölzen durch dieTracheiden, in den Laubbäumen durch die effektiveren Gefäße (Poren) bewerkstelligt. Letztere sind bei den Laubbäumen entweder zerstreut (zum Beispiel bei Buche, Ahorn, Pappel) oder ringförmig (zum Beispiel bei Eiche, Ulme, Esche) imJahresringangeordnet. Beispielsweise kann eine Eichenpore mit 400 µm Durchmesser 160.000-mal mehr Wasser als eine Nadelholztracheide mit 20 µm Durchmesser im gleichen Zeitraum transportieren. Nach überwiegend vertretener Lehre funktioniert der Wassertransport der Bäume durch Saugspannungen in den Leitgeweben infolge Verdunstung an denStomatader Blätter (Kohäsionstheorie). Dabei müssen Baumhöhen bis über 100 Meter überwunden werden können, was nach dieser Theorie nur mit enormen Drücken möglich ist. Kritiker dieser Lehre behaupten, dass schon bei wesentlich geringeren Höhen die Saugspannung zum Abriss des Wasserfadens in den Kapillaren führen müsste. Als gesichert gilt allerdings, dass im Frühjahr Zucker in den Speicherzellen mobilisiert werden und durch den aufgebauten osmotischen Druck Wasser aus den Wurzeln nachfließt. Dabei werden im Bodenwasser gelöste Nährsalze (vor allem K, Ca, Mg, Fe) vom Baum aufgenommen. Erst nach Ausdifferenzierung der Blätter werden die in der Krone erzeugtenAssimilateüber den Bast stammabwärts transportiert und stehen für das Dickenwachstum zur Verfügung. Eine Ausnahme bilden die ringporigen Laubbäume, bei denen die ersten Frühholzporen aus den im Vorjahr gebildetenReservestoffengebildet werden. Die süßen „Baumsäfte“ wurden von Menschen durch Einschneiden der Rinde abgezapft und durchEinkochenzuSirupenweiterverarbeitet, beispielsweiseAhornsirupoder der Saft derManna-Esche.Palmzuckeroder Palmsirup allerdings ist ein Extrakt aus dem Blütensaft derNipa- undZuckerpalme(Unterfamilie Arecoideae),Agavensirupstammt aus dem „Saft“ der zu den Stauden gehörendenAgaven,Birkenzuckerwurde ursprünglich in Finnland direkt aus derBirkenrindegewonnen. DieHydrologiebeziehungsweiseBodenökologieunterscheidet zwischen demNiederschlag, welcher im Bereich derBaumkroneauf den Boden trifft (Kronendurchlass) und dem Anteil, welche am Stamm herabfließt (Stammabfluss). Ein Teil des Niederschlags verdunstet direkt vom Baum (Interzeption) und erreicht den Boden nicht. Um die physikalisch grenzwertige Wasserversorgung sehr hoher Bäume von den Wurzeln zur Krone auszugleichen, ist etwa der Küstenmammutbaum in der Lage, zusätzlich Wasser mit den Nadeln aufzunehmen.[11] Dort wo Bäume ausreichend Licht, Wärme und Wasser vorfinden, bilden sie Wälder. Im Jahr 2000 waren lautFAO30 Prozent der Festlandmasse der Erde bewaldet. Pro Hektar binden Waldbäume zwischen 60 und 2000 Tonnen organisches Material und sind damit die größten Biomassespeicher der Kontinente. Die Gesamtmenge der 2005 weltweit in den Wäldern akkumulierten Holzmasse betrug 422 Gigatonnen. Da etwa die Hälfte der Holzsubstanz ausKohlenstoffbesteht, sind Wälder nach denOzeanendie größten Kohlenstoffsenken derBiosphäreund damit für dieCO2-BilanzderErdatmosphärebedeutsam. Die mit der Bestandsbildung von Bäumen einhergehende Konkurrenz um Ressourcen führt zu einer Anpassung desHabitusgegenüber den freistehenden Exemplaren (Solitäre). Natürlicher Astabwurf innerhalb der Schattenkrone sowie Verlagerung derAssimilationin die Lichtkrone sind Optimierungsreaktionen der Bäume, die zu einem hohen, schlanken Wuchs mit kleinen Kronen und oft zu hallenartigen Beständen führen (zum Beispiel Buchen-Altbestände). Eine Ausnahme und Besonderheit bezüglich derBiomasseproduktionstellen die über dasKronendachragendenEmergenten(Urwaldriesen) vielerRegenwälderdar. Die heutige Ausbreitung und Artenzusammensetzung der Wälder steht stark unter dem Einfluss der wirtschaftlichen Tätigkeit des Menschen. Der Übergang von der Jäger- und Sammlerkultur zum Ackerbau ging in den dicht besiedelten Regionen mit der Zurückdrängung der Wälder einher. Nützlich waren Bäume den Menschen zunächst vorwiegend als Brennholz (Niederwald­wirtschaft). Im Laufe der Entwicklung wurde die Gewinnung von Nutzholz ausHochwäldernimmer wichtiger. Diese Entwicklung hält an. Laut FAO wurden noch Ende der 1990er-Jahre weltweit 46 Prozent des weltweiten Holzeinschlags (3,2 Milliarden m³) als Brennholz genutzt, in den Tropen waren es sogar 86 Prozent. Die extensiveWaldvernichtungin Zentraleuropa während des Mittelalters hat in der Neuzeit zur Einführung des Prinzips dernachhaltigenWaldbewirtschaftung geführt, nach dem nur so viel Holz entnommen werden darf, wie nachwächst. In den Primärwäldern der feuchtenTropenfindet sich die größte Artenvielfalt aller Waldtypen.[12]Wichtige tropische Familien sind dieWolfsmilchgewächse(Euphorbiaceae),Seifenbaumgewächse(Sapindaceae),Bombacaceae,Byttnerioideae(zu denMalvaceae),Mahagonigewächse(Meliaceae),Hülsenfrüchtler(Fabaceae),Caesalpiniaceae,Verbenaceae,Sterculiaceae,DipterocarpaceaeundSapotaceae. In dersubtropischenZone findet man Bäume unter den immergrünenMyrtengewächsen(Myrtaceae) undLorbeergewächsen(Lauraceae) sowieSilberbaumgewächsen(Proteaceae), denen sich in der wärmeren gemäßigten Zone andere immergrüne Bäume anschließen, so die immergrünenEichen,Granatbäume,Orangen- undZitronenbäumesowieÖlbäume. Dagegen sind in dergemäßigten Zonedie laubwechselnden Bäume vorherrschend. Hier sind Wälder vonEichen,BuchenundHainbuchencharakteristisch. Zu den inMitteleuropaheimischenLaubbäumenzählen dieAhorne,Birken,Buchen, Eichen,Erlen,Eschen,Linden,Mehlbeeren,Pappeln,UlmenundWeiden. TypischeNadelbäumesind dieFichten,Kiefern,Lärchen,TannenundEiben. In Mitteleuropa häufig vorkommende Baumarten, die in diesem Gebiet ursprünglich nicht beheimatet sind, sind dieGewöhnliche Robinie, derWalnussbaumund vieleObstbäume. Eine detaillierte Aufstellung bietet dieListe von Bäumen und Sträuchern in Mitteleuropa. Und obgleich auch hier bereitsNadelhölzerin zusammenhängenden Waldungen auftreten, werden die Nadelwälder erst in dersubarktischen(borealen) Zonevorherrschend, wo dieLaubbäumenach und nach verdrängt werden. Artenvielfalt wie auch Wuchshöhe der Bäume nehmen mit zunehmender Annäherung an denPolarkreisab. Eichen, Linden, Eschen, Ahorne und Buchen finden sich inSchwedennur noch diesseits des 64. Grades nördlicher Breite. Jenseits dieser Breite besteht die Baumvegetation hauptsächlich aus Fichten und Tannen, die in zusammenhängenden Wäldern nordöstlich noch über den 60. Grad hinausreichen, sowie aus Birken, die in zusammenhängenden Beständen sich fast bis zum 71. Grad nördlicher Breite erstrecken, und zum Teil aus Erlen undWeiden. Auch die Höhe über demMeeresspiegelhat auf die Ausbreitung und Höhe der Bäume (in Abhängigkeit von der geographischen Breite) einen bedeutenden Einfluss. In denAndenfinden sich noch bis in 5000 m HöhePolylepis-Bäume. Unter 30 Grad nördlicher Breite, wo die Schneegrenze bei 4048–4080 m liegt, kommen auf demHimalaja, nördlich vonIndien, noch in 3766 m Höhe Baumgruppen vor, die aus Eichen und Fichten bestehen. Ebenso sind inMexiko, unter 25–28 Grad nördlicher Breite, die Gebirge bis 3766 m mit Fichten und bis 2825 m hoch mit mexikanischen Eichen bedeckt. In denAlpendes mittleren Europas endet der Holzwuchs bei einer Höhe von 1570 m, imRiesengebirgebei 1193 m und auf demBrockenbei 1005 m. Eichen und Tannen stehen auf denPyrenäennoch bis zu einer Höhe von 1883 m; dagegen wächst die Fichte auf demSulitelmainLappland, bei 68 Grad nördlicher Breite, kaum in einer Höhe von 188 m, die Birke kaum in einer von 376 m. Insgesamt gibt es auf der Erde etwa 73.200 Baumarten, 19 % dieser Arten kommen in Eurasien vor, 8 % in Nordamerika, 13 % in Afrika, 8 % in Ozeanien und der artenreichste Kontinent mit 49 % aller Arten ist Südamerika. Von den etwa 73.000 Arten sind (Stand Januar 2022) etwa 9200 Arten laut einer Einschätzung von Wissenschaftlern nicht entdeckt und beschrieben.[13]Der weltweite Datensatz der erfassten Baumarten umfasst Stand Januar 2022 insgesamt 64.100 Baumarten.[14] Die wissenschaftliche Lehre von den Bäumen (Gehölzen) ist dieDendrologie. Anpflanzungen von Bäumen in systematischer oder pflanzengeographischer Anordnung, dieArboreten(von lateinischarborfür ‚Baum‘), dienen ihr zu Beobachtungs- und Versuchszwecken. Gehölze könnenvegetativ, das heißt durch Pflanzenteile, oder generativ durch Aussaatvermehrt werden. InBaumschulenfindet eine gezielte Auslese, Anzucht und Vermehrung von Bäumen und Sträuchern statt. Neben der forstlichen Nutzung finden Bäume reichliche Verwendung imGarten- und Landschaftsbau. Mit derBaumpflegehat sich ein eigener Berufsstand zum Erhalt und zur fachgerechten Behandlung von Bäumen in urbanen Regionen entwickelt. „Kein anderes Geschöpf ist mit dem Geschick der Menschheit so vielfältig, so eng verknüpft wie der Baum.“ Das schrieb der HistorikerAlexander Demandtund hat dem Baum mitÜber allen Wipfeln – Der Baum in der Kulturgeschichteein umfangreiches Werk gewidmet. Für ihn beginnt die Kulturgeschichte mit dem Feuer, das der Blitz in die Bäume schlug, und mit dem Werkzeug, für das Holz zu allen Zeiten unentbehrlich war. Neben der wichtigen Funktion der Bäume bei der Gestaltung von Kulturlandschaften begleitet vor allem die Holznutzung die Entwicklung der Menschheit. Abgesehen von der vor allem in Entwicklungsländern immer noch weit verbreiteten Brennholznutzung ist Holz ein vielseitiger Bau- und Werkstoff, dessen produzierte Menge die Produktionsmengen vonStahl,AluminiumundBetonweit übersteigt. Damit ist Holz nach wie vor der wichtigste Bau- und Werkstoff weltweit; Bäume sind dementsprechend eine bedeutende Rohstoffquelle. Neben der Holznutzung werden Bäume auch zur Gewinnung von Blüten, Früchten, Samen oder einzelnen chemischen Bestandteilen (Terpentin,Zucker,Kautschuk,Balsame,Alkaloideund so weiter) genutzt. In derForstwirtschaftder industrialisierten Länder spielen diese Nutzungen eine untergeordnete Rolle. Lediglich derObstbauals Teilbereich derLandwirtschaftist in vielen Regionen ein wichtiger Wirtschaftsfaktor. Der Anbau erfolgt in Form vonPlantagen. Hochwertige Obstsorten werden meist durchOkulationoderPfropfenveredelt. Dies erfolgt durch den Einsatz ausgewählter Obstsorten, wobei die bekannten und gewollten Eigenschaften der Früchte einer Obstsorte auf einen jungen Baum übertragen werden. Zurückgegangen ist dagegen die Nutzung vonStreuobstwiesen, die früher in vielen Gebieten Mitteleuropas landschaftsprägend waren. Als großeKohlenstoffsenkeleisten Bäume einen wichtigen Beitrag gegen die derzeitigeglobale Erwärmung. Außerdem wird der wichtige Beitrag derStraßenbäumezur Verbesserung der Luftqualität im Rahmen desStadtklimaszunehmend als Teil der Städteplanung mit berücksichtigt, denn Bäume verbessern die Stadtluft durch Sauerstoffproduktion, Staubfilterung und kühlende Verdunstung.[15]Dabei steigen die Ansprüche an die Stadtbäume durch denKlimawandel, der an vielen Orten u. a. für häufigere und längereHitzewellensorgt. Zu den am besten geeigneten Baumarten zählen, wenn man Faktoren wie den Wasserbedarf und den Kühlungseffekt betrachtetRobinieundLinde.[16]Auf der Suche nach Stadtbäumen die höhere Temperaturen, Schadstoffbelastung und Schädlinge besser verkraften als andere Sorten, erwiesen sich insbesondere Baumarten gut, die bisher noch nicht zum typischen Stadtbild zählen. Als besonders geeignet für den Einsatz im städtischen Bereich erwiesen sich – anhand von in Bayern durchgeführten Versuchsbepflanzungen –Silber-Linde,Europäische Hopfenbuche,Amberbäume,Ginkgos,Zürgelbäume, derFranzösische Ahornsowie die zu denUlmengewächsengehörendenZelkoven.[17] Dieser Bedeutung entsprechend ist ein vielfältiges Brauchtum mit dem Baum verknüpft. Das reicht vom Baum, der zur Geburt eines Kindes zu pflanzen ist, über denMaibaum, der in manchen Regionen in der Nacht zum ersten Mai der Liebsten verehrt wird, überKirmesbaumundWeihnachtsbaum, unter denen man feiert, und über denRichtbaumauf dem Dachstuhl eines neu errichteten Hauses bis zum Baum, der auf dem Grab gepflanzt wird. Nationen und Völkern werden bestimmte, für sie charakteristische Bäume zugeordnet.EicheundLindegelten als typisch „deutsche“ Bäume. DieBirkesymbolisiert Russland, und derBaobabgilt als der typische Baum derafrikanischenSavanne. Unter derGerichtslindewurde Recht gesprochen (siehe auch →Thing) und unter derTanzlindegefeiert. Seit 1989 wird jedes Jahr im Oktober für das darauffolgende Jahr derBaum des Jahresbestimmt, zunächst vom „Verein Baum des Jahres e. V.“, seit 2008 von der „Dr. Silvius Wodarz Stiftung“ und durch deren Fachbeirat, das „Kuratorium Baum des Jahres“ (KBJ).[18]Im Jahr 2000 wählte die Stiftung denGinkgo-Baum (Ginkgo biloba) zum Baum des Jahrtausends als Mahnmal für Umweltschutz und Frieden.[19] Bäume, die als Risiko- oder Gefahrenquellen in Erscheinung treten, werden mitunter alsGefahrenbaumklassifiziert. ZahlreicheMythenerzählen von einem Lebens- oderWeltenbaum, der die Weltachse im Zentrum des Kosmos darstellt. Bei den nordischen Völkern war es zum Beispiel die WeltescheYggdrasil, unter deren Krone dieAsenihr Gericht abhielten. So spielt der Baum in den Mythen der Völker als Lebensbaum wie dieSykomorebei den Ägyptern oder in der jüdischen Mythologie eine Rolle.Kelten,Slawen,GermanenundBaltenhaben einst in Götterhainen Bäume verehrt, und das Fällen solcher Götzenbäume ist der Stoff zahlreicher Legenden, die von der Missionierung Nord- und Mitteleuropas berichten. In vielen alten Kulturen und Religionen wurden Bäume oder Haine als Sitz der Götter oder anderer übernatürlicher Wesen verehrt. Solche Vorstellungen haben sich als abgesunkenes religiöses Gut bis in die heutige Zeit erhalten. Als Baum der Unsterblichkeit gilt der Pfirsichbaum in China. DerBodhibaum, unter demBuddhaErleuchtung fand, ist imBuddhismusein Symbol desErwachens. Auch in derBibelwerden Bäume immer wieder erwähnt.Tanachwie auch dasNeue Testamentnennen unterschiedliche Baumarten, wie zum Beispiel denOlivenbaumoder denFeigenbaum, mit dessen relativ großen Blättern das erste MenschenpaarAdam und Evalaut 1. Mose/Genesis 3:7 nach ihremSündenfallihreBlößebedeckte. Im1. Buch Mose, derGenesis, wird in Kapitel 1 in den Versen 11 und 12 berichtet, dassGottdie Bäume und insbesondere die fruchttragenden Bäume in seinerSchöpfungder Welt hervorbrachte. Zwei Bäume spielen in der Schöpfungsgeschichte eine entscheidende Rolle: DerBaum des Lebensund derBaum der Erkenntnis(von Gut und Böse). So hat der Baum auch in der christlichenIkonographieeine besondere Bedeutung. Dem Baum als Symbol des Sündenfalls, um dessen Stamm sich eine Schlange windet, steht häufig das hölzerne Kreuz als Symbol der Erlösung gegenüber. Ein dürrer und ein grünender Baum symbolisieren in denDogmenallegorienderReformationszeitdenAltenund denNeuen Bund. In der Pflanzensymbolik haben verschiedene Baumarten wie auch ihre Blätter, Zweige und Früchte eine besondere Bedeutung. So weist dieAkazieauf die Unsterblichkeit der menschlichen Seele hin, derÖlbaumauf den Frieden und ist ein altesmarianisches Symbolfür die Verkündigung an Maria. Der Zapfen derPinieweist auf die Leben spendende Gnade und Kraft Gottes hin, dieStechpalme, aus deren Zweigen nach der Legende dieDornenkronegefertigt war, auf die PassionChristi. DerArbre de Diane(Dianes Baum) ist eine Platane inLes Clayes-sous-Bois, Frankreich, die 1556 vonDiana von Poitiers, der Mätresse Heinrichs II., gepflanzt worden sein soll. Gedenkbäumesind Bäume, die zum Gedenken an ein Ereignis oder zum Gedenken an eine Person gepflanzt wurden. Informationen über verschiedene Baumarten: Informationen über seltene mitteleuropäische Baumarten: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Entwicklung in der Erdgeschichte 2Systematik 3Morphologie 3.1Wachstum 3.2Alter 3.3Schädigungen 3.4Stamm 3.4.1Definition 3.4.2Bestandteile 3.5Wurzel 3.6Blätter 3.7Blüten 3.8Frucht- und Samenbildung 4Physiologie 4.1Wuchs 4.2Wasserleitung 5Ökologie 5.1Wald 5.2Verbreitungszentren, Diversität 6Bäume und Menschen 6.1Nutzung 6.2Gesellschaftliches 6.3Mythologie und Religion 6.4In der Geschichte 7Superlative 8Siehe auch 9Filmografie 10Literatur 10.1Einführungen/Übersichten 10.2Bestimmungsbücher 10.3Kulturgeschichte"
  },
  {
    "label": 0,
    "text": "Bier – Wikipedia Bier Inhaltsverzeichnis Etymologie Geschichte Bierhandwerk Einteilung der Biere Arten Bierkonsum und Brauwirtschaft Biermaße Inhaltsstoffe Gesundheit und Risiken Kuriosa Tag des Bieres Siehe auch Literatur Weblinks Einzelnachweise Brauprozess Bierbeprobung Einteilung nach Stammwürzegehalt (Biergattung) Einteilung nach Art der verwendeten Hefe (Bierart) Einteilung nach dem Brauort (Biertyp) Einteilung nach weiteren unterschiedlichen Merkmalen (Biersorte) Alkoholfreie Biere Biermischgetränke Weltweit Europäische Union Vitamine und Mineralstoffe Aromastoffe Herz und Kreislauf Glutenunverträglichkeit Werbung zu gesundheitsbezogenen Wirkungen Stärkstes Bier der Welt Sonstiges Deutschland Österreich Schweiz Deutschland Österreich Deutschland Österreich Schweiz Tschechien Bierist einGetränk, das durchGärungaus stärkehaltigen Stoffen gewonnen und nichtdestilliertwird. Bei der Herstellung des meistkohlensäurehaltigenGetränks werden oftHopfenoder andere Würzstoffe zugesetzt,[1]etwaFrüchte,KräuterwieGrutoder andereGewürze. Weinund Bier entstehen durch Gärung vonZucker. Für Weine werden Zucker aus pflanzlichen oder tierischen Quellen (zum BeispielHonig) vergoren, während der Ausgangsstoff für die Gewinnung von Zucker bei Bier immerStärkeist. Der Zucker wird aus der Stärke vonGetreide(Gerste,Weizen,Roggen,Hafer,Hirse,Reis,Mais) durchMälzenoder andere enzymatische Verfahren gewonnen, seltener wird Stärke ausKartoffelnoder anderem Gemüse wieErbsenherangezogen. Bier gibt es praktisch ebenso lange, wie esAgrarkulturengibt. In Anbaugebieten Zentraleuropas für Gerste und Roggen sowie in den USA und einigen anderen, den so genanntenBierländern, ist es das beliebteste alkoholische Getränk. Bier und Braukunst wird weltweit als Identifikationsmerkmal und wesentlichen Bestandteildeutscher Kulturangesehen, auch wenn in vielen anderen Ländern ebenfalls Bier gebraut wird. In Deutschland ist der Vertrieb von alkoholischen Getränken unter der Bezeichnung „Bier“ nur erlaubt, wenn sie demdeutschen Reinheitsgebot, einem Kanon von Rechtsvorschriften für die Bierbrauerei in Deutschland, entsprechen. Der Alkoholgehalt der meisten Biersorten liegt in Deutschland und Österreich zwischen 4,5Vol.-%und 6 Vol.-%, je nach Sorte aber auch darüber.Alkoholfreie Bierewerden durch zwei verschiedene Verfahren – Abbrechen der Gärung oder Extraktion des Alkohols aus normalen Bieren – erzeugt. DieEtymologieist ungeklärt. Über den Ursprung des Wortes „Bier“ gibt es unterschiedliche Erklärungsansätze, von denen sich bisher jedoch keiner in den Sprachwissenschaften durchsetzen konnte. Gesichert sind jedoch die Wortformen in den historischen Sprachformen des Deutschen und seiner verwandten germanischen Sprachen, soalthochdeutschbior,mittelhochdeutschbier,mittelniederdeutschber,altenglischbeor,altfriesischbiarundaltnordischbjórr. Die beiden wichtigsten Wörterbücher zur Etymologie der deutschen Sprache – vonKluge/Seebold[2]undPfeiferet al.[3]– verzeichnen die folgenden Herleitungstheorien: Den ältesten bisher bekannt gewordenen Braubetrieb gab es in derRakefet-Höhle(heutiges Israel) im Gebiet derNatufien-Kultur vor rund 13.000 Jahren.[4]Das älteste überlieferte Bierrezept ist ca. 5000 Jahre alt und stammt aus China.[5]Frühe Nachweise für Bier gibt es aus demaltmesopotamischenRaum.[6]DieÄgypterließen halbfertig gebackenesBrotmitWasservergären und bekamen so eine Art Bier. Wie beiNaturvölkernnoch üblich (siehe dazuChichaundKava) wurde Getreide oder Brot gekaut und dabei eingespeichelt und der erhaltene Brei in einen Vorratstopf gespuckt,[7]die Einspeichelung führte zur Aufspaltung derStärkezu vergärbaren Zuckern (siehe dazuSpeichel#FunktionundSpeichel-Amylase#Katalysierte Reaktion). DieKeltenkannten mehrere Biersorten, insbesondere das weit verbreitetekormabzw.curma, ein einfaches Gerstenbier, und diecervisiabzw.cervesia(vgl.spanischcerveza), ein Weizenbier mit Honig für die wohlhabendere Bevölkerung.[8] ImMittelalterwurde Bier noch aus sehr vielen unterschiedlichen Zutaten gebraut. Es wurde Bier überwiegend mitobergärigerHefegebraut. Erst zwischen dem 13. und dem 16. Jahrhundert wurden dieKräuterbierein Mitteleuropa zunehmend vomHopfenbierverdrängt.[9]Der Hopfen verleiht dem Bier einen erfrischend bitteren Geschmack, wirkte (in Zeiten ohne künstliche Kühlung) schwach konservierend[10]und bildet ein Gegengewicht zum süßen Malz.[11] Die scherzhaft gebrauchte Bezeichnung „Flüssiges Brot“ hat einen ernsthaften historischen Hintergrund: In früheren Zeiten galt Bier als geeignetes Getränk für Kinder, da es einen geringeren Alkoholgehalt hatte und durch das Kochen derBierwürzeweitgehend keimfrei war, was vom damaligen Trinkwasser nicht behauptet werden konnte. In Zeiten von Missernten und Hunger war es wegen seinesEnergiegehaltseine wichtige Ergänzung der oft knappen Nahrung, da minderwertiges Getreide nicht weggeworfen werden musste, sondern durch das Bierbrauen halbwegs genießbar wurde. Beliebte Frühstücksspeise war bis zum 19. Jahrhundert noch dieBiersuppe. Unterernährte Wöchnerinnen erhielten alkoholfreies „Heil-Bier“ auf Rezept.[12]Im 17. Jahrhundert übernahmen Mönche den Begriff für ihr Fastenbier, denn flüssige Nahrung bricht das Fasten nicht.[13] Angesichts des hohen Bierkonsums im Mittelalter und in der frühen Neuzeit war Bier für den städtischenFiskusund die um 1500 entstehenden Landessteuerbehörden von großem Interesse. Bereits im Spätmittelalter wurden fast überall Produktions- und Verkaufssteuern auf Bier erhoben. Bierkellergab es vielerorts in Naturhöhlen. Als Bier in Kühlhäusern, die der Wiener BrauerAdolf Ignaz Mautner von Markhofunter dem Patentnamen „Normal-Bierlagerkeller System Mautner“ entwickelte, auch bei niedrigen Temperaturen gelagert werden konnte, setzte sich bald dieuntergärigeBrauweise durch. Bereits 1841 wurde das untergärigeLagerbiervonAnton DreherinSchwechatund von Adolf Ignaz Mautner in Wien gebraut; dies läutete die Epoche der untergärigen Biersorten ein. Als wichtiger Punkt in der Geschichte des untergärigen Bierbrauens gilt die „Erfindung“ derPilsner Brauart. Sie ging aus der schon damals berühmtenBayerischen Brauarthervor, die vor allem auf nur leichtgedarrtemMalzund auf der langsamenGärungdurch Lagerung in kalten Höhlen und tiefen Kellern beruhte.Josef Grollbraute am 5. Oktober 1842 den ersten Sud nach Pilsner Brauart. Dieser wurde erstmals am 11. November 1842 öffentlich ausgeschenkt und eröffnete so den weltweiten Siegeszug dieser Bierspezialität, die als OriginalPilsner Urquellvertrieben wird. InDeutschlandwird Bier nach der Bierverordnung von 2005 basierend auf demBiersteuergesetz, umgangssprachlich alsReinheitsgebotbekannt, gebraut. Bier ist das in Deutschland und vielen anderen Ländern meistkonsumierte alkoholische Getränk. InÖsterreichgeht die gewerbliche Erzeugung mit derBrauerei Hofstettenin Oberösterreich auf das Jahr 1229 zurück. Im weiteren Mittelalter entstanden zahlreiche Klosterbrauereien, die erst im 15. und 16. Jahrhundert durch Hausbrauereien zurückgedrängt wurden. Wurde bis in das 19. Jahrhundert Bier mit obergäriger Hefe produziert, änderte sich das mit der Erfindung von Presshefe durch Mautner schlagartig. Mit demSchwechater Lagerbierwurde Österreich eines der bedeutendsten Bierexportländer. Die österreichischen Brauer erzielten bei derWeltausstellung Paris 1867zahlreiche Preise. Wurde in den Weingegenden früher wesentlich mehr Wein als Bier getrunken – in Wien waren es 1732 dreimal so viel Wein wie Bier – so änderte sich das bis zum Ende des 18. Jahrhunderts. In der Zeit desVormärzMitte des 19. Jahrhunderts wurde Bier ein Modegetränk der Intellektuellen, Beamten, Studenten und Künstler. Zu Beginn des 21. Jahrhunderts lag der durchschnittliche Bierverbrauch in Österreich pro Kopf bei etwa 105 Liter im Jahr, wobei das Lager und dasMärzendie häufigsten Sorten sind.[14]Auf Grund der langen Geschichte der Biererzeugung wurde das Getränk in das Register der Traditionellen Lebensmittel aufgenommen.[15] Beim Bierbrauen werden die Bierzutaten Wasser, Malz und Hopfen miteinander vermischt und durchFermentationmittels Zugabe vonHefebiochemisch verändert. Es existieren unterschiedliche Brauverfahren an dessen Ende je nach Art der Zutaten und derBrauprozessführungunterschiedliche Biersorten entstehen. Nachdem aus Getreide, meist Gerste,Malzhergestellt wurde, wird diesesgeschrotet. Der eigentliche Brauprozess beginnt mit demMaischen. Dabei wird Wasser erwärmt, das geschrotete Malz hinzugefügt und die so entstandene Maische unter ständigem Rühren je nach Verfahren bis auf etwa 75 °C erhitzt. Bei verschiedenen Temperaturen setzenEnzyme(Diastase) die nicht vergärbare Stärke aus dem Malz in vergärbarenMalzzuckerum. Mit einerIodprobewird anschließend festgestellt, ob die gelöste Stärke vollständig verzuckert ist. Daraufhin wird die Maische im Läuterbottichgeläutert. DerMalztreberund die sogenannteWürze, der flüssige, vergärbare Teil der Maische, werden voneinander getrennt. Durch Nachgüsse mit heißem Wasser wird die Würze aus dem Treber gespült und anschließend in der Kochpfanne mit Hopfen oder auch anderen Kräutern gekocht. Bei dem nun folgendenAusschlagenwird der Sud aus der Würzepfanne in einenWhirlpooloder durch einen Filter gepumpt, um die vorhandenen Schwebstoffe wie Hopfenrückstände oder geronnenesEiweißvon der Ausschlagwürze zu entfernen. Zuletzt wird die nunAnstellwürzegenannte Flüssigkeit in einemKühlerauf die optimale Gärtemperatur abgekühlt und die Hefekultur zugesetzt. Bei der nachfolgendenalkoholischen Gärungwerden die in der Würze gelösten Zucker zuEthanolundKohlendioxidumgesetzt. Das Gas bleibt zum Teil im fertigen Bier unterDruckalsKohlensäuregebunden. Nach der Hauptgärung, die etwa eine Woche dauert, muss dasJungbiernoch etwa vier bis sechs Wochen nachgären und lagern. Das gereifte Bier wird oft nochmals gefiltert und schließlich inFlaschen,FässeroderDosenabgefüllt. Vom abgefüllten Bier werden in denBrauereienregelmäßig Stichproben entnommen und einer sensorischen Beurteilung unterzogen. Unterschieden wird zwischen Ziel ist es, wie bei den während des gesamten Brauprozesses überwachten chemisch-technischen Parametern, für die einzelnen Produkte eine gleichbleibende Qualität zu gewährleisten und Fehler rechtzeitig zu erkennen. Treten Abweichungen von den verschiedenen Qualitätsnormen der jeweiligen Brauerei auf, wird durchVerschnittmit anderen Chargen versucht, die Betriebsstandards zu erreichen. Sensorische Prüfungen werden bei Vergleichen zwischen verschiedenen Biersorten und Biermarken durchgeführt. Zusätzlich wird oft auf diesorten- und regionalspezifische Reintönigkeitgeachtet. Bei der geschmacklichen Prüfung erfolgt die Einteilung inAntrunk,MittelteilundAbgang. Zur Beprobung gehört der visuelle Eindruck, dabei wird neben der Farbe der Flüssigkeit die Beständigkeit und Porengröße desBierschaumsbeurteilt. Diese Merkmale lassen Rückschlüsse auf die Qualität des Bieres zu. Eine Klassifizierung von Bieren ist nach vielen unterschiedlichen Gesichtspunkten wie Rechtsvorschriften, Geschmack, Braustil und -zutaten oder auch Orten und Regionen möglich.[16]Vor allem im englischen Sprachraum hatte der AutorMichael Jacksonmit seinem BuchThe World Guide to Beereinen großen Einfluss auf die Kategorisierung von Biersorten. UnterStammwürzeversteht man den Gehalt anlöslichem Extraktim noch unvergorenen Bier, der wie beiSuppedie Kräftigkeit ausmacht. Aus dieser Stammwürze resultieren dann Süße, Trockenrückstand und Alkoholgehalt des vergorenen Bieres. Aufgrund von steuer- oder lebensmittelrechtlichen Erwägungen gliedert der Gesetzgeber die Biere anhand der Höhe der Stammwürze in unterschiedliche Biergattungen. In Deutschland sind Biergattungen über dieBierverordnungin vier Gruppen unterteilt. In Österreich gelten diesbezüglich folgende Sachbezeichnungen:[17] In der Schweiz lauten die Sachbezeichnungen für Biere:[18] Anhand der Art der verwendeten Hefe werden die Biersorten inobergärige Biere(z. B.AltbieroderKölsch) unduntergärige Biere(z. B.PilsoderHelles) aufgeteilt. Daneben gibt es noch die Besonderheit derspontangärigen Biere(z. B.LambicoderGeuze), bei denen die Hefe nicht kontrolliert zugesetzt wird, sondern die örtlichen, frei in der Luft über dem offenen Gärbottich fliegenden Hefen und Milchsäurebakterien genutzt werden. Die Einteilung nach Biertypen ist historisch bedingt und heutzutage nicht mehr sehr geläufig. Die Bezeichnungen für Bier leiten sich von berühmten Brauorten ab, deren Biersorten typisch für sie waren. Am Bekanntesten ist heutzutage noch dasPilsner Bier, dasMünchner,Dortmunderoder dasKölsch. Die Biersorten selbst erhalten ihre Bezeichnung nach diversen Merkmalen wie Nach demÖsterreichischen Lebensmittelbuchgibt es folgende Sorten: Schüttung zu mindestens 75 % aus Gerstenmalz: Mindestens 50 % Weizenmalz: Meist enthält „alkoholfreiesBier“ noch eine geringe Menge Restalkohol, die auch gesetzlich bis zu einem Gehalt von 0,5 Vol.-% als „alkoholfrei“ toleriert wird[19]. Der Alkoholgehalt liegt je nach Herstellungsverfahren zwischen 0,0 Vol.-% und 0,5 Vol.-%. Die meistenFruchtsäfteoderKefir[20]enthalten von Natur aus durch Gärprozesse vergleichbare Alkoholmengen. Biere mit 0,0 Vol.-% Ethanol gibt es erst seit 2006.[21]Für trockeneAlkoholiker, Schwangere, Autofahrer und Kinder ist zwar der Alkoholgehalt einer 0,5-Liter-Flasche alkoholfreien Biers kein Problem, dennoch ist alkoholfreies Bier aus psychologischen Gründen vor allem für Alkoholkranke nicht unbedenklich.[22] Es existieren zwei unterschiedliche Herstellungsverfahren: Das Abbrechen des Gärprozesses bei einem Alkoholgehalt von 0,5 Vol.-%, wie es beimMalzbierdurchgeführt wird; oder das nachträgliche Entfernen von Ethanol bis zum gewünschten Alkoholgehalt. Bei der vorzeitigen Unterbrechung der Gärung wird eine extraktschwache Würze mit 7–8 % Gehalt an Stammwürze eingesetzt. Zur Entfernung von Alkohol aus einem normalen Bier wird entweder ein Verfahren genutzt, bei dem durchUmkehrosmoseüber eineMembranoder über einenVakuum-Verdampferder Alkoholgehalt reduziert wird.[23]Teils wird eine Kombination von UmkehrosmoseundDestillationverwendet, bei der aus dem durch die Membran hindurchgetretenenPermeataus Ethanol und Wasser das Ethanol abdestilliert und das zurückbleibende Wasser, mit eventuellen weiteren Geschmacksstoffen, wieder in das hinter der Membran zurückgebliebene Konzentrat zurückgeführt wird. In Deutschland lag der Marktanteil alkoholfreier Biere 2009 bei 3,5 %, stieg bis 2015 auf ca. 5 % an und stagniert seit 2017 bei rund 7,5 %.[24] Bier wird auch mit anderen Getränken gemischt angeboten. Meist wird mit Erfrischungsgetränken oder Fruchtsäften gemischt. Sie bestehen meist zu wenigstens 50 % aus Bier. Diese Mischgetränke werden zunehmend als Fertigprodukt in den Handel gebracht. Längere Zeit sind aber bereits Mischungen bekannt, die erst unmittelbar vor dem Genuss in Lokalen bereitet wurden. Die absatzstärksten Mischgetränke sind in Deutschland mit einem Anteil von über 40 Prozent Bier-/Limomischungen, also v. a. Radler.[25] 2022 wurden insgesamt 1,9 Milliarden Hektoliter Bier gebraut. Die Länder mit der größten Produktion von Bier waren China, die USA und Brasilien.[26] Nach Angaben von „The Brewers of Europe“ lag dieEuropäische Unionim Jahr 2017 mit einer Jahresproduktion von ca. 395 Millionen hl (2018: ca. 405 Millionen hl) auf Rang 2 der weltgrößten Bierproduzenten. Für das Jahr 2018 werden EU-weit 10285 aktive Brauereien angegeben, ca. 7200 davon sind Kleinbrauereien mit einem Jahresausstoß von unter 1000 hl. Im Vergleich zum Jahr 2012 (4827 Brauereien) hat sich die Gesamtzahl der Brauereien seitdem mehr als verdoppelt. Von den aktiven Brauereien befinden sich 2030 in Großbritannien, 1600 in Frankreich und 1542 in Deutschland. Der Bierkonsum in der EU betrug 2018 ca. 370 Millionen hl und ca. 32 Millionen hl wurden aus der EU exportiert.[27][28] Im Jahr 2019 wurden in Deutschland 92,2 Millionen hl Bier abgesetzt, das waren 1,8 Millionen hl weniger als im Vorjahr. Nicht enthalten sind der Absatz von alkoholfreien Bieren und Malzbier sowie das aus Nicht-EU-Ländern eingeführte Bier. Biermischgetränke waren 2019 mit circa 4,4 Millionen hl am Bierabsatz beteiligt, dies entspricht etwa 4,8 % des gesamten Bierabsatzes. 76,1 Millionen hl (ca. 82,6 %) des gesamten Bierabsatzes waren für den deutschen Inlandsverbrauch bestimmt und wurden versteuert. Der steuerfreie Absatz betrug 16,1 Millionen hl Bier. 9 Millionen hl davon gingen in die EU-Länder, 7 Millionen hl in Drittländer und ca. 124.000 hl alsHaustrunkan die Beschäftigten der Brauereien.[30] Von der im Jahr 2019 produzierten Gesamtmenge von 86,2 Millionen hl kamen 24 Millionen hl aus Bayern, gefolgt von Nordrhein-Westfalen mit 19,4 Millionen hl. Von den insgesamt 1548 erfassten Braubetrieben bilden 862 kleine Brauereibetriebe mit weniger als 1000 hl/Jahr die größte Gruppe.[31] Nach Angaben desStatistischen Bundesamtesist der Konsum von alkoholhaltigem Bier in Deutschland seit Jahren rückläufig. 1994 wurden noch 107 Millionen hl in Deutschland abgesetzt, 2019 waren es noch 76 Millionen hl, im Jahr 2020 rund 72 Millionen hl.[32]Selbiges gilt für die Entwicklung des Pro-Kopf-Verbrauchs. Während ein deutscher Bürger im Jahr 2009 im Durchschnitt noch circa 105 l Bier trank, waren es 2013 bereits unter 100 l, 2019 nur noch 92 l.[31]2020 wurde ein deutlicher Rückgang mit einem Minus von 5,4 % auf 86,9 Liter verzeichnet. Der stärkste Einbruch innerhalb der letzten 10 Jahre lässt sich unter anderem mit den Maßnahmen zur Bekämpfung der Corona-Pandemie im Jahr 2020 begründen.[33]2023 sank der Bierabsatz in Deutschland gegenüber dem Vorjahr um weitere 4,5 Prozent, auf rund 8,4 Milliarden Liter.[34]2024 ging er um weitere 2,0 Prozent zurück, auf rund 8,3 Milliarden Liter.[35] Touristenstraßen wie dieAischgründer Bierstraßeund dieBayerische Bierstraßeerschließen die touristische Vermarktung von Bier und Brauwirtschaft. In der Schweiz wurden 2017 4,62 Mio. Hektoliter Bier getrunken. Davon wurden 3,46 Mio. hl von Schweizer Brauereien produziert und 1,59 Mio. hl aus 86 Ländern importiert. Mit 75 % ist Lagerbier am beliebtesten, gefolgt von 10 % Spezialbier (Pilsener Brauart), die restlichen 15 % verteilen sich auf diverse Spezialbiere wie Zwickel, Kellerbiere, obergärige Biere und Biermischgetränke. Der Pro-Kopf-Konsum betrug 2017 54,3 Liter Bier. Mit 921 Brauereien[36]auf 8,4 Mio. Einwohner weist die Schweiz wohl die höchste Brauereidichte der Welt auf. Als Brauerei zählt in der Schweiz eine Braustätte, die mehr als 400 Liter Bier im Jahr produziert und sich daher bei derEidgenössischen Alkoholverwaltungregistrieren muss und zwischen 17 und 34Rappen[37]pro Liter Biersteuer[38]zahlt.[39][40] In unterschiedlichen Gegenden Deutschlands, der Schweiz, Österreichs und der übrigen Welt haben sich regionale Bezeichnungen für verschiedene Größen von Biergläsern undBierflaschenetabliert, die ihren Ursprung zum Teil in alten (teilweise regionalen) Maßeinheiten haben. Bedingt durch die Art der verwendeten Zutaten, den Brau- und Gärprozess sowie die Lagerung und Reifung des Bieres sind im fertigen Getränk eine Vielzahl an unterschiedlichen Stoffen wieKohlenhydrate,Vitamine,AromastoffesowieSpurenelementevorhanden. Bei moderatem Konsum kann Bier eine gute Quelle vieler wasserlöslicherVitaminesein, darunter dieB-VitamineRiboflavin,Folsäure,Pantothensäure,PyridoxinundNiacin.Thiaminist nur in geringen Mengen vorhanden, da es während der Gärung von der Hefe abgebaut wird. Da Alkohol die Thiaminaufnahme hemmt, kann das vorhandene Thiamin schlechter verwertet werden. Fettlösliche Vitamine werden während des Brauprozesses abgeschieden und gelangen dadurch nicht ins Endprodukt.Vitamin Cist zwar in Gerste und grünem Malz vorhanden, wird aber beimDarrenzerstört. Manchen Bieren wird es alsAntioxidationsmittelzugesetzt.[43] Bier ist reich anKalium,Magnesium,SelenundSilicium, enthält aber nur wenigCalcium,EisenundZink. Das hohe Kalium-Natrium-Verhältnis macht Bier gut geeignet für eine natriumarme Ernährung. Da Alkoholdiuretischwirkt, kann Bier einen Mineralstoffverlust begünstigen.[43] Bier enthält zahlreiche Aromastoffe, die für den Geruch und Geschmack verantwortlich sind. Dabei unterscheiden sich Art und Menge der Aromastoffe je nach Biersorte. Alkoholfreie Biere enthalten – neben dem Alkohol – auch andere Aromastoffe nur in geringerer Konzentration als herkömmliche Biere. Eine Auswahl ist in der folgenden Galerie zu sehen.[44] Als alkoholhaltiges Getränk kann Bier eine starke psychische und im späteren Verlauf körperlicheAbhängigkeithervorrufen – alsosüchtigmachen – und zurAlkoholkrankheitführen. Da in vielen Regionen der Konsum von Bier[45]und Wein in größeren Mengen gesellschaftlich anerkannt ist und so nicht als auffälliges Verhalten gilt, wird das Suchtverhalten von den Betroffenen und ihrem Umfeld tendenziell später erkannt als bei anderen Substanzen. Bier ist eine wesentliche QuellepolyphenolischerAntioxidantien, welche vorArterioskleroseschützen können. In den Vereinigten Staaten steht Bier an Platz 3 der Antioxidantienlieferanten in Getränken und liefert pro Kopf etwa die doppelte Menge wie Rotwein. InTiermodellenkonnte eine Schutzwirkung von Bier mit und ohne Alkohol nachgewiesen werden, teils war der Effekt in alkoholhaltigem Bier aber stärker.[47] Der Polyphenolgehalt von Bier unterscheidet sich zwischen verschiedenen Sorten deutlich. Den größten Beitrag zum antioxidativen Effekt liefernSyringasäure,Sinapinsäure,KaffeesäureundFerulasäure.[46] Für Menschen mit einerGlutenunverträglichkeit(Zöliakie) sind praktisch alle konventionell gebrauten Biere aufgrund des im Braugetreide (Gerste und Weizen) enthaltenenGlutensfür den Konsum nicht oder nur eingeschränkt geeignet. Aus Getreidesorten, die kein Gluten enthalten, wirdglutenfreies Biergebraut. Verwendet werden dafür unter anderemMais,Reis,Hirse,SorghumoderBuchweizen. Diese alternativen Getreidesorten sind jedoch oft nur ineffizient zu verarbeiten oder weisen geschmackliche Abweichungen auf.[48]Traditionell zubereitetes Bier oder bierähnliche Getränke auf der Basis dieser Sorten sind in verschiedenen Regionen der Welt verbreitet, so das japanischeSakeaus Reis, das aus Mais hergestellteChichain Südamerika oder die auf Hirse basierenden GetränkeTella,Dolo,PombeundMerisain Afrika. Industriell produziertes glutenfreies Bier wird nach modernen Brauverfahren hergestellt und orientiert sich geschmacklich oft an handelsüblichen Bieren.[48] Nach derHealth-Claims-Verordnungist Werbung für Bier mit gesundheitsbezogenen Angaben nicht gestattet.[49] Da die Bierhefe ab 12 % Alkoholgehalt abstirbt, bedarf es zum Erreichen höherer Konzentrationen unterstützender Verfahren. Diese sind die nachträgliche Zugabe frischer Hefe, das Entfernen abgestorbener Hefekulturen, das Entziehen von Wasser (Eisbock) oder andere Techniken. Dabei werden mittlerweile Werte über 60 % erreicht. Da es keine einheitlichen internationalen Maßstäbe dafür gibt, wie diese Biere hergestellt werden dürfen, sind sowohl die Rekorde als auch die Getränke selbst mit Vorsicht zu genießen. Dieser Artikel ist als Audiodatei verfügbar: Mehr Informationen zur gesprochenen Wikipedia Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie 2Geschichte 3Bierhandwerk 3.1Brauprozess 3.2Bierbeprobung 4Einteilung der Biere 4.1Einteilung nach Stammwürzegehalt (Biergattung) 4.1.1Deutschland 4.1.2Österreich 4.1.3Schweiz 4.2Einteilung nach Art der verwendeten Hefe (Bierart) 4.3Einteilung nach dem Brauort (Biertyp) 4.4Einteilung nach weiteren unterschiedlichen Merkmalen (Biersorte) 4.4.1Deutschland 4.4.2Österreich 5Arten 5.1Alkoholfreie Biere 5.2Biermischgetränke 6Bierkonsum und Brauwirtschaft 6.1Weltweit 6.2Europäische Union 6.2.1Deutschland 6.2.2Österreich 6.2.3Schweiz 6.2.4Tschechien 7Biermaße 8Inhaltsstoffe 8.1Vitamine und Mineralstoffe 8.2Aromastoffe 9Gesundheit und Risiken 9.1Herz und Kreislauf"
  },
  {
    "label": 0,
    "text": "Biologie – Wikipedia Biologie Inhaltsverzeichnis Geschichte Einteilung der Fachgebiete Arbeitsmethoden der Biologie Anwendungsbereiche der Biologie „Galerie des Lebens“ (Vertreter verschiedener Organismengruppen) Siehe auch Literatur Weblinks Einzelnachweise Allgemeines Besondere Fortschritte (Auswahl) Mikrobiologie Botanik / Pflanzenwissenschaft Zoologie / Tierbiologie Humanbiologie Molekularbiologie Zellbiologie Entwicklungsbiologie Physiologie Genetik Verhaltensbiologie Ökologie / Umweltbiologie Evolutionsbiologie und Systematik Synthetische Biologie Theoretische Biologie Systembiologie Biologie(vonaltgriechischβίοςbíos„Leben“ undλόγοςlógoshier: „Lehre“, siehe auch-logie) oder historisch auchLebenskunde[1]ist dieWissenschaftvon der belebten Materie, denLebewesen. Sie ist ein Teilgebiet derNaturwissenschaftenund befasst sich sowohl mit den allgemeinen Gesetzmäßigkeiten des Lebendigen als auch mit den Besonderheiten der einzelnen Lebewesen: zum Beispiel mit ihrerEntwicklung, ihremBauplanund denphysikalischenundbiochemischenVorgängen in ihrem Inneren. Im FachBiologiewird in zahlreichen Teilgebieten geforscht. Zu den ganz allgemein auf das Verständnis des Lebendigen ausgerichteten Teilgebieten gehören insbesondereBiophysik,Genetik,Molekularbiologie,Ökologie,Physiologie,Theoretische BiologieundZellbiologie. Mit großen Gruppen der Lebewesen befassen sich dieBotanik(Pflanzen), dieZoologie(Tiere) und dieMikrobiologie(KleinstlebewesenundViren). Die Betrachtungsobjekte der Biologie umfassen u. a.Moleküle,Organellen,Zellenund Zellverbände,GewebeundOrgane, aber auch dasVerhalteneinzelnerOrganismensowie deren Zusammenspiel mit anderen Organismen in ihrerUmwelt. Diese Vielfalt an Betrachtungsobjekten hat zur Folge, dass im Fach Biologie eine Vielfalt anMethoden,TheorienundModellenangewandt und gelehrt wird. Die Ausbildung vonBiologenerfolgt an Universitäten im Rahmen einesBiologiestudiums, von Biologie-Lehramtsstudierenden zumindest zeitweise auch im Rahmen derBiologiedidaktik. In neuerer Zeit haben sich infolge der fließenden Übergänge in andere Wissenschaftsbereiche (z. B.Medizin,PsychologieundErnährungswissenschaften) sowie wegen desinterdisziplinärenCharakters derForschungneben der BezeichnungBiologieweitere Bezeichnungen für die biologischen Forschungsrichtungen und Ausbildungsgänge etabliert wie zum BeispielBiowissenschaften,Life SciencesundLebenswissenschaften. Überlegungen zumLebengab es bereits um 600 v. Chr. bei dem griechischenNaturphilosophenThales von Milet, der dasWasserals den Anfang – den Urgrund – aller Dinge bezeichnet haben soll. Von derAntikebis insMittelalterberuhte die Biologie allerdings hauptsächlich auf derBeobachtungderNatur, also nicht aufExperimenten. In die Interpretation der Beobachtungen flossen zudem häufigTheorienwie dieVier-Elemente-Lehreoder verschiedenespirituelleHaltungen ein, so auch derSchöpfungsmythosder biblischenGenesis, demzufolge „Gott der HERR den Menschen aus Staub von der Erde“ formte (Adam) und ihm „den Odem des Lebens in seine Nase“ blies – „und so ward der Mensch ein lebendiges Wesen.“[2] Erst mit Beginn derwissenschaftlichen Revolutionin der frühenNeuzeitbegannenNaturforscher, sich vomÜbernatürlichenzu lösen. Im 16. und 17. Jahrhundert erweiterte sich zum Beispiel das Wissen über dieAnatomiedurch die Wiederaufnahme vonSektionenund Erfindungen wie dasMikroskopermöglichten ganz neue Einblicke in eine bis dahin nahezu unsichtbare Welt. Der Brüsseler Arzt und PhilosophJohan Baptista van Helmontgelangte, Gedanken vonParacelsusweiterentwickelnd, im 17. Jahrhundert zu einer biologischen Lebens- und Krankheitsauffassung.[3]Die Entwicklung derChemiebrachte auch in der Biologie Fortschritte. Experimente, die zur Entdeckung von molekularen Lebensvorgängen wie derFermentationund derFotosyntheseführten, wurden möglich. Im 19. Jahrhundert wurden die Grundsteine für zwei große neue Wissenschaftszweige der Naturforschung gelegt:Gregor MendelsArbeiten an Pflanzenkreuzungen begründeten die Vererbungslehre und die spätereGenetikund Werke vonJean-Baptiste de Lamarck,Charles DarwinundAlfred Russel Wallacebegründeten dieEvolutionstheorien. Die Bezeichnung Biologie, im modernen Sinne verwendet, scheint mehrfach unabhängig voneinander eingeführt worden zu sein.Gottfried Reinhold Treviranus(Biologie oder Philosophie der lebenden Natur, 1802) undJean-Baptiste Lamarck(Hydrogéologie, 1802) verwendeten und definierten ihn erstmals. Das Wort selbst wurde schon 1797 vonTheodor Gustav August Roose(1771–1803) im Vorwort seiner SchriftGrundzüge der Lehre von der Lebenskraftverwendet und taucht im Titel des dritten Bands vonMichael Christoph HanowsPhilosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologiavon 1766 auf. Zu den Ersten, die „Biologie“ in einem umfassenden Sinn prägten, gehörte der deutsche Anatom und PhysiologeKarl Friedrich Burdach. Mit der Weiterentwicklung der Untersuchungsmethoden drang die Biologie in immer kleinere Dimensionen vor. Im 19. Jahrhundert erhielt die Medizin mit der vonTheodor Schwannbegründeten tierischen Zellenlehre eine neue biologische Grundlage. Weitere Errungenschaften des Jahrhunderts waren die Anwendung derDeszendenztheorie, dervergleichenden Morphologieund Entwicklungsgeschichte auf dieAnatomiesowie wegbereitende ErkenntnissebiochemischerundbiophysikalischerLebensvorgänge.[4]Im 20. Jahrhundert kamen die TeilgebietePhysiologieundMolekularbiologiezur Entfaltung. Grundlegende Strukturen wie die DNA, Enzyme, Membransysteme und die gesamte Maschinerie der Zelle können seitdem auf atomarer Ebene sichtbar gemacht und in ihrer Funktion genauer untersucht werden. Zugleich gewann die Bewertung von Datenerhebungen mit HilfestatistischerMethoden immer größere Bedeutung und verdrängte die zunehmend als bloß anekdotisch empfundene Beschreibung von Einzelphänomenen. Als Zweig derTheoretischen Biologiebegann sich seit den 1920er Jahren zudem, einemathematische Biologiezu etablieren. Bis zur Mitte des 20. Jahrhunderts befassten sich Biologen neben Themen der Vererbung, Entwicklung und Beeinflussung des Menschen durch die Umwelt auch mit Fragen nach dem Wesen und Sinn des Lebens. So breitete sich der vonHans DrieschvertreteneNeovitalismusaus.Jakob Johann von Uexküllstellte 1909 seine Umwelttheorie auf,Hans Spemannund seine Schüler lieferten ab 1921 bahnbrechende Untersuchungen zum Organisatoreffekt, es erfolgten experimentelle Studien zu den biologischen Trägern der Erbmasse in der Zelle und 1927 wurden Mutationen als Folge von Röntgenbestrahlung vonHermann Joseph Mullernachgewiesen.[5] Seit dem Ende des 20. Jahrhunderts entwickeln sich aus der Biologie neue angewandte Disziplinen: Beispielsweise ergänzt dieGentechnikunter anderem die klassischen Methoden der Tier- und Pflanzenzucht und eröffnet zusätzliche Möglichkeiten, die Umwelt den menschlichen Bedürfnissen anzupassen. Die praktische Biologie und Medizin gehörten zu den Disziplinen, in denen im Deutschen Reich noch Ende des 19. Jahrhunderts im Vergleich mit anderen Disziplinen am vehementesten Gegenwehr gegen die Zulassung von Frauen geübt wurde. So versuchten unter anderem E. Huschke, C. Vogt, P. J. Möbius und T. a.L. a.W. von Bischoff die geistigeInferioritätvon Frauen nachzuweisen, um deren Zulassung zum Studium zu verhindern.[6][7]Hingegen waren die beschreibenden biologischen Naturwissenschaften (aber auch andere beschreibende Naturwissenschaften wie Physik und Mathematik) weiter. Hier zeigten sich die noch ausschließlich männlichen Lehrenden in einer Studie A. Kirchhoffs (1897) zumeist offen für die Zulassung von Frauen zum Studium.[8][9] Die Biologie als Wissenschaft lässt sich durch die Vielzahl von Lebewesen, Untersuchungstechniken und Fragestellungen nach verschiedenen Kriterien in Teilbereiche untergliedern: Zum einen kann die Fachrichtung nach den jeweils betrachteten Organismengruppen (Pflanzen in der Botanik, Bakterien in der Mikrobiologie) eingeteilt werden. Andererseits kann sie auch anhand der bearbeiteten mikro- und makroskopischen Hierarchie-Ebenen (Molekülstrukturen in der Molekularbiologie, Zellen in der Zellbiologie) geordnet werden. Die verschiedenen Systeme überschneiden sich jedoch, da beispielsweise die Genetik viele Organismengruppen betrachtet und in der Zoologie sowohl die molekulare Ebene der Tiere als auch ihr Verhalten untereinander erforscht wird. Die Abbildung zeigt in kompakter Form eine Ordnung, die beide Systeme miteinander verbindet. Im Folgenden wird ein Überblick über die verschiedenen Hierarchie-Ebenen und die zugehörigen Gegenstände der Biologie gegeben. In seiner Einteilung orientiert er sich an der Abbildung. Beispielhaft sind Fachgebiete aufgeführt, die vornehmlich die jeweilige Ebene betrachten. Sie ist die Wissenschaft und Lehre von denMikroorganismen, also von den Lebewesen, die als Individuen nicht mit bloßem Auge erkannt werden können:Bakterienund andereEinzeller, bestimmtePilze, ein- und wenigzelligeAlgen(Mikroalgen) undViren. Die Botanik (auch Pflanzenwissenschaft) ging aus derHeilpflanzenkundehervor und beschäftigt sich vor allem mit dem Bau, der Stammesgeschichte, der Verbreitung und dem Stoffwechsel der Pflanzen. Die Zoologie (auch Tierbiologie) beschäftigt sich vor allem mit dem Bau, der Stammesgeschichte, der Verbreitung und den Lebensäußerungen der Tiere. Die Humanbiologie ist eine Disziplin, die sich im engeren Sinn mit der Biologie des Menschen sowie den biologischen Grundlagen der Humanmedizin und im weiteren Sinn mit den für den Menschen relevanten Teilbereichen der Biologie befasst. Die Humanbiologie entstand als eigenständige Wissenschaftsdisziplin erst in der zweiten Hälfte des 20. Jahrhunderts. Ihr verwandt ist diebiologische Anthropologie, welche jedoch zurAnthropologiegezählt wird. Ziel der biologischen Anthropologie mit ihren TeilgebietenPrimatologie,Evolutionstheorie,Sportanthropologie,Paläoanthropologie, Bevölkerungsbiologie,Industrieanthropologie,Genetik, Wachstum (Auxologie),KonstitutionundForensikist die Beschreibung, Ursachenanalyse und evolutionsbiologische Interpretation der Verschiedenheit biologischer Merkmale derHominiden. Ihre Methoden sind sowohl beschreibend als auch analytisch. Die grundlegende Stufe der Hierarchie bildet die Molekularbiologie. Sie ist jene biologische Teildisziplin, die sich mit Molekülen in lebenden Systemen beschäftigt. Zu den biologisch wichtigen Molekülklassen gehörenNukleinsäuren,Proteine,KohlenhydrateundLipide. Die NukleinsäurenDNAundRNAsind als Speicher der Erbinformation ein wichtiges Objekt der Forschung. Es werden die verschiedenenGeneund ihre Regulation entschlüsselt sowie die darin codierten Proteine untersucht. Eine weitere große Bedeutung kommt den Proteinen zu. Sie sind zum Beispiel in Form vonEnzymenals biologische Katalysatoren für beinahe alle stoffumsetzenden Reaktionen in Lebewesen verantwortlich. Neben den aufgeführten Gruppen gibt es noch viele weitere, wieAlkaloide,TerpeneundSteroide. Allen gemeinsam ist ein Grundgerüst ausKohlenstoff,Wasserstoffund oft auchSauerstoff,StickstoffundSchwefel. Auch Metalle spielen in sehr geringen Mengen in manchen Biomolekülen (z. B.ChlorophylloderHämoglobin) eine Rolle. Biologische Disziplinen, die sich auf dieser Ebene beschäftigen, sind: Zellensind grundlegende strukturelle und funktionelle Einheiten von Lebewesen. Man unterscheidet zwischenprokaryotischenZellen, die keinenZellkernbesitzen und wenig untergliedert sind, undeukaryotischenZellen, deren Erbinformation sich in einem Zellkern befindet und die verschiedeneZellorganellenenthalten. Zellorganellen sind durch einfache oder doppelte Membranen abgegrenzte Reaktionsräume innerhalb einer Zelle. Sie ermöglichen den gleichzeitigen Ablauf verschiedener, auch entgegengesetzter chemischer Reaktionen. Einen großen Teil der belebten Welt stellen Organismen, die nur aus einer Zelle bestehen, dieEinzeller. Sie können dabei aus einer prokaryotischen Zelle bestehen (die Bakterien), oder aus einer eukaryotischen (wie manche Pilze). In mehrzelligen Organismen schließen sich viele Zellen gleicher Bauart und mit gleicher Funktion zuGewebenzusammen. Mehrere Gewebe mit Funktionen, die ineinandergreifen, bilden einOrgan. Biologische Disziplinen, vornehmlich auf dieser Ebene (Beispiele): Jedes Lebewesen ist Resultat einerEntwicklung. NachErnst Haeckellässt sich diese Entwicklung auf zwei zeitlich unterschiedlichen Ebenen betrachten: Die Physiologie befasst sich mit den physikalischen, biochemischen und informationsverarbeitenden Funktionen der Lebewesen. Physiologisch geforscht und ausgebildet wird sowohl in den akademischen Fachrichtungen Biologie und Medizin als auch in der Psychologie. Als Begründer der Genetik giltGregor Mendel. So entdeckte er die später nach ihm benanntenMendelschen Regeln, die in der Wissenschaft allerdings erst im Jahr 1900 rezipiert und bestätigt wurden. Der heute weitaus wichtigste Teilbereich der Genetik ist dieMolekulargenetik, die in den 1940er Jahren begründet wurde. Die Verhaltensbiologie erforscht das Verhalten der Tiere und des Menschen. Sie beschreibt das Verhalten, stellt Vergleiche zwischen Individuen und Arten an und versucht, das Entstehen bestimmter Verhaltensweisen im Verlauf der Stammesgeschichte zu erklären, also den „Nutzen“ für das Individuum. Das Fachgebiet Ökologie (auch Umweltbiologie) setzt sich mit den Wechselwirkungen zwischen den Organismen und den abiotischen und biotischen Faktoren ihres Lebensraumes auf verschiedenen Organisationsebenen auseinander. EinePopulationist eine Fortpflanzungsgemeinschaft innerhalb einer Art in einem zeitlich und räumlich begrenzten Gebiet. Die Populationsökologie betrachtet vor allem die Dynamik der Populationen eines Lebensraumes auf Grund der Veränderungen der Geburten- und Sterberate, durch Veränderungen im Nahrungsangebot oder abiotischer Umweltfaktoren. Diese Ebene wird auch von der Verhaltensbiologie und derSoziobiologieuntersucht. Im Zusammenhang mit der Beschreibung und Untersuchung sozialer Verbände wie Herden oder Rudel können auch die auf den Menschen angewandtenGesellschaftswissenschaftengesehen werden. Die Lebewesen können sich positiv (z. B.Symbiose), negativ (z. B.Fressfeinde,Parasitismus) oder einfach gar nicht beeinflussen. Lebensgemeinschaft (Biozönose) und Lebensraum (Biotop) bilden zusammen einÖkosystem. Biologische Disziplinen, die sich mit Ökosystemen beschäftigen (Beispiele): Da dieEvolutionder Organismen zu einer Anpassung an eine bestimmte Umwelt führen kann, besteht ein intensiver Austausch zwischen beiden Fachdisziplinen, was insbesondere in der Disziplin derEvolutionsökologiezum Ausdruck kommt. DiePhylogenesebeschreibt die Entwicklung einer Art im Verlauf von Generationen. Hier betrachtet die Evolutionsbiologie die langfristige Anpassung an Umweltbedingungen und die Aufspaltung in neueArten. Auf der Grundlage der phylogenetischen Entwicklung ordnet die biologische Taxonomie alle Lebewesen in ein Schema ein. Die Gesamtheit aller Organismen wird in drei Gruppen, dieDomänen, unterteilt, welche wiederum weiter untergliedert werden: Mit der Klassifizierung der Tiere in diesem System beschäftigt sich dieSpezielle Zoologie, mit der Einteilung der Pflanzen dieSpezielle Botanik, mit der Einteilung der Archaeen, Bakterien undPilzedieMikrobiologie. Als häufige Darstellung wird einphylogenetischer Baumgezeichnet. Die Verbindungslinien zwischen den einzelnen Gruppen stellen dabei die evolutionäre Verwandtschaft dar. Je kürzer der Weg zwischen zwei Arten in einem solchen Baum, desto enger sind sie miteinander verwandt. Als Maß für die Verwandtschaft wird häufig die Sequenz eines weitverbreiteten Gens herangezogen. Als in gewissem Sinne eine Synthese von Ökologie, Evolutionsbiologie und Systematik hat sich seit Ende der 1980er Jahre dieBiodiversitäts­forschung etabliert, die auch den Brückenschlag zu Schutzbestrebungen für die biologische Vielfalt und zu politischen Abkommen über Schutz und Nachhaltigkeit bildet. In diesem Fachgebiet versuchen Bio-Ingenieure, künstliche lebensfähige Systeme herzustellen, die wie naturgegebene Organismen von einem Genom gesteuert werden. Die Theoretische Biologie (auch Systemische Biologie) befasst sich mit mathematisch formulierbaren Grundprinzipien biologischer Systeme auf allen Organisationsstufen. Die Systembiologie versucht, Organismen in ihrer funktionellen Gesamtheit zu verstehen. Sie folgt derSystemtheorieund nutzt neben mathematischen Modellen auch Computersimulationen. Sie überschneidet sich mit der Theoretischen Biologie. Die Biologie nutzt viele allgemein gebräuchlichewissenschaftliche Methoden, wie strukturiertes Beobachten, Dokumentation (Notizen, Fotos, Filme),Hypothesen­bildung, mathematische Modellierung, Abstraktion und Experimente. Bei der Formulierung von allgemeinen Prinzipien in der Biologie und der Knüpfung von Zusammenhängen stützt man sich sowohl auf empirische Daten als auch auf mathematische Sätze. Je mehr Versuche mit verschiedenen Ansatzpunkten auf das gleiche Ergebnis hinweisen, desto eher wird es als gültig anerkannt. Diese pragmatische Sicht ist allerdings umstritten; insbesondereKarl Popperhat sich gegen sie gestellt. Aus seiner Sicht können Theorien durch Experimente oder Beobachtungen und selbst durch erfolglose Versuche, eine Theorie zu widerlegen, nicht untermauert, sondern nur untergraben werden (sieheUnterdeterminierung von Theorien durch Evidenz). Einsichten in die wichtigsten Strukturen und Funktionen der Lebewesen sind mit Hilfe von Nachbarwissenschaften möglich. DiePhysikbeispielsweise liefert eine Vielzahl von Untersuchungsmethoden. Einfache optische Geräte wie das Lichtmikroskop ermöglichen das Beobachten von kleineren Strukturen wie Zellen und Zellorganellen. Das brachte neues Verständnis über den Aufbau von Organismen und mit derZellbiologieeröffnete sich ein neues Forschungsfeld. Mittlerweile gehört eine Palette hochauflösenderbildgebender Verfahren, wieFluoreszenzmikroskopieoderElektronenmikroskopie, zum Standard. Als eigenständiges Fach zwischen den Wissenschaften Biologie undChemiehat sich dieBiochemieherausgebildet. Sie verbindet das Wissen um die chemischen und physikalischen Eigenschaften von den Bausteinen des Lebens mit der Wirkung auf das biologische Gesamtgefüge. Mit chemischen Methoden ist es möglich bei biologischer Versuchsführung zum Beispiel Biomoleküle mit einem Farbstoff oder einem radioaktivenIsotopzu versehen. Das ermöglicht ihre Verfolgung durch verschiedeneZellorganellen, den Organismus oder durch eine ganze Nahrungskette. DieBioinformatikist eine sehr junge Disziplin zwischen der Biologie und derInformatik. Die Bioinformatik versucht mit Methoden der Informatik biologische Fragestellungen zu lösen. Im Gegensatz zur theoretischen Biologie, welche häufig nicht mit empirischen Daten arbeitet, um konkrete Fragen zu lösen, benutzt die Bioinformatik biologische Daten. So war eines der Großforschungsprojekte der Biologie, die Genomsequenzierung, nur mit Hilfe der Bioinformatik möglich. Die Bioinformatik wird aber auch in der Strukturbiologie eingesetzt, hier existieren enge Wechselwirkungen mit der Biophysik und Biochemie. Eine der fundamentalen Fragestellungen der Biologie, die Frage nach dem Ursprung der Lebewesen (auch als phylogenetischer Baum des Lebens bezeichnet, siehe Abbildung oben), wird heute mit bioinformatischen Methoden bearbeitet. DieMathematikdient als Hauptinstrument der theoretischen Biologie der Beschreibung und Analyse allgemeinerer Zusammenhänge der Biologie. Beispielsweise erweist sich die Modellierung durch Systeme gewöhnlicherDifferenzialgleichungenin vielen Bereichen der Biologie (etwa derEvolutionstheorie, Ökologie, Neurobiologie und Entwicklungsbiologie) als grundlegend. Fragen der Phylogenetik werden mit Methoden der diskreten Mathematik und algebraischen Geometrie bearbeitet. Zu Zwecken der Versuchsplanung und Analyse finden Methoden derStatistikAnwendung. Die unterschiedlichen biologischen Teildisziplinen nutzen verschiedene systematische Ansätze: Die Biologie ist eine naturwissenschaftliche Disziplin, die sehr viele Anwendungsbereiche hat. Durch biologische Forschung werden Erkenntnisse über den Aufbau des Körpers und die funktionellen Zusammenhänge gewonnen. Sie bilden eine zentrale Grundlage, auf der dieMedizinundVeterinärmedizinUrsachen und Auswirkungen von Krankheiten bei Mensch und Tier untersucht. Auf dem Gebiet derPharmaziewerden Medikamente, wie beispielsweiseInsulinoder zahlreicheAntibiotika, aus genetisch veränderten Mikroorganismen statt aus ihrer natürlichen biologischen Quelle gewonnen, weil diese Verfahren preisgünstiger und um ein Vielfaches produktiver sind. Für dieLandwirtschaftwerden Nutzpflanzen mittelsMolekulargenetikmitResistenzengegen Schädlinge versehen und unempfindlicher gegen Trockenheit und Nährstoffmangel gemacht. In derGenussmittel- undNahrungsmittelindustriesorgt die Biologie für eine breite Palette länger haltbarer und biologisch hochwertigerer Nahrungsmittel. Einzelne Lebensmittelbestandteile stammen auch hier von genetisch veränderten Mikroorganismen. So wird dasLabzur Herstellung von Käse heute nicht mehr aus Kälbermagen extrahiert, sondern mikrobiell erzeugt. Weitere angrenzende Fachgebiete, die ihre eigenen Anwendungsfelder haben, sindEthnobiologie,[17]Bionik,Bioökonomie,BioinformatikundBiotechnologie. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 1.1Allgemeines 1.2Besondere Fortschritte (Auswahl) 2Einteilung der Fachgebiete 2.1Mikrobiologie 2.2Botanik / Pflanzenwissenschaft 2.3Zoologie / Tierbiologie 2.4Humanbiologie 2.5Molekularbiologie 2.6Zellbiologie 2.7Entwicklungsbiologie 2.8Physiologie 2.9Genetik 2.10Verhaltensbiologie 2.11Ökologie / Umweltbiologie 2.12Evolutionsbiologie und Systematik 2.13Synthetische Biologie 2.14Theoretische Biologie 2.15Systembiologie 3Arbeitsmethoden der Biologie 4Anwendungsbereiche der Biologie 5„Galerie des Lebens“ (Vertreter verschiedener Organismengruppen) 6Siehe auch 7Literatur 8Weblinks 9Einzelnachweise Аԥсшәа Afrikaans Alemannisch አማርኛ Aragonés"
  },
  {
    "label": 0,
    "text": "Boston – Wikipedia Boston Inhaltsverzeichnis Geographie Einwohnerentwicklung Geschichte Politik Religion Flagge und Siegel Demografie Wirtschaft und Verkehr Sehenswürdigkeiten (Auswahl) Bedeutende Museen (Auswahl) Sonstige National Park Service Panorama Kultur Boston im Film Boston in Computerspielen Sport Persönlichkeiten Städtepartnerschaften Weblinks Einzelnachweise Geographische Lage Metropolregion Greater Boston Administrative Gliederung von Boston Stadtviertel Klima Voreuropäische Geschichte Frühe Besiedlung 1773 Boston Tea Party 1780–1890 Wachstum und Ausdehnung Melassekatastrophe 1919 Seit dem frühen 20. Jahrhundert Überblick Allgemeine Wirtschaftsstruktur Bedeutung des Finanzsektors Transport- und Verkehrssysteme Bildung Überblick Freedom Trail Black Heritage Trail Boston National Historical Park Boston Athenæum Isabella Stewart Gardner Museum Museum of Science Museum of Fine Arts New England Aquarium Gedenkanlagen Faneuil Hall Old South Meeting House Temple of the Grand Lodge of Massachusetts Boston[[ˈbɔstən]ⓘ] ist die größte Stadt inNeuengland, einem Gebiet im Nordosten derUSA, undHauptstadtdesBundesstaatesMassachusettsan derOstküste der Vereinigten Staaten. DasU.S. Census Bureauhat bei derVolkszählung 2020eine Einwohnerzahl von 675.647[2]ermittelt. Die Metropole ist eine der ältesten, wohlhabendsten und kulturell reichsten Städte der USA. Im kulturellen Bereich sind dieSymphony Hallund das in ihr residierendeBoston Symphony Orchestraweltbekannt. Die Wirtschaftskraft der Region wird hauptsächlich durch Bildungseinrichtungen,Gesundheitswesen,FinanzwirtschaftundTechnologiebestimmt. Boston (Fläche: 233,1 km²) liegt im Nordosten derVereinigten Staatenan dernordamerikanischenOstküste an derMassachusetts Bay. Deren starke Zergliederung ermöglichte die Anlage natürlicherHäfen. DerMystic Riverim Westen, derNeponset Riverim Süden und derCharles Riverim Norden begrenzen das Gebiet der Stadt grob. Die ursprünglichen im Stadtgebiet vorhandenen Hügel wurden mit der Zeit abgetragen und im Bereich Back Bay und South End abgelagert, um der östlich anliegenden Bucht weitere Landflächen abzuringen. Der zentrale Bereich Bostons bildet sowohl den Kern der Metropolregion Greater Boston mit 4,4 Millionen Einwohnern, die die StädteCambridge,Brookline,Quincyund viele suburbane Gemeinden umfasst, wie auch das Boston CMSA (Consolidated Metropolitan Statistical Area), der siebtgrößten in den USA (nach anderen Rankings an zehnter Stelle). An Boston grenzen die StädteRevere,Chelsea,Everett,Somerville,Cambridge,Watertown,Newton,Quincy,Winthrop,Brookline,Needham,Dedham,CantonundMilton. In Cambridge liegen die weltberühmteHarvard Universityund das ähnlich bedeutendeMassachusetts Institute of Technology(MIT). Boston gehört zumSuffolk CountydesCommonwealth of Massachusettsund ist beiderHauptstadt. Am 4. Oktober 2002 schloss die Stadtverwaltung Bostons die Neueinteilung des Stadtgebietes ab, das nun in 23 Neighborhoods untergliedert ist:Allston,Back Bay,Bay Village,Beacon Hill,Brighton,Charlestown,Chinatown,Dorchester,East Boston,Fenway–Kenmore,Hyde Park,Jamaica Plain,Mattapan, Mid Dorchester,Mission Hill,North End, Roslindale,Roxbury,South Boston,South End,West End,West Roxbury. Boston ist dafür bekannt, einen der attraktivsten und lebenswertesten urbanen Stadtkerne des Landes zu besitzen. Jedes Stadtviertel weist eine individuelle Atmosphäre auf, das durch die ansässigen Bewohner und ihre soziale und ethnische Zugehörigkeit bestimmt wird. Downtown, die Innenstadt, ist der Sitz des Finanzdistrikts und dort befinden sich auch Chinatown und das kommunale Verwaltungszentrum mit dem Rathaus,Boston City Hall. Back Bay, westlich desBoston Public Gardengelegen, ist einer der reichsten Stadtteile der USA. Nicht weniger wohlhabend ist Beacon Hill mit dem Regierungssitz des Staates Massachusetts, demState House. South End war früher Heimstatt einer starken Mittelstandsgesellschaft von Händlern und Seeleuten sowie deren Familien. Heutzutage leben dort eine großeLGBT-Bevölkerung, Künstler, Yuppies, Afroamerikaner und Latins. Anziehungspunkte sind die dortigen Restaurants und die Bohème-Atmosphäre. North End und East Boston haben einen dominierendenitalienischenEinfluss, beherbergen aber auch andere Ethnien. Am Nordufer des Charles River gibt es mit Charlestown einen irischen Stadtteil. Diese Bevölkerungsgruppe findet sich auch in South Boston. Allston und Brighton werden hauptsächlich von Studenten der nahegelegenenNortheastern University,Boston Universityund desBoston Collegebewohnt. Die größte Vielfalt und ethnische Diversität weist Dorchester auf. Latinos und Afroamerikaner stellen gemeinsam mit aus teurer gewordenen Wohnvierteln vertriebenen Mittelklassenfamilien den größten Anteil in den südlich an die Innenstadt angrenzenden Bezirken Roxbury und Mattapan. Die Sommermonate Juli und August können in Boston heiß und luftfeucht sein. Die langjährige Durchschnittstemperatur für Juli ist 23 °C. Während der Wintermonate November bis Februar kann das Wetter nass mit viel Schnee sein. Die durchschnittliche Temperatur im Januar ist −1 °C.[3] ¹1950–2020: Volkszählungsergebnisse Der erste Europäer, der sich in diesem Gebiet ansiedelte, war im Jahr 1625 derbritischeSiedlerWilliam Blaxton. 1629 verkaufte er Ländereien an Siedler. Die im Juni 1630 ankommendenPuritanerhatten Besitzrechte über die ganze Kolonie und überließen ihrerseits nunmehr Blaxton Land zur Bebauung. Die englischenEmigrantengruppierten sich in einer Siedlung auf einerHalbinsel, die von den indianischen UreinwohnernShawmutund von den Engländern zuerst Trimountaine (eigentlich „drei Berge“) genannt wurde.[4]Damals war die von derMassachusetts Bayumgebene Halbinsel nur durch eine sehrschmale Landverbindungmit dem Festland verbunden. Der zweite Gouverneur derMassachusetts Bay Colony,John Winthrop, gab der Siedlung den Namen Boston und gründete damit am 7. Septemberjul./17. September 1630greg.die Stadt. Der Name geht zurück auf eineStadt gleichen NamensinLincolnshire,England, aus der einige derPilgerväterstammten oder hier inhaftiert gewesen sind. John Winthrop hatte eine als „A Model of Christian Charity“ berühmt gewordene Predigt gehalten, in der er die neue Stadt alsCity upon a Hillbezeichnete und zum Ausdruck brachte, dass die Puritaner sich in einem besonderen Vertrag mit Gott verbunden wussten. Die puritanischen Werte, insbesondere Arbeit, Bildung und Gottesfürchtigkeit stabilisierten die Gesellschaft in hohem Maße und sind auch noch heute Bestandteil des gesellschaftlichen Leitbildes in Boston undNeuengland. So wurde bereits 1635, wenige Jahre nach der Siedlungsgründung, mit derBoston Latin Schooleine ersteLateinschuleund 1636 mit derHarvard Universitydie ersteUniversitätAmerikas gegründet. Am 1. Juni 1660 wurde die QuäkerinMary Dyerauf Grund eines Gesetzes von 1658, das ihren Glauben verbot, öffentlich hingerichtet. Sie gilt als die letzte religiöseMärtyrerinNordamerikas und die erste Märtyrerin desQuäkertums.[5] In der Stadt brachen mehrmals diePockenaus, Epidemien traten1721, 1730, 1752, 1764, 1776, 1778 und 1792 auf.[6] Am 20. März 1760 brach ein Stadtbrand aus, der 174 Häuser und 175 Läden zerstörte. Das Feuer vernichtete etwa ein Zehntel der Stadt. Über Personenschäden wurde nicht berichtet.[7] Bekannt wurde die Stadt durch die Boston Tea Party vom 16. Dezember 1773. Damals lösten Proteste gegen eine Erhöhung der Teesteuer durch das britische Parlament denUnabhängigkeitskriegaus. Eine rote Pflasterspur, der ungefähr vier Kilometer langeFreedom Trail, Amerikas „Pfad der Freiheit“, führt zu 16 historischen Stätten in der Stadt, darunter derBoston Common, dasMassachusetts State Housesowie dasOld State House. Nach deramerikanischen Revolutionwurde Boston zu einem der reichsten Handelshäfen der Welt. Zu den wichtigsten Handelsgütern gehörtenFische,Rum,SalzundTabak. Seit den 1820er Jahren begann sich, bedingt durch die zunehmende Einwanderung, auch das Bild der Bevölkerung deutlich zu verändern: Gaben bislang fast ausschließlichprotestantischeAbkömmlinge englischer Einwanderer den Ausschlag, so nahm nun der Anteil derKatholikeninsbesondereirischerund später auchitalienischerAbstammung deutlich zu. Dem enormen Flächenbedarf, der durch die wachsende Einwohnerzahl entstanden war, wurde die Stadt auf eine ungewöhnliche Weise gerecht: Zwischen 1630 und 1890 verdreifachte sich das Stadtgebiet nicht nur durch Eingemeindungen, sondern auch durchLandgewinnungen, insbesondere in den flachen Gewässern desCharles Riverund derMassachusetts Bay. Die dafür erforderlichen Erdmassen wurden vor allem durch die allmähliche Abtragung der drei Hügel gewonnen, die sich ursprünglich auf der Halbinsel befanden (daher 'Trimountain'). Nur in den Straßen vonBeacon Hillkann man noch Reste der einstigen Steigungen erahnen. Das dortigeMassachusetts State House, der Sitz desGouverneursvonMassachusetts, befindet sich auf der Spitze des verkleinerten Hügels. Von 1842 bis 1846 wurden in Boston durchHorace Wells,Charles Thomas JacksonundWilliam Thomas Green Mortondie ersten modernenNarkosenbei chirurgischen Operationen durchgeführt.[8] 1872 wurde die Stadt Boston von dem größten Brand in ihrer Geschichte heimgesucht. Die umfangreichen Trümmer der verbrannten Gebäude wurden danach ebenfalls als Füllmaterial für Landgewinnungen verwendet. Am 15. Januar 1919 ereignete sich ein weiteres schweres Unglück in der Nähe von North End Park, bei dem 21 Menschen zu Tode kamen und weitere 150 verletzt wurden. EinMelasse-Tank hielt aufgrund fehlerhafter Konstruktion dem Druck seines Inhalts nicht mehr stand und zerbarst, woraufhin sich 14.000 Tonnen Melasse in die Straßen Bostons ergossen und 21 Passanten ertränkte.[9] In den 1920er und 1930er Jahren durchlebte die Stadt Boston einen Strukturwandel, der durch den Rückzug alter Industrien gekennzeichnet war, der mit der im Vergleich zu anderen Gebieten in den USA hohen Lohnstruktur zu tun hatte. DerProduktionsfaktorArbeit war für viele Industriebetriebe zu teuer geworden, so dass sie abwandern mussten.[10]In der Folge reagierte die Stadt mit verschiedenen Initiativen, die unter dem NamenUrban Renewalbekannt geworden sind. Diese Initiativen beinhalteten diverse Stadtentwicklungsprogramme, die durch dieBoston Redevelopment Authority (BRA)nach der Initiierung im Jahr 1957 durchgeführt wurden. Bei den frühen Projekten wurde viel Kredit verspielt, da die Stadterneuerungsaktivitäten in vielen Fällen zu Lasten der Bevölkerungsgruppen mit unterdurchschnittlichem Einkommen ging. Diese Politik führte zu der Verdrängung großer Bevölkerungsteile aus der Kernstadt.[11]In späteren Projekten wurde diese Politik abgeändert, und es wurde das sogenannteLinkage-Prinzipals eine Art sozialer Ausgleich eingeführt. Dieses Prinzip, das bis heute fortgeführt wird, ist durch einen finanziellen Ausgleich zwischen strukturstarken und -schwachen Kernstadtgebieten gekennzeichnet. Den Entwicklern lukrativer Bauprojekte wird eine Abgabe auferlegt, die im Anschluss für die Revitalisierung verfallender Wohn- und Gewerbegebiete eingesetzt wird.[12][13] Mit dem in den 1970er Jahren einsetzenden Wirtschaftswachstum, das insbesondere durch den Medizinsektor getragen wurde, entwickelte sich die wirtschaftliche Dynamik zum ersten Mal seit mehr als 30 Jahren wieder positiv. In dieser Zeit zählten die in Boston ansässigen KlinikenMassachusetts General Hospital,Beth Israel Deaconess Medical CenterundBrigham and Women's Hospitalzu den national führenden medizinischen Einrichtungen unter anderem in Bezug auf medizinische Innovationen. Doch auch der allgemeine Wissenschaftssektor, der in Boston schon immer für das wirtschaftliche Wachstum von Bedeutung war, konnte in dieser Zeit vermehrt Studenten anziehen. Der bereits seit den 1920er Jahren immer stärker zunehmende Autoverkehr führte allerdings auch zu neuen Problemen. Diese waren gerade in Boston nicht einfach zu lösen, da die Innenstadt zu den ältesten und am dichtesten bebauten in den ganzen USA gehörte. Genau diesen Umstand sahen die verantwortlichen Lokalpolitiker Ende der 1940er Jahre jedoch als Teil des Problems. So wurde in den folgenden Jahren eine etwa vier Kilometer lange, sechsspurige Stadtautobahn als aufgeständerte Hochstraße quer durch das Herz der Stadt gelegt. Dies löste die Verkehrsprobleme jedoch nur für kurze Zeit und hatte auch eine Zerschneidung der Innenstadt bewirkt. Um die städtebaulichen Folgen desStraßendurchbruchsabzumildern und gleichzeitig die Verkehrsprobleme zu lösen, wurde in den 1980er Jahren das alsBig Digbekannte größte Straßenbau- und Revitalisierungsprojekt der Vereinigten Staaten initiiert.[14]Die Fertigstellung dieses Projektes erfolgte wegen seines Umfangs allerdings erst im Jahr 2007. Im frühen 21. Jahrhundert wuchs Boston schließlich zu einem internationalen, intellektuellen und technologischen Zentrum heran. Auch spielte die Stadt eine national bedeutende Rolle im politikwissenschaftlichen Bereich. Allerdings kam es auch zu einem Bedeutungsverlust von regionalen Institutionen im Finanzbereich und innerhalb des journalistischen Sektors. So wurde die ZeitungThe Boston Globevon derNew York Timesund die in Boston ansässige VersicherungFleetBoston Financialvon derBank of Americaübernommen.[15]Weitere problematische Entwicklungen warenGentrifizierungsprozesse, das heißt soziale Verdrängung aus Wohngebieten. Diese gingen seit den 1970er Jahren mit steigenden Grundstücks- und Wohnungspreisen einher.[16]Im Jahr 2004 hatte derGroßraum Bostondas national höchste Niveau bei den Lebenshaltungskosten und der Staat Massachusetts war der einzige US-Bundesstaat, bei dem die Bevölkerungszahl rückläufig war.[17] Ein weiterer Trend der Stadtentwicklung seit Beginn der zweiten Hälfte des 20. Jahrhunderts ist auch die Revitalisierung brachgefallener Hafenflächen. Projekte, die in Boston in diesem Bereich vorangetrieben wurden und werden, sind der sogenannteHarborwalk, ein durchgängiger Gehweg entlang der sogenanntenWaterfrontimNorth End, der durch Querungen auch an die Innenstadt anschließt, die Revitalisierung vomKaiRowes Wharfdurch einen Mischnutzungskomplex, die Revitalisierung des ArealsFort Point Channelund der Bau desBoston Exhibition und Convention Centersowie die Revitalisierung desCharlestown Navy Yard(größtes zusammenhängendes Revitalisierungsgebiet der USA).[12] Das politische System der Stadt ist durch ein starkesexekutivesSystem gekennzeichnet. Der Bürgermeister, dessen reguläre Amtszeit vier Jahre beträgt, ist mit weitreichenden Entscheidungskompetenzen ausgestattet. Die Wahl erfolgt in zwei Runden: eine unparteiliche Erstwahl und eine zweite November-Wahl zwischen den zwei Kandidaten der Erstwahl mit den meisten Stimmen.Bürgermeister von Bostonist seit November 2021 Michelle Wu.[18]Sie löste den zuvor seit 2014 amtierendenMarty Walshab, welcher ab dem 23. März 2021Arbeitsminister der Vereinigten StaatenimKabinett Bidenwar.[19] Das Stadtparlament(City Council) wird alle zwei Jahre neu zusammengesetzt. Aus den neun Wards oder Neighborhoods wird je ein Vertreter gesandt. Hinzu kommen vier von der gesamten Stadtbevölkerung bestimmte Kandidaten. Das Schulkomitee wird vom Bürgermeister, wie auch die einzelnen Ressortleiter in der Stadtverwaltung ernannt. Darüber hinaus haben weitere Behörden einen großen Einfluss auf die Stadtentwicklung. Hier seien vor allem dasMassachusetts Department of Conservation and Recreation, dieMassachusetts Port Authority(Massport) und dieBoston Redevelopment Authoritygenannt, die einen besonders großen Einfluss für das Leben in der Stadt Boston haben. Als Hauptstadt des US-Bundesstaats Massachusetts besitzt Boston aber auch überregionale Regierungsfunktionen. Durch zahlreiche Politiker, einschließlich mehrerer US-Präsidenten (zum Beispiel wurdeJohn F. Kennedyim Bostoner NachbarortBrooklinegeboren), hat Boston bzw. der Staat Massachusetts ebenfalls ein starkes politisches Gewicht bei der Bundespolitik derVereinigten Staaten. Dieser bedeutende Einfluss kommt auch durch verschiedene staatliche Behördeneinrichtungen der USA in Boston zum Ausdruck. So befinden sich hier unter anderem dasJohn F. Kennedy Federal Office Buildingund dasThomas P. O'Neill Federal Buildingund neuerdings auch dasJohn Joseph Moakley United States Courthouse, dem Hauptsitz desUnited States Court of Appeals for the First Circuit Im Jahr 2006 wurde die Stadt Boston respektive ihr Wahlamt vom Innenminister des Staats Massachusetts dafür gerügt, dass nicht genügend Wahlurnen für eine ordnungsgemäße Durchführung einer Wahl aufgestellt wurden.[20] Die Stadt Boston ist in den letzten Jahren durch eine sehr niedrige Kriminalitätsrate bekannt geworden. Dieses wird auch der engen Zusammenarbeit zwischen dem städtischen Polizeiapparat und den in den Stadtteilen vorhandenen lokalen Aktionsgruppen zugeschrieben, die maßgeblich zum Nicht-Abgleiten von Jugendlichen in die Kriminalität beiträgt.[21][22][23] Boston ist Sitz eines katholischen Erzbischofs (Erzbistum Boston). Auch der Sitz der US-amerikanischenUnitarier-Universalistenbefindet sich in Boston, Boston und Massachusetts waren frühe Zentren des nordamerikanischen antitriniatischenUnitarismus[24]. Das Stadtsiegel wurde 1823 angenommen. Gesetzlich geregelt ist, dass das Siegel folgendermaßen aussehen soll: Rund in der Form, einen Teil der Stadt darstellend, das Motto: „SICUT PATRIBUS, SIT DEUS NOBIS“ (Gott sei mit uns, wie er mit unseren Vätern war) und die Inschrift „BOSTONIA CONDITA AD. 1630 CIVITATIS REGIMINE DONATA AD. 1822“ (Boston gegründet 1630 AD Stadtrechte erhalten 1822 AD) beinhaltend.[25]Die Flagge hat einen hellblauen Hintergrund und trägt in der Mitte das Stadtwappen.[26]Eine gesetzliche Vorschrift schützt die Flagge. Die Bevölkerung bestand laut Schätzungen desUnited States Census Bureauim Jahr 2022 zu 44,2 Prozent aus Weißen und zu 22,5 Prozent aus Afroamerikanern (2015: 46,2%, 24,7%); 9,7 Prozent waren asiatischer Herkunft (2015: 9,1%). 19,6 Prozent der Bevölkerung warenHispanics[27] DerMediandes Einkommens je Haushalt lag im Mittel der Jahre 2018 bis 2022 bei 89.212US-Dollarpro Jahr (nachKaufkraftindex2022), 2015 bei 55.777 US-Dollar. Das jährliche Pro-Kopf-Einkommen belief sich im Durchschnitt der Jahre 2018 bis 2022 auf 55.949 US-Dollar. 2022 lebten 17,5% der Bevölkerung Bostons unterhalb derArmutsgrenze, 2015 21,5%.[27] Die Metropolregion von Boston erbrachte 2016 eine Wirtschaftsleistung von 422,7 Milliarden US-Dollar und belegte damit Platz 9 unter den Großräumen der USA und gehört auch weltweit zu den leistungsstärksten Wirtschaftsregionen.[28][29]Die Arbeitslosenquote betrug nur 2,9 Prozent (Stand: Mai 2018).[30] Nach einer Statistik von 2016 war Boston die US-Region mit der größten Ungleichheit im Einkommen. Die Top-5 % erzielten ein Einkommen von 266.224 $, während die untersten 20 % im Schnitt 14.942 $ verdienten.[31]Noch gravierender ist der Unterschied zwischen einzelnen Ethnien. Ein durchschnittlicher weißer Haushalt hat ein Vermögen von 247.500 $, der durchschnittliche schwarze Haushalt besitzt 8 $.[32] In einer Rangliste der Städte mit der höchsten Lebensqualität weltweit belegte Boston im Jahre 2018 den 35. Platz von 231 untersuchten Städten und den zweiten innerhalb der Vereinigten Staaten.[33] Die Wirtschaftsstruktur Bostons wird maßgeblich durch die hier ansässigen höheren Bildungseinrichtungen mitgeprägt. So zählen diese Einrichtungen nicht nur zu den größten Arbeitgebern der Stadt, sondern sind auch Kristallisationspunkt für die Ansiedlung vieler High-Tech-Unternehmen, wie zum Beispiel aus dem IT-Bereich und dem Biotechnologiesektor. Im Großraum Boston spielen zudem viele produzierende Unternehmen aus dem Verteidigungsbereich eine große Rolle. An den insgesamt 32 Hochschulen (inklusive der sonstigen höheren Bildungseinrichtungen) sind laut einem Bericht der Boston Redevelopment Authority aus dem Jahr 2003 mehr als 135.000 Studenten eingeschrieben. Diese tragen mit jährlich 4,8 Mrd. US-Dollar zum Wirtschaftswachstum der Stadt Boston bei.[34]Gleichzeitig nimmt die Stadt Boston den ersten Platz aller US-amerikanischen Städte bei der Akquisition von Forschungsgeldern im Gesundheitssektor ein.[35] Darüber hinaus nimmt inzwischen auch der Tourismus eine zentrale Rolle beim Wirtschaftswachstum der Stadt Boston ein. Im Jahr 2005 besuchten geschätzte 17,6 Mio. Personen die Hauptstadt des Bundesstaats Massachusetts. Diese große Zahl an Besuchern verteilten sich auf die Typen klassischer Tourismus, Geschäftsreisende sowie Messe- und Kongressbesucher. Hieraus wurden geschätzte 9,8 Mrd. US-Dollar für Hotelübernachtungen, Verpflegung, Unterhaltungs- respektive Freizeitaktivitäten und die Benutzung der Verkehrsmittel generiert.[36]Weitere wichtige Wirtschaftszweige sind der Finanzsektor, hier insbesondere der Versicherungsbereich, das Druck- und Verlagswesen (beispielsweiseThe Boston Globe). Darüber hinaus existieren in Boston inzwischen vier Messe- und Kongresszentren. Dies sind unter anderem dasHynes Convention Centerim Stadtteil Back Bay und das neu erbaute World Trade Center Boston an der Waterfront des Stadtteils South Boston. Aufgrund des Regierungssitzes des Staates Massachusetts sind auch die politik- und wirtschaftsnahen Dienstleistungen, wie zum Beispiel Unternehmens- und Politikberater sowie Juristen stark in Boston vertreten. Führende Unternehmen mit Hauptsitz in Boston sind unter anderemGillette, eine Tochtergesellschaft vonProcter & Gamble, sowieTeradyne, einer der weltweit führenden Hersteller von Testsystemen für Mikroprozessoren und weiteren elektronischen Bausteinen. Darüber hinaus stammt die UnternehmensberatungThe Boston Consulting Groupaus Boston und hat noch heute hier wie dasWayfair-E-Commerce-Versandhaus ihren Hauptsitz.[37]Der Immobilien-REITAmerican Tower, dem Sendemasten für Mobilfunkanlagen gehören, hat ebenfalls seinen Hauptsitz in der Stadt. Die pharmazeutische Industrie wird durch den PharmaherstellerAlexion Pharmaceuticalsrepräsentiert, der seinen Hauptsitz 2018 nach Boston verlagerte. Weitere Unternehmen befinden sich im Umland der Stadt. Hier sei insbesondere auf die sogenannteRoute 128verwiesen. DerFIRE-Sektor(Abkürzung fürFinance,Insurance andRealEstate) ist für die wirtschaftliche Entwicklung der Stadt Boston neben dem Bildungssektor von besonderer Bedeutung. Dieser Einfluss resultiert insbesondere aus dem Einfluss des Finanzsektors auf Beschäftigung, Ausbildung, wirtschaftliche Entwicklung und Wohnungsbau. Für diese Bereiche spielt die Kapitalverfügbarkeit innerhalb der Region eine besondere Rolle. Der Finanzsektor beschäftigt in der Stadt Boston geschätzte 111.000 Angestellte. Diese gilt als eines der Finanzzentren im Nordosten der USA. Die FirmaFidelity Investmentsmit 11.250 Mitarbeitern war 2001 der Platzhirsch unter den Finanzunternehmen der Stadt, gefolgt vom UnternehmenJohn Hancock Insurancemit 4.793 Angestellten, das inzwischen vom kanadischen KonzernManulife Financialübernommen wurde. Die enorme Finanzkraft der Stadt spiegelt sich auch in der Architektur wider. So wurde dasPrudential Centerwährend des Baus mehrfach kernsaniert und gehört heute zu den teuersten und schwersten Gebäuden der Welt. In einer Rangliste der wichtigsten Finanzzentren weltweit belegt Boston den 10. Platz und den dritten innerhalb der Vereinigten Staaten hinter New York und San Francisco.(Stand: 2018).[38][39] Boston ist Standort eines internationalen Flughafens. DerGeneral Edward Laurence Logan International Airport, kurzLogan International Airport, steht nach Passagierzahlen von 27,7 Mio. auf Nr. 19 der US-amerikanischen Flughäfen, aber auf Nummer 1 in denNeuenglandstaaten. DerSeehafenPort of Bostonist nach dem Containerumschlag die Nummer 12 an der Atlantikküste der USA. Sowohl der Flughafen wie auch große Teil des Hafens sind im Besitz der bundesstaatlichenMassachusetts Port Authority(massport), einer von der Stadt Boston unabhängigen Gesellschaft. Das öffentliche Nahverkehrssystem wird von derMassachusetts Bay Transportation Authority(MBTA)betrieben. Das System besteht aus S- (MBTA Commuter Rail) und vier U-Bahnlinien (subway –blue,green,red,orange) sowie lokalen Omnibuslinien und Umland-Express-Bussen. Im allgemeinen Sprachgebrauch der Bostonians wird das öffentliche Transportsystem einfach alsThe Tbezeichnet. Bedeutende Bahnhöfe in Boston sindSouth StationundNorth Station. Die South Station ist unter anderem auch Endpunkt derAmtrak-Verbindung zwischenWashington, D.C., New York und Boston, des sogenanntenNordost-Korridors. Hier kommt derAcela Express, ein Hochgeschwindigkeitszug auf Basis des französischenTGVzum Einsatz. Der Individualverkehr wird durch verschiedene Hauptverkehrsachsen bedient. Im Rahmen des sogenannten „Big Dig“ („das große Graben“; eines der aufwendigsten Tiefbauprojekte der Welt) wurden Autobahnen, die die Innenstadt durchschneiden, vor 2007 in Tunnel unter die Erde gelegt. Der sogenannteMassachusetts Turnpike, dieInterstate 90, führt westwärts in Richtung der Staatsgrenze zum US-Bundesstaat New York. DieInterstate 95verbindet als Nord-Süd-Trasse Boston mit Portland in Maine (nordwärts) bzw.New Yorkund Washington, D.C. (in südlicher Richtung). Hinzu kommt mit derInterstate 93ein weiterer Nord-Süd Interstate-Highway, der alsJohn F. Fitzgerald Expresswayvom südlichen Ende der Stadt bis in den Norden nach New Hampshire führt.[36] In Boston und Umland sind verschiedene bekannte Universitäten ansässig. In der Innenstadt gehören dazu die folgenden: Außerhalb der Kernstadt liegen: Boston weist vor allem geschichtliche und kulturelle Attraktionen mit architektonisch interessanten Ortsteilen innerhalb der Stadt auf, wie unter anderem Back Bay und Beacon Hill. Erholungsgebiete im weiteren Umland sind unter anderemCape Cod(ca. 85 km südöstlich) und die InselMartha’s Vineyard(ca. 130 km südlich). DieSkylinevon Boston bietet ebenfalls viele sehenswerteWolkenkratzer. Ein die Bürgersteige entlangführender, roter Strich mit vier Kilometern Länge führt zu Stätten der US-amerikanischen Unabhängigkeitsbewegung. Beginnend am StadtparkBoston Common, dem ältesten öffentlichen Park der USA, führt er nach Charlestown; dabei werden alle wichtigen 16 Stätten dieser geschichtlichen Periode durchlaufen. Eine Station ist unter anderem dasOld State House, von dessen BalkonJohn Adams1776 dieUnabhängigkeitserklärungverkündete.[40] DerBlack Heritage Trailverbindet auf einer Länge von über 2,5 km historisch bedeutende Stätten der schwarzen amerikanischen Geschichte. Seit demMemorial Day2012 beginnt die Tour an derFaneuil Hallund führt an mehr als 15 Orten durch Beacon Hill. Die Stätten dokumentieren wichtige Stationen desAbolitionismusund derUnderground Railroad.[41]Einige der Orte sind wegen ihrer herausragenden geschichtlichen Bedeutung zu einerNational Historic Sitezusammengefasst und bilden dieBoston African American National Historic Site.[42][40] Im National Historical Park sind Sehenswürdigkeiten zusammengefasst, die die Bedeutung der Stadt Boston innerhalb der amerikanischen Revolution deutlich werden lassen. Zu diesen Stätten zählen: Die sieben erstgenannten Sehenswürdigkeiten befinden sich am Freedom Trail.[40] Eine der ältesten Kultureinrichtungen Massachusetts mit bedeutender Bibliothek und Kunstsammlungen. Neben historischen Dokumenten und Fotografien werden hier Zeugnisse der ersten US-Präsidenten sowie zahlreiche Gemälde amerikanischer Künstler gezeigt. DasIsabella Stewart Gardner Museumwurde um die Jahrhundertwende aus antikem Baumaterial im Stil der historistischen Neorenaissance gebaut, das aus Europa eingeführt wurde. Viele interaktive Ausstellungen und Experimente laden zum Mitmachen ein. Nicht nur Kinder können hier spielerisch die Naturwissenschaften und Ingenieurwissenschaften erkunden. Angeschlossen an das Museum ist dasHaydenPlanetariummit Lasershows und aktuellen Präsentationen. Ferner findet sich hier das „Mugar Omni Theatre“, einIMAX-Kino und einSchmetterlingsgarten, in dem der Besucher auch die Züchtung von Schmetterlingen besichtigen kann. Zudem bietet das Museum weitere Ausstellungsflächen, auf denen Sonderausstellungen besucht werden können, wie zum BeispielGunther von Hagens’ „Body Worlds 2“ bis Ende Januar 2007. DasMuseum of Fine Arts, Bostonzählt zu den bedeutendsten Kunstmuseen in den USA.[44]Es beherbergt eine bedeutende Sammlung ägyptischer undnubischerAltertümer und eine bedeutende Sammlung chinesischer Objekte. Gemälde vonRembrandtundEl Grecosind zu sehen, ebenso Werke vonVincent van Gogh,Paul Gauguin,Pierre-Auguste Renoir,Édouard Manet,Edvard Munch,Alberto Giacometti,Georges RouaultundMax Beckmann. Direkt am Meer gelegen gibt es imNew England Aquariumvon Pinguinen über Seelöwen bis hin zu Quallen vieles zu sehen, was im Meer lebt. In einem riesigen, zylindrischen Salzwassertank, der von allen Seiten einsehbar ist, beherbergt einkünstliches Riffmehrere hundert verschiedene Fischarten sowie Schildkröten. Gleichsam als „Außenposten“ organisiert das NEA täglich eine Ausfahrt, bei der Buckelwale, Delfine und andere beobachtet werden können. Von 1971 bis 1985 beheimatete das Aquarium den SeehundHoover, der als sprechendes Tier berühmt und eine wissenschaftliche Sensation wurde. Diese Halle ist eines der ältesten Gebäude der Stadt Boston. Erbaut von 1740 bis 1742 mit finanzieller Unterstützung durch den KaufmannPeter Faneuil. Das aus dem Jahre 1729 stammende Gebäude (Ecke Washington/Milk Street) hat viele historisch bedeutsame Versammlungen erlebt, die letztendlich in derBoston Tea Partymündeten. Eine einprägsame Ausstellung illustriert die damaligen Ereignisse. Er ist der älteste erhalteneFreimaurertempel. DerNational Park Serviceweist für Boston einenNational Historical Parkaus, denBoston National Historical Park, und eineNational Historic Site, dieBoston African American National Historic Site. Insgesamt liegen 52National Historic Landmarks in Boston.[46]271 Bauwerke und Stätten der Stadt sind imNational Register of Historic Places(NRHP) eingetragen (Stand 5. November 2018).[47] Boston ist ein künstlerisches und intellektuelles Zentrum. DasBoston Symphony Orchestra(sowie das daraus rekrutierteBoston Pops Orchestra) genießt ebenso Weltruf wie dieUniversitätenHarvardundMIT, die in der VorstadtCambridgeliegen. Jährlich findet dasBoston Film Festivalstatt. Die lange sehr restriktive Zensurpraxis der Stadtverwaltung wurde unter dem SchlagwortBanned in Bostonzu einem inoffiziellen Markenzeichen der erotischen Literatur und offenherzigen oder kontroversen Theaterstücken und Filmen in den USA. In Boston wurde mehrere erfolgreiche Filme und Fernsehserien gedreht,[48]darunter: Die Stadt ist Heimat verschiedener Sport-Franchises: Außerdem ist Boston Zielort desBoston-Marathons, der kontinuierlich seit 1897 ausgerichtet wird und damit eine Tradition aufweist, die demMarathonderOlympischen Spieleebenbürtig ist. Bei dieser Veranstaltung wurde am 15. April 2013 einBombenanschlagverübt, bei dem drei Menschen getötet und über 170 zum Teil schwer verletzt wurden.[49] Jährlich im Oktober findet auf demCharles RiverdieHead of the Charles Regattamit 10.000 Teilnehmern und bis zu 400.000 Zuschauern statt.[50][51] Partnerstädte von Boston sind:[52] Zusätzlich zu den oben aufgeführten offiziellen Partnerstädten (englisch:sister cities) pflegt Boston zu folgenden vier Städten eine Freundschaft (englisch:less formal friendship): Allston/Brighton|Back Bay|Bay Village|Beacon Hill|Charlestown|Chestnut Hill|Chinatown|Columbia Point|Dorchester|Downtown Crossing|East Boston|Fenway–Kenmore|Financial District|Forest Hills|Fort Point|Government Center|Hyde Park|Jamaica Plain|Leather District|Longwood|Mattapan|Mission Hill|North End|Readville|Roslindale|Roxbury|South Bay|South Boston|South End|West End|West Roxbury Massachusetts Turnpike (I-90)•I-93•I-95 Route 1A•Route 2•Route 2A•Route 3•Route 9•Route 28•Route 30•Route 99•Route 128•Route 145•Route 203 Arborway•Blue Hills Parkway•Day Boulevard•Fenway•Jamaicaway•Memorial Drive•Morrissey Boulevard•Morton Street•Park Drive•Riverway•Stony Brook Reservation Parkways•Soldiers Field Road•Storrow Drive•Truman Parkway•VFW Parkway•West Roxbury Parkway Beacon Street•Bennington Street•Boylston Street•Commonwealth Avenue•Dorchester Avenue•Huntington Avenue•Massachusetts Avenue•Newbury Street•State Street•Tremont Street•Washington Street Anderson Memorial Bridge•Boston University Bridge•Bowker Overpass•Charlestown Bridge•Eliot Bridge•Harvard Bridge•Leverett Circle Connector Bridge•Longfellow Bridge•North Beacon Street Bridge•Tobin Bridge•Leonard P. Zakim Bunker Hill Memorial Bridge Callahan-Tunnel•Dewey Square Tunnel•Sumner-Tunnel•Thomas P. O’Neill Jr. Tunnel•Ted-Williams-Tunnel East Boston Expressway•John F. Fitzgerald Expressway(Central Artery) •Northeast Expressway•Northern Expressway•Northwest Expressway•Southeast Expressway•Southwest Expressway(Southwest Corridor) •Western Expressway Chester Square•Cleveland Circle•Copley Square•Dewey Square•Dock Square•Dudley Square•Kenmore Square•Louisburg Square•North Square•Packard’s Corner•Park Square•Post Office Square•Union Square•Uphams Corner Big Dig•Boston Post Road•Emerald Necklace•Liste der Brücken über den Charles River• Boston•Chelsea•Revere•Winthrop Town Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geographie 1.1Geographische Lage 1.2Metropolregion Greater Boston 1.3Administrative Gliederung von Boston 1.4Stadtviertel 1.5Klima 2Einwohnerentwicklung 3Geschichte 3.1Voreuropäische Geschichte 3.2Frühe Besiedlung 3.31773 Boston Tea Party 3.41780–1890 Wachstum und Ausdehnung 3.5Melassekatastrophe 1919 3.6Seit dem frühen 20. Jahrhundert 4Politik 5Religion 6Flagge und Siegel 7Demografie 8Wirtschaft und Verkehr 8.1Überblick 8.2Allgemeine Wirtschaftsstruktur 8.3Bedeutung des Finanzsektors 8.4Transport- und Verkehrssysteme 8.5Bildung 9Sehenswürdigkeiten (Auswahl) 9.1Überblick 9.2Freedom Trail 9.3Black Heritage Trail 9.4Boston National Historical Park 10Bedeutende Museen (Auswahl) 10.1Boston Athenæum"
  },
  {
    "label": 0,
    "text": "Chemie – Wikipedia Chemie Inhaltsverzeichnis Wortherkunft Geschichte Allgemeines Wirtschaftliche Bedeutung der Chemie Ausbildung Ansehen Berühmte Chemiker Fachrichtungen Organisationen Quellen und weiterführende Informationen Literatur Weblinks Einzelnachweise Schulunterricht Beruf Allgemeine Chemie Anorganische Chemie Organische Chemie Physikalische Chemie Biochemie Theoretische Chemie Präparative Chemie Analytische Chemie Technische Chemie Kosmochemie Lexika Sachbücher Datensammlungen Allgemeinverständliche Chemie-Zeitschriften Chemische Fachzeitschriften (Auswahl) Chemie(bundesdeutsches Hochdeutsch: [çeˈmiː];süddeutsch,Schweizerdeutsch,österreichisches Hochdeutsch: [keˈmiː]) ist diejenigeNaturwissenschaft, die sich mit dem Aufbau, denEigenschaftenund der Umwandlung vonStoffen(Substanzen oder Materialien) beschäftigt. Ein Stoff besteht ausAtomen,MolekülenoderIonen. Zentrale Begriffe der Chemie sindchemische Reaktionenundchemische Bindungen. Durch chemische Reaktionen werden chemische Bindungen gebildet oder gespalten. Dabei verändert sich die Elektronenaufenthaltswahrscheinlichkeit in den Elektronenhüllen der beteiligten Stoffe und damit deren Eigenschaften. Die Herstellung von Stoffen (Synthese) mit von der Menschheit benötigten Eigenschaften ist heute das zentrale Anliegen der Chemie. Traditionell wird die Chemie in Teilgebiete unterteilt. Die wichtigsten davon sind dieorganische Chemie, diekohlenstoffhaltigeVerbindungen untersucht, dieanorganische Chemie, die alle Elemente desPeriodensystemsund deren Verbindungen behandelt, sowie diephysikalische Chemie, die sich mit den grundlegenden Phänomenen, die der Chemie zu Grunde liegen, beschäftigt. Die Chemie in ihrer heutigen Form als exakte Naturwissenschaft entstand im 17. und 18. Jahrhundert allmählich aus der Anwendung rationalen Schlussfolgerns, basierend auf Beobachtungen undExperimentenderAlchemie. Einige der ersten bedeutendenChemikerwarenRobert Boyle,Humphry Davy,Jöns Jakob Berzelius,Joseph Louis Gay-Lussac,Joseph Louis Proust,MarieundAntoine Lavoisierund im 19. JahrhundertJustus von Liebig. Diechemische Industriezählt zu den wichtigsten Industriezweigen. Sie stellt Stoffe her, die zur Herstellung von Alltagsgegenständen (z. B.Grundchemikalien,Kunststoffe,Lacke), Lebensmitteln (auch als Hilfsmittel dazu wieDüngemittelundPestizide) oder zur Verbesserung der Gesundheit (z. B.Pharmazeutika) benötigt werden. Die BezeichnungChemieentstand aus dem von χέω, „gießen“,[1]abgeleitetenaltgriechischenWortχύμείαchymeíabzw.χημείαchēmeía[2]„[Kunst der Metall-]Gießerei“ im Sinne von „Umwandlung“. Die heutige SchreibweiseChemiewurde vermutlich erstmals vonJohann Joachim Langeim Jahre 1750–1753[3]eingeführt und ersetzte zu Beginn des 19. Jahrhunderts das seit dem 17. Jahrhundert bestehende WortChymie, das wahrscheinlich eine Vereinfachung und Umdeutung des seit dem 13. Jahrhundert belegten AusdrucksAlchemie„Kunst des Goldherstellens“ war, welches wiederum selbst eine mehrdeutigeEtymologieaufweist (zu denKonnotationenvergleiche die Etymologie des WortesAlchemie:[4][5]Das Wort wurzelt wohl inarabischal-kīmiyá, welches unter anderem „Stein der Weisen“ bedeuten kann, eventuell aus altgriechischχυμείαchymeía„Gießung“ oder auskoptisch/altägyptischkemi„schwarz[e Erden]“, vergleiche hierzu auchKemet). Bis zum Anfang des 19. Jahrhunderts galten die Begriffe „Scheidekunde“ und „Scheidekunst“ als Alternativen für das Wort Chemie.[6][7][8] DieChemie in der Antikebestand im angesammelten praktischen Wissen über Stoffumwandlungsprozesse und den naturphilosophischen Anschauungen der Antike. DieChemie im Mittelalterentwickelte sich aus derAlchemie, die in China, Europa und Indien schon seit Jahrtausenden praktiziert wurde. Die Alchemisten beschäftigten sich sowohl mit der erhofften Veredlung der Metalle (Herstellung vonGoldaus unedlen Metallen, siehe auchTransmutation) als auch mit der Suche nach Arzneimitteln. Insbesondere für die Herstellung von Gold suchten die Alchemisten nach einemElixier(Philosophen-Stein,Stein der Weisen), das die unedlen („kranken“) Metalle in edle („gesunde“) Metalle umwandeln sollte. Im medizinischen Zweig der Alchemie wurde ebenfalls nach einem Elixier gesucht, dem Lebenselixier, einem Heilmittel für alle Krankheiten, das schließlich auch Unsterblichkeit verleihen sollte. Kein Alchemist hat allerdings je den Stein der Weisen oder das Lebenselixier entdeckt. Bis zum Ende des 16. Jahrhunderts basierte die Vorstellungswelt der Alchemisten in der Regel nicht auf wissenschaftlichen Untersuchungen, sondern auf Erfahrungstatsachen und empirischen Rezepten. Alchemisten führten eine große Auswahl Experimente mit vielen Substanzen durch, um ihre Ziele zu erreichen. Sie notierten ihre Entdeckungen und verwendeten für ihre Aufzeichnungen die gleichenSymbole, wie sie auch in derAstrologieüblich waren. Die mysteriöse Art ihrer Tätigkeit und die dabei oftmals entstehenden farbigenFlammen,RauchoderExplosionenführten dazu, dass sie alsMagierundHexerbekannt und teilweise verfolgt wurden. Für ihre Experimente entwickelten die Alchemisten mancheApparaturen, die auch heute noch in derchemischen Verfahrenstechnikverwendet werden. Ein bekannter Alchemist warAlbertus Magnus. Er befasste sich alsKlerikermit diesem Themenkomplex und fand bei seinen Experimenten ein neueschemisches Element, dasArsen. Erst mit den Arbeiten vonParacelsusundRobert Boyle(The Sceptical Chymist, 1661) wandelte sich die Alchemie von einer reinaristotelischgeprägten zu einer mehr empirischen und experimentellen Wissenschaft, die zur Basis der modernen Chemie wurde. DieChemie in der Neuzeiterhielt alsWissenschaftentscheidende Impulse im 18. und 19. Jahrhundert: Sie wurde auf die Basis von Messvorgängen undExperimentengestellt, v. a. durch Gebrauch derWaage, sowie auf die Beweisbarkeit vonHypothesenund Theorien über Stoffe und Stoffumwandlungen. Die Arbeiten vonJustus von Liebigüber die Wirkungsweise vonDüngerbegründeten dieAgrarchemieund lieferten wichtige Erkenntnisse über dieanorganische Chemie. Die Suche nach einem synthetischen Ersatz für denFarbstoffIndigozum Färben vonTextilienwaren der Auslöser für die bahnbrechenden Entwicklungen derorganischen Chemieund derPharmazie. In beiden Gebieten hatte Deutschland bis zum Beginn des 20. Jahrhunderts eine absolute Vorrangstellung. Dieser Wissensvorsprung ermöglichte es beispielsweise, den zur Führung desErsten WeltkriegesnotwendigenSprengstoffstatt aus importiertenNitratenmithilfe derKatalyseaus demStickstoffderLuftzu gewinnen (sieheHaber-Bosch-Verfahren). DieAutarkie­bestrebungen der Nationalsozialisten gaben der Chemie als Wissenschaft weitere Impulse. Um von den Importen vonErdölunabhängig zu werden, wurden Verfahren zur Verflüssigung vonSteinkohleweiterentwickelt (Fischer-Tropsch-Synthese). Ein weiteres Beispiel war die Entwicklung von synthetischemKautschukfür die Herstellung von Fahrzeugreifen. In der heutigen Zeit ist die Chemie ein wichtiger Bestandteil der Lebenskulturgeworden. Chemische Produkte umgeben uns überall, ohne dass wir uns dessen bewusst sind. Allerdings habenUnfälleder chemischen Großindustriewie beispielsweise die vonSevesoundBhopalder Chemie ein sehr negativesImageverschafft, so dassSloganswie „Weg von der Chemie!“ sehr populär werden konnten. DieForschungentwickelte sich um die Wende zum 20. Jahrhundert so weit, dass vertiefende Studien des Atombaus nicht mehr zum Bereich der Chemie gehörten, sondern zurAtomphysikbzw.Kernphysik. Diese Forschungen lieferten dennoch wichtige Erkenntnisse über das Wesen der chemischen Stoffwandlung und der chemischen Bindung. Weitere wichtige Impulse gingen dabei auch von Entdeckungen in derQuantenphysikaus (Elektronen-Orbitalmodell). Die Chemie befasst sich mit den Eigenschaften der Elemente und Verbindungen, mit den möglichen Umwandlungen eines Stoffes in einen anderen, macht Vorhersagen über die Eigenschaften für bislang unbekannte Verbindungen, liefert Methoden zur Synthese neuer Verbindungen und Messmethoden, um die chemische Zusammensetzung unbekannter Proben zu entschlüsseln. Obwohl alle Stoffe aus vergleichsweise wenigen „Bausteinsorten“, nämlich aus etwa 80 bis 100 der 118 bekanntenElementeaufgebaut sind, führen die unterschiedlichen Kombinationen und Anordnungen der Elemente zu einigen Millionen sehr unterschiedlichen Verbindungen, die wiederum so unterschiedliche Materieformen wie Wasser, Sand, Pflanzen- und Tiergewebe oder Kunststoff aufbauen. Die Art der Zusammensetzung bestimmt schließlich die chemischen und physikalischen Eigenschaften der Stoffe und macht damit die Chemie zu einer umfangreichen Wissenschaft. Neben den Schulkenntnissen können besonders Interessierte und Studenten der Chemie ihre Kenntnisse durch diechemische Literaturvertiefen. Fortschritte in den verschiedenen Teilgebieten der Chemie sind oftmals die unabdingbare Voraussetzung für neue Erkenntnisse in anderen Disziplinen, besonders in den BereichenBiologieundMedizin, aber auch im Bereich derPhysikund derIngenieurwissenschaften. Außerdem erlauben sie es häufig, die Produktionskosten für viele Industrieprodukte zu senken. Beispielsweise führen verbesserteKatalysatorenzu schnelleren Reaktionen und dadurch zur Einsparung von Zeit und Energie in der Industrie. Neu entdeckte Reaktionen oder Substanzen können alte ersetzen und somit ebenfalls von Interesse in der Wissenschaft und Industrie sein. Diechemische Industrieist – gerade auch in Deutschland – ein sehr bedeutenderWirtschaftszweig: In Deutschland lag derUmsatzder 20 umsatzstärksten deutschen Chemieunternehmen 2017 bei über 250 Milliarden Euro,[9]die Zahl der Beschäftigten lag nach der Wiedervereinigung Deutschlands bei über 700.000 und ist Stand 2017 auf über 900.000 angewachsen.[9]Sie stellt einerseitsGrundchemikalienwie beispielsweiseSchwefelsäureoderAmmoniakher, oft in Mengen von Millionen von Tonnen jährlich, die sie dann zum Beispiel zur Produktion vonDüngemittelnundKunststoffenverwendet. Andererseits produziert die chemische Industrie viele komplexe Stoffe, unter anderem pharmazeutische Wirkstoffe (Arzneistoffe) undPflanzenschutzmittel(Pestizide), maßgeschneidert für spezielle Anwendungen. Auch die Herstellung vonComputern,Kraft-undSchmierstoffenfür dieAutomobil­industrie und vielen anderen technischen Produkten ist ohne industriell hergestellte Chemikalien unmöglich. Es ist Aufgabe des Chemieunterrichts, einen Einblick in stoffliche Zusammensetzung,Stoffgruppenund stoffliche Vorgänge der Natur zu geben. Stoffumwandlungen in der belebten und unbelebten Natur beruhen ebenfalls auf chemischen Reaktionen und sollten als solche erkannt werden können. Ebenso sollte aus der VermittlungnaturwissenschaftlicherErkenntnisse Verständnis für die moderne Technik und eine positive Einstellung dazu aufgebaut werden, da gerade die Chemie durch Einführung neuer Produkte einen wesentlichen Beitrag zur Verbesserung der Lebensbedingungen des Menschen geleistet hat. Nicht zuletzt dient der Chemieunterricht auch dazu, die Schüler zu mündigen Verbrauchern zu erziehen. Er wird aus diesem Grund nach Lehrplänen (Curricula) und pädagogischen Konzepten gestaltet (Chemiedidaktik). Es ist möglich, alsChemielaborantin Betrieb und Berufsschule im so genannten Dualen System ausgebildet zu werden. Ein weiterer Ausbildungsberuf für die Arbeit im Chemielabor ist derChemisch Technische Assistent(CTA). DerChemikant(auch Chemie- und Pharmatechnologe oder früher Chemiefacharbeiter) ist ein Ausbildungsberuf für Mitarbeiter in der chemischen Industrie. Viele Universitäten bieten einenStudiengang Chemiean. Ein Großteil derChemikerschließt im Anschluss an das Studium einePromotionan. Die öffentliche Wahrnehmung der Chemie hat sich im Laufe der Zeit gewandelt. Herrschte in den Industriestaaten des 19. Jahrhunderts noch Begeisterung für die technologischen Möglichkeiten, die die moderne Chemie eröffnete, trübte sich dieses Bild unter dem Eindruck des Ersten Weltkriegs mit seinem umfangreichen Einsatz an Explosivstoffen undchemischen Waffen. Im weiteren Verlauf des 20. Jahrhunderts fügten derContergan-Skandal, dieKatastrophe von BhopalundUmweltproblemedem öffentlichen Bild von der Chemie weiteren Schaden zu. Teilweise ging die chemische Industrie mitSchmutzkampagnengegen kritische Wissenschaftler vor, etwa gegenRachel Carsonnach Veröffentlichung ihres BuchesSilent Spring1962 oder gegenFrank Sherwood RowlandundMario J. Molinanach Veröffentlichung ihrer Studie zumOzonloch1974.[10] Die öffentliche Wahrnehmung der Chemie ist im deutschsprachigen Raum tendenziell negativ geprägt. Die auf Laien abgehoben wirkende, teils unverständliche Formelsprache fürchemische VerbindungensowieReaktionsgleichungenund die Berichterstattung mit Fokus aufChemiekatastrophenundUmweltskandalenhat womöglich zu einer negativen Konnotation geführt. Insbesondere in Europa ist heute unter anderem aufgrund der strikten Gesetzgebung (Chemikaliengesetz,Gefahrstoffverordnung) eine weitgehend sichere Handhabung vonChemikaliengewährleistet.[11]Um das Ansehen der Chemie zu verbessern, wurde das Jahr 2003 von verschiedenen Trägerorganisationen zum „Jahr der Chemie“ erklärt. 2011 wurde von der UN (in Zusammenarbeit mit derUNESCOund derIUPAC) zum „Internationalen Jahr der Chemie“ erklärt.[12] Irrationale Ablehnung von Chemie wird in jüngerer Vergangenheit unter dem SchlagwortChemophobiediskutiert. Diese richtet sich allerdings in erster Linie gegen chemische Stoffe, weniger gegen die Chemie als Wissenschaft oder die forschenden Chemiker selbst. Für das Vereinigte Königreich war eine Untersuchung derRoyal Society of Chemistry2015 zu dem überraschenden Ergebnis gekommen, dass die Chemie in der Öffentlichkeit einen weitaus weniger schlechten Ruf genießt, als dies von Chemikern selbst gemeinhin angenommen wird.[13]Wesentlich hierfür ist eineassoziativeTrennung zwischen Chemikern und der Chemie einerseits und chemischen Stoffen andererseits. Schädliche Auswirkungen der chemischen Industrie werden nicht den Chemikern zugeschrieben, sondern den Entscheidungsträgern in den Unternehmen. Während den Forschern eher noble Motive zugestanden und sie nur wenig mit den Endprodukten ihrer Arbeit in Verbindung gebracht werden, wird dieProfitorientierungder Unternehmen, die potentiell schädlichen Entscheidungen zugrunde liegt, kritisch gesehen.[14]Der Chemie als Wissenschaft standen die meisten Befragten neutral bis positiv, wenn auch distanziert gegenüber. 59 % gingen davon aus, dass der Nutzen der Chemie größer ist als mögliche schädliche Effekte, und 72 % erkannten die Bedeutung chemischerForschung und EntwicklungzumWirtschaftswachstuman.[13] Traditionell wird die Chemie in dieorganischeundanorganischeChemie unterteilt, etwa um 1890 kam diephysikalischeChemie hinzu. Seit derHarnstoffsynthese1828 vonFriedrich Wöhler, bei der die organische SubstanzHarnstoffaus der anorganischen VerbindungAmmoniumcyanathergestellt wurde, verwischen sich die Grenzen zwischen Stoffen aus der unbelebten (den „anorganischen“ Stoffen) und der belebten Natur (den organischen Stoffen). So stellen Lebewesen auch eine Vielzahl anorganischer Stoffe her, während im Labor fast alle organischen Stoffe hergestellt werden können. Die traditionelle, aber auch willkürliche Unterscheidung zwischen anorganischer und organischer Chemie wurde aber dennoch beibehalten. Ein Grund besteht darin, dass die organische Chemie stark vomMolekülbestimmt wird, die anorganische Chemie jedoch oft vonIonen,Kristallen,KomplexverbindungenundKolloiden. Ein weiterer ist, dass sich die Reaktionsmechanismen und Stoffstrukturen in der Anorganik und Organik vielfach unterscheiden. Eine weitere Möglichkeit ist es, die Chemie nach der Zielrichtung in die untersuchende, 'zerlegende' Analytische Chemie und in die aufbauende, produktorientierte Präparative- oder Synthetische Chemie aufzuspalten. In der Lehrpraxis der Universitäten ist die Analytische Chemie oft als Unterrichtsfach vertreten, während die Präparative Chemie im Rahmen der organischen oder anorganischen Chemie behandelt wird. Es gibt noch weitere Fachgebiete (etwa dieForensische Chemieals Teilgebiet der angewandten Chemie[15]). Unter Allgemeiner Chemie werden die Grundlagen der Chemie verstanden, die in fast allen chemischen Teilgebieten von Bedeutung sind. Sie stellt somit das begriffliche Fundament der gesamten Chemie dar: den Aufbau desAtoms, dasPeriodensystem der Elemente(PSE), dieChemische Bindung, die Grundlagen derStöchiometrie,Säuren,BasenundSalzeundchemische Reaktionen. Im Gegensatz zu anderen naturwissenschaftlichen Disziplinen gibt es in der Chemie den Terminus Technicus „Allgemeine Chemie“ (eine „Allgemeine Physik“ gibt es nicht). Insofern steht die Allgemeine Chemie am Anfang jeder näheren Beschäftigung mit der Chemie. Diese auch Anorganik genannte Richtung umfasst, einfach ausgedrückt, die Chemie aller Elemente und Verbindungen, die nicht ausschließlich Kohlenstoffketten enthalten, denn diese sind Gegenstände der organischen Chemie. Die anorganische Chemie beschäftigt sich beispielsweise mit denMineralsäuren,Metallen, und anderen kohlenstofffreien Verbindungen, aber auch mitKohlendioxid, den SäurenCyanwasserstoff(Blausäure) undKohlensäuresowie mit deren Salzen. Verbindungen, die sich nicht genau einteilen lassen fallen in den Bereich derOrganometallchemie. DieBioanorganische Chemieüberschneidet sich hingegen thematisch mehr mit der Biochemie. In der klassischen Anorganik geht es um kleine Moleküle oder überhaupt um Salze bzw. Metalle, daher reicht eine Summenformel meist aus. In derKomplexchemie, wo es dennoch Isomere gibt, werden verständlicherweise wie in der organischen Chemie systematische Namen und Strukturformeln benötigt. Oft orientieren sich diese dabei sogar an denen von ähnlich aufgebauten Substanzen in der organischen Chemie (siehe beispielsweiseSilane). Die moderne anorganische Chemie befasst sich damit der Strukturbildung (Strukturchemie) von Molekülen und Festkörpern (Festkörperchemie), um zum Beispiel neue Werkstoffe mit speziellen physikalischen und chemischen zu erschaffen oder dem komplexen Verhalten von Teilchen in Lösungen (Kolloidchemie). Historische Definition:Die Anorganische Chemie befasst sich mit den chemischen Elementen und Reaktionen der Stoffe, die nicht von organischemLeben(mithilfe der hypothetischenLebenskraft) erzeugt werden. Die organische Chemie (auch Organik) ist die Chemie des ElementesKohlenstoffund nur wenigen anderen Elementen, besitzt dennoch die größte Vielfalt an chemischen Verbindungen. Durch die Vielzahl an Strukturelementen enthält schon alleine die Chemie derKohlenwasserstoffeeine gewaltige Zahl an unterschiedlichen Substanzen, die sich nur in unterschiedlichen Bindungsarten, Anordnungen (Isomerie) oder überhaupt nur an der Struktur (Stereochemie) unterscheiden. Hinzu kommt noch, dass häufig auch Fremdatome im Kohlenwasserstoffgerüst eingebaut sind. Um diese Unzahl an Verbindungen einwandfrei zu identifizieren, genügen keine Summenformeln mehr. Aus diesem Grund gibt es dieIUPAC-Nomenklatur, die jeder Substanz (auch jeder anorganischen) einen eindeutigen, systematischen Namen zuweisen, obwohl gerade bei organischen Stoffen oft Trivialnamen (gewohnte Bezeichnungen; z. B.: Essigsäure) vorhanden sind. Die organische Chemie teilt daher ihre Verbindungen infunktionelle Gruppenmit ähnlichen chemischen Eigenschaften ein und wird anhand von vergleichbarenReaktionsmechanismengelehrt. Historische Definition: Früher wurde gedacht, dass organische Substanzen, wie schon das Wort „organisch“ sagt, nur von Lebewesen hergestellt werden können. Dies wurde einer so genannten „vis vitalis“, also einer „Lebenskraft“ zugeschrieben, die in diesen Substanzen verborgen sei. Diese Theorie war lange Zeit unangefochten, bis esFriedrich Wöhler1828 gelang, erstmals eine anorganische Substanz im Labor in eine organische umzuwandeln. Wöhlers berühmteHarnstoffsyntheseausAmmoniumcyanatdurch Erhitzen auf 60 °C. Die Strukturaufklärung und Synthese von natürlichen Stoffen ist Bestandteil derNaturstoffchemie. Heutzutage ist der Erdölverarbeitende Sektor (Petrochemie) wirtschaftlich von Bedeutung, da er Ausgangsstoffe für zahlreiche großtechnische Synthese liefert. Bei der physikalischen Chemie handelt es sich um den Grenzbereich zwischenPhysikund Chemie. Während in der präparativen Chemie (Organik, Anorganik) die Fragestellung zum Beispiel ist: „Wie kann ich einen Stoff erzeugen?“, beantwortet die physikalische Chemie stärker quantitative Fragen, zum Beispiel „Unter welchen Bedingungen findet eine Reaktion statt?“ (Thermodynamik), „Wie schnell ist die Reaktion?“ (Kinetik). Sie liefert auch die Grundlage für analytische Verfahren (Spektroskopie) oder technische Anwendungen (Elektrochemie,MagnetochemieundNanochemie). In Überschneidung mit der Meteorologie auchAtmosphärenchemie. Die an Bedeutung gewinnende theoretische Chemie, Quantenchemie oder Molekularphysik versucht, Eigenschaften von Stoffen, chemischer Reaktionen undReaktionsmechanismenanhand von physikalischen Modellen, wie zum Beispiel derQuantenmechanikoderQuantenelektrodynamikund numerischen Berechnungen zu ergründen. Die Physikalische Chemie wurde um 1890 vor allem vonSvante Arrhenius,Jacobus Henricus van ’t HoffundWilhelm Ostwaldbegründet. Letzterer war auch erster Herausgeber der 1887 gemeinsam mit van ’t Hoff gegründetenZeitschrift für physikalische Chemieund hatte inLeipzigden ersten deutschen Lehrstuhl für Physikalische Chemie inne. Das erste eigenständige Institut für Physikalische Chemie wurde 1895 vonWalther Nernst, der sich bei Ostwald habilitiert hatte, inGöttingengegründet. Weitere spezifisch der Physikalischen Chemie gewidmete Institute folgten dann in rascher Folge in Leipzig (1897),Dresden(1900),Karlsruhe(1903), Breslau,Berlin(1905) und andernorts. ChemikerundPhysiker, die vorwiegend im Bereich der Physikalischen Chemie tätig sind, werden auch als Physikochemiker bezeichnet. Die Biochemie ist die Grenzdisziplin zurBiologieund befasst sich mit der Aufklärung vonStoffwechsel-Vorgängen, Vererbungslehre auf molekularer Ebene (Genetik) und der Strukturaufklärung und der Synthese (Molekulardesign) von großen Biomolekülen. Die Anwendung der Biochemie im technischen Bereich wird alsBiotechnologiebezeichnet. Sie überschneidet sich mit den angrenzenden DisziplinenPharmazeutische ChemieundMedizinische Chemie. Theoretische Chemie ist die Anwendung nichtexperimenteller (üblicherweise mathematischer oder computersimulationstechnischer) Methoden zur Erklärung oder Vorhersage chemischerPhänomene. Die Theoretische Chemie kann grob in zwei Richtungen unterteilt werden: Einige Methoden basieren auf Quantenmechanik (Quantenchemie), andere auf der statistischen Thermodynamik (Statistische Mechanik). Wichtige Beiträge zur theoretischen Chemie bzw.physikalischen ChemieleistetenLinus Carl Pauling,John Anthony Pople,Walter KohnundJohn C. Slater. Dieses Teilgebiet der Chemie ist gewissermaßen das Gegenteil deranalytischen Chemieund befasst sich mitSynthesenvonchemischen Verbindungen. Die anderen Teilbereiche sind im Wesentlichen präparativ ausgerichtet, da es eine Hauptaufgabe der Chemie ist, Verbindungen entweder im kleinen Maßstab oder in großen Mengen, wie im Rahmen dertechnischen Chemie, zu synthetisieren. Insofern ist die präparative Chemie ein wesentlicher Bestandteil der Chemikerausbildung. Sie spielt ebenfalls eine bedeutende Rolle in sich mit der Chemie überschneidenden Gebieten, wie derpharmazeutischen Chemiebzw.pharmazeutischen Technologie. Die Analytische Chemie beschäftigt sich mit derqualitativen Analyse(welcheStoffe sind enthalten?) und derquantitativen Analyse(wie vielvon der Substanz ist enthalten?) von Stoffen. Während die klassische analytische Chemie noch stark auf aufwendigeTrennungsgänge, um verschiedene Substanzen zu isolieren undNachweisreaktionenim Reagenzglas aufbaute, so werden heutzutage diese Fragestellungen in derinstrumentellen Analytikmit hohem apparativen Aufwand bearbeitet. Auch hier wird inAnorganische analytische ChemieundOrganische analytische Chemieunterteilt. Hier haben sich zahlreiche Spezialgebiete herausgestellt, beispielsweise dieklinische Chemiein Überschneidung mit der Medizin (vergleicheLabormedizin) undToxikologie(alstoxikologische Chemie)[16]) oder dieLebensmittelchemie. Für manche Verfahren in derMikrochemieundSpurenanalytikwerden nur noch kleinste Substanzmengen benötigt. Die Technische Chemie beschäftigt sich mit der Umsetzung von chemischen Reaktionen im Labormaßstab auf großmaßstäbliche Industrieproduktion. Chemische Reaktionen aus dem Labor lassen sich nicht ohne weiteres auf die großindustrielle Produktion übertragen. Die technische Chemie beschäftigt sich daher mit der Frage, wie aus einigen Gramm Produkt im Labor viele Tonnen desselben Produktes in einer Fabrik entstehen. Etwas abstrakter ausgedrückt: Die technische Chemie sucht nach den optimalen Bedingungen für die Durchführung technisch relevanter Reaktionen; dies geschieht empirisch oder mehr und mehr durch eine mathematische Optimierung auf der Grundlage einer modellhaften Beschreibung des Reaktionsablaufs und des Reaktors. Nahezu jede Produktion in der chemischen Industrie lässt sich in diese drei Schritte gliedern. Zunächst müssen dabei dieEduktevorbereitet werden. Sie werden eventuell erhitzt, zerkleinert oder komprimiert. Im zweiten Schritt findet die eigentliche Reaktion statt. Im letzten Schritt wird schließlich das Reaktionsgemisch aufbereitet. Mit der Vorbereitung und der Aufbereitung beschäftigt sich die chemische Verfahrenstechnik. Mit der Reaktion im technischen Maßstab beschäftigt sich dieChemische Reaktionstechnik. Die Kosmochemie befasst sich mit chemischen Vorgängen imWeltraum. Ihr Gegenstand sind chemische Substanzen und Reaktionen, die im interstellaren Raum, auf interstellaren Staubkörnern und aufHimmelskörpernwie z. B.Planeten,Kometen,PlanetoidenundMondenablaufen können. Allgemeine Chemie·Anorganische Chemie·Biochemie·Organische Chemie·Physikalische Chemie·Technische Chemie·Theoretische Chemie Agrochemie·Analytische Chemie·Atmosphärenchemie·Bauchemie·Bioanorganische Chemie·Biogeochemie·Bioorganische Chemie·Biophysikalische Chemie·Chemoinformatik·Chemometrik·Elektrochemie·Femtochemie·Festkörperchemie·Geochemie·Kernchemie·Klinische Chemie·Kohlechemie·Kolloidchemie·Kombinatorische Chemie·Kosmochemie·Lebensmittelchemie·Magnetochemie·Medizinische Chemie·Meereschemie·Metallorganische Chemie·Naturstoffchemie·Oberflächenchemie·Oleochemie·Petrochemie·Pharmazeutische Chemie·Photochemie·Physikalische Organische Chemie·Polymerchemie·Quantenchemie·Radiochemie·Supramolekulare Chemie·Stereochemie·Strahlenchemie·Strukturchemie·Textilchemie·Thermochemie·Umweltchemie Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Wortherkunft 2Geschichte 3Allgemeines 4Wirtschaftliche Bedeutung der Chemie 5Ausbildung 5.1Schulunterricht 5.2Beruf 6Ansehen 7Berühmte Chemiker 8Fachrichtungen 8.1Allgemeine Chemie 8.2Anorganische Chemie 8.3Organische Chemie 8.4Physikalische Chemie 8.5Biochemie 8.6Theoretische Chemie 8.7Präparative Chemie 8.8Analytische Chemie 8.9Technische Chemie 8.10Kosmochemie 9Organisationen 10Quellen und weiterführende Informationen 11Literatur 11.1Lexika 11.2Sachbücher 11.3Datensammlungen 11.4Allgemeinverständliche Chemie-Zeitschriften 11.5Chemische Fachzeitschriften (Auswahl) 12Weblinks 13Einzelnachweise Адыгабзэ"
  },
  {
    "label": 0,
    "text": "Cricket – Wikipedia Cricket Inhaltsverzeichnis Geschichte Spielprinzip Austragungsformen Ausrüstung Cricket auf nationaler Ebene Cricket in der Gesellschaft und Medien Weblinks Einzelnachweise Anfang Organisation und Weiterentwicklung Entwicklung zum weltweiten Sport Allgemeiner Spielablauf Regeln Test Cricket und First-Class Cricket One-Day International und List A Cricket Twenty20 Cricket Cricket in Deutschland Cricket in der Schweiz Cricket in Österreich Besondere Ereignisse Anekdotisches und Cricket-Ausdrücke Cricket in der Literatur Cricket in Film und Fernsehen Spieler und Offizielle Spielfeld Ball Bowlingtechnik Runs (Punkte) Ausscheiden des Schlagmanns (Dismissals) Extras Ergebnis Bodyline Series (1932/1933) Underarm incident(1981) Ball tampering (2001/02) Spielabbruch (2006) Attentat auf die sri-lankische Mannschaft (2009) Spot Fixing Skandal (2010) Ball tampering (2018) Cricket(englisch [ˈkɹɪkɪt]; in Deutschland amtlich auchKricket,[1][2]in den Anfängen auchThorball) ist einSchlagballspiel, bei dem zweiMannschaftenmit jeweils elf Spielern gegeneinander antreten. Kernelement des Spiels ist das Duell zwischen demBowler(Werfer) und demBatter(Schlagmann). Während der Bowler versucht, den Batter zu einem Fehler zu bewegen und damit ausscheiden zu lassen, versucht der Batter seinerseits, durch Wegschlagen desBallssogenannteRuns(Punkte) zu erzielen. Der Bowler wird in seiner Aufgabe durch die anderenFeldspielerunterstützt, die versuchen, denBallso schnell wie möglich zurückzubringen. Die Sportart entwickelte sich seit mindestens dem 16. Jahrhundert vor allem in Südostengland und wird seit dem 19. Jahrhundert professionell betrieben. Nachdem das erste Länderspiel 1844 zwischen denVereinigten StaatenundKanadastattfand, etablierte sich das bis heute betriebene, mehrere Tage dauernde,Test Cricketseit 1877. Seit 1971 werden internationale Ein-Tages-Spiele (ODIs) durchgeführt und kurz darauf in dieser Spielform Weltmeisterschaften fürFrauenundMännerausgetragen. Heute wird Cricket vor allem in den Ländern desCommonwealthals Sommersport betrieben und hat sich in einigen Ländern, vor allem in Südasien, alsNationalsportetabliert. Seit den frühen 2000er Jahren entwickelte sich die kürzere Form desTwenty20-Cricket, die neben kommerziell erfolgreichen Ligabetrieben ab denOlympischen Spielen 2028dazu führt, dass Cricket wieder olympische Sportart wird. Eine frühe Form von Cricket, die von Schafhirten und Bauern gespielt wurde, kann bis ins 13. Jahrhundert zurückverfolgt werden. Es liegen schriftliche Belege über ein Spiel namenscreagaus dem Jahr 1300 inKentvor, das vonPrince Edwardgespielt wurde.[3] 1598 berichtet ein Gerichtsfall über eine Sportart namensKreckett, die an einer Schule inGuildfordausgeübt wurde.[3]Dies ist auch laut demOxford English Dictionaryder erste gültige Nachweis des WortesCricketin der Englischen Sprache. Während des 17. Jahrhunderts wurdeCreckettvor allem im Südosten Englands immer populärer. Gegen Ende dieses Jahrhunderts wurde es zu einer organisierten Sportart (die vermutlich auch die ersten Profis auf diesem Gebiet hervorbrachte), da nachweislich im Jahr 1697 eingreat cricket matchmit 11 Spielern pro Mannschaft inSussexabgehalten wurde.[4] Im 18. Jahrhundert wurden wesentliche Bestandteile des Spiels weiterentwickelt undCricketwurde zum Nationalsport in England. Adelige und reiche Kaufleute begannen ihre eigenen Mannschaften(select XIs)aufzustellen. Spielstätten in London wurden bereits 1707 auf demArtillery GroundinFinsburybereitgestellt, bis 1787 letztendlich der legendäreLord’s Cricket Grounderöffnet wurde. Der im selben Jahr gegründeteMarylebone Cricket Clubavancierte rasch zur ersten Adresse in diesem Sport und ist bis heute auch der Hüter derLaws of Cricket(Cricketregeln).[4] Im 19. Jahrhundert wurde der bis dahin angewandteUnderarm Bowling(„Unterarmwurf“) zuerst durch dasRoundarm Bowling(eine Bowling-Bewegung in Höhe des Beckens) und schließlich 1864 durch denOverarm Bowling(„Oberarmwurf“) ersetzt, der heute noch ein typisches Erkennungsmerkmal vonCricketist.[5]1877 wurde auch das ersteTest CricketMatch auf demMelbourne Cricket Groundzwischen Australien und England ausgetragen. Auf nationaler Ebene wird Cricket seit dieser Zeit in Ligabetrieben professionell betrieben.[4] Cricket war – ebenso wieCroquetundPelota– auch eine Sportart bei denOlympischen Spielenin Paris im Jahr1900. Die Dauer des olympischen Cricketspiels betrug nur zwei Tage, Sieger wurde Großbritannien.[6] Während der Sport auf internationaler Ebene zum Ende des 19. Jahrhunderts vornehmlich in England,AustralienundSüdafrikaausgetragen wurde, wurde er mit der Unabhängigkeit der britischen Kolonien auch in derKaribikund inSüdasien(IndienundPakistan) relevant. Zum Ende der 1970er Jahre erfolgte eine Modernisierung des Sportes, die Cricket auch medial und kommerziell weiter etablierte.[7]Dauerten Spiele bis dahin grundsätzlich mehrere Tage, wurden mit Ein-Tagesspielen (ODIs) und zu Beginn der 2000er Jahre mitTwenty20-Spielen kürzere Formen etabliert.[8][9]Seit ihrer Einführung gibt es in diesen beiden Formen auch Weltmeisterschaften (Cricket World Cup,ICC Men’s T20 World Cup,Women’s Cricket World Cup,ICC Women’s T20 World Cup), die sich neben dentraditionellen Tourenzwischen Mannschaften als heutiger Kern des internationalen Spiels etabliert haben. Auf dem ovalen Spielfeld befinden sich elf Spieler der Feldmannschaft und zwei Spieler der Schlagmannschaft. In der Mitte des Spielfeldes ist ein Streifen (Pitch) von rund 20 m Länge und 3 m Breite aufgebracht, an dessen schmalen Enden sich jeweils eine Holzkonstruktion, das sogenannteWicket, befindet. Ziel der Feldmannschaft ist es, mit Hilfe ihres Bowlers mit dem Ball das Wicket des Gegners zu zerstören. Um dieses zu verhindern, steht vor jedem der Wickets ein Batter der Schlagmannschaft und wehrt den Ball mit einem Schläger ab. Der Bowler läuft vom gegenüberliegenden Ende des Pitches an und wirft (bowlt) den Ball in einer vorgegebenen Bowlingtechnik in Richtung des Batters. Verfehlt dieser den Ball und der Ball zerstört das Wicket, ist der Batter ausgeschieden; genauso gilt dies, wenn er den Ball trifft und dieser dann direkt aus der Luft von einem Mitglied der Feldmannschaft gefangen wird. In diesem Fall kommt ein neuer Batter der Schlagmannschaft aufs Feld und nimmt den Platz des ausscheidenden ein. Gelingt dem Batter ein Schlag, so läuft er zur anderen Seite des Pitches und tauscht mit seinem Partner die Plätze. Für jeden gelungenen Seitenwechsel bekommt die Schlagmannschaft einen Punkt (Run). Gelingt es dem Schlagmann, dass der Ball über die Spielfeldbegrenzung rollt, gibt es dafür 4 Runs. Sollte ihm sogar gelingen, dass der Ball das Spielfeld verlässt, ohne dass er zuvor den Boden berührt hat, so gibt es 6 Runs.[10] Ein Spiel ist dabei in zwei oder vier Spielabschnitte (Innings; die Form mit „s“ am Ende ist sowohl Plural, wie auch Singular) geteilt, in denen jede Mannschaft abwechselnd jeweils ein Innings lang Schlag- oder Feldmannschaft ist. Das Innings ist dabei noch einmal in Unterabschnitte (Over) von sechs regulär gebowlten Bällen des Bowlers geteilt, in denen dieser immer von einem Ende des Pitches anläuft und bowlt. Sind alle Bälle des Overs gebowlt, folgt ein anderer Bowler von der anderen Seite des Pitches und absolviert von dort sein Over. Das Innings ist dann beendet, wenn bei der Schlagmannschaft zehn Spieler ausgeschieden sind oder wenn eine vorher festgelegte Anzahl von Overn absolviert ist. Anschließend tauschen die beiden Mannschaften ihre Aufgaben und absolvieren ihr Innings als Schlag- bzw. Feldmannschaft. Nach Abschluss aller Innings gewinnt die Mannschaft mit den meisten Runs.[10] Als oberste Instanz der Cricket-Regeln ist derMarylebone Cricket Club(MCC) in London anerkannt, obwohl internationales Cricket unter der Führung desInternational Cricket Council(ICC) durchgeführt wird. Der MCC gibt dieLaws of Cricketheraus, die in 42 Regeln den Ablauf des Spieles festlegen.[11][12] Eine Cricketmannschaft besteht aus elf Spielern. In sehr eingeschränktem Maße ist auch der Einsatz vonSubstitutesundRunnersfür verletzte Spieler möglich; taktische Einwechslungen gibt es jedoch nicht. Im Verlauf eines Cricketspiels übernehmen die Spieler je nach Spielsituation verschiedene Rollen. Cricket wird auf einem großen, ovalen Platz gespielt, dessen äußere Begrenzung(Boundary)beispielsweise durch ein Seil markiert ist. In der Mitte des Spielfeldes befindet sich ein besonders präparierter, 20,12 Meter (22 Yards) langer und 3,05 Meter (10 feet) breiter Streifen – die so genanntePitch. An den beiden Enden dieserPitchsind jeweils drei Stäbe(Stumps)aufgestellt, auf denen lose je zwei kleinere Querstäbe(Bails)liegen. Diese beiden Anordnungen von Stäben werdenWicketsgenannt.[10] Der Cricketball ist traditionell dunkelrot und hat einen Kern ausKork, der eng mit Schnur umwickelt ist. Die Hülle besteht aus vier Stücken Leder, die mittels einer leicht erhabenen Naht verbunden sind. Der Ball wiegt 155,9–163 g (5½–5¾ Unzen) und hat einen Umfang von 22,4–22,9 cm (8 13/16–9 Zoll; Ø≈7,2 cm).[13]In Ein-Tages-Spielen und Twenty20s werden generell weiße Bälle verwendet und bei Mehrtagesspielen, die in den Abendstunden stattfinden, pinke Bälle. Die Bälle im Jugend- und Damenbereich sind etwas leichter und kleiner. Die Bowlingtechnik des Bowlers ist durch die Regeln größtenteils vorgeschrieben. Der Bowlingarm darf, sobald er die Höhe der Schulter erreicht hat, nicht mehr gestreckt werden, was in der Praxis fast immer dazu führt, dass der Arm in diesem Moment schon voll gestreckt ist und bleibt. Anderenfalls wird der gebowlte Ball als ungültig gewertet (sieheNo Ball). Diese Technik wirdBowlengenannt. Im Cricket wird streng zwischen den Begriffenwerfenundbowlenunterschieden, wobeiwerfen(in dem der Arm nachträglich gestreckt wird) als verbotene Technik gilt.[14][15] Eine weitere Besonderheit ist die Tatsache, dass derBallso gebowlt wird, dass er vor dem Batter auf dem Boden aufkommt. Die Regeln schreiben dies zwar nicht vor, sie verbieten allerdings, dass der Ball den Battervolleyüber Hüfthöhe erreicht. Fürvolleybenutzt man im Cricket den Begrifffull toss. Obwohl der Ball beim Aufkommen viel an Geschwindigkeit verliert, wird dem Batter das Schlagen des Balles erschwert. Grundsätzlich gibt es zwei Techniken, nach denen man das Bowlen der Bowler unterscheidet:[16] Welche Technik Anwendung findet, hängt von dem Zustand des Pitches und Balls, sowie den äußeren Bedingungen ab.[17]Klassisch beginnen die Fast Bowler das Innings, und wenn der Ball Abnutzung zeigt und der Pitch durch die auftreffenden Bälle Unebenheiten aufweist, kommen langsamere Bowler und Spin-Bowler zum Einsatz.[10]Auch sind die klimatischen Zustände entscheidend, welche Typen von Bowlern bevorzugt eingesetzt werden. Während in Europa, Südafrika und Australien Fast Bowler Vorteile haben, sind es in Südasien vorwiegend die Spin-Bowler, die einen größeren Einfluss aufs Spiel ausüben können. Der schlagende Batter hat zwei Ziele. Zum einen verteidigt er sein Wicket, d. h., er verhindert, dass es vom gebowlten Ball getroffen wird. Zum anderen wird er, wann immer ihm dies möglich erscheint, versuchen, den Ball so wegzuschlagen, dass er genug Zeit hat, zum anderen Wicket hinüberzulaufen. Sein Partner, welcher am anderen Ende der Pitch neben dem dortigen Wicket steht, damit der Bowler der Feldmannschaft Platz zum Bowlen hat, muss dann ebenfalls zum gegenüberliegenden Pitchende laufen.[10]Den Battern bleibt die Entscheidung, ob sie loslaufen oder nicht, jedoch selbst überlassen. Kommt einer der Batter dabei nicht rechtzeitig hinter die Schlaglinie(popping crease)an seinem Pitchende, bevor die Feldmannschaft den Ball auf das entsprechende Wicket geworfen und dieses dadurch zerstört hat – d. h., mindestens einer der kleinen, querliegenden Stäbe(bails)heruntergefallen ist – so ist dieser Batter ausgeschieden (siehe 1. unterAusscheiden des Schlagmannsunten). Gelingt den Batsmen aber dieser Seitenwechsel, wird diessinglegenannt und zählt einen Punkt(Run). Wenn genug Zeit ist, können die Batsmen beliebig oft hin und her laufen und bekommen eine dementsprechende Punktzahl. In der Praxis geschieht dieser Seitenwechsel selten öfter als drei Mal. Wenn der schlagende Batter den Ball hoch über die Spielfeldgrenze schlägt, gibt es sechs Runs.[10]Der Schiedsrichter streckt dann beide Arme nach oben. Wenn der Ball vorher den Boden berührt, gibt es nur vier Runs. In diesem Fall führt der Schiedsrichter mit einem Arm eine wellenartige Bewegung von einer Seite zur anderen aus. Für die Statistiken eines Batters wichtig sind die sogenanntenFiftiesoderHalf-Century, wenn dieser in einem Innings 50 oder mehr Runs erzielt undCenturies, wenn 100 oder mehr Runs erzielt werden. Die Feldmannschaft hat das Ziel, die gegnerischen Schlagleute so schnell wie möglich ausscheiden zu lassen. Jeder Batter spielt so lange, bis er ausgeschieden ist und wird dann durch den nächsten Batter, der in diesemInnings(Spieldurchgang) noch nicht an der Reihe war, ersetzt. Sobald zehn der elf Schlagleute ausgeschieden sind (man spricht dann vonall out),ist dieses Innings abgeschlossen. Das Schlagrecht wechselt bzw. das Spiel ist zu Ende. Das Ausscheiden eines Batters wird auch alsWicketbezeichnet, unabhängig davon, ob das eigentlicheWicketdaran beteiligt war oder nicht. Ein solches Wicket kann auf insgesamt zehn verschiedene Arten geschehen. Die wichtigsten sind: Das Ausscheiden von Battern, das am häufigsten vorkommt, ist das Fangen des Balles, gefolgt von Treffern des Wickets, entweder durch den Ball direkt oder viaLBW.Run outundStumpingskommen deutlich weniger vor.[18]Wichtig dabei ist, dass der Schiedsrichter nur dann ins Spiel kommt, wenn er vom bowlenden Team darum gebeten wird. Ohne Antrag darf der Umpire keine Entscheidung über das Ausscheiden eines Spielers treffen.[10]Gelingt dem Bowler einWicketbei drei aufeinander folgenden gebowlten Bällen, so spricht man von einemHattrick. Für die Statistik des Bowlers ist es von besonderer Bedeutung, wenn dieser fünf Wickets in einem Innings, einem sogenannten Five-for, erzielt oder insgesamt zehn Wickets in den beiden Innings eines First-Class-Spiels. Von den regulären Runs unterscheidet man die sogenannten Extras. Diese werden bei Regelverstößen oder irregulären Schlägen vergeben. Dabei gibt es fünf unterschiedliche Arten: SowohlNo Ballsals auchWideszählen automatisch einen Punkt (Run) für die Schlagmannschaft. In beiden Fällen muss der Ball auch noch zusätzlich vom Bowler wiederholt werden.Penaltieswerden jeweils in Blöcken von fünf Runs vergeben. Alle diese oben genannten Punkte werden nicht dem Schlagmann gutgeschrieben, sondern in der jeweiligen Kategorie unterExtrasvermerkt.[10] Das Ergebnis eines Cricketspiels ist entweder ein Sieg einer der beiden Mannschaften (Win), ein Unentschieden (Tie) oder ein Remis (Draw).[10] Eine Mannschaft gewinnt, wenn sie mehr Punkte als der Gegner erzielt hat, nachdem dieser sein(e) Innings abgeschlossen hat. Möglich ist auch ein durch den Schiedsrichter zuerkannter Sieg wegen Spielverweigerung der gegnerischen Mannschaft, ein Sieg durch Aufgabe des Gegners oder ein Sieg durchPenalties. Das Spiel endet unentschieden, wenn alle Innings abgeschlossen und die Punktzahlen beider Mannschaften gleich sind.[10]Dieses Ergebnis ist sehr selten. In allen anderen Fällen – das heißt in der Regel nach einem Zeitablauf ohne Abschluss aller Innings – wird das Spiel als Remis gewertet.[10] Cricket wird generell in nationales und internationales Cricket geteilt, bei der heute drei unterschiedliche Spielweisen dominieren. Auf internationaler Ebene werden die drei Formen zumeist in Form vonTourenzwischen zwei Mannschaften und Turnieren mit mehreren Mannschaften absolviert. Auf nationaler Ebene gibt es in allen drei Formen Ligen und Turniere. AlsTestoderTest Cricketbezeichnet man eine spezielle Form eines internationalen Cricketspiels, die traditionell als höchste Form der Ausübung des Sports gilt. Bei dieser Spielform wird das Spiel an bis zu fünf Tagen – in täglich drei rund zweistündigen Spielabschnitten – ausgetragen. Tests sind wiederum meist in eine Serie von zwei bis sechs Tests eingebettet, so dass die entsprechenden Duelle sich über mehrere Wochen hinziehen können. Die Berechtigung, Tests auszutragen ist nur auf wenige, derzeit 12, Nationalmannschaften beschränkt. Tests sind dabei eine Sonderform des First-Class Cricket, das vor allem auf nationaler Ebene ausgetragen wird. Die Spiele zwischen Profimannschaften finden überwiegend in einerLigastatt und dauern drei bis vier Tage. Auch hierbei werden zweiInningsje Mannschaft mit derselben Tageseinteilung wie beim Test Cricket gespielt. Am bekanntesten ist die englischeCounty Championship(Grafschaftsmeisterschaft), die seit 1890 ausgetragen wird. Aufgrund der Bedürfnisse vor allem desFernsehenswurde seit den 1960er Jahren ein kürzeres und dramatischeres Format eingeführt, dasOne-Day Cricket(Ein-Tages-Cricket), seit den 1970er Jahren auf internationaler Ebene alsOne-Day Internationals. Diese neue Art des Crickets erfreute sich schnell wachsender Popularität, auch wenn es von Traditionalisten zunächst weitgehend abgelehnt wurde. Im Gegensatz zum First-Class Cricket ist ein Innings nicht erst dann abgeschlossen, wenn alle Schlagleute „aus“ sind, sondern schon nach einer festgesetzten Zahl vonOver, üblicherweise 50 Over. Sobald also eines dieser beiden Kriterien erfüllt wird, ist das Innings vorbei. Aus diesem Grund wird diese Spielform auch oftLimited-Overs Cricketgenannt. Auch gibt es weitere Regeländerungen im Vergleich zum First-Class Cricket. Wie beimTest Cricketwerden auch One-Day Internationals meist als Teil einer Serie (Tour) ausgetragen (drei bis sieben Spiele) oder gar in Form eines Drei-Nationenturniers. Alle vier Jahre wird im One-Day-Modus eine Weltmeisterschaft (Cricket World Cup, bzw.Women’s Cricket World Cupbei den Frauen) ausgetragen. In den vier Jahren zwischen den World Cups findet eineChampions Trophystatt. Das Äquivalent auf nationaler Ebene oder Spiele von weniger etablierten Nationalmannschaften wird alsList A Cricketbezeichnet. Zu Beginn der 2000er Jahre starteten Versuche, das Spiel für moderne Medien attraktiver zu gestalten.[9]Dabei etablierte sich dasTwenty20-Cricket, bei dem die Spielabschnitte auf je 20 Over verkürzt werden und eine Maximalspieldauer von 75 Minuten je Innings festgelegt ist. Auch in diesem Format wird eine Weltmeisterschaft jeweils für Männer und Frauen ausgetragen (ICC Men’s T20 World Cup,ICC Women’s T20 World Cup). Twenty20-Cricket wird ab2028Teil des Programms bei denOlympischen Spielensein.[19] Das Twenty20-Format hat sich insbesondere auch als Spielform nationaler Profiligen etabliert, wie derIndian Premier League, bei der international besetzte Mannschaften innerhalb kurzer Zeit ein Turnier bestreiten, was häufig von großen Zuschauerzahlen verfolgt wird. Ein Cricketspieler benötigt ein Trikot, oft zusätzlich einen Sweater und eine lange Hose. Die Schuhe sind weiß und haben meist ein besonderes Profil, Schuhe für Bowler sogar Spikes. Bei Testspielen ist weiße Spielkleidung vorgeschrieben; beiOne-Day Cricketwird, zumindest bei den Profis, heute fast immer in bunter Spielkleidung gespielt, früher oft als „Pyjama Cricket“ belächelt.[20] Jeder Schlagmann hat einen Schläger und trägt Schutzausrüstung. Das wichtigste Schutzbekleidungsstück sind diePads, die seine Beine vor dem mit einer sehr hohen Wucht auftreffenden Ball schützen. Zusätzlich sollte der Schlagmann einen Helm und Handschuhe tragen, sowie eine so genannteBox, einSuspensoriumzum Schutz der sogenanntenGentleman’s Region. Einige Spieler tragen darüber hinaus noch einen Schutz für die Arme oder gar einen Brustschutz unter dem Hemd. Bei der Feldmannschaft trägt nur derWicket Keeper, der immer hinter dem Schlagmann und dessenWicketsteht, eine besondere Ausrüstung. Wie der Schlagmann hat auch er einen (meist kleineren) Beinschutz(Wicket Keeping Pads)und er ist der einzige Spieler der Feldmannschaft, der Fanghandschuhe(Wicket Keeping Gloves)tragen darf.[21] Die beiden Schiedsrichter tragen in der Regel weißes oder cremefarbenes Hemd und Jacket sowie eine schwarze Hose. Auch hier hat sich in den kürzeren Formaten eine farbige Kleidung durchgesetzt. Keine Nationalmannschaft aus dem deutschsprachigen Raum konnte sich bisher für eine Weltmeisterschaft qualifizieren. Cricket wird in Deutschland seit 1988 vomDeutschen Cricket-Bund(DCB) mit Sitz inEssenund seit 2015 zusätzlich von derDeutschen Cricket-Union(DCU) mit Sitz inEngelskirchengefördert. Beide sind über verschiedene Landesverbände organisiert, wobei der DCB mit acht Landesverbänden und rund 7.000 Mitgliedern der größere der beiden Bundesverbände ist. Die meisten Cricket-Vereine befinden sich inNordrhein-Westfalen,Hessen,Baden-WürttembergundBayern. Zahlenmäßige Hochburg des Cricket in Deutschland ist dasRhein-Main-Gebiet. Leistungsmäßig liegt Deutschland auf der Cricket-Weltrangliste derzeit auf Platz 42 und stellt in Europa außerhalb der britischen Inseln die größte Zahl an aktiven Cricket-Spielern. Der Spielbetrieb in Deutschland ist in einer mehrgleisigen Bundesliga und darunter angesiedelten Regionalligen organisiert. Der Ursprung von Cricket in Deutschland liegt inBerlin. Seit spätestens 1873 spielten dort Herren aus Großbritannien, Kanada und den USA diesen Sport. Ab 1888 begannen die VereineBFC Germania 1888,BFC Viktoria 1889,Union 1892,Britannia 1892undTSV Helgoland 1897mit einem geregelten Spielbetrieb. Über viele Jahrzehnte bestimmte der BFC Viktoria 1889 den Sport und errang zahlreiche deutsche Meisterschaften (21 mal bei den Herren, zuletzt 1959). Ähnlich der Regelung im Fußball darf der BFC Viktoria 1889 für die errungenen Meisterschaften vier Sterne auf dem Trikot tragen. Die erste deutsche Meisterschaft der Damen (2006 erstmals ausgespielt) wurde von der Frauenmannschaft des BFC Viktoria 1889 mit einem Sieg über den Schweriner BBCCC gewonnen. Das Team wechselte nach dem Sieg in der deutschen Meisterschaft jedoch geschlossen zu denReinickendorfer Füchsen. DieFlüchtlingskrise in Deutschlandbrachte den deutschen Cricket-Vereinen ab 2015 zahlreiche neue Spieler, vor allem ausAfghanistan. Cricket wird in der Schweiz offiziell vonCricket Switzerlandvertreten. Dem 1980 gegründeten Verband gehören zurzeit 20 assoziierte Mitgliedsvereine an sowie eine Schiedsrichtervereinigung. Von 1985 bis 2012 gehörte der Verband alsaffiliated memberdem International Cricket Council (ICC) an. Aufgrund der Gründung eines konkurrierenden Verbandes wurde diese Mitgliedschaft jedoch ausgesetzt, bis der Verband durch eine Mitgliedschaft beiSwiss Olympicals offizieller Cricket-Dachverband anerkannt ist. Cricket Switzerland veranstaltet im Sommer jeweils eine 20-over-Meisterschaft, die in einer Ost- und Westgruppe plus Finalspiele ausgespielt wird. Zusätzlich gibt es mit dem Mr Pickwick T20 Cricket Cup ein Twenty20-Turnier. Internationale Turniere in der Schweiz finden jeweils im Februar mit demCricket on IceinSt. Moritzund im Sommer mit dem «International Cricket Festival» inZuozstatt. Bereits 1892 gründeten inWienansässige Briten denVienna Cricket and Football-Club.Cricketwird inÖsterreichheute vom Österreichischen Cricket Verband (ÖCV-ACA) im ICC vertreten, der am 4. September 1981 gegründet wurde. Seit Anfang der 1990er Jahre wird in Österreich eineOpen Leagueund eineTwenty20Liga ausgetragen, an der auch ein Cricket-Verein ausSlowenienteilnimmt. In Österreich gibt es aktuell drei Cricket-Grounds, die dem internationalen Standard entsprechen. Heimländerspiele der österreichischen Nationalmannschaft finden meistens inSeebarnbei Wien statt. InWien-Donaustadtbefindet sich im Austria Cricket Stadium das Zentrum der Nachwuchsausbildung mit der Austrian Cricket Academy. Der dritte Cricket-Ground liegt inKärntenin der Nähe vonVelden am Wörtherseebei Latschach an derDrau. Cricket ist in zahlreichen Ländern des Commonwealth eine der wichtigsten Sportarten. Dies spiegelt sich in der medialen Aufmerksamkeit wider, aber auch in der politischen und kulturellen Reflexion. Beispiel dafür ist dieCricket-Diplomatie, bei der der Stellenwert dieses Sports in den StaatenIndienundPakistandazu genutzt wurde, diplomatische Fortschritte zu erzielen. Heute gibt es zwölf Verbände, die als Vollmitglieder des International Cricket Councils anerkannt sind und in dessen Ländern Cricket eine hohe Aufmerksamkeit erfährt, da es die Nationalmannschaften dieser Verbände sind, die an den wichtigsten Wettbewerben des Sports teilnehmen und jeweils Profiligen unterhalten. Gesellschaftliche Diskussionen um Cricket flammen immer wieder dann auf, wenn es zu größeren Ereignissen kommt, die über lange Zeit die (Sport-)Berichterstattung in zahlreichen Ländern dominiert. Diese Diskussionen führen oft zu Veränderungen des Spiels und dessen Regeln. Als Bodyline Series wird dieAshes Tour 1932/33in Australien zwischen den Gastgebern und England bezeichnet. Dabei spielten die Bowler der englischen Mannschaft gezielt auf den Körper der australischen Batsmen, um die Dominanz des damals überragendenDonald Bradmanzu brechen. Die australische Seite wertete diese Strategie als einen gezielten Versuch, die australischen Spieler zu verletzen und reklamierte Unsportlichkeit. Da die englische Seite dafür kein Verständnis hatte und die mediale Aufregung groß war, kam es zu einer kurzen diplomatischen Krise zwischen den beiden Ländern. Die Cricketregeln wurden in der Folge angepasst, so dass diese Form des Spiels nicht mehr möglich ist.[22] Als beschämendster Moment der Cricketgeschichte gilt der sogenannteUnderarm incident(„underarm“ steht für eine Bowling-Bewegung, bei dem sich die Hand unterhalb der Hüfte befindet) in einemOne-Day Internationalzwischen Australien und Neuseeland am 1. Februar 1981 in Melbourne. Die neuseeländischen Schlagleute lagen beim letzten Ball des Spiels sechs Punkte hinter Australien und hätten mit einer großen Boundary noch einen Gleichstand erzielen können. Um dies zu verhindern, rollte der australische Bowler Trevor Chapell auf Anweisung seines Bruders und Kapitäns Greg den Ball am Boden zum neuseeländischen Schlagmann, so dass es für diesen unmöglich war, ihn weit und hoch in die Luft zu schlagen.[23][24] Diese Technik des Bowlens existierte zwar im ausgehenden 19. Jahrhundert, war aber zum Zeitpunkt des Vorfalles lange unüblich, wenn auch nicht von den Regeln verboten. Dies wurde durch Medien und Fans als sehr unsportliches Verhalten und grober Verstoß gegen „the spirit of the game“ aufgefasst. Australien wurde zum Sieger des Spieles erklärt, was langjährige Konflikte mit dem neuseeländischen Cricketteam zur Folge hatte. Bei derTour Indiens in Südafrika 2001/02wurde beim zweiten Test der indische SpielerSachin Tendulkarvom UmpireMike Dennessder Ballmanipulation beschuldigt.[25]Dies führte zu Beschuldigungen des Rassismus seitens indischer Medien gegen den Schiedsrichter und sorgte für die sofortige Beendigung der offiziellen Tour, um einen Einsatz Denness im letzten Test zu verhindern.[26]Konsequenz dieser Entwicklung waren eine Reform und die Professionalisierung des Schiedsrichterwesens im internationalen Cricket.[27]Tendulkar wurde letztendlich freigesprochen.[28] Der erstmalige Abbruch eines internationalen Cricket-Spiels in der langjährigen Geschichte der Sportart sorgte für Anfeindungen zwischen Australien und Pakistan. Der Vorfall ereignete sich am 20. August 2006 in London imSpiel zwischen England und Pakistan. Nachdem der australische SchiedsrichterDarrell Hairentschieden hatte, Pakistan hätte den Ball manipuliert, weigerte sich die pakistanische Mannschaft, nach einer Pause wieder das Spielfeld zu betreten. Daraufhin wurde das Spiel für England gewertet. Aus Südasien gab es daraufhin Vorwürfe des Rassismus gegen den Australier, während Hair in Australien als Held gefeiert wurde.[29][30] Am 3. März 2009 kam es zu einemAngriff auf das sri-lankische Cricket-Team in Pakistan. Bewaffnete Männer feuerten inLahoremit Raketenwerfern und Maschinengewehren auf den Bus der Mannschaft – mindestens sieben Menschen starben danach, u. a. sechs Spieler und ein Trainer wurden verletzt. Dieangesetzte Länderspielseriewurde daraufhin abgebrochen.[31]Seither hatte die pakistanische Mannschaft ihre Heimspiele in den Vereinigten Arabischen Emiraten gespielt.[32]Erstmals seit 2009 wurde im Jahr 2017 mit derPakistan Super League 2016/17ein Finale einer nationalen Meisterschaft mit internationalen Spielern wieder in Pakistan gespielt.[33] Am dritten Tag des vierten Testsder Tour Pakistans in England in 2010wurden Presseberichte bekannt, dass pakistanische Spieler inWettbetrugverwickelt seien und gegen hohe Geldbeträge in vorgegebenen Overn No Balls bowlen würden. Als dieses bestätigt wurde, wurden die involvierten SpielerMohammad Asif,Mohammad Amirund KapitänSalman Buttzu Gefängnisstrafen verurteilt und langjährig gesperrt.[34][35] Beim dritten Test derTour Australiens in Südafrika 2018wurde der australische SpielerCameron Bancroftmit Fernsehkameras derBallmanipulationüberführt. Als herauskam, dass diese Manipulation mit dem Führungsteam seiner Mannschaft abgesprochen und von dieser initiiert worden war, mussten die KapitäneSteve SmithundDavid Warnerzurücktreten, und auch der TrainerDarren Lehmannlegte seinen Posten nieder.[36] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 1.1Anfang 1.2Organisation und Weiterentwicklung 1.3Entwicklung zum weltweiten Sport 2Spielprinzip 2.1Allgemeiner Spielablauf 2.2Regeln 2.2.1Spieler und Offizielle 2.2.2Spielfeld 2.2.3Ball 2.2.4Bowlingtechnik 2.2.5Runs (Punkte) 2.2.6Ausscheiden des Schlagmanns (Dismissals) 2.2.7Extras 2.2.8Ergebnis 3Austragungsformen 3.1Test Cricket und First-Class Cricket 3.2One-Day International und List A Cricket 3.3Twenty20 Cricket 4Ausrüstung 5Cricket auf nationaler Ebene 5.1Cricket in Deutschland 5.2Cricket in der Schweiz 5.3Cricket in Österreich 6Cricket in der Gesellschaft und Medien 6.1Besondere Ereignisse 6.1.1Bodyline Series (1932/1933) 6.1.2Underarm incident(1981) 6.1.3Ball tampering (2001/02) 6.1.4Spielabbruch (2006) 6.1.5Attentat auf die sri-lankische Mannschaft (2009)"
  },
  {
    "label": 0,
    "text": "Deutschland – Wikipedia Deutschland Inhaltsverzeichnis Begriffsgeschichte: Deutsch und Deutschland Geographie Bevölkerung Geschichte Politik Wirtschaft Verkehr Kultur Sport Medien Gesellschaft Siehe auch Literatur Weblinks Anmerkungen Physische Geographie Flora Fauna Humangeographie Demografie Sprachen Religionen Urgeschichte, Kelten, Germanen und Römer Völkerwanderung und Frühmittelalter (375–962) Vom Ostfrankenreich zum Heiligen Römischen Reich (962–1806) Rheinbund, Deutscher Bund, Norddeutscher Bund (1806–1871) Deutsches Reich (1871–1945) Deutschland unter alliierter Besatzung (1945–1949) Bundesrepublik Deutschland und DDR (1949–1990) Wiedervereinigtes Deutschland (seit 1990) Staatsgründung Staatsgebiet Politisches System Staatshaushalt Parteienlandschaft Europapolitik Politische Indizes Außen- und Sicherheitspolitik Militär Feuerwehr Polizei und Nachrichtendienste Kriminalität Recht Grundlagen Außenhandel und Wirtschaftsentwicklung Automobilindustrie Informationstechnik und Telekommunikation Energie Tourismus Finanzwirtschaft Straßenverkehr Schienenverkehr Nahverkehr Luftverkehr Schiffsverkehr Soziales Gesundheit Bildung Wissenschaft Geologie Relief Klima Klimawandel Gewässer Inseln Verwaltungsgliederung Ballungsgebiete Traditionen Verhältnis von Staat und Religion Bevölkerungsanteile Deutsches Kaiserreich (1871–1918) Weimarer Republik (1919–1933) Nationalsozialistische Diktatur (1933–1945) Legislative Exekutive Ursprung Diktatur und Nachkriegszeit Gegenwart Verfassungsrechtsprechung durch EU-Gerichte Unabänderliches Recht 23. Mai 1949: Bundesrepublik Deutschland(Grundgesetz)[6](siehe Abschnitt „Staatsgründung“) Deutschland(anhörenⓘ/?; Vollform des Staatsnamens seit 1949:Bundesrepublik Deutschland) ist einBundesstaatin Mitteleuropa. Es besteht aus 16Ländernund ist alsfreiheitlich-demokratischerundsozialerRechtsstaatverfasst. Die 1949 gegründete Bundesrepublik Deutschland stellt die jüngste Ausprägung des 1871 erstmals gegründeten deutschenNationalstaatsdar. Im Rahmen derWiedervereinigung DeutschlandswurdeBerlin1990Bundeshauptstadtund 1991 zum Parlaments- undRegierungssitzbestimmt. Das Land grenzt an neun Nachbarstaaten und liegt in dergemäßigten KlimazonezwischenNord-undOstseeim Norden sowieBodenseeundAlpenim Süden. Deutschland hat circa 83,6 Millionen Einwohner und zählt bei einer Fläche von 357.588 Quadratkilometern mit durchschnittlich 237 Einwohnern pro Quadratkilometer zu dendicht besiedeltenFlächenstaaten. Diebevölkerungsreichste deutsche Stadtist Berlin; weitere Metropolen mit mehr als einer Million Einwohnern sindHamburg,MünchenundKöln; der größteBallungsraumist dasRuhrgebietmit über fünf Millionen Einwohnern. Funde desHomo heidelbergensissowie zahlreicherprähistorischer Kunstwerkeaus der späterenAltsteinzeitbelegen, dass seit 600.000 Jahren Menschen auf dem Gebiet des heutigen Deutschland leben, einige Steinwerkzeuge wurden sogar auf über 1,3 Millionen Jahre datiert.[7]Während derJungsteinzeit, um 5600 v. Chr., wanderten die ersten Bauern ausVorderasienein. DieRömerbezeichneten die Siedlungsgebiete dergermanischen Stämmein derAntikealsGermania magna. Durch die EroberungenKarls des Großenwurden weite Teile des heutigen Deutschland um 800 erstmals in einem Herrschaftsgebiet zusammengefasst. Infolge der Teilungen desFränkischen Reichsunter Karls Enkeln entstand im 9. Jahrhundert dasOstfrankenreich, das ab dem 10. Jahrhundert auch alsRegnum Teutonicumbezeichnet wurde und aus dem das bis 1806 bestehendeHeilige Römische Reich Deutscher Nationhervorging. An dessen Stelle wiederum trat 1815 derDeutsche Bund, der sich aus lose miteinander verbundenen souveränen Staaten zusammensetzte. Nach der gescheitertenMärzrevolutionvon 1848 kam es erst 1871 zurGründungeines deutschen Nationalstaats, desDeutschen Reichs. Dierasche Entwicklung vom Agrar- zum Industriestaatvollzog sich während derGründerzeitin der zweiten Hälfte des 19. Jahrhunderts. Nach demErsten Weltkriegwurde 1918 die Monarchie abgeschafft und die demokratischeWeimarer Republikkonstituiert. Ab 1933 führte dienationalsozialistische Diktaturzu politischer und rassistischer Verfolgung und gipfelte in derErmordung von sechs Millionen Judenund Angehörigen anderer Minderheiten wieSinti und Roma. Der vomNS-Staat1939 begonneneZweite Weltkriegendete 1945 mit der Niederlage derAchsenmächte. Das von denSiegermächtenbesetzte Land wurde 1949geteilt, nachdem bereits 1945 seineOstgebieteteils unterpolnische, teilssowjetischeVerwaltungshoheit gestellt worden waren. DerGründung der Bundesrepublikals demokratischerwestdeutscherTeilstaat mitWestbindungam 23. Mai 1949 folgte die Gründung der sozialistischenDDRam 7. Oktober 1949 alsostdeutscherTeilstaat unter sowjetischerHegemonie. Dieinnerdeutsche Grenzewar nach dem BerlinerMauerbau(ab 13. August 1961) abgeriegelt. Nach derfriedlichen Revolution in der DDR1989 erfolgte die Lösung derdeutschen Fragedurch die Wiedervereinigung beider Landesteile am3. Oktober 1990, womit auch die Außengrenzen Deutschlands als endgültig anerkannt wurden. Durch den Beitritt der fünfostdeutschen Ländersowie die Wiedervereinigung vonOst-undWest-Berlinzur heutigen Bundeshauptstadt zählt die Bundesrepublik Deutschland seit 1990 sechzehn Bundesländer. Seit der Wiedervereinigung 1990 hat sich Deutschland zu einer der führenden Wirtschaftsnationen weltweit entwickelt. Anfangs stellte die Integration der DDR eine große Herausforderung dar, doch durch umfangreiche Investitionen und Reformen konnte die Wirtschaft stabilisiert werden. Insbesondere die Arbeitsmarktreformen derAgenda 2010führten zu einer deutlichen Reduzierung der Arbeitslosigkeit und erhöhten die Wettbewerbsfähigkeit des Landes. Das Land verfügt über eine gut entwickelte Infrastruktur,ein starkes Bildungssystemund eine hoch qualifizierte Arbeitskraft, was es zu einem attraktiven Standort für Unternehmen und Investitionen macht. Deutschland gilt als eine der stabilsten und wohlhabendsten Nationen der Welt.[8] Die Bundesrepublik Deutschland ist Gründungsmitglied der Europäischen Union und ihrer Vorgänger (Römische Verträge1957) sowie deren bevölkerungsreichstes Land. Mit 19 anderenEU-Mitgliedstaatenbildet sie eine Währungsunion, dieEurozone. Deutschland ist Mitglied derUN, derOECD, derOSZE, derNATO, derG7, derG20und desEuroparates. Bereits 1951 eröffnete derHohe Flüchtlingskommissar(UNHCR) ein Verbindungsbüro in der damaligen BundeshauptstadtBonn, seit 1991 unterhalten die Vereinten Nationen dort ihren deutschen Sitz („UNO-Stadt“).[9]Die Bundesrepublik Deutschland gilt als einer der politisch einflussreichsten Staaten Europas und ist ein gesuchtes Partnerland auf globaler Ebene.[10] Gemessen am nominalen Bruttoinlandsproduktist Deutschland die größte Volkswirtschaft Europas und die drittgrößte der Welt nach denVereinigten StaatenundChina(Stand 2024).[11]Die Deutschen waren 2024 diedrittgrößte Export-undImportnation.[12]Sie bilden eineInformations-undWissensgesellschaft, deren Entwicklung vonAutomatisierung,Digitalisierungunddisruptiven Technologiengeprägt ist. Die Verbesserung desdeutschen Bildungssystemsund dienachhaltige Entwicklungdes Landes gelten als zentrale Aufgaben der Standortpolitik. Gemäß demIndex der menschlichen Entwicklungzählt Deutschland zu den sehr hoch entwickelten Ländern.[13][14] Muttersprache der Bevölkerungsmehrheit istDeutsch. Daneben gibt esRegional- und Minderheitensprachenund sowohl Deutsche als auch Migranten mit anderen Muttersprachen, von denen die bedeutendstenTürkischundRussischsind.[15] DieetymologischenVorformen des Adjektivsdeutsch,etwa dasgermanische*þeudiskoder dasalthochdeutschethiutisk,bedeuteten ursprünglich „zum Volk gehörig“ und bezogen sich auf die Dialekte deskontinental-westgermanischen Dialektkontinuums. Die BezeichnungDeutschlandwird seit dem 15. Jahrhundert verwendet, ist in einzelnen Schriftstücken aber schon früher bezeugt; in derFrankfurter ÜbersetzungderGoldenen Bulle(um 1365) heißt esDutschelant.Davor sind nur Wortfügungen desAttributsdeutschmitLandbelegt, beispielsweise in der unbestimmten Singularform „ein deutsches Land“ oder der bestimmten Pluralform „die deutschen Länder“, nicht aber in der bestimmten Singularform „das deutsche Land“. Gemeint waren Länder mit einer Führungsschicht, die sich auf den politischen Herrschaftsanspruch des(Ost-)Fränkischen, ab dem 10. Jahrhundert desHeiligen Römischen Reiches(962–1806) bezog. Die Bezeichnung wurde damit vor allem für (vor-)staatliche Gebilde im deutschenSprach- oder Herrschaftsgebiet verwendet, das über Jahrhunderte große Veränderungen erlebte. DasHeilige Römische Reich,ursprünglich nur als „Reich“ (lateinischImperium) bezeichnet, erhielt mehrere Namenszusätze: „Heilig“ seit Mitte des 12. Jahrhunderts, „Römisch“ seit Mitte des 13. Jahrhunderts und seit dem ausgehenden 15. Jahrhundert „Deutscher Nation“(Heiliges Römisches Reich Deutscher Nation).Erst im 16. Jahrhundert kam die Bezeichnung„Teutschland“für die vorher so bezeichneten „deutschen Lande“ auf.[16]Bald bürgerte sich in der zeitgenössischen Literatur eine Gleichsetzung vonReichundTeutschlandein, die schließlich als Synonyma verwendet wurden (etwa durch den Hallenser JuristenJohann Peter von Ludewig1735).[17] Ein Bewusstsein, dass nicht der jeweiligeTerritorialstaat, sondern Deutschland als Ganzes alsVaterlandanzusehen sei, begann sich erst in dennapoleonischen Kriegenauszubreiten. Zuvor hatte etwaFriedrich Schillerstreng zwischen einem geistigen und einem politischen Deutschland unterschieden: „Deutschland, aber wo liegt es? Ich weiß das Land nicht zu finden.Wo das gelehrte beginnt, hört das politische auf.“ Auch die Möglichkeit einer geeinten Nation sah er skeptisch: „Zur Nation euch zu bilden, ihr hoffet es, Deutsche, vergebens.Bildet, ihr könnt es, dafür freier zu Menschen euch aus.“ Deutsche Größe,so der Titel eines unvollendeten Gedichts aus dem Jahr 1801, sah er allein im Geistigen.[18]Noch 1813 sprachAchim von Arnimvon Deutschland als einem „hohlen Wortideale“, dem er „alles Herrliche der einzelnen deutschen Völker“ (im Plural) gegenüberstellte.[19] Ein politisches Verständnis des NamensDeutschlandging zunächst nur von einer kleinen Gruppe von Intellektuellen und Politikern wieErnst Moritz Arndt,Friedrich Ludwig Jahn,Johann Gottlieb FichteoderHeinrich Friedrich Karl vom und zum Steinaus, entfaltete aber bereits während derBefreiungskriegeeine erhebliche Mobilisierungswirkung.[20]Auch dasKaisertum Österreichund dasKönigreich Preußenbezogen sich nun positiv auf Deutschland: ErzherzogKarl von Österreich-Teschenerließ 1809 zu Beginn desFünften Koalitionskriegseinen AufrufAn die deutsche Nation,in dem er versicherte: „Unsere Sache ist die Sache Deutschlands“.[21]Der preußische KönigFriedrich Wilhelm III.kündigte in der Proklamation von Kalisch am 19. März 1813 „den Fürsten und Völkern Deutschlands die Rückkehr der Freiheit und Unabhängigkeit“ an.[22]Dieses Deutschland wurde als der deutscheSprachraumdefiniert (Arndt:Des Deutschen Vaterland,1813; ähnlich noch 1841Hoffmann von FallerslebensLied der Deutschen).[23]Es wurde nicht mehr als Reich, sondern alsNationverstanden; die deutsche Nationalbewegung setzte sich in den folgenden Jahrzehnten dafür ein, alle deutschen Territorien in einemNationalstaatzusammenzufassen. Dies misslang zunächst, auf demWiener Kongress1814/15 wurden stattdessen die Territorialstaaten wiederhergestellt und in einemStaatenbundzusammengefasst, demDeutschen Bund.[24]Dieser wurde ebenfalls als Deutschland bezeichnet, umfasste aber einige mehrheitlich nicht-deutschsprachige Territorien wieBöhmen und Mähren, während andere, mehrheitlich deutschsprachige Gebiete wieOstpreußennicht dazugehörten. Gleichwohl blieb die Nationalbewegung zunächst ein Elitenprojekt. Massenwirksamkeit entfaltete sie erst in derRheinkrise1840.[25] Ab derReichsgründung 1871setzte ein Bedeutungswandel ein, vonDeutschlandals Kulturnation hin zur Staatsbezeichnung, unter geografischer Einschränkung auf das heutige Gebiet: DasKaisertum Österreichwurde 1871 nicht Teil des Deutschen Reichs. Die deutschsprachigen Bewohner Österreichs empfanden sich aber auch weiterhin als Deutsche. Als am Ende desErsten Weltkriegsder Vielvölkerstaat zerfiel, wollten dieDeutschösterreichersich dem Deutschen Reich anschließen. Das untersagte jedoch derVertrag von Saint-Germain. So begannen sich unterschiedliche nationale Identitäten zu entwickeln. Die BegriffedeutschundDeutschlandwurden zunehmend nur mit dem Deutschen Reich identifiziert. Dieser Prozess wurde zunächst unterbrochen, als 1938 unternationalsozialistischer HerrschaftderAnschluss Österreichsan das Deutsche Reich erfolgte. Die Distanzierung vom Nationalsozialismus nach demZweiten Weltkriegführte in Österreich zur Distanzierung vom BegriffDeutschlandund zur Festigung einer eigenen nationalenIdentität der Österreicher. Im Zuge der politischen Neuorganisation des fortbestehenden Gesamtstaates lehnte derParlamentarische Ratin Westdeutschland eine Fortführung des StaatsnamensDeutsches Reichwegen seines „aggressiven Akzents“ ab und verwendete erstmalsDeutschlandals Staatsbezeichnung in der damals konstituierten „BundesrepublikDeutschland“. In den Beratungen sagteTheodor Heuss1948: „Mit dem WortDeutschlandgeben wir dem Ganzen ein gewissesPathos… sentimentaler und nicht machtpolitischer Art.“[26]DieDeutsche Demokratische Republik(DDR) nutzteDeutschlandnicht im Staatsnamen, aber als synonyme Bezeichnung fürDDRin Art. 1 derVerfassung von 1949. Später verwendete die DDR fast nur noch das Attributdeutschbeziehungsweise den Namenszusatz „… der DDR“ für staatliche Hoheitsbezeichnungen. Mit derdeutschen Einheit1990 konnteDeutschlandzur amtlichen Kurzform der Staatsbezeichnung werden.[27] Dienaturräumlichen Großregionensind von Nord nach Süd: dasNorddeutsche Tiefland, dieMittelgebirgszoneund dasAlpenvorlandmit denAlpen. Deutschland gehört geologisch zuWesteuropa, das heißt zu jenem Teil des Kontinents, der dempräkambrischkonsolidierten „Ur-Europa“ (Osteuropa einschließlich eines Großteils Skandinaviens, vgl.Baltica) erst im Verlauf desPhanerozoikumssukzessive durchKontinent-Kontinent-Kollisionen(Gebirgsbildungen) angegliedert wurde. Die entsprechenden Krustenprovinzen (Grundgebirgsprovinzen) werden klassisch vereinfachend (Ost-)Avalonia(vgl.kaledonische Gebirgsbildung) undArmorica(vgl.variszische Gebirgsbildung) genannt. Die jüngste Krustenprovinz ist das Alpen-Karpaten-Orogen(vgl.alpidische Gebirgsbildung), an dem Deutschland nur mit dem äußersten Süden Bayerns Anteil hat und das im Gegensatz zu den beiden anderen tektonischen Provinzen ein aktives Orogen darstellt. Die heutige Oberflächengeologie Deutschlands, das heißt das Muster aus verschieden alten und verschieden aufgebauten Gesteinskomplexen, wie es ingeologischen Kartenvielfach abgebildet ist, entstand erst im Verlauf der letzten 30 bis 20 Millionen Jahre im jüngerenKänozoikumund wurde von zwei Ereignissen maßgeblich geprägt: derAlpidischen Gebirgsbildungund demQuartären Eiszeitalter. Das Quartäre Eiszeitalter formte die vergleichsweise eintönige Oberflächengeologie Norddeutschlands und desAlpenvorlandesmit ihrenMoränen­ablagerungen und sonstigen Begleiterscheinungengroßflächiger Vergletscherungen(vgl.glaziale Serie). Die Oberflächengeologie der Mitte und des überwiegenden Teils des Südens Deutschlands ist das Ergebnis bedeutender bruchtektonischer Hebungen und Senkungen, die auf die Fernwirkung der Alpidischen Gebirgsbildung zurückgehen. Hierbei wurden teils alte (überwiegendPaläozoikum), variszisch gefaltete Grundgebirgskomplexe (Schiefergebirge undKristallin) aus dem Untergrund herausgehoben und großflächig freigelegt (u. a.Rheinisches Schiefergebirge,Harz,Erzgebirge), teils sank die Erdkruste ein und bildete Sedimentationsräume, die mehr oder weniger mächtige känozoische Sedimentabfolgen aufnahmen (Oberrheingraben,Niederrheingraben,Hessische Senke,Molassebecken). Eine tektonische Zwischenstellung nehmen die Tafelländer mit ihren ungefaltetenmesozoischenSchichtenfolgen ein, dominiert vonTriasundJura(Thüringer Becken,Süddeutsches Schichtstufenland). Das geologisch jungeFaltengebirgeder Alpen ist das einzigeHochgebirge, an dem Deutschland Anteil hat. Die deutschen Alpen, die sich zur Gänze im BundeslandBayernbefinden, weisen die einzigen Gebirgsgipfel mit mehr als2000m ü.NHNauf. Der Gipfel derZugspitze(2962m ü. NHN), den sich Deutschland mit Österreich teilt, ist der höchstgelegene Punkt des Landes. Die deutschenMittelgebirgeerstrecken sich vom Nordrand derMittelgebirgsschwellebis zum Alpenrand und zumOberrheinmit dem Bodensee. Sie nehmen tendenziell von Nord nach Süd an Höhe und Ausdehnung zu. Höchster Mittelgebirgsgipfel ist derFeldbergimSchwarzwald(1493m ü. NHN), gefolgt vomGroßen ArberimBayerischen Wald(1456m ü. NHN). Gipfel über1000m ü. NHNbesitzen außerdem dasErzgebirge, dasFichtelgebirge, dieSchwäbische Albund derHarz, der sich recht isoliert als nördlichstes unter den höchsten deutschen Mittelgebirgen mit demBrockenauf1141m ü. NHNerhebt. Nördlich der Mittelgebirgsschwelle erreichen nur noch einige Berge innerhalb der eiszeitlichenEndmoränenzügemehr als100m ü. NHN, von denen dieHeidehöheinSchraden(Südlicher Landrückenim brandenburgisch-sächsischen Grenzgebiet) mit201m ü. NHNder höchste ist. Die tiefste allgemein zugängliche Landesstelle Deutschlands liegt bei3,54m unter NHNin einerSenkebeiNeuendorf-Sachsenbandein derWilstermarsch(Schleswig-Holstein).[28]Ebenfalls in diesem Bundesland befindet sich die tiefsteKryptodepression: Sie liegt mit39,6m unter NHNam Grund desHemmelsdorfer Seesnordöstlich vonLübeck. Der tiefste künstlich geschaffene Geländepunkt liegt bei267m unter NHNam Grund desTagebaus Hambachöstlich vonJülichin Nordrhein-Westfalen. Deutschland gehört vollständig zurgemäßigten KlimazoneMitteleuropas im Bereich derWestwindzoneund befindet sich im Übergangsbereich zwischen demSeeklimain Westeuropa und demKontinentalklimainOsteuropa. DasKlimain Deutschland wird unter anderem vomGolfstrombeeinflusst, durch den das durchschnittliche Temperaturniveau für die Breitenlage ungewöhnlich hoch ist. Die mittlereJahresdurchschnittstemperatur, bezogen auf die Normalperiode 1961–1990,[29]beträgt im bundesweiten Gebietsmittel 8,2 °C, die mittleren Monatsdurchschnittstemperaturen liegen zwischen −0,5 °C im Januar und 16,9 °C im Juli. Der mittlere jährliche Niederschlag beträgt 789 Millimeter. Die mittlere monatliche Niederschlagshöhe liegt zwischen 49 Millimeter im Februar und 85 Millimeter im Juni. Die tiefste offiziell anerkannte in Deutschland gemessene Temperatur betrug −37,8 °C; sie wurde 1929 inWolnzachregistriert. Die bisher höchste Temperatur betrug 41,2 °C und wurde am 25. Juli 2019 inDuisburg-Baerlund inTönisvorstam Niederrhein gemessen.[30] Auswertungen der Wetterbeobachtungen zeigen, dass die mittlere Temperatur in Deutschland im Zeitraum 1881 bis 2022 um ca. 1,7 °C gestiegen ist (linearer Trend), während der weltweite Temperaturanstieg (über See- und Landflächen) im gleichen Zeitraum nur 1,1 °C betrug.[31] GegenwärtigePrognosenfür dasKlima in Deutschlandstellen bis zur Mitte des 21. Jahrhunderts einen Anstieg um 2,3 °C im Jahresmittel in Aussicht, der sich bis 2100 weiter auf 4,7 °C erhöhen könnte (bis 6 °C in Sommermonaten).[32][33]Bis 2050 könnte der volkswirtschaftliche Schaden 900 Milliarden Euro betragen, zuzüglich Folgekosten durch gesundheitliche Schäden, vorzeitige Todesfälle undsozioökologische Kettenreaktionen.[34][35] Von den sechsStrömenmit den größten Einzugsgebieten entwässernRhein,Elbe,WeserundEmsüber dieNordseeund dieOderüber dieOstseein denAtlantik, während dieDonauinsSchwarze Meerfließt und somithydrographischdemMittelmeerzuzurechnen ist. Die Einzugsgebiete dieser beiden Systeme werden durch dieeuropäische Hauptwasserscheidevoneinander getrennt. Der in der Schweiz entspringende Rhein dominiert den Südwesten und Westen. Auf 865 Kilometern fließt er durch bzw. an der Grenze zu Deutschland, bevor er über die Niederlande in die Nordsee mündet. Seine wichtigsten deutschen Zuflüsse sindNeckar,Main,MoselundRuhr. Der Rhein hat eine große wirtschaftliche Bedeutung und ist eine der am stärksten befahrenenWasserstraßenEuropas. Die Donau entwässert im Süden auf 647 Kilometern fast das gesamte deutscheAlpenvorlandund fließt weiter nach Österreich undSüdosteuropa. Ihre wichtigsten deutschen Zuflüsse sindIller,Lech,IsarundInn. Den Osten Deutschlands durchfließt auf 725 Kilometern die in Tschechien entspringende Elbe. Ihre wichtigsten deutschen Nebenflüsse sindSaaleundHavel. Auf 179 Kilometern ist die Oder, wie im weiteren Verlauf auch ihr wichtigster Zufluss, dieNeiße, derGrenzfluss zu Polen. Einzig das Einzugsgebiet der 452 Kilometer langen Weser liegt vollständig in Deutschland. Sie speist sich aus den FlüssenWerraundFuldaund entwässert den mittleren Norden. Die Ems durchfließt auf 371 Kilometern den äußersten Nordwesten des Landes. Ihr Einzugsgebiet erstreckt sich auch auf Teile der Niederlande. Die natürlichenSeensind überwiegend glazialen Ursprungs. Daher finden sich die meisten der großen Seen im Alpenvorland, in derHolsteinischen Schweizund inMecklenburg. Der größte vollständig zumdeutschen Staatsgebietgehörende See ist dieMüritz, die Teil derMecklenburgischen Seenplatteist. Der größte See mit deutschem Anteil ist derBodensee, an den auch Österreich und die Schweiz grenzen. Im Westen und Osten Deutschlands gibt es viele, durch die Rekultivierung vonBraunkohletagebauenoderIndustriebrachenentstandene, künstliche Seen, wie dasLeipziger Neuseenlandoder denDortmunderPhoenix-See. ImWattenmeer, der niederländischen, deutschen und dänischen Nordseeküste unmittelbar vorgelagert, liegen dieFriesischen Inseln. Während dieNordfriesischen InselnFestlandsreste sind, die durch Landsenkung und nachfolgende Überflutung von der Küste getrennt wurden, handelt es sich bei denOstfriesischen InselnumBarriereinseln, die aus durch küstenparallele Strömungen sowie Wellen- und Gezeitendynamik angespülten Sedimenten entstanden. Das inmitten derDeutschen BuchtgelegeneHelgolandist die am weitesten vom Festland entfernt liegende bewohnte deutsche Insel. Sie geht auf den Aufstieg einesSalzstockesim Untergrund der Nordsee zurück. Die größten deutschen Inseln in der Ostsee sind (von West nach Ost)Fehmarn,Poel,Hiddensee,RügenundUsedom. Rügen ist zugleich die größte deutsche Insel. GrößteHalbinselistFischland-Darß-Zingst. Mit Ausnahme von Fehmarn sind diese Landflächen Teil einerBoddenküste, das heißt einer nacheiszeitlich gefluteten und nachfolgend durchAnlandungsvorgängemodifiziertenGrundmoränenlandschaft. Die größten und bekanntestenInseln in BinnengewässernsindReichenau,MainauundLindauim Bodensee sowie dieHerreninselimChiemsee. Der Naturraum Deutschland liegt in derkühlgemäßigten Klimazone; von West nach Ost kennzeichnet seine natürlicheVegetationden Übergang vomWestseitenseeklimazumKontinentalklima. DieFlorawäreohne menschlichen Einflusshauptsächlich vonLaub- und Mischwälderngeprägt, ausgenommen nährstoffarme oder trockene Standorte wie Felskuppen,HeideniederungenundMoorlandschaftensowie diealpinenundsubalpinenHochlagen, die äußerst vegetationsarm sind und in ihrem Klima derkaltgemäßigten Klimazoneähneln. Örtlich weist die Flora in Deutschland eine hoheDiversifikationdurch Standortfaktoren des Geländes und dermesoklimatischenLage auf. Der Gesamtbestand der in Deutschland wild lebenden Pflanzenarten wird auf über 9.500Artengeschätzt, davon sind fast 3.000 Arten Samenpflanzen, 74 Farnpflanzen, über 1.000 Moose und etwa 3.000 Kieselalgen. Dazu kommen rund 14.000Pilz-und 373Schleimpilzarten.[36]Insbesondere auf Brach- und Störflächen finden sich heute eine Reiheeingeführter Artenwie dieRobinieund dasDrüsige Springkraut. Derzeit bedeckt derWald in Deutschlandrund 30 Prozent der Landfläche. Damit ist Deutschland eines der waldreichsten Länder in der Europäischen Union. Die aktuelle Baumartenzusammensetzung entspricht nur zum geringen Teil den natürlichen Gegebenheiten und wird hauptsächlich von derForstwirtschaftbestimmt. Die häufigsten Baumarten sind mit 26,0 Prozent Flächenanteil dieGemeine Fichte, gefolgt von derWaldkiefermit 22,9 Prozent, derRotbuchemit 15,8 Prozent und denEichenmit 10,6 Prozent.[37] Rund die Hälfte der Staatsfläche wirdlandwirtschaftlichgenutzt; lautStatistischem Bundesamtwaren es 179.891 Quadratkilometer am 31. Dezember 2023.[38]Neben der Nutzung alsDauergrünlandwird auf einem GroßteilAckerbaubetrieben, seit derStein-bzw. derBronzezeitüberwiegend mitNutzpflanzen, die nicht natürlich in Mitteleuropa vorkommen (die meisten derGetreideartenaus demVorderen Orient,KartoffelundMaisaus Amerika). In den Flusstälern, unter anderem von Main, Mosel,Ahrund Rhein, wurde die Landschaft vielfach für den Weinanbau umgestaltet. Die Bewahrung der Natur ist in Deutschland öffentliche Aufgabe und inArt. 20aGrundgesetz verankertesStaatsziel. DemNaturschutzdienen 16Nationalparks(sieheNationalparks in Deutschland), 19Biosphärenreservate, 105Naturparkssowie tausende vonNaturschutzgebieten,LandschaftsschutzgebietenundNaturdenkmälern. In Deutschland sind etwa 48.000Tierartennachgewiesen, darunter 104 Säugetier-, 328 Vogel-, 13 Reptilien-, 22 Lurch- und 197 Fischarten sowie über 33.000 Insektenarten, womit das Land „aufgrund der erdgeschichtlichen Entwicklung und der geographischen Lage zu den eher artenärmeren Gebieten“ zählt.[39]Zu diesen Arten kommen über 1.000 Krebs-, fast 3.800 Spinnen-, 635 Weichtiere sowie über 5.300 andere Wirbellose. Zu den in Deutschland heimischen wildenSäugetierenzählen unter anderemRehe,Wildschweine,Rot-undDamhirschesowieFüchse,MarderundLuchse.BiberundOttersind seltene Bewohner der Flussauen, mit teilweise wieder steigenden Beständen. In denbayerischen AlpenlebenAlpensteinbock,Alpenmurmeltierund dieGämse; letztere ist auch in verschiedenen Mittelgebirgen anzutreffen. Andere Großsäuger, die in früherer Zeit auf dem Gebiet des heutigen Deutschlands lebten, wurden ausgerottet:Wildpferd,Auerochse(15. Jahrhundert),Wisent(16. Jahrhundert),Braunbär(19. Jahrhundert),Wolf(19. Jahrhundert),Elch(20. Jahrhundert). Während Elche heute gelegentlich aus Nachbarländern zuwandern, haben sich Wölfe ausPolenkommend wieder fest in Deutschland etabliert und um die Jahrtausendwende erstmals Nachwuchs geboren. Im Zeitraum 2023/2024 existierten in Deutschland insgesamt 209 bestätigte Rudel, 46 Paare und 19 territoriale Einzeltiere, die größtenteils in den Ländern Sachsen, Brandenburg, Niedersachsen und Mecklenburg-Vorpommern leben.[40][41]2013 wurde eine Herde Wisente imRothaargebirgeausgewildert und hat sich inzwischen auf etwa 40 Tiere vermehrt. Im Oktober 2019 wurde im LandkreisGarmisch-Partenkirchenein vermutlich aus Italien zugewanderter Braunbär von einer Wildtierkamera fotografiert. In den darauf folgenden Monaten konnte das Tier mehrfach erneut nachgewiesen werden.[42]Bärensichtungen gab es in den folgenden Jahren im deutschen Alpenraum immer wieder. Im Jahr 2023 lagen 13 Bärennachweise in verschiedenen Landkreisen vor.[43]Bereits im Jahr 2006 war mit dem „Problembären“Brunoein Bär nach Deutschland zugewandert. Inzwischen leben auch ursprünglichhier heimische Luchsewieder in Deutschland, allerdings in geringerBestandsdichte, weil sie immer wieder Opfer vonWildereiund Straßenverkehr werden. VomSeeadler, der als Vorlage für das deutscheWappentiergilt, gibt es wieder etwa 500 Paare, vor allem inMecklenburg-VorpommernundBrandenburg. DerSteinadlerkommt nur noch in denBayerischen Alpenvor, wo auch der dort ausgerotteteBartgeieraus der Schweiz und Österreich wieder Einzug hält. Die häufigstenGreifvögelsind heuteMäusebussardundTurmfalke, der Bestand anWanderfalkenist deutlich geringer. Über die Hälfte des Gesamtbestandes anRotmilanenbrütet in Deutschland, ist aber aufgrund der intensiven Landwirtschaft rückläufig. Dagegen profitieren vieleVögelalsKulturfolgervon der Anwesenheit des Menschen, insbesondere die in Städten lebendenStadttauben,Amseln(frühere Waldvögel),SpatzenundMeisen, für deren Überleben auch dieWinterfütterungsorgt, sowieKrähenundMöwenaufMüllkippen. Das Wattenmeer ist Rastplatz für zehn bis zwölf MillionenZugvögelpro Jahr. Der früher in den Flüssen häufigeLachswurde im Zuge derIndustrialisierungweitgehend ausgerottet, aber in den 1980er-Jahren imRheinwieder angesiedelt. In Deutschland wurde der letzteStör1969 gefangen. In vielen Teichen werden die von denRömerneingeführtenKarpfengehalten. Die von Berufsfischern Mitte des 20. Jahrhunderts als Beutekonkurrenten nahezu ausgerotteten und inzwischen geschützten ArtenSeehundundKegelrobbe– letztere das größte in Deutschland heimische Raubtier – sind heute wieder mit einigen tausend Exemplaren an den deutschen Küsten vertreten. In Nord- und Ostsee kommen achtWalartenvor, darunter derSchweinswal, und mit demGemeinen Delfinauch eineDelfinart. Die artenarmeReptilienfaunaumfasst beispielsweiseRingelnatter,Kreuzotter,Zauneidechseund die vom Aussterben bedrohteEuropäische Sumpfschildkröte.AmphibienwieSalamander,Frösche,Kröten,UnkenundMolchestehen in Deutschland alle unter Artenschutz, und die Hälfte der rund 20 Arten wird auf der nationalenRoten Liste bedrohter Tierartengeführt.[44] Zu den – teilsinvasiven–Neozoen in Deutschland(eingeschleppten Tierarten) gehörenWaschbär,Marderhund,Bisamratte,Nutria,Halsbandsittich,KanadagansundNilgans. Deutschland hat insgesamt neunNachbarstaaten: Im Norden grenzt Deutschlandan Dänemark, im Nordostenan Polen, im Ostenan Tschechien, im Südostenan Österreich, im Südenan die Schweiz, im Südwestenan Frankreich, im Westenan Luxemburgundan Belgienund im Nordwestenan die Niederlande. Die Grenzlänge beträgt insgesamt 3876 Kilometer.[45] In Deutschland werden insgesamt 51 Prozent der Landesflächelandwirtschaftlichgenutzt (2016),Wälderbedecken weitere 30 Prozent. 14 Prozent werden als Siedlungs- und Verkehrsfläche genutzt. Wasserflächen kommen auf zwei Prozent, die restlichen drei Prozent verteilen sich auf sonstige Flächen, zumeistÖdlandund auchTagebaue.[38] Dieföderal aufgebaute Bundesrepublikbesteht aus 16Gliedstaaten, die offiziell alsLänder(Bundesländer) bezeichnet werden. DieStadtstaatenBerlin und Hamburg bestehen jeweils aus gleichnamigenEinheitsgemeinden, während dieFreie Hansestadt Bremen, als dritter Stadtstaat, mitBremenundBremerhavenzwei separate Stadtgemeinden umfasst. Im Unterschied zu anderen Föderalstaaten gibt es in Deutschland keinebundesunmittelbaren Gebiete. DieGemeindensind die kleinstendemokratischverfassten, rechtlich selbstständigenGebietskörperschaftenund Verwaltungseinheiten Deutschlands.[46]Sie haben aufgrund ihrer genossenschaftlichen Prägung, die bis insMittelalterzurückreicht, eine lange Tradition.[47]Heute sind die Gemeinden in Deutschland, mit Ausnahme der Stadtstaaten und der meistenkreisfreien Städte, inLandkreisenund anderenGemeindeverbändenzusammengefasst.[48]Es existieren 400 Gebietskörperschaften auf Kreisebene, davon 294 Kreise sowie 106 kreisfreie Städte. Sie sind untergliedert in insgesamt 10.751 Gemeinden (Stand Januar 2025),Tendenz sinkend, sowie 198 größtenteils unbewohntegemeindefreie Gebiete. Kreise und Gemeinden unterliegen demKommunalverfassungsrechtdes jeweiligen Bundeslandes und sind daher bundesweit unterschiedlich organisiert. Der Landkreis ist damit sowohl überörtliche kommunale Gebietskörperschaft als auch untere staatliche Verwaltungsbehörde, er hat eine eigeneVertretungskörperschaft, denKreistag(Art. 28Abs. 1 S. 2 GG), und nimmt verschiedene Aufgaben der „überörtlichen Gemeinschaft“ für die kreisangehörigen Gemeinden wahr.[49] Die Kommunen sindstaatsrechtlichTeil der Länder, womit sie deren Aufsichts- undWeisungsrechtunterliegen und demnach über keine eigenestaatliche Hoheitsmachtverfügen.[50]DieSelbstverwaltungsgarantiedes Art. 28 Abs. 2 GG – einerseits eine sogenannteinstitutionelle Rechtssubjektsgarantie,woraus folgt, dass es Gemeinden im Staatsaufbau überhaupt geben muss, andererseits ein subjektiv-öffentliches Recht mit Verfassungsrang – unterscheidet zwischen den Gemeinden, denen dieses Recht vollumfänglich zugesprochen wird, und den Gemeindeverbänden (Landkreisen), denen es in lediglich abgestufter Form zugesprochen wird. Somit besteht für die Aufgabenabgrenzung zwischen Gemeinden und Kreisen ein klares Regel-Ausnahme-Verhältnis zugunsten der Gemeinden (Subsidiaritätsprinzip).[51]DasBundesverfassungsgerichthat hinsichtlich der „Angelegenheiten der örtlichen Gemeinschaft“, also der in Art. 28 Abs. 2 Satz 1 GG gewährleisteten Befugnis, in diesem Aufgabenbereich die Geschäfte eigenverantwortlich zu führen (sogenannteobjektive Rechtsinstitutionsgarantie), den Vorrang der Gemeindeebene vor der Kreisebene nach Maßgabe der Gesetze festgestellt: Hiernach gilt für die Gemeinden „als Essentiale“ und „identitätsbestimmendes Merkmal der gemeindlichen Selbstverwaltung“ das Prinzip der „‚Universalität‘ des gemeindlichen Wirkungskreises“,[52]im Gegensatz zur speziellen Kompetenz der Gemeindeverbände kraft ausdrücklicher gesetzlicher Zuweisung, womit es auch keine feststehenden Gemeindeverbandshoheitengibt.[53] In Deutschland werden Verdichtungs- und Ballungsräume (Agglomerationen) nicht statistisch genau abgegrenzt. Es gibt (Stand 31. Dezember 2024) 80 Großstädte (ab 100.000 Einwohner), davon 15 mit mehr als 500.000 Einwohnern, historisch bedingt überwiegend im Westen und Südwesten Deutschlands. Diese entlang des Rheins verlaufenden Ballungsräume bilden den Mittelteil der zentralen europäischen Bevölkerungskonzentration (Blaue Banane). Die meisten Agglomerationen sind monozentrisch, dasRuhrgebiethingegen ist eine (polyzentrische)Konurbation. Mit seinen zahlreichen Zentren hat Deutschland, anders als etwa die NachbarländerÖsterreichmit seiner HauptstadtWienundDänemarkmitKopenhagen, keinePrimatstadt. Trotz der Vielzahl großer Städte lebte zum 31. Dezember 2024 etwas weniger als ein Drittel (26,8 Millionen) der Einwohner Deutschlands in Großstädten. Nahezu 15 Millionen Einwohner leben in den 15 deutschen Städten mit mehr als einer halben Million Einwohnern, was etwa 18 % aller Einwohner entspricht.[70][71] Auf dem Gebiet Deutschlands wurden von derMinisterkonferenz für Raumordnungzwischen 1995 und 2005 insgesamt elfEuropäische Metropolregionenfestgelegt. Diese gehen über die entsprechenden Agglomerationen weit hinaus. Köln/Düsseldorf/Dortmund/Essen gehören zurMetropolregion Rhein-Ruhr, Leipzig/Halle/Chemnitz zurMetropolregion Mitteldeutschland. Eine weitere ist dieMetropolregion Rhein-Neckarum Ludwigshafen/Mannheim/Heidelberg. Die folgende Tabelle zeigt alle deutschen Städte mit über 500.000 Einwohnern mitsamt der Agglomeration und Metropolregion, zu der sie gehören: BerlinHamburgMünchenKöln Frankfurt am MainDüsseldorfStuttgartLeipzig *) Stand: 31. Dezember 2024 Nach Fortschreibung desZensus 2022lebten am 31. Dezember 2024 in Deutschland 83.557.140Einwohner[75]auf einer Fläche von 357.381 Quadratkilometern.[2]Das Land gehört mit knapp 237 Menschen pro Quadratkilometer zu den dicht besiedeltenFlächenstaaten.[3]Zum Jahresende 2023 waren 50,7 Prozent der Bevölkerung Frauen und 49,3 Prozent Männer.[75]Im Jahr 2019 waren 18,4 Prozent der Einwohner unter 20 Jahre, 24,6 Prozent zwischen 20 und 40 Jahre und 28,4 Prozent zwischen 40 und 60 Jahre alt. Im Alter von 60 bis 80 Jahren waren 21,7 Prozent der Bevölkerung, 6,8 Prozent waren älter.[76]Das Durchschnittsalter steigt seit der Wiedervereinigung 1990 kontinuierlich und lag 2023 bei 44,6 Jahren.[77]Deutschland gehört damit zu den ältesten Gesellschaften der Welt. Nach demZensus 2022lebten zum Stichtag 15. Mai 2022 82.711.282 Menschen in Deutschland.[78] Neben derFamilieals der am häufigsten angestrebten Form des Zusammenlebens sind viele Lebensmodelle in der deutschen Gesellschaft vertreten.[79]Die Anzahl der lebend geborenen Kinder lag im Jahr 2015 bei 737.575, das war die höchste Geburtenzahl seit 15 Jahren. Dies entspricht einerGeburtenratevon 1,50 Kindern pro Frau bzw. 9,6 Geburten pro 1000 Einwohner.[80]Im selben Zeitraum wurden 925.200 Sterbefälle registriert, etwa 11,2 Fälle pro 1000 Einwohner.[81]Im Jahr 2021 erhöhte sich die Geburtenrate pro Frau auf 1,58 Kinder, bereits im Folgejahr war sie wieder auf 1,46 Kinder pro Frau gesunken.[82] Weil seit 1972 dieSterberatealljährlich über der Geburtenrate liegt, wird politisch die Orientierung zu einerfamilienfreundlichen, kinder- und nachwuchsfördernden Gesellschaft mitMehrkindfamilienangestrebt (Pronatalismus). Als zentrale Voraussetzung dafür werten Experten dieVereinbarkeit von Familie und Beruf. Bei fortgesetzt niedrigen Geburtenraten, insbesondere in Bevölkerungsschichten mit mittleren und höheren Bildungsabschlüssen, wurden für Deutschland soziale, ökonomische und geopolitische Probleme vorhergesagt (Stand 2012).[83] Etwa 71,633 Millionen Personen in Deutschland besaßen zum Stichtag 30. September 2024 diedeutsche Staatsangehörigkeit. Dies entspricht 84,5 Prozent der Wohnbevölkerung.[75]Hatten im Jahr 2017 rund 18,9 Millionen Personen (23 Prozent der Gesamtbevölkerung) einenMigrationshintergrund,[84][85]waren es 2022 mit 23,8 Millionen Menschen 28,7 Prozent der Gesamtbevölkerung, die einen Migrationshintergrund hatten.[86][87]Davon waren fast zwei Drittel Zugewanderte und mehr als die Hälfte deutsche Staatsangehörige.[87] Im Jahr 2022 hatten in der Altersgruppe der unter 15-Jährigen 41 Prozent einen Migrationshintergrund, in der Altersgruppe der 15- bis 49-Jährigen hatten 36 Prozent einen Migrationshintergrund und in der darüberliegenden Altersklasse (50+) waren es 19 Prozent.[88][87] Als Personen mit Migrationshintergrund zählten im Zensus 2011 alle Ausländer sowie alleDeutschen, die nach 1955 auf das Gebiet der heutigen Bundesrepublik Deutschland zugewandert sind oder mindestens einen nach 1955 zugewanderten Elternteil haben. Unter ihnen bilden mit Stand 2009 dieAussiedler und Spätaussiedlerdie größte Gruppe, gefolgt von Bürgern derTürkei, anderer Staaten der Europäischen Union und des ehemaligenJugoslawien.[89]Zwischen 1950 und 2002 wurden insgesamt 4,3 Millionen Menschen, entweder im Land geboren oder lange dort lebend, auf eigenen Antrageingebürgert. DasInstitut der deutschen Wirtschaft(IW) prognostizierte 2017, die Bevölkerung Deutschlands werde durch Einwanderung weiter wachsen und im Jahr 2035 rund 83,1 Millionen Menschen umfassen.[90]Im Jahr 2018 wuchs die deutsche Bevölkerung um 227.000 Einwohner, womit Deutschland die Marke von 83 Millionen Einwohnern überschritt.[91]Im Jahr 2019 wuchs sie um 147.000 Personen (+0,2 %) auf 83,2 Millionen.[92]Ende September 2020 lag die Einwohnerzahl bei 83.190.556.[93]Im Jahr 2022 überstieg die Bevölkerungszahl Deutschlands erstmals den Wert von 84 Millionen, zum 30. Juni 2022 lag sie laut Statistischem Bundesamt bei 84.080.000 Einwohnern.[94] Deutschland istde factoseit Jahrzehnten, seit 1958 mit kurzen Unterbrechungen, einEinwanderungsland.[95][96]Die höchste Nettozuwanderung ihrer Geschichte hatte die Bundesrepublik Deutschland im Jahr 2022, als knapp 1,5 Millionen Menschen mehr zu- als fortzogen.[95][87] In Deutschland ist hauptsächlich diedeutsche Sprache(Hochdeutsch) verbreitet. Sie wird alsStandardsprachein den überregionalen Medien und als Schriftsprache verwendet; als Sprache des Alltags wird Deutsch in vielen Regionen fast ausschließlich gesprochen (oft regional leicht eingefärbt). Der Übergang zu dendeutschen Dialektenist fließend. Bei denAmtssprachen innerhalb Deutschlandsist Deutsch die wichtigsteVerwaltungssprache.[98]Die Zuständigkeit liegt im Grundsatz in derKulturhoheit der Länder, der Gesamtstaat legt solche Sprachen nur zur Erfüllung seiner eigenen Aufgaben fest. Sofern europäisches Recht anwendbar ist, können vor Gericht Anträge und Schriftstücke in jederAmts-oderGerichtssprachejedes Mitgliedslands derEuropäischen Uniongestellt werden. Angestammtenationale MinderheitensindDänen,Friesen,SorbenundSinti und Roma. Einige Regional- und Minderheitensprachen dürfen als Amts-, Gesetzes- oder Gerichtssprachen verwendet werden. Grundlage ist dieEuropäische Charta der Regional- oder Minderheitensprachen, nach der DeutschlandNiederdeutschals Regionalsprache und folgende Minderheitensprachen anerkennt:Dänisch(etwa 50.000 Sprecher, sowohlReichsdänisch, überwiegend in der VarianteSydslesvigdansk, als auchSønderjysk),Friesisch(etwa 10.000,Nordfriesischin Schleswig-Holstein,Saterfriesischin Niedersachsen),Sorbisch(etwa 30.000,Obersorbischin Sachsen,Niedersorbischin Brandenburg) undRomanesder Roma (etwa 200.000 in ganz Deutschland). Andere neue oder in Deutschland kaum noch gesprochene Minderheitensprachen wieJiddischoder diejenische Sprachewurden nicht in die Charta aufgenommen.[99]Die Sprachen von Zuwanderern fallen ausdrücklich nicht unter die Charta.[100]Die vonGehörlosenverwendeteDeutsche Gebärdensprache(DGS) wurde mit Einführung desBehindertengleichstellungsgesetzes(BGG) im Jahr 2002 in Deutschland als eigenständige Sprache anerkannt.[101]In früherer Zeit existierten im Gebiet des heutigen Deutschland noch weitere Minderheitensprachen wie diePolabische Sprache(im 18. Jahrhundert ausgestorben) oder noch früher dasMoselromanische(im 11. Jahrhundert ausgestorben). Die niederdeutsche Sprache wird sowohl sprachwissenschaftlich (der grammatischen und lexikalischen Substanz nach) als auch politisch und rechtlich teilweise als eigenständige Sprache aufgefasst; sie verhält sich allerdings soziolinguistisch wie ein Dialekt und ist daher auch als „Scheindialekt“ des Deutschen bezeichnet worden und insofern als eine regionaleVarietät(sieheNiederdeutsche Sprache: Die Stellung des Niederdeutschen). Niederdeutsch hatte 2007 etwa 2,6 Millionen aktive Sprecher, passive Kenntnis hatten etwa drei Viertel der Bevölkerung des Sprachgebiets.[102]2016 war das passive Verstehen bei knapp der Hälfte der Einwohner des Sprachgebietes gut bis sehr gut, in Mecklenburg-Vorpommern 70 Prozent, in Schleswig-Holstein knapp 60 Prozent, in Niedersachsen knapp 50 Prozent.[103]Aktiv beherrschten die Niederdeutsche Sprache in Mecklenburg-Vorpommern knapp 21 Prozent, in Schleswig-Holstein knapp 25 Prozent, in Nordrhein-Westfalen und Sachsen-Anhalt jeweils knapp 12 Prozent und in Brandenburg knapp 3 Prozent.[104] Norddeutsche verwenden die niederdeutsche Sprache oder regionale Mundarten tendenziell weniger ausgeprägt, während immittel-undoberdeutschenRaum der Gebrauch derfränkischen,bairischenundalemannischen Mundartenselbst im akademischen Milieu verbreiteter ist. Immer wieder brachtenZuwandererihre Sprachen mit, zum Beispiel dieRuhrpolenim 19. Jahrhundert. Während die Nachkommen der älteren Zuwanderungswellen sich inzwischen sprachlich weitgehend angepasst haben, verwenden Zuwanderer der vergangenen Jahrzehnte (etwaGastarbeiter) untereinander neben dem Deutschen noch häufig ihre Muttersprache, vor allemTürkisch(etwa zwei Millionen). Daneben ist auch dierussische Spracheverbreitet, unterKontingentflüchtlingenund unterRusslanddeutschen, zu denen nicht nur deutsche oderplautdietsche, sondern auch russische Muttersprachler gehören (drei bis vier Millionen). Ein Teil der älteren jüdischen Bevölkerung, welcher aus der ehemaligenSowjetunionzugewandert ist, spricht auch Jiddisch; jedoch fand zumeist schon vor der Auswanderung keine Weitergabe dieser Sprache an nachfolgende Generationen mehr statt, sodass sie durch Russisch bzw. Deutsch verdrängt wurde.Haredim, die heutigen Hauptträger der jiddischen Sprache inIsraelund denUSA, machen in Deutschland nur eine kleine Minderheit der jüdischen Gemeinden aus. Die Zahl der Personen, welchePolnischals Alltagssprache verwenden, wird als relativ hoch vermutet. Aufgrund der starkenAssimilationder polnischen und polnischstämmigen Bevölkerungsgruppe ist der Gebrauch des Polnischen außerhalb des eigenen Haushalts jedoch verhältnismäßig selten. Die an öffentlichen Schulen in Deutschland vorrangig gelehrteFremdspracheistEnglisch. Als zweite Fremdsprache werden häufigFranzösisch,LateinoderSpanischangeboten, seltener auchRussischoderItalienisch– je nach schulischem Angebot und den Vorgaben der einzelnen Bundesländer. In der ehemaligen DDR warRussischdie vorherrschende Fremdsprache im Schulunterricht. Englisch wurde zwar ebenfalls unterrichtet, spielte jedoch eine deutlich untergeordnete Rolle.[105] Wie der Großteil West- und Mitteleuropas ist das heutige Deutschland bis zurSpätantikezurückreichend christlich-abendländisch und seit dem 18. Jahrhundert aufgeklärt-wissenschaftlich geprägt. Dem liegen Einflüsse aus der antiken griechischen und römischen Kultur ebenso zugrunde wie jüdische und christliche Traditionen, die sich seit Beginn der ChristianisierungNordwesteuropas, ab etwa dem 4. Jahrhundert, mit germanischen Traditionen vermischt hatten. Das Judentum ist in Deutschland schon in römischer Zeit nachweisbar, wurde aber immer wieder Opfer vonPogromenund erzwungener Konversion, in der schlimmsten Form dieShoahderNS-Zeit. Das Gebiet Deutschlands wurde seit dem frühen Mittelalterchristianisiert. In derfränkischenZeit wurde im Reich Karls des Großen die Missionierung, teilweise durch Zwang, abgeschlossen. MitMartin LuthersThesenanschlag 1517 begann die christlicheReformationund in der Folge die Bildungprotestantischer Konfessionen, die in Deutschland neben der katholischen Konfession die religiöse Landschaft prägen. Aufgrund der Regelungcuius regio, eius religio(d. h. der Landesherr bestimmt, welcher Religion die Untertanen anzugehören haben) war die konfessionelle Landschaft über Jahrhunderte stark zersplittert mit nahezu rein katholischen und evangelischen Regionen in unmittelbarer Nachbarschaft zueinander. Der Zustrom der Heimatvertriebenen nach dem Zweiten Weltkrieg, die zunehmende Mobilität der Bevölkerung sowie anhaltendeSuburbanisierungundSäkularisierunghaben diese Kontraste jedoch im 20. und 21. Jahrhundert verwischt. DieReligionsfreiheit in DeutschlandgarantiertArt. 4des Grundgesetzes, individuell alsGrundrechtund institutionell im Verhältnis von Religion und Staat. So wird die weltanschauliche Neutralität des Staates und dasSelbstbestimmungsrechtderReligionsgemeinschaftenfestgeschrieben. Auf dieser Basis ist das Verhältnis von Religionsgemeinschaften und Staat partnerschaftlich; es gibt also keine strikteTrennung von Kirche und Staat, sondern in vielen sozialen und schulisch-kulturellen Bereichen bestehen Verflechtungen, beispielsweise über eine kirchliche, aber staatlich mitfinanzierte Trägerschaft von Kindergärten, Schulen, Krankenhäusern oder Pflegeheimen. Ebenso berufen sich einige deutsche Parteien auf die christliche Tradition des Landes. Die christlichen Kirchen besitzen den Status vonAmtskirchenund sindKörperschaften des öffentlichen Rechts, aufgrund des geltendenStaatskirchenrechtsjedochsui generis.Alsöffentlich-rechtlichen Religionsgesellschaftensollen den Kirchen bestimmte Gestaltungsmöglichkeiten eingeräumt werden, ohne dass sie dabei einer Staatsaufsicht unterliegen; stattdessen wird sowohl der kirchlicheÖffentlichkeitsauftragteilweise inKirchenverträgenmit den Ländern oder den entsprechenden Regelungen in denLandesverfassungenanerkannt wie auch die besondere, originäreKirchengewaltrechtlich bekräftigt. Bestimmte christliche Kirchen sowie die jüdischen Gemeinden erheben eineKirchensteuer, die derStaatgegen eine Aufwandsentschädigung einzieht und an die jeweiligen Kirchen beziehungsweise an denZentralrat der Juden in Deutschlandweiterleitet. Des Weiteren ist derReligionsunterrichtlaut Grundgesetz fakultatives, aber dennoch ordentliches Unterrichtsfach in den öffentlichen Schulen (mit Ausnahme von Bremen, Berlin und Brandenburg). Dieses Fach wird oft von einem Vertreter einer der beiden großen Kirchen unterrichtet. Fast 47 % der Bevölkerung gehörten 2024 keinerGlaubensgemeinschaftan; fast 48 % der Bevölkerung gehörten 2024 einerchristlichen Konfessionan: derrömisch-katholischen Kirche23,7 % (überwiegend in West- und Süddeutschland), derevangelischen Kirche(Lutheraner,ReformierteundUnierte) 21,5 %, anderen christlichen Kirchen wieorthodoxenundaltorientalischen Kirchen, denZeugen Jehovas, derneuapostolischen Kircheund denFreikircheninsgesamt ca. 2,5 %.[106] Ende 2023 stellte in keinem der 16 Bundesländer eine der beiden großen Kirchen (römisch-katholisch, evangelisch) die absolute Mehrheit der Einwohner; in zehn Ländern war eine absolute Mehrheit der Bevölkerung weder Mitglied der katholischen noch der evangelischen Kirche.[107] Die Anzahl der Gottesdienstbesucher ist wesentlich geringer als die Anzahl der Kirchenmitglieder. An den sogenannten Zählsonntagen (zweiter Fastensonntag und zweiter Sonntag im November) des Jahres 2016 besuchten 2,4 Millionen Menschen (2,9 % der Gesamtbevölkerung) die katholischen Gottesdienste[108]und 0,8 Millionen (1 %) jene der evangelischen Kirche. An hohen kirchlichen Feiertagen, insbesondere zuHeiligabend, nehmen deutlich mehr Menschen an Gottesdiensten teil.[109]DasSyrische Christentumist durch den kontinuierlichen Zuzug vonAssyrernausMesopotamienmit ca. 130.000 Mitgliedern eine stetig wachsende christliche Konfession in Deutschland.[110]Davon gehören rund 100.000 Assyrer derSyrisch-Orthodoxen Kirche von Antiochienan.[111] Ende 2024 lebten in Deutschland etwa 3,3 MillionenMuslime. Ihr Anteil an der Gesamtbevölkerung betrug 3,9 %.[106]Etwa die Hälfte hat einen türkischen Migrationshintergrund. Als Dachverband der vielen islamischen Organisationen und Ansprechpartner für Außenstehende wurde derKoordinierungsrat der Muslime in Deutschlandgegründet. DieDeutsche Buddhistische Uniongeht von etwa 270.000Buddhisten in Deutschlandaus. Die Hälfte davon sind eingewanderteAsiaten. Dies entspricht 0,3 Prozent der Bevölkerung.[112] Etwa 200.000Judenleben in Deutschland,[112]dies entspricht 0,25 Prozent der Bevölkerung (Stand 2022). Davon sind etwa die Hälfte injüdischen Gemeindenorganisiert. Seit den 1990er-Jahren verzeichnen diese einen starken Zuwachs durch Zuwanderer aus den ehemaligenOstblockstaaten, vor allem aus derUkraineundRussland. Funde der PrimatenartDanuviusauf dem Gebiet des heutigen Deutschland sind über 11 Millionen Jahre alt und stellen wohl einen der ersten aufrecht gehenden Vorfahren des Menschen dar. Der ältestefossileBeleg für die Anwesenheit derGattungHomoauf deutschem Gebiet – derUnterkiefer von Mauer– ist rund 600.000 Jahre alt, er wurde nach seinem Fundort in der Nähe vonHeidelbergalsHomo heidelbergensisbenannt.[113]Die mindestens 300.000 Jahre altenSchöninger Speeresind die ältesten vollständig erhaltenenJagdwaffender Menschheit und haben das Bild der kulturellen und sozialen Entwicklung des frühen Menschen revolutioniert, ebenso wie dieFundstätte Bilzingslebenmit einem der ältesten Bauwerke und dem vielleicht ältesten Kunstwerk der Welt.[114] Zu denNeandertalern, nach einem Fundort imNeandertal, östlich vonDüsseldorf, benannt, gesellte sich vor mindestens 47.500 Jahren der aus Afrika zugewanderteHomo sapiens, der anatomisch moderne Mensch – zuerst nachweisbar in Thüringen.[115]Die Neandertaler verschwanden zwar, doch ließ sich belegen, dass beide zumindest einige gemeinsame Nachkommen hatten. Diejungpaläolithische KleinkunstderSchwäbischen Albist die älteste bekannte Kunst der Menschheit.[116] Aus dem Nahen Osten kommendejungsteinzeitlicheBauern, die mit ihrem Vieh und ihren Kulturpflanzen über Anatolien und den Balkan zuwanderten (Linearbandkeramiker), verdrängten ab etwa 5700/5600 v. Chr. dieJäger und SammlerderMittelsteinzeitaus der Südhälfte Deutschlands. Erst um 4000 v. Chr. wurden die aneignenden Kulturen der Jäger, Sammler und Fischer auch in Norddeutschland von bäuerlichen, nun durchgehend sesshaften Kulturen abgelöst; als letzte Kultur der Jäger in Norddeutschland gilt dieErtebølle-Kultur. Mit über 1000 Jahren Verzögerung begann auf deutschem Gebiet dieBronzezeitum 2200 v. Chr. Zu ihren bedeutendsten Funden zählt dieHimmelsscheibe von Nebra. Mit Beginn derHallstattzeit(1200–1000 v. Chr.) waren Süd- und Mitteldeutschland vonKeltenbesiedelt, als bedeutendstes Metall begann sich das Eisen durchzusetzen. Um 600 v. Chr. kam es inNorddeutschlandzur Herausbildung derJastorf-Kultur, die alsgermanischeKultur angesehen wird. Der Begriff „Germanen“ (lateinischGermani) wurde im 1. Jahrhundert v. Chr. von antiken Autoren erstmals erwähnt. Es handelt sich hierbei um einenethnographischen, wenig präzisen Sammelbegriff, der aus methodischen Gründen nicht als Bezeichnung für ein einheitliches Volk missverstanden werden darf.[117] Von 58 v. Chr. bis etwa 455 n. Chr. gehörten die Gebiete links desRheinsund südlich derDonauzumRömischen Reich, von etwa 80 bis 260 n. Chr. auch ein TeilHessenssowie der größte Teil des heutigen Baden-Württemberg südlich desLimes. Diese römischen Gebiete verteilten sich auf die ProvinzenGallia Belgica,Germania superior,Germania inferior,RaetiaundNoricum. Dort gründeten die Römer Legionslager, eine Reihe von Städten wieTrier,Köln,AugsburgundMainz– dieältesten Städte Deutschlands. Verbündete germanische Stämme sicherten dieseProvinzen, zudem wurden Siedler aus anderen Reichsteilen hier sesshaft. Der außerhalb der römischen ProvinzenGermania InferiorundGermania Superiorliegende Teil des Siedlungsgebiets der Germanen wurde von den Römern in derfrühen und hohen Kaiserzeitund in der Spätantike alsGermania magnabezeichnet.[118] Versuche, den Einflussbereich weiter in dieses germanische Gebiet auszudehnen, scheiterten mit derVarusschlachtim Jahr 9 n. Chr. Die Bemühungen der Römer zur Errichtung von Provinzen bis zurElbeendeten schließlich.Tacitus’ frühestens im Jahr 98 entstandene SchriftGermaniaist die älteste Beschreibung der germanischen Stämme. Nach dem Einfall derHunnenum 375 setzte die Völkerwanderung ein, gleichzeitig bildeten sich im Übergang von der Spätantike zumFrühmittelaltermehrere Großstämme heraus, nämlich die derFranken,Alamannen,Sachsen,BayernundThüringer. Wichtig in der neueren Forschung ist in diesem Kontext der komplexe Vorgang derEthnogeneseder unterschiedlichengentes(Stämme). Die Entstehung von ethnischen Identitäten (Ethnizität) in der Spätantike bzw. dem beginnenden Frühmittelalter im Zusammenhang mit der sogenannten Völkerwanderung[119]wird heute nicht mehr als biologische Kategorie verstanden. Identitäten entstehen vielmehr in einem wechselhaften sozialen Prozess, bei dem mehrere Faktoren eine Rolle spielen.[120] Das Ziel der in das Imperium eingedrungenen Gruppen war vor allem Teilhabe am Wohlstand des Imperiums, dessen Strukturen und Kultur sie keineswegs zerstören wollten. Doch die folgenden militärischen Konflikte und innerrömische Machtkämpfe führten zu einem politischen Erosionsprozess des Westreichs.[121]Im Zuge des UntergangsWestroms(der letzte Kaiser in Italien wurde 476 abgesetzt) kam es zur Bildung germanisch-romanischer Nachfolgereiche auf dem Boden des westlichen Imperiums.[122]DasOströmische Reich(„Byzanz“) bestand hingegen bis 1453 fort und unterhielt weiterhin Kontakte in den Westen. In die weitgehend entvölkerten Gebiete des heutigenOstdeutschlandswanderten im 7. Jahrhundertslawische Stämmeein. Erst im Zuge derhochmittelalterlichenOstsiedlungwurden sie assimiliert. West- und Mitteleuropa wurde von dem am Ende des 5. Jahrhunderts entstandenenFrankenreichdominiert, das heutige Norddeutschland von den Sachsen und Slawen. Alle heute zu Deutschland gehörigen Gebiete des Frankenreichs lagen im östlichen TeilreichAustrasien. Unter denMerowingernkam es allerdings wiederholt zu dynastischen Konflikten. Mitte des 8. Jahrhunderts trat im FrankenreichPippin der Jüngereaus der Dynastie derKarolingerdie Königsnachfolge der bis dahin herrschenden Merowinger an. Nach der Unterwerfung und Zwangsmissionierung der Sachsen und Eroberungen in Italien, Nordspanien und im östlichen Grenzraum unterKarl dem Großenwurde dasVielvölkerreichneu organisiert. Kirchenorganisation und Kulturförderung knüpften partiell an römische Traditionen an (Karolingische Renaissance). Zu Weihnachten 800 ließ sich Karl vom Papst inRomzum Kaiser krönen und erhob damit Anspruch auf die Nachfolge des Römischen Reiches (Translatio imperii), was zur Konkurrenz mit den byzantinischen Kaisern führte (Zweikaiserproblem). Nach Karls Tod 814 kam es zu Kämpfen unter seinen Nachkommen, die 843 imVertrag von Verdunzur Dreiteilung des Reiches in dasOstfrankenreichunter„Ludwig dem Deutschen“, dasWestfrankenreichundLotharingienführten.[123] Im ostfränkischen Reich bildeten sich um 900 fünf große Herzogtümer heraus, nämlich die StammesherzogtümerSachsen,Baiern,Schwaben,FrankenundLothringen. Im 10. Jahrhundert starb die karolingische Dynastie in West- wie auch in Ostfranken aus, beide Reichsteile blieben politisch fortan getrennt. DieSchlacht auf dem Lechfeldbeendete 955 jahrzehntelangeUngarneinfälle, führte zu einem Prestigegewinn KönigOttos, der 962 in Rom zum Kaiser gekrönt wurde, und zur Zuordnung desErzengels MichaelalsSchutzpatronder Deutschen. Die Dynastie derOttonenwar für die Ausformung desOstfrankenreichswesentlich, sie gilt aber nicht mehr als Beginn der eigentlichen „deutschen“ Reichsgeschichte. Der damit verbundene Prozess zog sich vielmehr mindestens bis ins 11. Jahrhundert hin.[124]Der Begriffregnum Teutonicorum(„Königreich der Deutschen“) findet sich erstmals zu Beginn des 11. Jahrhunderts in denQuellen, er war aber nie Titel des Reiches(Imperium),sondern diente den Päpsten zur Relativierung des Herrschaftsanspruchs derrömisch-deutschen Könige.[125] Im Jahr 951 nahmOtto I.dielangobardischeKönigswürde an. Dies verband dasRegnum TeutonicummitReichsitalien. 962 wurde Otto zum Kaiser gekrönt, damit vereinte er die römisch-deutsche Königswürde mit dem Anspruch auf daswestliche „römische“ Kaisertum(Reichsidee). Diesesrömisch-deutsche Reichnahm unter den Ottonen einehegemonialeStellung im westlichen Europa ein. 1024 traten dieSalierdie Königsnachfolge an, die bis zum Ende des Mittelalters stets an eine Wahl durch verschiedeneGroßedes Reichs gekoppelt war. Die Art und Weise, wie weltliche und geistliche Macht miteinander verzahnt wurden, nennt man heute teilweise „Reichskirchensystem“. Die Frage, wer Bischöfe einsetzen durfte, führte zumInvestiturstreitmit demreformiertenPapsttum, zumGang nach Canossa1077 und zur Zwischenlösung desWormser Konkordats1122. Einen Höhepunkt erreichte die Auseinandersetzung zwischen Kaiser und Papst instaufischerZeit, insbesondere unterFriedrich II.Er gab im deutschen Reichsteil vieleRegalienauf. Mit seinem Tod 1250 brach die staufische Königsherrschaft zusammen; das folgendeInterregnumvergrößerte die Macht der Fürsten. Das Kaisertum bestand als politischer Ordnungsfaktor fort, verlor aber auf europäischer Ebene zunehmend an Einflussmöglichkeiten. In Form derTerritorialstaatenverselbstständigten sich zahlreiche Feudalherrschaften zu Lasten der königlich-kaiserlichen Macht, die aber nie stark ausgeprägt gewesen und deshalb aufkonsensuale Herrschaftmit denGroßendes Reiches angewiesen war. KaiserHeinrich VI.war Ende des 12. Jahrhunderts mit dem Versuch gescheitert, durch denErbreichsplandieErbmonarchieeinzuführen. Während sich das Westfrankenreich zum französischen Zentralstaat entwickelte, blieb das ostfränkische oder römisch-deutsche Reich durchLandesherrenund das Recht derKönigswahlgeprägt. In der Mitte des 13. Jahrhunderts setzte sich im Heiligen Römischen Reich – die BezeichnungSacrum Imperium (Heiliges Reich)wurde bereits 1157 gebraucht,Sacrum Imperium Romanum (Heiliges Römisches Reich)ist erstmals urkundlich gesichert 1184 belegt (die ältere Forschung ging von 1254 aus)[126]– die Auffassung durch, dass einem Kollegium vonKurfürstendieWahl des Königszustehe, was durch dieGoldene Bulle1356 verbindlich festgeschrieben wurde. Bis zu seinem Ende 1806 blieb das Reich somit formal eine Wahlmonarchie. Obwohl die Kaiser wiederholt versuchten, ihre Position zu stärken, blieb das Reich einsupranationalerVerband vieler verschieden großerTerritoriensowieReichsstädte. Dasspätmittelalterliche14. und 15. Jahrhundert war vom Wahlkönigtum geprägt: Drei große Familien – dieHabsburger, dieLuxemburgerund dieWittelsbacher– verfügten über den größten Einfluss im Reich und über die größteHausmacht. Als bedeutendster König giltKarl IV., der eine geschickteHausmachtpolitikbetrieb. Trotz Krisen wie derPest(Schwarzer Tod), derAgrarkriseund desabendländischen Schismasflorierten die Städte und der Handel; es begann der Übergang in dieRenaissance. Im Reich traten die Habsburger das Erbe der Luxemburger an, die 1437 in männlicher Linie ausstarben, und stellten bis zum Ende des Reichs fast kontinuierlich die römisch-deutschen Herrscher. Durch geschickte Politik sicherten sich die Habsburger zusätzliche Territorien im Reich und sogar die spanische Königskrone: Habsburg stieg damit zur europäischen Großmacht auf. An der Wende zum 16. Jahrhundert betrieb KaiserMaximilian I.eine umfassendeReichsreform, die denReichstag, die Gerichtsbarkeit (Schaffung vonReichskammergerichtundReichshofrat) und die innere Ordnung durch denEwigen Landfriedenund die Einteilung inReichskreisestärkte. Durch das Scheitern desGemeinen Pfennigsund desReichsregimentsblieb die Reform aber unvollständig. Ab 1519 verfolgte KaiserKarl V., zugleichspanischer Königmit überseeischemKolonialreich, das Konzept einerUniversalmonarchie. Seine Vorherrschaft in Europa begründete den jahrhundertelangenhabsburgisch-französischen Gegensatz. Im Jahr 1517 stießMartin Lutherdurch Forderungen nach innerkirchlichen und theologischen Reformen und eine anti-päpstliche Haltung dieReformationan, was zur Herausbildung „protestantischer“Konfessionenführte. Der Katholizismus reagierte mit derGegenreformation, doch behaupteten sich die neuen evangelischen Kirchen in weiten Teilen des Reiches. DerAugsburger Religionsfrieden1555 schaffte einen vorläufigen Ausgleich: Die Landesherren durften bestimmen, welche Konfession für ihre Untertanen galt(cuius regio, eius religio). Konfessionelle und machtpolitische Gegensätze lösten denDreißigjährigen Krieg(1618–1648) mit vielen Todesopfern und verheerten Landschaften aus, beendet durch denWestfälischen Frieden, der den Einfluss der Territorien gegenüber dem Kaiser stärkte und festschrieb (sieheJüngster Reichsabschied). DieReichsfürstendurften nunmehr eigene Truppen aufstellen und konnten mit auswärtigen MächtenVerträgeabschließen. Das Reich wurde dadurch de facto zu einemStaatenbund,de jureblieb es ein monarchisch geführtes und ständisch geprägtes Herrschaftsgebilde. Ab 1663 wandelte sich der Reichstag zu einem permanenten Gesandtenkongress (Immerwährender Reichstag), der inRegensburgtagte. Im Rahmen seinerReunionspolitikführteLudwig XIV.ab 1688 denPfälzischen Erbfolgekrieg, der 1697 mit demFrieden von Rijswijkendete. Frankreich wirkte als Vorbild desAbsolutismus, der im Reich nicht die königliche Zentralgewalt, sondern einzelne Fürstentümer zu bürokratisch organisierten Staaten werden ließ. Manche Herrscher, insbesondereFriedrich II. von Preußen, öffneten sich dem philosophischen Zeitgeist und führten Reformen durch (Aufgeklärter Absolutismus). Der politische AufstiegPreußensim 18. Jahrhundert führte zumDualismusmit dem Hause Habsburg. Nach derFranzösischen Revolutionbesetzten deren Truppen daslinke Rheinufer. Nach dem SiegNapoleon Bonapartesim ZweitenKoalitionskriegkam es 1803 zumReichsdeputationshauptschluss. 1806 legte der letzte KaiserFranz II.die Krone nieder, womit das Reich erlosch. Unter Napoleons Einfluss war zwischen 1801 und 1806 die Anzahl der Länder im Gebiet des „Alten Reiches“ von rund 300 auf etwa 60 verringert worden. Frankreich annektierte den deutschen Westen und Nordwesten und schuf deutscheVasallenstaaten, deren Throne Napoleon mit Familienangehörigen besetzte (Großherzogtum Berg,Königreich Westphalen,Großherzogtum Frankfurt). Einige deutsche Staaten baute Napoleon zu Bündnispartnern auf, vor allem das 1805 imFrieden von PressburgneugeschaffeneKönigreich Bayern,WürttembergundBaden, indem er sie um die Gebiete dersäkularisiertenundmediatisiertenKleinstaaten erweiterte und in dem mit Frankreich verbündetenRheinbundvereinigte. Dieser folgte mit den von Napoleon besiegten GegnernPreußenundÖsterreichdem dadurch dreigeteilten, als Machtfaktor ausgeschalteten Heiligen Römischen Reich nach. Die „Franzosenzeit“ brachte den Rheinbundstaaten erhebliche Modernisierungsanstöße, unter anderem bürgerliche Freiheiten, durch die Einführung desZivilrechtsbuchsCode civil. Auch in Preußen wurden ab 1806 tiefgreifendeReformenunternommen, um ausUntertanenStaatsbürger(vgl.Citoyen) und den Staat wieder handlungs- und wehrfähig zu machen. Ab 1809 regte sich Widerstand gegen französische Besatzung und Herrschaft; diverse Aufstände, etwa vonAndreas HoferinTirolundFerdinand von Schillin Preußen, wurden zunächst niedergeschlagen. Nach Napoleons Niederlage imRusslandfeldzug 1812begannen Preußen und Österreich im Bündnis mit demRussischen ReichdieBefreiungskriege(1813–1815), die das deutscheNationalgefühlstärkten, zunächst unter protestantischen Akademikern, etwa imLützower Freikorps, das auch als Ursprung der FarbenSchwarz-Rot-Goldgilt.[127]Die meisten Rheinbundstaaten schlossen sich den Verbündeten an, die nach dem Sieg bei derLeipziger Völkerschlacht1813 Napoleon bis 1815 endgültig besiegten. AnschließendrestauriertederWiener Kongress(1814–1815) weitgehend die monarchische Herrschaft. ImDeutschen Bund, einem von Österreich und Preußen dominiertenStaatenbund, organisierten sich 38 Staaten (→Drittes Deutschland) mit dem FrankfurterBundestagals Entscheidungsgremium. 1833/1834 wurde derDeutsche Zollvereinunter preußischer Vormacht geschaffen. ImVormärzunterdrückte die alte Herrschaftselite das wirtschaftlich erstarkendeBürgertum(Demagogenverfolgung), das weiter politische Teilhabe und die Bildung eines Nationalstaats forderte, so 1817 beim studentischenWartburgfestund 1832 beimHambacher Festmit dem Hissen von Schwarz-Rot-Gold, den späteren Nationalfarben. Mit der bürgerlichenMärzrevolution 1848mussten viele konservative Politiker abtreten, unter ihnen der epochenprägende österreichische StaatskanzlerFürst Metternich. Unter dem Revolutionsdruck in Berlin akzeptierte der Deutsche Bundestag die Wahl derFrankfurter Nationalversammlung. Sie richtete eine Regierung ein und erließ diePaulskirchenverfassung, die einen föderativen Nationalstaat als „Deutsches Reich“ mitkonstitutioneller Monarchieund das Ausscheiden Österreichs aus Deutschland beinhaltete.[128] Doch der preußische KönigFriedrich Wilhelm IV.lehnte die ihmangetragene Kaiserkroneab. Nach Niederschlagung desMaiaufstandsendete die Revolution am 23. Juli 1849 mit der Einnahme derFestung Rastattdurch preußische Truppen. Das Scheitern derdemokratischen Bewegungführte zu Flucht und Auswanderung derForty-Eightersund zu einerReaktionsärain den deutschen Staaten. Anfang der 1860er-Jahre brach der Konflikt Preußens mit Österreich um die Vormacht im Deutschen Bund auf (deutscher Dualismus), der in Preußens Sieg imDeutschen Krieg1866 endete. Der Deutsche Bundwurde aufgelöst,Preußen annektierteetliche Gebiete nord- und mitteldeutscher Kriegsgegner. 1866 wurde unter Vorherrschaft Preußens derNorddeutsche Bundzunächst alsMilitärbündnisgegründet. Seine Verfassung von 1867 machte ihn zum souveränenBundesstaatund bildete die Grundlage für diekleindeutsche Lösung.[129] ImDeutsch-Französischen Kriegtraten die süddeutschen Staaten dem Norddeutschen Bund bei (1. Januar 1871). Dieser Bund wurde so zum deutschenNationalstaatohne Österreich und ersten Bundesstaat in Deutschland.[130][129]Am 18. Januar 1871 nahm der preußische KönigWilhelm I.in Versailles denKaisertitelan, den er mit der neuen Verfassung erhalten hatte. Dies wurde später alsReichsgründungstaggefeiert. Otto von Bismarck, seit1862 preußischer Ministerpräsident, hatte die Reichsgründung betrieben und wurde ersterReichskanzler. DieBismarcksche Reichsverfassungstützte die Macht der konstitutionellen Monarchie, war aber auch auf Modernisierung ausgelegt und ambivalent; Gesetze zur Schule und Zivilehe waren teils liberal. Für denReichstaggalt einallgemeines Wahlrecht(für Männer). Gegen die katholische Kirche führte Bismarck denKulturkampf, gegen dieSozialdemokratieerließ er ab 1878 dieSozialistengesetzeund versuchte, die Arbeiter durch eineSozialgesetzgebungan den Staat zu binden. DieHochindustrialisierung in Deutschlandsorgte für Wirtschafts- und Bevölkerungswachstum,Landfluchtund eine breite Steigerung des Lebensstandards; Deutschland stieg zur größten Volkswirtschaft Europas auf. DieBündnispolitik Otto von Bismarckszielte auf die Isolierung Frankreichs mit Deutschland als halbhegemonialer Macht in der Mitte Europas. Nachdem deutsche Kaufleute und Vereine private Kolonialpolitik betrieben hatten, eignete sich das Reich 1884 afrikanische Gebiete an. Diesedeutschen Kolonienwurden als „Schutzgebiete“ bezeichnet. Neben Kolonialbegeisterung gab es allerdings auch Skepsis und Ablehnung, zeitweise sogar von Bismarck. Die Gebiete wurden ausgebeutet; einige deutsche Kolonialherren begingen Verbrechen an den Einheimischen, zum Beispiel imVölkermord an den Herero und Nama(1904–1908). Im „Dreikaiserjahr“ 1888 wurdeWilhelm II.Deutscher Kaiser und begründete das Zeitalter des militärisch geprägtenWilhelminismus. Er forderte für das Deutsche Reich die Anerkennung der bisherigen Großmächte („Platz an der Sonne“) und bemühte sich um neue Kolonien undFlottenaufbauimImperialismus. Großbritannien schloss allerdings in einem neuen Bündnissystem (Triple Entente) statt Frankreich nun Deutschland aus. DieJulikrisevon 1914 mündete in denErsten Weltkrieg, einen verlustreichenMehrfrontenkrieg; mehr als zwei Millionen deutsche Soldaten starben, rund 800.000 Zivilisten verhungerten. Auch in anderen Ländern führte der Krieg zu vielen Toten und politischen Umwälzungen. Mit derNovemberrevolutionund derAusrufungderRepublikam 9. November 1918 endete dasDeutsche Kaiserreich, das mit seiner Kapitulation die Niederlage im Ersten Weltkrieg einräumte. Nach der Wahl der verfassunggebendenNationalversammlung– bei der erstmals Frauen aktiv und passivwahlberechtigtwaren – trat dieWeimarer Verfassungam 14. August 1919 in Kraft. ImFriedensvertrag von Versailleswurden erhebliche Gebietsabtretungen, dieAlliierte RheinlandbesetzungundReparationenauf Grundlage einer festgeschriebenen deutschenAlleinschuld am Kriegbestimmt. Diese Ausgangslage belastete das politische Klima; Rechtsextreme verbreiteten dieDolchstoßlegendegegen die „Novemberverbrecher“, was zupolitischen MordenundPutschversuchenführte (Kapp-Putsch1920 undHitlerputsch1923). AuchkommunistischeAufstände wie derRuhraufstand1920, dieMärzkämpfe in Mitteldeutschland1921 und derHamburger Aufstand1923 sorgten für Instabilität. Unzureichende Reparationsleistungen nahmen Belgien und Frankreich zum Anlass derRuhrbesetzungvon 1923 bis 1925. In den kurzen „goldenen Zwanzigern“ blühte die Kultur und ab 1924 auch dieKonjunktur.Berlinwar mit über vier Millionen Einwohnern die drittgrößte und eine der dynamischsten Städte der Welt. DieProsperitätendete 1929 mit derWeltwirtschaftskrise, auf deren Höhepunkt 1932 es in Deutschland mehr als sechs MillionenArbeitslosegab, die größtenteils in Elend lebten. Radikale Parteien fanden starken Zulauf, sodass es für die gemäßigten Parteien zunehmend schwieriger wurde, stabile Regierungen zu bilden. Nach dem sehr deutlichen Wahlsieg der Nationalsozialisten bei derReichstagswahl 1930verfügten die in rascher Folge wechselnden Reichskanzler über keine parlamentarische Mehrheit mehr; ihrePräsidialkabinettewaren vomReichspräsidentenPaul von Hindenburgund dessenNotverordnungenabhängig. DieDeflationspolitikdes ReichskanzlersHeinrich Brüningverschärfte die wirtschaftliche Krise. Dessen NachfolgerFranz von Papen(Juni–November 1932) unterstellte die demokratische Regierung Preußens einem Reichskommissar (Preußenschlag) und ließ Neuwahlen abhalten, bei denen die Nationalsozialisten noch stärker wurden. ReichskanzlerKurt von Schleicherversuchte, eineMachtübernahmeAdolf Hitlersdurch eine „Querfront“ vonGewerkschaftenund Teilen der Nationalsozialisten zu verhindern; von Papen aber überredete den widerwilligen Hindenburg, Hitler am 30. Januar 1933 zum Reichskanzler zu ernennen. Am 27. Februar kam es zum – bis heute unaufgeklärten –Reichstagsbrand, den Hitler zur „Reichstagsbrandverordnung“ nutzte, mit der auf unbestimmte Zeit dieGrundrechteaußer Kraft gesetzt wurden. Die folgenden Massenverhaftungen politischer Gegner, insbesondere von Kommunisten und Sozialdemokraten, prägten dieReichstagswahl am 5. März 1933, bei der die NSDAP die absolute Mehrheit knapp verfehlte und mit der reaktionärenDNVPweiterregierte. Die endgültige Machtübernahme erfolgte kurz darauf, als der Reichstag mit den Stimmen der bürgerlichen Parteien, allein gegen die Stimmen derSPD, dasErmächtigungsgesetz vom 24. März 1933verabschiedete und damit Hitlers Regierung auch dieGesetzgebungüberließ. DieNSDAPerrichtete im Deutschen Reich innerhalb einiger Monate einentotalitärenEinparteienstaatunter Führung Adolf Hitlers und derGleichschaltungder Institutionen. Missliebige Personen und politische Gegner, insbesondere Kommunisten, Sozialdemokraten und Gewerkschafter, wurden aus allen Behörden entfernt,erste Konzentrationslager, ab 1935 unter Aufsicht derInspektion der Konzentrationslager, wurden errichtet,Bücher verbrannt[131]und missliebige Kunst als „entartet“ diffamiert.NS-Propagandadurchdrang auch das Privatleben; bereits auf Kinder wurde Druck ausgeübt, den Parteiorganisationen beizutreten. Im Oktober 1933 verkündete Hitler den Austritt Deutschlands aus demVölkerbund. Er sicherte seine Herrschaft im Inneren, indem er während derRöhm-Mordeam 30. Juni 1934 innerparteiliche Gegner und ehemalige Weggefährten ermorden ließ, wobei die SA zugunsten der Hitler bedingungslos ergebenenSSentmachtet wurde. Die Generalität derReichswehrlegte auf ihn persönlich denFührereidab. DieGestapowurde als politische Polizei zur Bekämpfung der politischen und ideologischen Gegner eingesetzt. Von Beginn an hatte Hitler zwei Ziele, einen Angriffs- undVernichtungskriegzur Schaffung von „Lebensraum im Osten“ und dieVerfolgung der Juden, die mit Diskriminierung, Demütigung und Ausgrenzung begann und als „Endlösung der Judenfrage“ imHolocaustendete. 1934 begann dieAufrüstung der Wehrmacht. Eine expansive Geldpolitik und Schuldenwirtschaft waren auf baldige Kriegsführung ausgerichtet. Mit demReinhardt-Programmwurde dieArbeitslosigkeitgesenkt; dies wurde von der Bevölkerung als Einlösung wirtschaftlicher Versprechen begrüßt. Diedeutschen Judenwurden immer schlechter gestellt; dieNürnberger Gesetze1935 bestraften Beziehungen zwischen „Ariern“ und Juden als „Rassenschande“ schwer. Juden verloren alle öffentlichen Ämter, wurden willkürlich verfolgt, bestohlen und erpresst und schließlich mit einem völligen Berufsverbot belegt, jüdische Vermögenarisiert. Immer häufiger wurden auch Juden in Konzentrationslager eingewiesen. Viele fassten den Entschluss zurEmigration, die meisten aber blieben in Deutschland. Die rassistische NS-Ideologie zur Schaffung einer „gesunden“ „Volksgemeinschaft“ (vgl.Herrenrasse) richtete sich gegen zwei weitere Gruppen,Romaund Slawen als „Untermenschen“. Nicht als „fremdrassig“, aber als die„Gesundheit“ des „Volkskörpers“bedrohend angesehen, wurden auchHomosexuelle,Behinderteund „Asoziale“ vom Regime drangsaliert und ermordet. Zugleich feierte das RegimePropaganda­erfolge; 1936 verbesserten dieOlympischen Spieledas Ansehen im Ausland, das entmilitarisierteRheinlandwurdebesetzt. Die Expansion begann mit dem erzwungenenAnschluss Österreichsim März 1938, worauf Deutschland alsGroßdeutsches Reichbezeichnet wurde. DasMünchner Abkommenim Oktober 1938 besiegelte die Annexion desSudetenlandes. Mit derZerschlagungderTschecho-Slowakischen Republikim März 1939 brach Hitler sein Versprechen, das Sudetenland sei seine letzte territoriale Forderung. Damit wurde klar, dass dieAppeasement-Politikder Westmächte gegenüber Deutschland ein Fehler gewesen war. Nachdem das Deutsche Reich am 1. September 1939 denÜberfall auf Polenbegonnen hatte, erklärtenGroßbritannien,Kanada,Australien,Indien,Neuseeland,SüdafrikaundFrankreichDeutschland den Krieg. DerZweite Weltkriegforderte in sechs Jahren etwa 55 bis 60 Millionen Tote. Deutschland gelangen zunächst einige als „Blitzkrieg“ bezeichnete militärische Erfolge.Polenwurde imNichtangriffspaktzwischen Hitler undStalinaufgeteilt, dieWehrmachtwarf anschließend ihre Armeen nach Westen, überfiel in der „Weserübung“ Dänemark und Norwegen und im „Westfeldzug“ die neutralen Staaten Luxemburg, Belgien und die Niederlande und besetzte 1940 innerhalb von sechs Wochen große Teile Frankreichs. Hitlers Popularität erreichte ihren Höhepunkt. Noch vor Kriegsbeginn verschärfte das Dritte Reich die Judenverfolgung. Im Laufe des Jahres 1938 wurden deutsche Juden aus dem Wirtschaftsleben ausgeschaltet und mittels verschiedener Verordnungen um ihr Vermögen gebracht.[132]Am 9. November 1938 wurden im Zuge derNovemberpogromejüdische Geschäfte und Synagogen verwüstet. 1941 wurde die Ausreise von Juden verboten. Seither mussten sie den „Judenstern“ tragen, und im gesamten deutschen Machtbereich begann ihresystematische Ermordung. Viele starben wegen unzureichender Versorgung und Seuchen bei derZwangsarbeit. Die mit der Ausführung vor allem beauftragte SS errichtete auf ehemals polnischem oder sowjetischem GebietVernichtungslager, in denen die meisten Opfer, inViehwaggonsherangebracht, sofort vergast wurden (sieheAktion Reinhardt). Allein in denGaskammern und Krematorien der Konzentrationslager Auschwitzwurden über eine Million Menschen ermordet. Insgesamt beläuft sich die Zahl der ermordeten Juden auf mindestens sechs Millionen.[133] Mit demUnternehmen Barbarossabegann am 22. Juni 1941 der (Russlandfeldzug 1941–1945). Das deutsche Heer marschierte auf diesowjetische Hauptstadtvor und wurde in derSchlacht um Moskauim Dezember 1941 gestoppt. Nachdem das kriegsverbündeteKaiserreich Japan(→Achsenmächte) im selben Monat die amerikanische Marine imAngriff auf Pearl Harborüberfallen hatte, erklärte Deutschland auch denVereinigten Staaten von Amerikaden Krieg. Mangelnde Ressourcen und die Übermacht des Gegners ließen bald die Kriegswende eintreten, die sich in der verlorenenSchlacht von Stalingradmit der völligen Aufreibung derdeutschen 6. Armeemanifestierte. Je unvermeidlicher die Niederlage wurde, desto härter wurde die Politik nach innen geführt. In seinerSportpalastredevom 18. Februar 1943 proklamierteJoseph Goebbelsden „totalen Krieg“, während die deutschen Armeen an fast allen Fronten zurückwichen und zahlreiche deutsche Städte durch denBombenkriegzerstört wurden. Zwei Tage vor der Eroberung der deutschen Hauptstadt durch dieRote Armeein derSchlacht um Berlinnahm sich Hitler am 30. April 1945 imFührerbunkerdas Leben. Diebedingungslose Kapitulation der Wehrmachtfolgte am 8. Mai. Am 23. Mai 1945 verhaftete die britische Armee dieletzte ReichsregierungunterKarl DönitzimSonderbereich MürwikbeiFlensburg. Überlebende politische, militärische und wirtschaftliche Hauptverantwortliche wurden ab November 1945 wegen ihrer individuellen Verantwortung anKriegsverbrechenundVerbrechen gegen die Menschlichkeitin denNürnberger Prozessenangeklagt. Deutschland wurde in denGrenzen vom 31. Dezember 1937aufgeteilt; am 5. Juni 1945 legten dievier Siegermächte– USA, UdSSR, Großbritannien undschließlich auch Frankreich– Besatzungszonen fest und übten sodann westlich derOder-Neiße-LiniedieHoheitsgewaltin ihrer jeweiligen Zone und gemeinsam mittels einerAlliierten KommandanturüberGroß-Berlinaus. Diedeutschen Ostgebiete, ein Viertel der Reichsfläche, bewohnt von einem Fünftel der Reichsbevölkerung, waren bereits vor Kriegsende nach ihrer Eroberung durch dieRote Armeeder Verwaltung derVolksrepublik Polenund im nördlichen Ostpreußen jener derSowjetunionunterstellt worden (Oblast Kaliningrad). Auf Betreiben Stalins billigten die Westmächte dies imPotsdamer Abkommenwie auch die begonneneVertreibung der Deutschen aus Mittel- und Osteuropa. DieRepublik Österreichwurde in den Grenzen von 1938 wiederhergestellt und ebenfalls in vierBesatzungszonen aufgeteilt. 1946/1947 wurde dasSaarlandaus dem Besatzungsgebiet ausgegliedert und unter direkte französische Verwaltung gestellt. Die Vier Mächte bemühten sich anfangs noch um eine gemeinsame Besatzungspolitik. Einig war man sich über eineDemilitarisierung,EntnazifizierungundZerschlagung der Kartelle; schon bei der Frage, was unterDemokratiezu verstehen sei, zeigten sich Differenzen zwischen der Sowjetunion und den Westmächten, die sich im beginnendenKalten Kriegverschärften. In den drei Westzonen stellten die Westalliierten die für den Wiederaufbau bedeutendeMontanindustrieunter dasRuhrstatut. Mit derWährungsreformim Juni 1948 und der zeitgleichen Aufhebung der Preisbindung und Bewirtschaftung setzte derWirtschaftsdirektor der WestzonenLudwig Erhardeine vor allem psychologisch bedeutsame wirtschaftliche Zäsur; mit der wenige Tage später folgenden Währungsreform in dersowjetisch besetzten Zone Deutschlandsund derBerlin-Blockadedurch die UdSSR vertiefte sich die Trennung zwischen Ost und West. Die Bundesrepublik Deutschland wurde am 23. Mai 1949 in den drei westlichen Besatzungszonen gegründet und dasGrundgesetzals provisorische Verfassung in Kraft gesetzt, dessen Präambel für eine Übergangszeit einWiedervereinigungsgebotenthielt; Bonn wurde Regierungssitz. In dersowjetischen Besatzungszonewurde viereinhalb Monate später die Deutsche Demokratische Republik gegründet. Beide Teilstaaten sahen sich jeweils in Kontinuität einesgesamtdeutschenStaates underkannten den jeweils anderen nicht an.[134]Beide blieben unterKontrolle der Besatzungsmächte. Mit der Integration in die entgegengesetzten Militärbündnisse vonNATOundWarschauer Vertragerhielten sie 1955 ihre formale Unabhängigkeit (siehePariser Verträge,Souveränitätserklärung der UdSSR für die DDR). Voraussetzung dafür war, dass im Juli 1951 die drei Westmächte die formelle Beendigung des Kriegszustandes mit Deutschland beschlossen; die Sowjetunion erklärte dies erst im Januar 1955, worauf weitere Staaten im östlichen Europa folgten.[135]Den Alliierten verblieben die Verantwortung fürDeutschland als Ganzesund ihre Rechte in Berlin. Während in der DDR eine staatlich gelenktePlanwirtschaftaufgebaut wurde, entschied sich die Bundesrepublik für die sogenanntesoziale Marktwirtschaftmit geringem staatlichem Einfluss. Die sowjetischeBesatzungsmachtsorgte mit hohen Reparationsforderungen (vor allemDemontagen) für schwierige Startbedingungen auf dem Gebiet der DDR, während in der Bundesrepublik mit ausländischer Hilfe (Marshallplan) ein „Wirtschaftswunder“ einsetzte, das zu anhaltend hohen Wachstumsraten, Vollbeschäftigung und Wohlstand führte. Im Westen orientierte man sich beimNeu- und Wiederaufbau der Städtean derCharta von Athen (CIAM)von 1933, während im Osten die nach sowjetischem Vorbild entwickelten16 Grundsätze des Städtebausverbindlich wurden. Im Ergebnis folgte der Wiederaufbau in beiden deutschen Staaten dennoch dem Leitbild derautogerechten Stadt.WohnenundGewerbewurden damit häufig voneinander getrennt. Fortan wurden auch zahlreichesuburbaneSatellitenstädte(„Schlafstädte“) geplant. Diese Art der Stadtentwicklung wurde bereits früh als verfehlt erkannt.[136] DerEiserne Vorhangdurch Mitteleuropa teilte auch Deutschland; die fortgesetzteAuswanderung besonders Junger und Hochqualifizierterließ die DDR dieinnerdeutsche Grenzezunehmend abriegeln, bis sie 1961 unter dem langjährigen SED-GeneralsekretärWalter Ulbrichtdurch den Bau derBerliner Mauervollständig geschlossen wurde, was selbst familiäre Kontakte zwischen West- und Ostdeutschland stark erschwerte. Wer dieRepublikfluchttrotzdem versuchte, wurde gewaltsam aufgehalten (sieheSchießbefehl, Grenz- undMauertote). Außenpolitisch setzte der langjährige BundeskanzlerKonrad Adenauerfür dieteilsouveräneBundesrepublik dieWestintegrationund die Beteiligung am wirtschaftlichen Zusammenschluss Westeuropas durch, der mit derMontanunion1952 begann. DerÉlysée-Vertrag1963 begründete diedeutsch-französische Freundschaftals Motor dereuropäischen Integration. Die DDR wurde im September 1950 Vollmitglied im östlichenRat für gegenseitige Wirtschaftshilfe(RGW). Im Innern der DDR wurde durch die StaatsparteiSEDund durchMassenorganisationenwie dieFDJder Sozialismus verbindlich festgeschrieben; freie Wahlen gab es nicht mehr, derAufstand vom 17. Juni 1953wurde niedergeschlagen. Abweichende Meinungen wurden durchZensurund die umfassende Überwachung der GeheimpolizeiStaatssicherheitverfolgt; dagegen bildete sich Protest in einerDissidenten- und Bürgerrechtlerbewegung, die sich durch dieAusbürgerung Wolf Biermanns1976 radikalisierte. In der sich durchWesternisierungliberalisierenden Bundesrepublik verstärkten sich Forderungen nach einem gesellschaftlichen Wandel und nachVergangenheitsbewältigung, da die NS-Eliten weitgehend unbehelligt geblieben waren – insbesondere durch diewestdeutsche Studentenbewegung der 1960er-Jahre. Gegen die 1966 gebildeteGroße Koalitionmit ihrenNotstandsgesetzenentstand eineaußerparlamentarische Opposition. Diesozialliberale KoalitionunterWilly Brandtbaute ab 1969 den Sozialstaat und gesellschaftliche Freiheiten aus; die aufEntspannungmit Osteuropa zielende „Neue Ostpolitik“ brachte Brandt 1971 denFriedensnobelpreisund Kritik von konservativer Seite ein. Im Jahr 1973 wurden Bundesrepublik und DDRMitgliedstaaten der UNO. Die Planwirtschaft der DDR hatte neben zunehmenden Versorgungsproblemen (Mangelwirtschaft) mit der demographischen Entwicklung zu kämpfen, der der von 1971 bis 1989 regierendeErich Honeckerdurch massive Familienförderung begegnete. DieFrauen- und Familienpolitik der DDRgilt ebenso wie die erreichte soziale Gleichheit und Sicherheit als teilweise erfolgreich. Die 1970er-Jahre waren in der Bundesrepublik durch steigende Verschuldung und Arbeitslosigkeit nach derÖlkriseund dem Terror der linksradikalenRote Armee Fraktiongeprägt. BundeskanzlerHelmut Schmidt(SPD) verlor wegen seiner Unterstützung desNATO-Doppelbeschlusses– angegriffen von der Friedensbewegung, Teil der entstehendenNeuen Sozialen Bewegungen– den Rückhalt in seiner Partei und wurde 1982 vonHelmut Kohl(CDU) abgelöst, der 1989 die Chance zur Wiedervereinigung Deutschlands ergriff. Die Unzufriedenheit der DDR-Bevölkerung war im ständigen, durch dasWestfernsehenunterstützten Systemvergleich angewachsen. Ende der 1980er-Jahre bildete sich mit derReformpolitikMichail Gorbatschowsin der Sowjetunion auch in der DDR eine Protestbewegung, die in dermaroden DDRim Herbst 1989 durch eine Ausreisebewegung über den löchrig gewordenen Eisernen Vorgang und durchMassendemonstrationendie politische Führung unter Druck setzte („Wir sind das Volk“) und zum Rücktritt Honeckers führte. Am 9. November 1989 führte die Gewährung derReisefreiheitdurch die DDR-Führung zu einem Massenansturm und zur Öffnung der Grenzübertrittsstellen derBerliner Mauer. Kohl lenkte die Entwicklung ab seinemZehn-Punkte-ProgrammEnde November in Richtung nationaler Einheit („Wir sind ein Volk“) unter Erhaltung der militärischen und politischen Westbindung. Bei der ersten freienVolkskammerwahl vom 18. März 1990gewann das von derOst-CDUgeführte Parteienbündnis „Allianz für Deutschland“, das auf eine schnelle Wiedervereinigung setzte. Diese wurde in den nächsten Monaten imEinigungsvertragund mit den Vertretern der Alliierten im Rahmen der „Zwei-plus-Vier-Gespräche“ ausgehandelt. Diedeutsche Wiedervereinigungwurde am 3. Oktober 1990 mit dem Beitritt der DDR zur Bundesrepublik Deutschland vollzogen; dieserTag der Deutschen EinheitwurdeNationalfeiertag. Der 1991 in Kraft getreteneZwei-plus-Vier-Vertragregelte diedeutsche Frageabschließend: DieVier Mächtegaben ihre Hoheitsbefugnisse auf, bis Ende 1994 verließen ihre Truppen das Land, das wiedervereinigte Deutschland erhielt seine volle staatlicheSouveränität. Es verpflichtete sich zur Abrüstung auf maximal 370.000 Soldaten. Mit dem am 14. November 1990 in Warschau unterzeichnetendeutsch-polnischen Grenzvertragerkannte Deutschland dieOder-Neiße-Grenzean; das Territorium östlich davon wurde damit völkerrechtlich endgültig polnisch. Das wurde durch eine Politik der Aussöhnung mit den östlichen Nachbarn ergänzt, zuerst1991 mit Polen, dann1997 mit Tschechien. Außenpolitisch setzte sich die Bundesregierung unter Bundeskanzler Kohl für einevertiefte Integrationmit Bildung derEuropäischen Union, der anschließendenEU-Osterweiterungund derEuro-Einführung ein. Der Bundestag machte 1991Berlin zur Hauptstadt, in die Regierung und Parlament 1999 zogen (sieheReichstagsgebäudeundRegierungsviertel). Nach kurzem Wiedervereinigungboom waren die 1990er-Jahre von wirtschaftlicher Stagnation, Massenarbeitslosigkeit und „Reformstau“ geprägt. Insbesondere dieneuen Länderentwickelten sich nach der Einführung der Marktwirtschaft nicht so schnell wie erhofft („blühende Landschaften“). 1991 bis 1993 kam es zu einerWelle von Ausschreitungen gegen Asylbewerber. Erst in den 2000er-Jahren stabilisierten sich die neuen Länder sozial und wirtschaftlich. Bei derBundestagswahl 1998verlor Kohlsschwarz-gelbe Koalitionihre Bundestagsmehrheit, die bisherigen Oppositionsparteien SPD undBündnis 90/Die Grünenbildeten die ersterot-grüne Koalitionunter BundeskanzlerGerhard Schröder(SPD), die tiefgreifende Veränderungen in der Sozial-, Renten- und Gesundheitspolitik durchsetzte. Ökologie erhielt stärkeres Gewicht, etwa mit dem Beginn desAtomausstiegs. Zu den gesellschaftspolitischen Liberalisierungen zählten dasLebenspartnerschaftsgesetzund ein neuesStaatsbürgerschaftsrecht. Der erste Kampfeinsatz deutscher Soldaten seit dem Zweiten Weltkrieg – 1999 imKosovokrieg– markierte einen Wendepunkt der Außenpolitik. Nach denTerroranschlägen vom 11. September 2001sicherte Schröder den USA die „uneingeschränkte Solidarität“ zu; Deutschland nahm amKrieg in Afghanistanteil, aber nicht amIrakkrieg, was den „Friedenskanzler“ Schröder populär machte. Die zweite Amtszeit Schröders ab 2002 war von derAgenda 2010und damit verbunden den Arbeitsmarktreformen desHartz-Konzeptsgeprägt. Sozialleistungen für Arbeitslose wurden reduziert und an individuelle Fördermaßnahmen gekoppelt, was von Betroffenen als ungerecht empfunden wurde. Dies führte zudeutschlandweiten Protestenund indirekt zu einer vorgezogenenBundestagswahl 2005, woraufAngela Merkel(CDU) als erste Bundeskanzlerin Deutschlands an die Regierung kam. Ihregroße Koalitionwar mit dem Zusammenbruch von Banken während derWeltfinanzkriseund der folgendenGroßen Rezessionkonfrontiert. Nach deren Überwindung erlebte Deutschland einen andauernden Wirtschaftsboom und einen nachhaltigen Rückgang der Arbeitslosigkeit.Eurokrise(ab 2010) undFlüchtlingskrise in Europa ab 2015stellen seitdem die wichtigsten Herausforderungen der Politik dar, deren Bewältigung der Wirtschaftsboom wesentlich erleichtert. Beide Ereignisse führten jedoch auch zu erheblichen gesellschaftlichen Zerwürfnissen und zu einem ErstarkenEU-skeptischerund islamfeindlicher Bewegungen (Pegida,Alternative für Deutschland). Mit derLegalisierung gleichgeschlechtlicher Ehen, der standesamtlichen Einführung einesdritten Geschlechtsund der Einstellung derEinberufung zum Wehrdienstin der Bundeswehr strebte Deutschland in den 2010er-Jahren nach weitergehender Liberalisierung seiner Gesellschaft. Angela Merkel beendete die letzte ihrer vier Amtszeiten während derCOVID-19-Pandemie, auf die Deutschland mit vorübergehenden Einschränkungen des wirtschaftlichen, kulturellen und öffentlichen Lebens reagierte und ihre Bekämpfung mit nationalen Impfprogrammen, unter anderem mit dem neuartigen, in Deutschland entwickeltenmRNA-ImpfstoffTozinameran, begann. Die überwiegende Mehrheit der Deutschen trug dieMaßnahmen zur Bekämpfung der Pandemiemit. Jedoch wurden einerseits soziale und wirtschaftliche Verwerfungen innerhalb der deutschen Gesellschaft, des deutschen Gesundheitssystems sowie technologische Rückstände Deutschlands im Vergleich zu anderen westlichen Ländern durch die Pandemie offenkundig. Andererseits mobilisierten Protestbewegungen gegen die Maßnahmen zur Bekämpfung der Pandemie und sprachen dabei gezieltÄngste in der Bevölkerung in Bezug auf Impfungenan. Nach derBundestagswahl 2021wurde Merkel vonOlaf Scholz(SPD) und die bis dahin in Koalitionen regierende CDU von einerrot-grün-gelben Koalitionabgelöst. Mit ihr setzt sich diedigitale TransformationDeutschlands sowie die wegen desKlimawandelsbegonneneVerkehrs-undEnergiewendehin zu nachhaltigen Energieträgern fort. Derrussische Überfall auf die Ukraine 2022führte zu umfassendenWirtschaftssanktionen des Westens gegen Russland, an denen sich auch Deutschland beteiligte. Unter anderem stoppte Deutschland die Inbetriebnahme der GaspipelineNord Stream 2, die zusammen mit der erstenNord-Stream-Pipeline im September 2022 durch einenAnschlagteilweise zerstört wurde. Die deutsche Wirtschaft, die sich von russischem Gas abhängig gemacht hatte, musste vorübergehend eine starke Teuerung im Energiesektor hinnehmen. Deutschland unterstützte dieUkraine finanziell und mit der Lieferung von Rüstungsgüternim Wert von mehreren Milliarden Euro sowie im Rahmen derEuropean Union Military Assistance Mission Ukrainedurch die militärische Fort- und Ausbildung ukrainischer Soldaten in Deutschland. Mehr als eine Million ukrainische Kriegsflüchtlinge suchten in Deutschland Schutz. DieUkrainer in Deutschlandwurden dadurch mit insgesamt rund 1,2 Millionen Einwohnern (Stand Ende 2023) diezweitgrößte ausländische Bevölkerungsgruppe, nach denTürken in Deutschland(siehe Tabelle im AbschnittDemografie).[137]Die veränderte Sicherheitslage in Europa führte zu einem verstärkten Engagement Deutschlands an der NATO-Ostgrenze, unter anderem inLitauen, wo diePanzerbrigade 45stationiert wird. Ebenfalls plantVerteidigungsministerBoris Pistorius, dass junge Männer in Zukunft über ihre Bereitschaft zum Dienst an der Waffe Auskunft geben müssen.[138] Dieerste KoalitionausSPD,Bündnis 90/Die GrünenundFDPin der 24. deutschen Bundesregierungendete vorzeitignach 3 Jahren und 15 Tagen, als am 6. November 2024 Bundeskanzler Olaf Scholz nach einer langen Reihe von Konflikten zwischen den Koalitionspartnern dem Bundespräsidenten Frank-Walter Steinmeier vorschlug,FinanzministerChristian Lindnerzu entlassen. In der Folge traten auch die anderen Bundesminister aus der FDP, mit Ausnahme vonVolker Wissing, zurück. Scholz schmähte die Arbeit Christian Lindners öffentlich und dessen FDP wurde von den Koalitionspartnern und der öffentlichen Meinung überwiegend als Hauptverantwortlicher für den Koalitionsbruch benannt.[139][140]Planungen der FDP, das Ende der Koalition vorzeitig herbeizuführen, soll es laut einer Berichterstattung vonDie Zeitschon im September 2024 gegeben haben. Scholz fragte am 16. Dezember 2024 den Bundestag, ob dieser ihm nochvertraue, und verlor die Abstimmung wie von ihm geplant, damitvorgezogene Neuwahlenabgehalten werden können. Am 27. Dezember 2024 verkündete Bundespräsident Steinmeier die Auflösung des Bundestags und setzte den Wahltermin auf den 23. Februar 2025 fest. Bei dieser Wahl erlitten die bisherigen Regierungsparteien SPD, Grüne und FDP Stimmenverluste, in der Folge wurde erneut eineschwarz-rote Koalition gebildet. Am 6. Mai 2025 wurdeFriedrich Merzzum Bundeskanzler Deutschlands gewählt und löste somit Olaf Scholz in dieser Funktion ab. Der Anfang des Jahrzehnts war geprägt von geopolitischen Umwälzungen, insbesondere durch denAngriffskrieg Russlands in der Ukraine, sowie derzweiten Amtszeit Donald Trumps. Dieser drohte mit hohen Zöllen auf EU-Produkte[141], die deutsche Exporte gefährdeten, und stellte wiederholt die Beistandsbereitschaft der USA in einem NATO-Bündnisfall infrage[142]. Um diesen Herausforderungen zu begegnen, plante die neue Regierung, deutlich mehr Schulden aufzunehmen, als es das Grundgesetz zu dieser Zeit erlaubt hätte. Infolgedessen lockerte die Koalition aus CDU/CSU und SPD mit Unterstützung der Fraktion Bündnis 90/Die Grünen die im Grundgesetz verankerteSchuldenbremse. Dies geschah nach derBundestagswahl 2025, jedoch noch vor der konstituierenden Sitzung, also mit den Stimmverhältnissen des19. Bundestags. Sowohl die Vorgehensweise als auch die inhaltlichen Änderungen wurden kontrovers in deutschen Medien diskutiert[143][144]. Die Bundesrepublik Deutschland ist alsStaatundVölkerrechtssubjektnachherrschender Lehreundständiger RechtsprechungdesBundesverfassungsgerichtsidentisch mit demDeutschen Reichund seinem Vorläufer, demNorddeutschen Bund, und steht damit seit 1867 in einer staatlichen Kontinuität (sieheRechtslage Deutschlands nach 1945). Diehistorisch verschiedenen Verfassungengeben Auskunft über das Selbstverständnis des jeweiligen Staates. Nachdem Deutschland 1945 von den Vier Mächten, den Siegermächten des Zweiten Weltkriegs, besetzt worden war, wurde dasGrundgesetzder inWestdeutschlandentstandenen Bundesrepublik am 23. Mai 1949 verkündet und zum Folgetag in Kraft gesetzt. Es war durch diedeutsche Teilungund bis 1955 durch dasBesatzungsstatutin seinem Geltungsbereich beschränkt. Im östlichen Teil Deutschlands wurde am 7. Oktober 1949 die DDR als eigener Staat gegründet und erhielt eineVerfassung, die 1968 ersetzt und 1974 revidiert wurde. Den provisorischen Charakter verlor das Grundgesetz mit derWiedervereinigung, indem die DDR seinem Geltungsbereich zum 3. Oktober 1990 beitrat. Mit dem Ende derViermächteverantwortungerlangte das vereinte Deutschland volle Souveränität. DasStaatsgebietDeutschlands ergibt sich aus der Gesamtheit der Staatsgebiete seiner Länder. DasHoheitsgebietwurde zweimal durch Beitritt nachArtikel 23 Satz 2 Grundgesetz alter Fassungerweitert: 1957 um dasSaarland, 1990 um dasBeitrittsgebietder DDR sowie Berlins (Ostteil BerlinsundWest-Staaken).[145] Nicht zum Staatsgebiet gehört dieausschließliche Wirtschaftszonein Nord- und Ostsee. Der Verlauf der Staatsgrenze ist heute bis auf Teile desBodenseesfestgelegt. Das einzige in Deutschland existierendeKondominiumist dasgemeinschaftliche deutsch-luxemburgische Hoheitsgebiet, welches die FlüsseMosel,SauerundOuran der Grenze zwischen dem GroßherzogtumLuxemburgund der Bundesrepublik Deutschland (mit den LändernRheinland-PfalzundSaarland) bilden.[146]Es geht zurück auf dieWiener Kongressaktevon 9. Juni 1815, deren Regelungen im Jahr 1984 in einem Grenzvertrag bestätigt wurden.[147]Das Gebiet ist jeweils das einzigegemeindefreie Gebietder Länder Rheinland-Pfalz und Saarland. Noch immer umstritten ist dieDeutsch-Niederländische Grenzfrageim Bereich desEms-Dollart-Gebiets (→Ems Dollart Region), weil beide Nachbarstaaten ihre unvereinbaren Rechtsstandpunkte zum Grenzverlauf aufrechterhalten.[148]Innerhalb Deutschlands ist der Verlauf der Ländergrenzen zwischenSchleswig-Holstein,Niedersachsenund eventuellHamburgim Bereich derUnterelbenicht abschließend geklärt worden. Für diesen Bereich haben die Länder durchVerwaltungsabkommenundStaatsverträgeVerwaltungs- und Gerichtszuständigkeiten geregelt, dieGebietshoheitist damit aber nicht geklärt.[149]Nicht restlos geklärt ist auch die Auseinandersetzung der LänderBaden-WürttembergundHessenum die staatsrechtliche Zugehörigkeit der in Baden-Württemberg eingegliederten GemeindeBad Wimpfen. ExklavischeTeile des deutschen Staatsgebietes sind das baden-württembergischeBüsingen am Hochrhein, das von der Schweiz umschlossen wird und zumSchweizer Zollgebietgehört, sowie einige kleine nordrhein-westfälische Gebiete, die durch die wenige Meter breite belgischeVennbahn-Trasse vom Hauptgebiet Deutschlands abgetrennt sind. Das Grundgesetz (GG) ist dieVerfassungder Bundesrepublik Deutschland.Staatsoberhauptist derBundespräsidentmit vor allem repräsentativen Aufgaben. Er wird von derBundesversammlunggewählt.Im protokollarischen Rangfolgen ihm derPräsident des Deutschen Bundestages, derBundeskanzler, der jeweils amtierendePräsident des Bundesrates, der den Bundespräsidenten vertritt, und derPräsident des Bundesverfassungsgerichts.SitzdesVerfassungsorgansBundesregierungist dieBundeshauptstadtBerlin (§ 3Abs. 3Berlin/Bonn-Gesetz). Artikel 20 GGlegt – durch dieEwigkeitsklauselgesichert – fest, dass Deutschland alsdemokratischer,sozialerRechtsstaatundföderativorganisiertsein muss.Regierungssystemist eineparlamentarische Demokratie. Die Bundesstaatlichkeit ist in zweiEbenenimpolitischen Systemgegliedert: dieBundesebene, die den Gesamtstaat Deutschland nach außen vertritt, und die Länderebene, die in jedem der 16Bundesländerexistiert. Jede Ebene besitzt eigeneStaatsorganederExekutive(ausführende Gewalt),Legislative(gesetzgebende Gewalt) undJudikative(rechtsprechende Gewalt). Die Länder wiederum bestimmen die Ordnung ihrer Städte und Gemeinden; beispielsweise sind fünf Länder in insgesamt 22Regierungsbezirkeuntergliedert. Die Länder haben sich eigeneVerfassungengegeben; ihnen kommt grundsätzlich Staatsqualität zu, sie sind jedoch beschränkte Völkerrechtssubjekte, die nur mit Einwilligung der Bundesregierung eigene Verträge mit anderen Staaten eingehen dürfen (Art. 32Abs. 3,Art. 24Abs. 1 GG). Die Bundesrepublik kann als diestaatsrechtlicheVerbindung ihrer Bundesländer angesehen werden und erhält erst dadurch Staatscharakter, ist alsoBundesstaat im eigentlichen Sinne(siehe auchFöderalismus in Deutschland). Gesetzgebungsorganedes Bundes sind derDeutsche Bundestag, derBundesratund imVerteidigungsfallunter weiteren Voraussetzungen derGemeinsame Ausschuss. Bundesgesetze werden vom Bundestag miteinfacher Mehrheitbeschlossen. Sie werden wirksam, wenn der Bundesrat keinenEinsprucheingelegt oderzugestimmthat (Art. 77GG). EineÄnderung des Grundgesetzesist nur mit derZweidrittelmehrheitder Mitglieder des Bundestages und des Bundesrates möglich (Art. 79Abs. 2 GG). In den Bundesländern entscheiden dieLandesparlamenteüber die Gesetze ihres Landes. Obwohl die Abgeordneten nach dem Grundgesetznicht weisungsgebundensind (Art. 38GG), dominieren in der Praxis derGesetzgebungVorentscheidungen in denParteien, die an der politischen Willensbildung mitwirken (Art. 21GG). DieZuständigkeitzur Gesetzgebung liegt bei den Bundesländern, wenn nicht eine Gesetzgebungsbefugnis des Bundes besteht (Art. 70bis 72 GG) – nämlich eineausschließlicheoder in bestimmten Fällen derkonkurrierenden Gesetzgebung. DieExekutivewird auf Bundesebene durch dieBundesregierunggebildet, die aus demBundeskanzleralsRegierungschefund denBundesministernbesteht. AlleBundesministerienhaben einenDienstsitzin Berlin und einen in derBundesstadt Bonn; einige haben ihren ersten Dienstsitz in Bonn. Auf Länderebene leiten dieMinisterpräsidenten, in denStadtstaatenHamburgundBremendie Präsidenten des Senats, in Berlin derRegierende Bürgermeisterdie Exekutive. Auch die Länder sindparlamentarische Demokratienund deren Regierungschefs durch dieLandtage,Bürgerschaftenbzw. dasAbgeordnetenhaus von Berlingewählt. Die Verwaltungen des Bundes und der Länder werden jeweils durch dieFachministergeleitet. Der Bundeskanzler wird auf Vorschlag des Bundespräsidenten vom Bundestag mit der Mehrheit seiner Mitglieder gewählt (Art. 63GG), seine Amtszeit endet mit der Wahlperiode des Bundestages (Art. 69Abs. 2 GG). Vor deren Ablauf kann der Bundeskanzler gegen seinen Willen nur dadurch aus dem Amt scheiden, dass der Bundestag mit der Mehrheit seiner Mitglieder einen Nachfolger wählt (Art. 67GG, sogenannteskonstruktives Misstrauensvotum). Die Bundesminister werden auf Vorschlag des Bundeskanzlers ernannt (Art. 64Abs. 1 GG), sie und der Bundeskanzler bilden die Bundesregierung (Art. 62GG), derenRichtlinienkompetenzder Bundeskanzler innehat (Art. 65Satz 1 GG). Die Führungsaufgabe in der deutschen „Kanzlerdemokratie“ kommt dem Bundeskanzler zu.[150]Der Kanzler nominiert auch den deutschen Kandidaten für das Amt einesEU-Kommissars. Die Ausübung der staatlichen Befugnisse und die Ausführung der Bundesgesetze obliegt grundsätzlich den Bundesländern, sofern das Grundgesetz keine abweichende Regelung trifft oder zulässt (Art. 30,Art. 83GG). DerStaatshaushaltwies im Jahr 2021 Einnahmen durch Steuern, steuerähnliche Abgaben und Gebühren von 1629 MilliardenEurosowie Ausgaben von 1762 Milliarden Euro auf.[151]Von den Einnahmen waren 833 Milliarden Euro Steuereinnahmen von Bund, Ländern, Gemeinden und derEU.[152]Aufgrund der auf etwa 33 Millionen gestiegenen Anzahl von sozialversicherungspflichtigenErwerbstätigenund steigender Löhne liegen wichtigeSteuereinnahmenwie dieEinkommensteuerund dieUmsatzsteuerauf einem weiter prozentual hohen Niveau für den Staat.[152] DieStaatsverschuldung Deutschlandsbetrug im Jahr 2021 entsprechend dem Bericht derDeutschen Bundesbanketwa 2500 Milliarden Euro.[153]Bei einem Bruttoinlandsprodukt von etwa 3600 Milliarden Euro für 2021 entsprach die Staatsschuldenquote damit etwa 70 Prozent desBruttoinlandsprodukts.[154][155]Im Jahr 2005 betrugen die Staatsschulden der Bundesrepublik Deutschland 1541 Milliarden Euro.[156]Bei einem Bruttoinlandsprodukt von etwa 2281 Milliarden Euro für 2005 entsprach dies einer Staatsschuldenquote von 67 %.[157] Die Bundesrepublik, deren StaatsanleihenBundesanleihengenannt werden, erhält von den drei großenRatingagenturenStandard & Poor’s,Moody’sundFitchdie bestmöglicheBonität. Die Nachfrage nach den als sichere Anlage geltenden Wertpapieren hat in den letzten Jahren die Zinsen deutlich gesenkt und teilweise sogar zuNegativzinsengeführt, was einen Hauptgrund für Deutschlands Haushaltsüberschuss darstellte.[158] Neben verschiedenenVerkehrsteuern(zum BeispielUmsatzsteuer) erzielt der Staat einen Großteil seiner Einnahmen aus Steuern vomEinkommenund Ertrag: Hierzu zählenEinkommen-,Körperschaft-sowieGewerbesteuer. Insofern Produkte oder Dienstleistungen derUmsatzsteuerunterliegen, beträgt der Steuersatz in Deutschland 19 (allgemeiner Satz) oder 7 Prozent (ermäßigter Satz, zum Beispiel Lebensmittel). Umgangssprachlich und imEU-Rechtwird die Umsatzsteuer auchMehrwertsteuergenannt. Laut einerOECD-Studie aus dem Jahr 2014 haben Deutsche durch die hohen Steuern und weitere Abgaben wie Sozialversicherungsbeiträge die weltweit höchsteAbgabenlast, noch vor denskandinavischenSozialstaaten.[159]Dabei wurde allerdings lediglich die Besteuerung der Einkommen betrachtet. Betrachtet man dieAbgabenquote(Verhältnis der Steuern und Sozialabgaben zum Bruttoinlandsprodukt), so liegt Deutschland laut einer Studie der OECD aus dem Jahr 2021 auf Platz 9 der OECD-Staaten.[160] Laut einer von derUNOveröffentlichten Studie gehört Deutschland zu den Ländern mit der höchsten Bereitschaft, durch Steuernöffentliche Güterzu finanzieren.[161]Der Bund kann teilweise Kredite über lange Laufzeiten (bis zu zehn Jahren) zu negativen Zinsen aufnehmen.[162] Parteien wirken gemäßArt. 21GG an der politischen Willensbildung des Volkes mit. Das Parteienspektrum wird durch die im Bundestag vertretenen Parteien geprägt, ihm gehören seit Bestehen dieVolksparteien, dieSPDund dieUnionsparteien(inFraktionsgemeinschaftCDUundCSU), an. Von den anderen Parteien sind dort nach derBundestagswahl 2025außerdemDie LinkeundBündnis 90/Die Grünen, derSSWsowie dieAfDvertreten; der SSW zog2021erstmals seit derBundestagswahl 1949wieder in den Bundestag ein. Alle genannten Parteien sind in denFraktionen des Europäischen Parlamentsvertreten. Nahezu allen einflussreichen Parteien stehenJugendorganisationenzur Seite, weitere politischeVorfeldorganisationenumfassen etwaSchülervertreter,Studentenverbände, Frauen- und Seniorenorganisationen, Wirtschaftsvereine, Kommunalorganisationen und internationale Verbände.Parteinahe Stiftungenbestimmen den politischen Diskurs – rechtlich unabhängig von den Parteien – mit. Deutschland ist Gründungsmitglied desEuroparatesund derEuropäischen Gemeinschaften, die mittels zunächst vorwiegend wirtschaftlicherIntegrationin den 1990er-Jahren zur politischenEuropäischen Union(EU) zusammenwuchsen. Die Bundesrepublik Deutschland trat 1990 derEuropäischen Währungsunionbei und ist Teil desEuropäischen Binnenmarktes. Seit 2002 ist derEuroals Zahlungsmittel eingeführt und hat in der Bundesrepublik dieDeutsche Markabgelöst. Deutschland ist zudem Teil desSchengenraumsund der justiziellen und polizeilichen Zusammenarbeit mithilfe vonEuropolundEurojust. DieGemeinsame Außen- und Sicherheitspolitikder EU bestimmt die deutsche Außenpolitik mit. Den Rechtsrahmen der deutschenEuropapolitikin der EU setztArtikel 23 des Grundgesetzes. In Deutschland haben dasEuropäische Patentamt(München) und mehrere EU-Institutionen ihren Sitz: dieEuropäische Zentralbankin Frankfurt am Main, dieEU-Versicherungsaufsichtsbehördeebenfalls in Frankfurt und dieEuropäische Agentur für Flugsicherheitin Köln. Deutschland hatte dreizehnmal denVorsitzimRat der Europäischen Unioninne,[163][164]zuletzt imzweiten Halbjahr 2020unter dem Motto „Gemeinsam. Europa wieder stark machen“.[165] Die Leitlinien deutscher Außenpolitik sind die Westbindung und die europäische Integration.Sicherheitspolitischzentral ist die Mitgliedschaft im transatlantischen VerteidigungsbündnisNATOseit 1955. Während desKalten Kriegeswar der Spielraum westdeutscher Außenpolitik begrenzt. Als eines der wichtigsten Ziele galt dieWiedervereinigung. Militäreinsätze imAuslandkamen nicht in Frage. Laut Grundgesetz darf sich die Bundeswehr anAngriffskriegennicht beteiligen, ihre Aufgabe besteht lediglich in der Landes- und Bündnisverteidigung. Die von der sozialliberalen Koalition ab 1969 initiierte „Neue Ostpolitik“ unter dem MottoWandel durch Annäherung,die wichtige Verbündete zunächst skeptisch sahen, konnte eigenständige Akzente setzen und wurde von der liberalkonservativen Regierung Helmut Kohls ab 1982 fortgeführt. Seit der Wiedervereinigung trägt Deutschland international größere Verantwortung; seit 1991 nimmt dieBundeswehrunter Aufsicht des Bundestages und zusammen mit verbündeten Armeen an friedenserhaltenden und -erzwingenden Einsätzen außerhalb Deutschlands und des Territoriums der NATO-Verbündeten teil (Out-Of-Area-Einsätze). Die rot-grüne Bundesregierung Gerhard Schröders lehnte denIrakkrieg2003 ab und stellte sich damit gegen den wichtigen Verbündeten USA. Traditionell spielt Deutschland zusammen mit Frankreich eine führende Rolle in der Europäischen Union. Deutschland treibt die Bemühungen voran, über dieWirtschafts- und Währungsunionhinaus eine einheitliche, wirkungsvolleeuropäische Außen- und Sicherheitspolitikzu schaffen. Weitere außenpolitische Ziele sind die Verwirklichung desKyoto-Protokollszum Klimaschutz sowie die weltweite Anerkennung desInternationalen Strafgerichtshofs. Besonderes Interesse hat Deutschland an einer friedlichen Lösung desNahostkonflikts, die es vor allem durch informelle Kontaktmöglichkeiten zwischen den beteiligten Parteien unterstützt. Zusammen mit den Verbündeten Großbritannien und Frankreich bemüht sich die Bundesrepublik, denIranim Dialog dazu zu bewegen, auf die Weiterführung seinesKernenergieprogrammszu verzichten. Am 13. Juli 2016 verabschiedete die Bundesregierung dasneue Weißbuchzur Sicherheitspolitik und zur Zukunft der Bundeswehr als oberstes sicherheitspolitisches Grundlagendokument Deutschlands.[172] Nach ihrer Gründung 1949 durfte die Bundesrepublik Deutschland aufgrund desBesatzungsstatutszunächst keine eigenen Streitkräfte aufstellen. Unter dem Eindruck desKoreakriegesund derSowjetisierungOsteuropas wurde es der Bundesrepublik im Rahmen derWiederbewaffnungjedoch gestattet, zunächst 1951 den paramilitärischenBundesgrenzschutzals Grenzpolizei und ab 1955 vollwertige Streitkräfte aufzustellen, um derNATObeizutreten. Die Aufstellung dieserBundeswehrals Voraussetzung des Beitritts war somit ein bedeutender Beitrag zurWestbindungund damit zur internationalen Anerkennung der Bundesrepublik, aber innenpolitisch unter dem Eindruck des Zweiten Weltkriegshöchst umstritten. Nach der Wiedervereinigung 1990 wurden Teile derNationalen Volksarmee(NVA) der DDR in diese Streitkräfte eingegliedert. Von 1956 bis 2011 kam in der Bundesrepublik gemäß Art. 12a des Grundgesetzes für alle Männer ab dem vollendeten 18. Lebensjahr eineallgemeine Wehrpflichtzur Anwendung. Sie wurde 2011 ausgesetzt und durch denfreiwilligen Wehrdienstersetzt. Seit 2001 haben auch Frauen uneingeschränkten Zugang zum Dienst in den Streitkräften. Ihr Anteil beträgt rund 13 Prozent der Soldaten (Stand Juli 2023).[173]Rund 1.600 deutsche Soldaten befanden sich 2023 im Ausland im Einsatz (Stand September 2023).[174] Die Bundeswehr gliederte sich zunächst in dieTeilstreitkräfteHeer,LuftwaffeundMarine. Im Mai 2024 kam mit dem – vorher bereits als Organisationsbereich existenten –Cyber- und Informationsraumeine vierte Teilstreitkraft hinzu. Daneben bestehen die unterstützenden OrganisationsbereicheStreitkräftebasisundZentraler Sanitätsdienst. Nach dem Ende desKalten Kriegeswurde die Gesamtstärke der Bundeswehr von rund 500.000 bis 2015 schrittweise auf unter 180.000 Soldaten reduziert, nachdem imZwei-plus-Vier-Vertrageine maximale Friedensstärke von 370.000 deutschen Soldaten völkerrechtlich bindend festgelegt worden war. Mit der Aussetzung derWehrpflicht2011 war zudem eine umfassende Reform der Bundeswehr verbunden, die in erster Linie die Festsetzung einer maximalen personellen Stärke von 185.000 Soldaten und 55.000 zivilen Mitarbeitern bedeutete.[175]Zudem wurden die Stückzahlen des schweren Geräts (Kampfpanzer,Artillerie) deutlich reduziert. Hintergrund dieser Strukturänderungen war die Fokussierung der Bundeswehr auf die Teilnahme an internationalen UN- und NATO-Missionen seit Mitte der 1990er-Jahre, für die weniger militärisches Personal und vor allem leichteres und rascher verlegbares Material benötigt wurden. Mit derAnnexion der Krim durch Russland 2014sowie demKrieg in der gesamten Ukraine seit 2022änderte sich der Aufgabenschwerpunkt der Bundeswehr zurück zur Landes- und Bündnisverteidigung im Rahmen von NATO und EU. Die Bundeswehr ist als erste Armee eines deutschen Nationalstaates eineParlamentsarmee, über deren Einsätze ausschließlich der Bundestag auf Vorschlag der Bundesregierung entscheidet. Oberbefehlshaber („Inhaber der Befehls- und Kommandogewalt“) ist in Friedenszeiten der jeweiligeBundesminister der Verteidigung; imVerteidigungsfallgeht diese Funktion auf denBundeskanzlerüber. Das Traditionsverständnis der Bundeswehr distanziert sich sowohl von derWehrmachtderNS-Zeitals auch von der NVA. Es bezieht sich auf diePreußische Heeresreformum 1810, die Befreiungskriege gegen Napoleon, den militärischenWiderstand gegen den Nationalsozialismusund ihre eigene Geschichte (sieheTraditionserlass).[176]Für die Soldaten gilt das Leitbild des „Bürgers in Uniform“. Als bedeutendstes militärisches Zeremoniell gilt derGroße Zapfenstreich; öffentlichkeitswirksam sind die häufig außerhalb militärischer Anlagen durchgeführtenVereidigungen und Gelöbnisseder Soldaten. Die Bundesrepublik Deutschland gab im Jahr 2024 71,75 Milliarden Euro für die Bundeswehr aus.[177]Damit gehört Deutschland zu den zehn Staaten der Welt mit den höchstenVerteidigungsetats; die deutschen Ausgaben liegen mittlerweile mit einem Anteil von etwa 2,12 Prozent des Bruttoinlandsprodukts über dem Durchschnitt von 2,02 % der europäischen NATO-Staaten und Kanadas. Der Durchschnitt unter Einbeziehung der Vereinigten Staaten beträgt 2,71 %.[178] Verteidigungshaushalt/Verteidigungsausgaben in Mrd. Euro 1Die Angaben der NATO für 2023 und 2024 sind geschätzt. In derFeuerwehr in Deutschlandwaren im Jahr 2021 rund 1.385.000 aktive Mitglieder, davon über 1.014.000freiwillige Feuerwehrleute, 35.800Berufsfeuerwehrleute, 34.000Werkfeuerwehrleuteund 301.000 Jugendliche und Kinder organisiert. Sie sind in knapp 24.000 Freiwilligen Feuerwehren, 111 Berufsfeuerwehren, 754 Werkfeuerwehren sowie 22.900Jugendfeuerwehrentätig. Die deutschen Feuerwehren wurden im selben Jahr zu über 4.344.500 Einsätzen alarmiert. Hierbei waren 197.834 Brände zu löschen, war fast 659.700 Maltechnische Hilfezu leisten, in der rettungsdienstlichen Notfallrettung bei rund 2.437.000 sowie bei 1.050.000 weiteren Einsätzen tätig zu werden.[186]Darüber hinaus gehören zusätzlich mehrere Millionen fördernde Mitglieder den örtlichenFeuerwehrvereinenan.[187]Die Feuerwehren sind überKreisfeuerwehrverbände, ggf.BezirksfeuerwehrverbändeundLandesfeuerwehrverbändezumDeutschen Feuerwehrverbandzusammengeschlossen, der sie im WeltfeuerwehrverbandCTIFvertritt. Zuständig für dieinnere Sicherheitder Bundesrepublik sind aufgrund desFöderalismus in Deutschlandgrundsätzlich die Bundesländer und damit insbesondere dieLandespolizeienundLandeskriminalämter. Innerhalb der Polizei wird häufig weiter unterschieden zwischenSchutzpolizei,Bereitschaftspolizei,Kriminalpolizei, Spezialeinheiten (wie demSpezialeinsatzkommando(SEK) oder demMobilen Einsatzkommando(MEK)) sowie denOrdnungsbehörden. Zur Wahrung der öffentlichen Ordnung werden diese zusätzlich in einigen Kommunen durchOrdnungsämterunterstützt. Dennoch existieren auch auf Bundesebene mehrere Organisationen zum Schutz der öffentlichen Sicherheit. Dazu gehört insbesondere dieBundespolizei(ehemalsBundesgrenzschutz), die etwa Aufgaben desGrenzschutzes, derBahnpolizeiund derTerrorabwehrübernimmt und dazu auch die SpezialeinheitGSG 9unterhält, sowie dasBundeskriminalamt, das unter anderem besonders schwere Straftaten verfolgt. Beide sind direkt demBundesministerium des Innern und für Heimatnachgeordnet. Hinzu kommen die Vollzugsbehörden derBundeszollverwaltung(etwa derZollfahndungsdienst, dasZollkriminalamtund dieZentrale Unterstützungsgruppe Zoll), die für die Durchsetzung von fiskalischen, handels- und arbeitsrechtlichen Regeln zuständig sind und demBundesministerium der Finanzenunterstehen. In Deutschland bestehen außerdem drei Nachrichtendienste desBundes: Der zivileBundesnachrichtendienst(BND) als Auslandsnachrichtendienstsammelt zivile und militärische Informationen über das Ausland und wertet diese aus. Zuständig für Aufgaben desVerfassungsschutzesund dieSpionageabwehrsind als Inlandsnachrichtendienste dasBundesamt für Verfassungsschutz(BfV), derMilitärische Abschirmdienst(MAD) für den Geschäftsbereich desBundesministeriums der Verteidigung(BMVg) und in den Bundesländern je eineLandesbehörde für Verfassungsschutz. Polizeiliche Vollzugsbefugnisse haben die Nachrichtendienste in Deutschland aufgrund desTrennungsgebotesnicht. Deutschland gehört zu den sichersten Ländern der Welt.[189]Wie in allen wohlhabenden Ländern derwestlichen Weltgab es von Anfang der 1960er- bis Anfang der 1990er-Jahre einen Anstieg der Kriminalität und bis 2014 einenRückgang, vor allem beiGewaltkriminalitätundDiebstahl.[190]2023 erreichte die Gewaltkriminalität jedoch ihren höchsten Stand seit 15 Jahren.[191] Für Vergleiche der Gewaltneigung über lange Zeiträume und große räumliche Distanzen hinweg wird die Rate der Tötungsdelikte pro Jahr als Index verwendet.[192]Deutschland kam hierbei im Jahr 2018 auf 0,9 Fälle pro 100.000 Einwohner, was dem Durchschnitt in Westeuropa entspricht. Der Durchschnitt in Gesamt-Europa lag bei 2,8 Fällen pro 100.000 Einwohner, der globale Durchschnitt bei 5,8. Ostasiatische Staaten liegen durchschnittlich bei 0,5, Singapur bei nur 0,2 Fällen pro 100.000 Einwohner.[193] Detaillierte, flächendeckende Daten werden seit 1953 (bis 1990 nur für die Altbundesländer) in derPolizeilichen Kriminalstatistikerfasst. Einen Höhepunkt derStraftaten insgesamtgab es 1993. Bis 2021 ist die Rate um 27 Prozent gefallen. Die Rate der Diebstähle sank von 1993 bis 2021 um 65 Prozent. Der Höhepunkt beiangezeigtenGewaltdelikten wurde jedoch nicht in den 1990er-Jahren, sondern 2007 erreicht. Der Rückgang lag hier bis 2021 bei 25 Prozent.[188]Es wird von einer steigenden Anzeigebereitschaft beziehungsweise einer sich verringerndenDunkelzifferausgegangen, vor allem beiGewalt gegen Frauen.[194] In Deutschland gingen im Jahr 2024 laut einer Studie desInstituts der deutschen Wirtschaftmindestens 3,3 Millionen Menschen einerSchwarzarbeitnach.[195] Das deutsche Recht gehört demkontinentalen Rechtskreisan und hat sich über die meiste Zeit seines Bestehens ohne die Ordnung durch einen deutschen Nationalstaat entwickelt. Es beruht daher auf dem historisch überliefertendeutschen Recht,das aufgermanische Stammesgesetzeund mittelalterliche Rechtssammlungen wie denSachsenspiegelzurückgeht, und derRezeption des römischen Rechtsab dem 12. Jahrhundert, das wegen seiner Exaktheit und Universalität als überlegen galt. Außer wenigen Rechtssetzungen wie derConstitutio Criminalis Carolina1532 war das Heilige Römische Reich vonPartikularrechtengeprägt. Erst im Lauf des 19. Jahrhunderts wurde eine Rechtsvereinheitlichung begonnen und im Deutschen Bund 1861 einAllgemeines Deutsches Handelsgesetzbuchsowie im Kaiserreich unter anderem dasReichsgericht1877 und dieReichsjustizgesetze1879 eingeführt. 1900 trat dasBürgerliche Gesetzbuchin Kraft. DerNationalsozialismuspervertierte das Recht zum Mittel der Gewaltherrschaft, wofür die Terrorurteile desVolksgerichtshofs, dieNürnberger Gesetzeund zahlreiche weitereRechtsaktestehen, welche erst durch alliiertesBesatzungsrecht, eine nicht-deutscheRechtsquelle,[196]wieder aufgehoben wurden. Auch wenn das Besatzungsrecht seinerseits in fünf Bundesgesetzen wieder aufgehoben wurde und seine Bestimmungen weitestgehend Eingang in deutsches Recht fanden, ist die deutscheRechtspflegebis zum heutigen Tag um die Wiederherstellung des vom nationalsozialistischenUnrechtsstaatzerschlissenen Rechts bemüht. Beispielsweise ist die aus der Zeit des Nationalsozialismus stammende strafrechtliche Definition vonMordunter deutschen Rechtspflegern umstritten. Die im Dritten Reich verschärfte Fassung des§ 175führte auch in der Bundesrepublik zu einer ausgedehnten Verfolgung vonHomosexualität; er wurde erst 1969 reformiert und 1994 aus dem Strafgesetzbuch gestrichen. In der DDR wurde das Recht durch dieEinparteienherrschaftder SED gelenkt; dieGewaltenteilungund Unabhängigkeit der Gerichte, die von derVerfassungvorgeschrieben waren, wurden in derVerfassungswirklichkeitumgangen.[197]In der Rechtspflege und der Gesetzgebung bemühte sich die DDR über die Zeit ihrer Existenz, sich von der bürgerlichen Rechtstradition, die im Kaiserreich begründet und in der Bundesrepublik fortgesetzt wurde, zu entfernen und rechtshistorisch eigenständige Rechtsquellen zu schaffen. Anders als die Bundesrepublik lehnte die DDR juristisch sowohl die Identität mit demDeutschen Reichals auch dessenRechtsnachfolgeab. ImZivilgesetzbuch der DDR, das 1976 in Kraft trat, standen die „Versorgungsbeziehungen“ der Bürger im Vordergrund.[198]Fragen desEigentumswurden unter deutlichen Vorzeichen der sozialistischen Planwirtschaft geregelt, eine Definition vonBesitzgab es mit der Einführung des Zivilgesetzbuches nicht mehr. Durch denBeitritt der DDRendete sowohl die Entwicklung als auch der Fortbestand des DDR-Rechts. Außer bei Altfällen in der Rechtspflege übt das Straf- und Zivilrecht der DDR auf das deutsche Recht der Gegenwart keinen Einfluss mehr aus. Durch Artikel 9 desEinigungsvertragsgingen einige Gesetze und Verordnungen aus der DDR in das Landesrecht derneuen Länderüber. DieTodesstrafewurde in Deutschland mitArt. 102des Grundgesetzes schon bei seiner Verkündung abgeschafft. In der DDR erfolgte die Abschaffung erst 1987, wenige Jahre vor ihrem Ende. Die Bundesrepublik Deutschland versteht sich alsRechtsstaat(Art. 20,Art. 28Abs. 1 Satz 1 GG), was bedeutet, dass staatliche Tätigkeit nur durch das Recht begründet werden kann und durch das Recht begrenzt wird. Inhalt deutscher Gesetze ist deshalb üblicherweise zuerst die Grenze ihres Wirkungskreises, bevor Recht begründet wird. Beispielsweise werden im§ 1imStrafgesetzbuchalle Taten straffrei gestellt, die zum Zeitpunkt der Tat durch das Gesetz nicht strafbar waren. Wer durch die öffentliche Gewalt in seinen Rechten verletzt wird, hat das Recht, bei Gericht um Rechtsschutz hiergegen nachzusuchen (Art. 19Abs. 4 GG). DieRichterunterliegen bei derRechtsprechungkeinerlei Weisungen und sind von anderen Gewalten staatlicher oder politischer Art unabhängig. Deutschland kenntSchöffengerichteundStrafkammern, in denen Urteile durchehrenamtliche RichterundBerufsrichtergemeinsam gefällt werden, sofern die Straferwartung bei Verbrechen vier Jahre nicht übersteigt.Geschworenengerichtewurden in Deutschland 1924 abgeschafft. UmfangreicheProzessordnungenwie dieStrafprozessordnungund dieZivilprozessordnungbestimmen den genauen Ablauf der Gerichtsprozesse, aber auch der vor-, außer- und nachgerichtlichen Verfahren. Die Rechtsprechung wird im Wesentlichen vonGerichtender Bundesländer ausgeübt: InZivil-undStrafsachendurch dieAmtsgerichte, dieLandgerichteund dieOberlandesgerichte(ordentliche Gerichtsbarkeit); anFachgerichtsbarkeitgibt es dieArbeits-,Verwaltungs-,Sozial-undFinanzgerichtsbarkeit. Für den gewerblichen Rechtsschutz besteht dasBundespatentgericht. Als Rechtsmittelgerichte dienen die obersten Gerichtshöfe des Bundes (Art. 95GG): DerBundesgerichtshofals oberstes Zivil- und Strafgericht, dasBundesarbeitsgericht, dasBundesverwaltungsgericht, dasBundessozialgerichtund derBundesfinanzhof. ÜberverfassungsrechtlicheStreitigkeiten urteilen dieVerfassungsgerichteder Länder und dasBundesverfassungsgericht(Art. 93GG), dessen Entscheidungen Gesetzeskraft entfalten können und so andere Gerichte binden (vgl.§ 31Bundesverfassungsgerichtsgesetz). Zunehmende Bedeutung haben dasEuroparechtund die Rechtsprechung desGerichtshofs der Europäischen Union. Infolge langjährigerVerträgeDeutschlands mit der Europäischen Union und der darauf beruhenden Rechtsaktivitäten wird deutsches Recht erheblich durch Unionsrecht beeinflusst. Im Dezember 2021 erklärte derEuropäische Gerichtshofin einem unionsweit wegweisenden Urteil, dass das von ihm gesprochene Recht auch die Rechtsprechung der Verfassungsgerichte der Mitgliedstaaten außer Kraft setzen könne. Damit beansprucht der Europäische Gerichtshof nach Beobachtern auch, die letzte Instanz der Rechtsprechung der Mitgliedstaaten zu sein; diese könnten sich nun nicht länger in Abgrenzung zu EU-Recht auf ihre Verfassung berufen.[199]Dem Urteil gingen verschiedene Konflikte zwischen der Europäischen Union und ihrer Mitgliedstaaten über die letztinstanzliche, verfassunggebende Rechtsprechung – unter anderem ein (eingestelltes) Vertragsverletzungsverfahren gegen Deutschland aufgrund eines dem EuGH widersprechenden Urteils seines Bundesverfassungsgerichts zur Finanzaufsicht[200]– voraus. Relativ neu in der deutschen Rechtsgeschichte ist die Existenz unabänderlichen Rechts. In der deutschen Verfassung schützt dieEwigkeitsklausel(Art. 79 Abs. 3 GG) ihren Wesenskern. Diese Klausel kann auch durch eine verfassungsändernde Mehrheit in Bundestag und Bundesrat nicht geändert oder abgeschafft werden, da sie sonst sinnlos wäre. Änderungen des Grundgesetzes, die gegen die durch die Klausel geschützten Normen verstoßen, dürfen nach Artikel 100 Absatz 1 des Grundgesetzes (nur) durch das Bundesverfassungsgericht für nichtig erklärt werden. In diesem Fall war das Gesetz bereits ab seiner Verkündung nichtig.[201] Mit einem nominalenBruttoinlandsproduktvon etwa 3,8 BillionenUS-Dollarim Jahr 2020 ist Deutschland die größteVolkswirtschaftEuropas und drittgrößte der Welt. Gemessen am nominalenBIP pro Kopfsteht Deutschland international an 18., in der Europäischen Union an 8. Stelle (Stand 2019).[202]Gemessen am Warenwert war das Land 2016 der drittgrößte Importeur und Exporteur der Welt.[203]DasEntwicklungsprogramm der Vereinten Nationenzählt Deutschland zu den Ländern mit sehr hoher menschlicher Entwicklung. ImGlobal Competitiveness Indexbelegte es 2019 den 7. Platz. Deutschlands Wettbewerbsfähigkeit speist sich vor allem aus der hohen Zahl an kleinen und mittleren Unternehmen (Mittelstand), die gerade in spezialisierten Bereichen der Industrie zu den Weltmarktführern gehören.[204] Die Gesamtwirtschaftsleistung wird zu 2,1 Prozent im primärenWirtschaftssektor(Landwirtschaft), 24,4 Prozent im sekundären (Industrie) und 73,5 Prozent im tertiären (Dienstleistung) erbracht. 2014 verzeichnete Deutschland mit durchschnittlich etwa 42,6 Millionen sozialversicherungspflichtig Beschäftigten einen Höchststand.[205]Die Zahl der Erwerbslosen lag im Schnitt 2014 bei 2,898 Millionen.[206]Deutschland wies gemäß demStatistischen Bundesamt2019 eineArbeitslosenquotevon 5,6 Prozent auf.[207]Ein wichtigerFaktorzur Schaffung neuer Arbeitsplätze ist dasUnternehmer- und Gründertum, worüber unter anderem der jährlicheKfW-Gründungsmonitor Auskunft gibt.[208] Deutschland verfügt über verschiedeneRohstoffvorkommenund weist eine langeBergbautraditionauf (unter anderemKohle,Edelsalze,IndustriemineraleundBaustoffesowieSilber,EisenundZinn). Die Industrie ist auf globale Rohstoffimporte angewiesen. DasHumanpotenzialmit guter Bildung und dieInnovationskulturgelten als Voraussetzungen für den Erfolg der deutschen Wirtschaft undWissensgesellschaft.[209]Als weltweit konkurrenzfähigste Branchen der deutschen Industrie gelten dieAutomobil-,Nutzfahrzeug-,elektrotechnische,Maschinenbau- undChemieindustrie. Global bedeutend sind auch dieLuft- und Raumfahrttechnik, dieFinanzbranche– etwa mit demFinanzplatz Frankfurt am Main[210]– und dieVersicherungswirtschaft, insbesondere dieRückversicherungen. Der Stellenwert derKultur- und Kreativwirtschaftnimmt zu. Als Mitglied derEuropäischen Uniongehört Deutschland zum größtenBinnenmarktder Welt mit zusammen rund 500 Millionen Einwohnern und einem nominalen BIP von 17,6 Billionen US-Dollar 2011. Deutschland ist auch Teil derEurozone, einer Währungsunion mit 19 Mitgliedsländern und etwa 337 Millionen Einwohnern. Deren Zahlungsmittel ist derEuro, dessen Währungspolitik von der Europäischen Zentralbank (EZB) gesteuert wird und der zweitwichtigsteReservewährungder Welt ist sowie gemessen am Bargeldwert die weltgrößte Währung im Umlauf. Die Arbeitslosenquote in Deutschland gehört zu den niedrigsten in der EU; sie beträgt 3 %, nur in Polen und Tschechien liegt sie niedriger. Der Durchschnitt aller EU-Länder beträgt 6,1 %, während die OECD-Länder eine Arbeitslosenquote von 4,8 % aufweisen (Stand März 2023).[211] DieEinkommensungleichheitin Deutschland lag 2005 knapp unter dem OECD-Durchschnitt.[212]2008 betrug ein mittleresverfügbares Einkommen1.252 bei einemGini-Indexvon 0,29.[213]DieVermögensverteilung in Deutschlandist mit einem Gini-Index von 0,78 deutlich stärker konzentriert als die Verteilung der Einkommen. Laut derCredit Suissebelief sich die Summe der Privatvermögen 2016 auf 12,4 Billionen Dollar. Im Durchschnitt verfügte jede erwachsene Person in Deutschland 2016 über ein Vermögen von 185.175 US-Dollar (Median-Vermögen: 42.833 US-Dollar). Das ist weltweit Platz 27 und weniger als in den meisten Nachbarländern Deutschlands – eine Ursache oder Folge (je nach Interpretation) ist ein niedriger Anteil an Immobilieneigentum.[214]2016 gab es in Deutschland 1.637.000 Millionäre und 2017 insgesamt 114 Milliardäre (in US-Dollar), die weltweit dritthöchste Anzahl.[215] Die deutsche Volkswirtschaft verzeichnete von 1986 bis 1988 sowie 1990 und von 2003 bis 2008 einen höherenExportüberschussals jedes andere Land („Exportweltmeister“).[216]Deutschland war in den 2010er-Jahren durchgehend das Land mit dem dritthöchsten Wert an Exporten weltweit.[217]Die Exporte erreichten im Jahr 2020 einen Gesamtwert von 1.205 Milliarden Euro, der Warenwert der Importe betrug 1.025 Milliarden Euro – ein Überschuss derAußenhandelsbilanzvon 180 Milliarden Euro.[218]DerLeistungsbilanzüberschusswar 2016 der höchste weltweit und lag bei über 7 Prozent der Wirtschaftsleistung, was teilweise auf Kritik aus dem In- und Ausland stößt.[219] Die wichtigsten Handelspartner (Im- und Exporte) im Jahr 2020 waren dieVolksrepublik China(213 Milliarden Euro Handelsvolumen), dieNiederlande(173 Milliarden Euro), dieVereinigten Staaten(172 Milliarden Euro),Frankreich(147 Milliarden Euro),Polen(123 Milliarden Euro) undItalien(114 Milliarden Euro). Die größten Exportmärkte waren die USA, die VR China, Frankreich und die Niederlande. Mehr als die Hälfte seines Außenhandels führte Deutschland mit den Staaten der Europäischen Union.[220]Der Wert aller Exporte von Gütern und Dienstleistungen machte 2019 47 Prozent der Wirtschaftsleistung aus, was unter den größeren Volkswirtschaften ein hoher Wert ist.[221]Das Land ist deshalb potenziell anfällig für Schwankungen im globalen Handel, auch wenn der Aufschwung der letzten Jahre vor allem konsumgetrieben war. Deutschland wurde Ende 2008 und 2009 von derinternationalen Finanzkriseerfasst, was zu einem Rückgang des Bruttoinlandsprodukts 2009 um 5,6 Prozent führte. Anschließend wuchs die deutsche Volkswirtschaft wieder deutlich um 4,1 und 3,7 Prozent (2010 und 2011) und 2012 und 2013 moderater mit jeweils 0,5 Prozent. 2014 beschleunigte sich das Wirtschaftswachstum wieder auf 1,9 Prozent und 2015 und 2016 weiter auf 1,7 bzw. 1,9 Prozent.[222]Für das Jahr 2017 lag das Wachstum bei 2,2 Prozent.[223]DieCOVID-19-Pandemieführte 2020 zu einem Einbruch der Wirtschaftsleistung um 4,6 Prozent. Im folgenden Jahr erholte sich die Wirtschaft wieder etwas und es wurde ein Wachstum von 2,7 Prozent verzeichnet.[224] Zwischen 2000 und 2011 lag die jährliche durchschnittliche Inflationsrate bei minimal 0,3 Prozent (2009) und bei maximal 2,6 Prozent (2008).[225]Anfang 2015 verzeichnete Deutschland durch den niedrigen Ölpreis erstmals seit 2009 eine leichte Deflation (−0,3 %).[226]Nach Jahren mit relativ moderaten Preissteigerungen erreichte die Inflationsrate in Deutschland im Rahmen derweltweiten Energiekrise2022 ihr höchstes Niveau seit den 1950er-Jahren mit Preissteigerungen im zweistelligen Bereich.[227] Deutschland ist weltweit für die Entwicklung und Produktion von innovativen und hochwertigenPKWbekannt. Das Automobil wurde 1886 vonCarl Benzin Deutschland erfunden,[228]was den Grundstein für die Entwicklung der gegenwärtig drittgrößtenAutomobilindustrieder Welt legte. Heute sind Konzerne wieVolkswagen,Mercedes-BenzundBMWein wichtiger Bestandteil der deutschen Wirtschaft. Die deutsche Autoindustrie erwirtschaftete 2017 mit ca. 800.000 Mitarbeitern in Deutschland mehr als 400 Milliarden Euro Umsatz,[229]etwa sieben Prozent des BIP gehen auf sie zurück.[230] DieInformations- und Kommunikationstechnik(IKT) gilt als wesentlicherStandortfaktor. Die Digitalisierung der deutschen Wirtschaft wird unter dem ProjektnamenIndustrie 4.0vorangetrieben. Das umsatzstärkste in Deutschland tätige Telekommunikationsunternehmen ist dieDeutsche Telekom.SAP, dieSoftware AGundDATEVzählen zu den bedeutendstenSoftwareherstellernder Welt mit Hauptsitz in Deutschland. Im Hardwarebereich ist vor allem die Entwicklung von Bedeutung, etwa beiInfineonundFTS. Neben angestammten Unternehmen der IKT-Branche gewinneninnovativeStartUpsundE-Venturesin Deutschland an Bedeutung. 2017 hatten 88 Prozent der Bevölkerung einenInternetzugang; etwa 87 Prozent konnten dabei auf einenBreitbandanschlusszurückgreifen.[231] Deutschland war im Jahr 2010 der viertgrößte Produzent anPrimärenergiein Europa und wurde auf Rang 24 unter den Energieproduzenten der Welt gelistet.[234]2012 betrug derPrimärenergieverbrauchin Deutschland 13.757PJ(2005: 14.238 PJ).[235]Daran gemessen ist das Land der zweitgrößte nationale Energieverbraucher in Europa undsiebtgrößte in der Welt. DieStromversorgungwurde im Jahr 2012 von 1059Unternehmenmit Hauptsitz in Deutschland gewährleistet.[236] Erneuerbare Energienlieferten im Jahr 2016 29,2 Prozent derBruttostromproduktion,[237]13,4 Prozent desEndenergiebedarfsim Wärmesektor und 5,1 Prozent derKraftstoffe.[238]Im Rahmen derEnergiewendeist geplant, bis 2050 den Anteil der Erneuerbaren Energien amStromverbrauchauf 80 Prozent zu steigern, den Primärenergieverbrauch gegenüber 2008 um 50 Prozent zu senken und denTreibhausgasausstoßin Einklang mit den EU-Zielen um 80–95 Prozent gegenüber 1990 zu reduzieren.[239]Insgesamt sollen 2050 mindestens 60 Prozent des Energieverbrauchs durch erneuerbare Energien gedeckt werden.[240] Deutschland stand 2023 mit knapp 35 Millionen ausländischen Übernachtungsgästen im Jahr auf Platz 8 dermeistbesuchtenLänder der Erde.[241] Etwa 4.000 der 11.116 Gemeinden Deutschlands sind in Tourismusverbänden organisiert, 310 davon sind alsHeilbäder,SeebäderundKurorteanerkannt. Es stehen 6.135Museen, 366Theater, 34Freizeit- und Erlebnisparks, 45.000Tennisplätze, 648Golfplätze, 190.000 kmWanderwegnetz, 40.000 KilometerRadfernwegesowieFerien- und Themenstraßenzur Verfügung. Von herausragender Bedeutung ist derGeschäfts- und Kongresstourismus; Deutschland ist der international bedeutendsteMessestandortmit mehrerenWeltleitmessen.[242]DieInternationale Tourismus-Börse Berlinist die weltweit führende Tourismusmesse. Zudem gibt es in Deutschland die größte Dichte anFestivals. DieWertpapierbörseder deutschen Volkswirtschaft ist dieDeutsche Börsein Frankfurt mit dem LeitindexDAX. Derzeit verzichtet der deutsche Staat im Altersvorsorgesystem aufKapitaldeckungund damit auf Pensionsfonds. Einen klassischen allgemeinenStaatsfondsgibt es bisher ebenso nicht; jedoch wurde mit demFonds zur Finanzierung der kerntechnischen Entsorgung(KENFO) der erste deutsche Staatsfonds und die größteöffentlich-rechtliche Stiftungin Deutschland errichtet. Der von derWeltbankerstellteLogistics Performance Index2018 weist Deutschland als das Land mit der weltweit besten Infrastruktur aus.[243] Aufgrund der dichten Besiedlung und zentralen Lage in Europa besteht in Deutschland ein sehr hohes Verkehrsaufkommen. Insbesondere für denGüterverkehrstellt es ein wichtigesTransitlanddar. Durch das Konzept derTranseuropäischen Netzewird Deutschland als Transferraum zwischen dem ersten europäischenKernwirtschaftsraum, der sogenanntenBlauen Banane, und dem Kernwirtschaftsraum inOstmitteleuropagefördert. Wichtige Projekte in diesen Netzen sind die Eisenbahnachsen Lyon/Genua–Rotterdam/Antwerpen,POS(Paris–Ostfrankreich–Südwestdeutschland),PBKA(Paris–Brüssel–Köln–Amsterdam),Berlin–Palermound dieMagistrale für Europa. Ferner ist Deutschland der westliche Ausgangspunkt einigerPaneuropäischer Verkehrskorridore. 2005 wurde eineAutobahnmautfür Lastkraftwagen eingeführt. Die Kohlendioxid-Emissionen des Straßengüterverkehrs stiegen in Deutschland von 1995 bis 2017 um 20 Prozent.[244] Bereits die Römer legtengepflasterte Straßenin Deutschland an, die wieder verfielen. Die erstenChausseenwurden im 18. Jahrhundert erbaut. Die Erfindung des Automobils gab dem Straßenbau neue Impulse. Die erste Autobahn der Welt, dieAVUS, wurde 1921 in Berlin eröffnet. DerStraßenverkehrhat in der zweiten Hälfte des 20. Jahrhunderts dieEisenbahnals wichtigsten Verkehrsträger abgelöst. Deutschland besitzt eines der dichtestenStraßennetzeder Welt. Im Jahr 2012 umfasste dasBundesfernstraßennetz12.845 KilometerAutobahnenund 40.711 KilometerBundesstraßen. Weiterhin umfasste das überörtliche Straßennetz 86.597 KilometerLandesstraßen, 91.520 KilometerKreisstraßenund dieGemeindeverbindungsstraßen. Am 1. Januar 2020 waren in Deutschland 47,7 MillionenPersonenkraftwagenzugelassen. Der Fahrzeugbestand allerKraftfahrzeugeundAnhängerbetrug 65,8 Millionen.[245]Von 1995 bis 2017 sind die absoluten Kohlendioxid-Emissionen des Straßengüterverkehrs in Deutschland um 20 Prozent gestiegen.[246] Um die Gefahren und Belastungen durch den Straßenverkehr zu verringern, wurden in vielen deutschen StädtenFußgängerzonen,verkehrsberuhigte ZonenundTempo-30-Zoneneingerichtet. Die Anzahl derim Straßenverkehr Getötetennahm seither kontinuierlich ab; 2015 waren es 3459 Menschen, 2019 noch 3046.[247]DerRadverkehrspielt eine zunehmende Rolle, sein Ausbau wird politisch etwa durch denRadverkehrsplanunterstützt. Am 7. Dezember 1835 wurde mit derLudwigseisenbahndie erste Eisenbahnstrecke in Deutschland eröffnet, in den folgenden Jahrzehnten wuchs das Schienennetz auf mehr als 18.000 Kilometer im Jahr 1870 an. Der Eisenbahnausbau beschleunigte auch das industrielle Wachstum im 19. Jahrhundert. DeutschlandsEisenbahnnetzist heute etwa 39.200 Kilometer lang[248]und wird täglich von bis zu etwa 41.500 Personen- und Güterzügen befahren.[249]Im Rahmen derBahnreformwurden dieStaatsbahnenDeutsche Bundesbahn(West) undDeutsche Reichsbahn(Ost) zum 1. Januar 1994 in das privatwirtschaftliche UnternehmenDeutsche Bahn AGüberführt. Es organisiert den Großteil des Eisenbahnverkehrs in Deutschland. Rund 350 weitereEisenbahnverkehrsunternehmenbefahren das deutsche Eisenbahnnetz. Während sich der Staat aus dem operativen Betrieb zurückgezogen hat, finanziert er den Großteil des Netzunterhalts und -ausbaus sowie (überRegionalisierungsmittel) weitgehend den Regionalverkehr. Regional-(Interregio-Express(IRE),Regionalbahn(RB),Regional-Express(RE) undS-Bahnen(S)) undFernverkehr(Intercity(IC),Eurocity(EC) undIntercity-Express(ICE)) fahren weitgehend nachTaktfahrplan. FürFernzügestehenSchnellfahrstreckenin einer Gesamtlänge von etwa 3.000 Kilometern zur Verfügung. DieVerkehrsleistungim Eisenbahnverkehr lag 2023 im Nah- und Fernverkehr bei insgesamt 104,2 MilliardenPersonenkilometernund erreichte damit einen Rekordwert.[250] 1881 eröffneteWerner von SiemensinLichterfeldebei Berlin die erste elektrische Straßenbahn der Welt. Dieses Verkehrsmittel dominierte in der ersten Hälfte des 20. Jahrhunderts denöffentlichen Nahverkehrder größeren Städte in Deutschland. Nach dem Zweiten Weltkrieg wurden vor allem in Westdeutschland viele stillgelegt, andere zuStadtbahnenmit innerstädtischen Tunnelstrecken umgebaut. Ersetzt wurden sie durchOmnibus-Verkehr, der auch auf dem Land flächendeckend vorhanden ist und nahezu jeden Ort erschließt. Allerdings wurden die Busnetze durch den Bevölkerungsrückgang im ländlichen Raum ausgedünnt und häufig durchRufbus-Systeme ersetzt. In den größten Städten wurden im 20. JahrhundertU-Bahnenangelegt und mitS-Bahnenzu einem Schnellbahnnetz für Stadt und Umland kombiniert. Die administrative Abwicklung erfolgt überÖPNV-Aufgabenträger. Seit den 1980er-Jahren wurdenRadwegnetzein den Städten und auf dem Land angelegt und ausgebaut, sodass heute das Fahrrad wieder eine zunehmende Rolle im Nahverkehr spielt. Im internationalen Vergleich ist der öffentliche Nahverkehr in den größeren Städten Deutschlands durch hohe Effektivität und Flächendeckung gekennzeichnet. Mit rund 700Flugplätzenverfügt Deutschland über eine der größten Dichten an Start- und Landebahnen weltweit. DerFlughafen Frankfurt Mainist nach Passagieren (2016: 60,77 Millionen)[251]der größte Deutschlands, der viertgrößte Europas und gemessen am Frachtaufkommen (2015: 2,1 Millionen Tonnen)[252]der größte Flughafen Europas. Die größte deutsche FluggesellschaftLufthansabetreibt in Frankfurt und auf dem zweitgrößten deutschenFlughafen in Müncheninterkontinentale Drehkreuze. Der Bund und die Länder Berlin und Brandenburg sind alleinige Gesellschafter derFlughafen Berlin Brandenburg GmbH, die denFlughafen Berlin Brandenburg„Willy Brandt“ betreibt. Einen eigenenWeltraumbahnhof(bzw. Raumhafen) für den Verkehr über dieKármán-Linie(100 km) hinaus bis in denWeltraumbesitzt Deutschland nicht. DieRaumfahrtdesDeutschen Zentrums für Luft- und Raumfahrtnutzt deshalb meist denCSG-Raumhafen inFranzösisch-Guayanaoder das russisch betriebeneKosmodrom Baikonur. Aufgrund des hohenAußenhandelsanteilsist Deutschland besonders auf den Seehandel angewiesen. Es verfügt über eine Anzahl modernerSeehäfen, wickelt aber auch große Anteile seines Handels nachÜberseeüber die Häfen von Nachbarländern, vor allem in den Niederlanden, ab. Die dreiumschlagsstärkstenSeehäfen in Deutschland sindHamburg,Wilhelmshavenund dieBremer Häfen. DerJadeWeserPortin Wilhelmshaven ist der einzigeTiefwasserhafenin Deutschland. Die wichtigsten Ostseehäfen sindRostock,LübeckundKiel.Rostock-Warnemündeist der meistfrequentierteKreuzfahrthafenDeutschlands. Die wichtigsten Seeschifffahrtsstraßen sindUnterelbeundUnterweser. DerNord-Ostsee-Kanalist die meistbefahrene künstliche Seeschifffahrtsstraße der Welt,[253][254]vor der deutschen Ostseeküste liegt mit derKadetrinnedie meistbefahrene Schiffsroute der Ostsee. Es gibt ein gut ausgebautes Netz vonWasserstraßenfür dieBinnenschifffahrt. Die wichtigsten schiffbaren Flüsse sindRhein,Main,Mosel,WeserundElbe. Bedeutende Binnenkanäle sind derMittellandkanal, derDortmund-Ems-Kanal, derRhein-Herne-Kanalund derElbe-Seitenkanal. DerMain-Donau-Kanalüberwindet dieeuropäische Hauptwasserscheideund ermöglicht so einen direkten Schifffahrtsweg von der Nord- und Ostsee zumSchwarzen Meer. Der Komplex derDuisburg-Ruhrorter Häfenist der umschlagsstärksteBinnenhafenDeutschlands und gilt als größter Binnenhafen Europas. Ebenfalls beginnt bzw. endet dort auch dieNeue Seidenstraße,ein Infrastrukturprojekt der Volksrepublik China, die an alteHandelsroutenanknüpfen will.[255][256] DerMasterplan Binnenschifffahrtwurde 2019 beschlossen. Die deutsche Kunst- und Kulturgeschichte, deren Wurzeln bis in die Zeit derKelten,GermanenundRömerzurückreichen, hat seit dem Mittelalter stil- und epochenprägende Persönlichkeiten hervorgebracht. In den verschiedensten Disziplinen wurden deutschsprachige Kulturschaffende Wegbereiter neuer geistiger Strömungen und Entwicklungen. Einige der einflussreichsten deutschen Künstler zählen zu den Protagonisten derwestlichen Zivilisation.[257]Die staatlichen Zuwendungen für Kultur (Theater, Museen, Kunsthochschulen etc.) durch die Bundesregierung, Landesregierungen sowie Gemeinden in Deutschland betrug im Jahr 2017 über elf Milliarden Euro.[258][259] Die deutsche Kultur hat sich, da Deutschland lange nicht alsNationalstaatexistierte, über Jahrhunderte vor allem über die gemeinsame Sprache definiert; auch über die Reichsgründung 1871 hinaus ist Deutschland häufig alsKulturnationverstanden worden. Durch die Verbreitung vonMassenmedienim 20. Jahrhundert hat diePopulärkulturin der deutschen Gesellschaft einen hohen Stellenwert erhalten. Die Verbreitung desInternetsim 21. Jahrhundert hat zu einer Differenzierung der Kulturlandschaft geführt und die mannigfaltigenNischenkulturenin ihren Ausprägungen verändert.[260] Der Verbreitung der deutschen Sprache und Kultur in der Welt dienen dieGoethe-Institute. Mit insgesamt 158 Standorten, inklusive Verbindungsbüros, ist das Institut im Jahr 2013 in 93 Ländern vertreten.[261]Laut einer Umfrage in 22 Staaten für dieBBCim Jahr 2013 genoss Deutschland international zum sechsten Mal in Folge seit dem Jahr 2008 das höchste Ansehen unter 16 untersuchten Ländern. Durchschnittlich bewerteten 59 Prozent der Befragten Deutschlands Einfluss und politisches Wirken als positiv, 15 Prozent hatten ein negatives Bild.[262] Für spezielle Bereiche der deutschen Kultur siehe: In Deutschland ist die Mehrheit der Sportvereine imDeutschen Olympischen Sportbund(DOSB) organisiert. Ihm gehörten im Jahr 2023 etwa 27,8 Millionen Mitglieder in 86.000 Turn- und Sportvereinen an.[263] Die am häufigsten betriebene und von Zuschauern verfolgte Sportart in Deutschland istFußball. Rund 6,8 Millionen Mitglieder und ungefähr 172.000 Mannschaften waren 2012 imDeutschen Fußball-Bundorganisiert,[264]der dieFußball-Weltmeisterschaften1974und2006sowie dieFußball-Europameisterschaften1988und2024ausrichtete und weltweit einer der größten und erfolgreichsten Sportfachverbände ist. Diedeutsche Fußballnationalmannschaftder Männer wurde 1954, 1974, 1990 und 2014Weltmeister. Deutschland ist das einzige Land, das im Fußball sowohl bei denFrauenals auch den Männern Europa- und Weltmeister werden konnte. Das größte Stadion, ausgelegt für internationale Begegnungen, ist dasOlympiastadion Berlinmit 74.475 Plätzen. DerSignal Iduna Parkin Dortmund ist für Fußballspiele auf Bundesliga-Ebene mit über 80.500 Plätzen das Stadion mit der größten Zahl genehmigter Zuschauerplätze. DieHandball-Bundesligawird oft als die beste Spielklasse der Welt angesehen, die Herren-Nationalmannschaft wurde 2007 zum dritten MalWeltmeister. Etwa 830.000 aktive Mitglieder gehören rund 4500 Vereinen an (Stand 2011).[265]Dachverband ist derDeutsche Handballbund. Volleyballund die VarianteBeachvolleyballwurden in den 1990er-Jahren zum beliebtenBreitensport. DerDeutsche Volleyball-Verbandzählte im Jahr 2016 rund 430.000 Mitglieder.[266]Diedeutschen Volleyball-Meisterder Frauen und Männer werden jährlich ermittelt, die europäischeVolleyball Champions Leaguewurde mehrfach von deutschen Vereinen gewonnen, die DDR–Männermannschaft wurde 1970Volleyball-Weltmeister. BasketballundEishockeywachsen bei Zuschauerzahlen und Medienpräsenz stetig. Rekordmeister derDeutschen Eishockey Ligasind dieEisbären Berlin. Im Basketball warDirk Nowitzki, der 2007 zumwertvollsten Spielerder nordamerikanischen ProfiligaNBAgewählt wurde und 2011 als erster Deutscher mit denDallas Mavericksden NBA-Titel gewann, einer der besten Spieler der Welt. DieBasketball-Bundesligagibt es seit 1966, Rekordmeister sind dieBayer Giants Leverkusen. Seit 2007 gibt es dieProA- undProB-Liga. Bei derBasketball-Weltmeisterschaft 2023wurde Deutschland erstmals Weltmeister. BeimMotorsportrichtet sich das öffentliche Interesse besonders auf dieFormel 1und dieDTM, in denen deutsche Fahrer und Konstrukteure Spitzenerfolge erzielen konnten.Michael Schumacherist mit sieben Weltmeistertiteln der erfolgreichste aller Formel-1-Rennfahrer undSebastian Vettelder jüngste Weltmeister in der Formel-1-Geschichte, weiterer Weltmeister istNico Rosberg. ImRallyesportgelangte WeltmeisterWalter Röhrlzu Weltruhm. DerMotorradsport, in dem sich das Augenmerk auf dieStraßen-WMbzw.MotoGPrichtet, hat deutsche Weltmeister wieAnton Mang,Dirk Raudies,Stefan BradlundSandro Cortesehervorgebracht. Zu den herausragendenBoxerndes 20. Jahrhunderts gehörenMax SchmelingundHenry Maske, im frühen 21. JahrhundertFelix Sturm,Robert Stieglitz,Sebastian Sylvester,Jürgen Brähmer,Regina Halmichund dieKickboxerinChristine Theiss. Die ukrainischen BrüderVitaliundWladimir Klitschkosowie die armenischstämmigenArthur AbrahamundSusianna Kentikianwurden mehrfache Weltmeister in ihren Gewichtsklassen und durchlebten ihre Profikarrieren vollständig in deutschenBoxställen. Wintersport hat eine lange Tradition in Deutschland. ImBobsport,Rennrodeln,Biathlon,LanglaufundEisschnelllaufkonnten deutsche Sportler regelmäßig Medaillen bei Welt- undEuropameisterschaftensowie bei den Olympischen Spielen gewinnen. Weitere populäre Wintersportarten sind dasSkispringen, mitSven Hannawald,Jens WeißflogundSeverin Freundals besonders erfolgreichen Sportlern, sowie verschiedenealpine Skisportarten.Katarina Wittzählt mit zwei Olympiasiegen imEiskunstlaufzu den größten Sportlern ihrer Disziplin. Das Deutsche Reich war 1936 Austragungsort derOlympischen Sommerspiele 1936inBerlinund derWinterspieleinGarmisch-Partenkirchen, die Bundesrepublik Deutschland für dieSommerspiele 1972. Imewigen MedaillenspiegelderOlympischen Spielenimmt Deutschland mit 613 Goldmedaillen den zweiten Platz ein (2025). Tenniserlebte vor allem seit den 1980er- und 1990er-Jahren durch die Erfolge der deutschen SportlerBoris Becker,Steffi GrafundMichael Stichgroße Popularität. Im frühen 21. Jahrhundert gewannenTommy HaasundAngelique Kerbereinige Titel. Die Beliebtheit desStraßenradsportshing von den aktuellen Erfolgen deutscher Fahrer ab.Rudi Altig(in den 1960er-Jahren),Jan Ullrich(1996–2005/07) undAndré Greipel(2010er) zählen zu den erfolgreichsten Radsportlern ihrer Zeit. ImHallenradsportist Deutschland nach der Anzahl der Weltmeistertitel die führende Nation. Mit über 70 Prozent der gewonnenen WM-Titel ist Deutschland – die DDR eingeschlossen – führend in den DisziplinenRadballundKunstradfahren. DerDeutsche Schützenbundhat etwa 1,4 Millionen Mitglieder, deutschlandweit gibt es rund sechs MillionenSportschützen.[267] ImDeutschen Schachbundsind 2024 knapp 95.000 Mitglieder in rund 2250 Vereinen organisiert.[268]Von 1894 bis 1921 war der DeutscheEmanuel Laskerder zweite Schachweltmeister. Heute ist Deutschland in der Weltspitze mitVincent Keymervertreten. Große Erfolge konnten deutsche Sportler auch in weiteren Sportarten wieFechten,Reiten,Ringen,Rudern,Kanusport, imHockey, in derLeichtathletikund imSchwimmsporterreichen. Im Tischtennis gehörenTimo BollundDimitrij Ovtcharov, imGolfStephan Jäger,Bernhard LangerundEsther Henseleitzu den besten Spielern der Welt. DerDeutsche Golf-Verbandzählt mit 680.000 Golfspielern im Jahr 2023 zu den zehn größten Verbänden des deutschen Sports.[263]Zu den wichtigstenMarathonläufender Welt zählt der seit 1974 durchgeführteBerlin-Marathon.[269] In Deutschland werden 352Zeitungen, 27Wochenzeitungen, 7Sonntagszeitungen, 2450Publikums-und 3753Fachzeitschriftenregelmäßig publiziert.[270]Einen Teil dieser Medien geben die großen KonzerneAxel Springer SE,Bauer Media Group,Bertelsmann,Hubert Burda Mediaund dieFunke Mediengruppeheraus. Es gibt 18 Nachrichtenagenturen, von denen dieDeutsche Presse-Agentur(dpa) und dasRedaktionsnetzwerk Deutschland(RND) die bedeutendsten sind. Die auflagenstärksten überregionalenTageszeitungen(Stand 2020) sind dieBild(Aufl. 1,27 Mio.), dieSüddeutsche Zeitung(Aufl. 0,3 Mio.), dieFrankfurter Allgemeine Zeitung(Aufl. 0,2 Mio.) und dasHandelsblatt(Aufl. 0,14 Mio.). Die mit Abstand auflagenstärkste Wochenzeitung istDie Zeit(Aufl. 0,55 Mio.). Daneben gibt es politische Magazine wieDer Spiegelund auf populäre Themen ausgerichtete Magazine wieSternundFocus. In Deutschland gibt es 145Fernsehsender.[271]Im Fernsehen gibt esöffentlich-rechtliche SenderwieDas ErsteundZDFund privat finanzierte Vollprogramme, vor allemRTL,Sat.1,Pro7,RTL Zwei,Kabel EinsundVox. In den letzten Jahren sind viele regionale Sender und Spartenprogramme hinzugekommen. Der Hörfunk in Deutschland ist dual organisiert und vor allem regional geprägt. Er teilt sich auf in öffentlich-rechtlichen Hörfunk, der sich durch denRundfunkbeitragfinanziert, und private Radioanbieter, die ihre Erlöse überwiegend aus Werbung erzielen. Ende 2016 waren weit über 300 Rundfunkanbieter registriert, davon rund 290 kommerzielle und mehr als 60 öffentlich-rechtliche Programme derARD, überwiegend überUKWausgestrahlt, aber zunehmend auch überDAB. Von großer Bedeutung für die Entwicklung sind zwei Urteile des Bundesverfassungsgerichts von 1981 und 1986, welche die Organisation und die Rahmenbedingungen festlegten. Als Onlinemedien werdenSpiegel Online(wöchentliche Reichweite: 15 %),T-Online(wöchentliche Reichweite: 14 %) und die Nachrichtenportale der ARD (wöchentliche Reichweite: 13 %) am häufigsten genutzt. Die aktive und passive Mediennutzung beträgt täglich rund 9 Stunden (Stand 2018).[272] LautWorld Values Surveywerden in Deutschland, das sich auf diepluralistischeTradition derAufklärungstützt,säkular-rationaleWerte undpersönliche Selbstentfaltunggeschätzt. Die Bevölkerung nennt in den Bereichen Bildung, Work-Life-Balance, Beschäftigung, Umwelt, Sozialbeziehungen, Wohnen, Sicherheit und subjektives Wohlbefinden Zufriedenheitswerte über dem Durchschnitt der entwickelten Industrienationen und liegt nur bei Gesundheit darunter. Insgesamt lag Deutschland 2015 beimOECD Better Life Indexmit 7 von 10 Punkten über dem OECD-Schnitt (6,5; Griechenland 5,5, Schweiz 7,6).[273] ImWorld Happiness Report2018 der UN belegte Deutschland Platz 15 von 156 Ländern.[274] Deutschland hat eine lange Tradition des gesetzlich beförderten sozialen Ausgleichs. LautGini-Indexgilt das Land im internationalen Vergleich als Gesellschaft mit geringer Einkommensungleichheit. Der deutsche Staat bietet seinen Bewohnern umfangreiche rechtliche Ansprüche auf Familienförderung und soziale Absicherung. DieGeschichte der Sozialversicherungbegann im Kaiserreich. Spätere Regierungen haben sie nach und nach erweitert und um zusätzliche soziale Transferleistungen ergänzt, wodurch heute ein großer Teil desStaatshaushaltsfür Soziales aufgewendet wird. Für Arbeitnehmer besteht eine Pflichtmitgliedschaft in derSozialversicherung, die aus fünf Säulen besteht:Kranken-,Unfall-,Renten-,Pflege-undArbeitslosenversicherung. Die soziale Grundsicherung wird in erster Linie durch Beiträge der Versicherten finanziert, Defizite durch Steuergelder ausgeglichen. Im Jahr 2010 hatten in Deutschland 830.000 Euro-Millionäre(1 % der Bevölkerung) ein Gesamtvermögen von 2.191 Milliarden Euro, während rund 12,4 Millionen Menschen (15,3 % der Bevölkerung) inrelativer Armutlebten oder als armutsgefährdet galten.[276]2016 waren 19,7 Prozent der Bevölkerung von Armut oder sozialer Ausgrenzung bedroht (EU: 23,5 %).[277]2024 beklagte derEuroparat, dass die soziale Ungleichheit in Deutschland im Wachsen begriffen sei, und forderte die Regierung auf, mehr gegen Armut und Wohnungsnot zu unternehmen.[278][279] Zu den innerstaatlichenTransferleistungenzählt derLänderfinanzausgleich, der Bundesländer mit hohemSteueraufkommendazu verpflichtet, einen Teil ihrer Einnahmen an schlechter gestellte Länder abzugeben, damit die Lebensverhältnisse in Deutschland nicht zu weit auseinandergehen. Der auf die Einkommensteuer erhobeneSolidaritätszuschlagsoll teilungsbedingte Lasten in den neuen Ländern mildern. DasAllgemeine Gleichbehandlungsgesetzsoll Benachteiligungen aufgrund vonGeschlecht,Rasse, derethnischenHerkunft, derReligionoderWeltanschauung, einerBehinderung, desAltersoder dersexuellen Identität(etwaHomosexualität) verhindern. Das deutsche Gesundheitswesen ist hoch entwickelt, wie die sehr niedrige Rate derSäuglingssterblichkeitvon etwa 3,5 Jungen und 3,0 Mädchen bei 1000 Geburten[280]und eine hoheLebenserwartungdeutlich machen, die im Jahr 2016 bei 78,2 Jahren für Männer und bei 83,1 für Frauen lag.[281]Dabei hatten 2015 arme Männer eine Lebenserwartung von 70,1, wohlhabende von 80,9 Jahren (Frauen: 76,9 und 85,3 Jahre).[282]2015 ergab eine Studie derOrganisation für wirtschaftliche Zusammenarbeit und Entwicklung, Patienten in Deutschland hätten kurze Wartezeiten, geringen eigenen Finanzaufwand und viel Auswahl. DieVorbeugungsei hingegen verbesserungswürdig, was eine hohe Zahl von Krankheiten wieHerz-Kreislauf-ErkrankungenundDiabeteszeige. Die Qualität zeige sich aber unter anderem dadurch, dass einSchlaganfallhäufig überlebt werde. Die Zahl an Krankenhausaufenthalten und Operationen liege international in der Spitzengruppe, aber auch die Kosten für Medikamente; 2013 machten die Gesundheitsausgaben 11 Prozent des BIP aus (OECD-Schnitt: knapp 9 %).[283] DasGesundheitssystemumfasst die Leistungserbringer wie Ärzte, Apotheker, Pflegepersonal, den Staat (Bund, Länder undGemeinden), dieKranken-, Unfall-, Pflege- und Rentenversicherungen, dieKassenärztlichen Vereinigungen, die Arbeitgeber- und Arbeitnehmerverbände, weitere Interessenverbände sowie diePatienten, zum Teil vertreten durch Verbände undSelbsthilfeorganisationen.Krankenhäuserwerden häufig in gemeinnütziger Trägerschaft geführt, zunehmend jedoch privatisiert. Weitere Versorgungsleistungen werden weitgehend privat von Freiberuflern erbracht (niedergelassene Ärzte und Apotheker und Unternehmen, beispielsweise der pharmazeutischen und medizintechnischen Industrie). Der Staat beteiligt sich als Leistungserbringer nur nachrangig mit Gesundheitsämtern, kommunalen Krankenhäusern und Hochschulkliniken. Der Großteil der Bevölkerung gehört dergesetzlichen Krankenversicherung(GKV) an, deren Beiträge sich hauptsächlich an der Einkommenshöhe orientieren. Familienmitglieder ohne eigenes Einkommen sind oft beitragsfrei mitversichert. Der Leistungsanspruch ist unabhängig von der Beitragshöhe. Etwa 10,8 Prozent der Versicherten waren 2017privat krankenversichert.[284] Das heutige deutsche Bildungswesen hat seine Wurzeln unter anderem im weltweit einstmals vorbildhaftenhumboldtschen Bildungsidealund denpreußischen Bildungsreformen. Seine Ausgestaltung liegt in der Verantwortung der Länder („Kulturhoheit“), wird jedoch durch bundesweite Konferenzen derKultusministerkoordiniert, die auch gemeinsameBildungsstandardssetzen. Je nach Bundesland gibt esVorschulzeitenund es besteht eine neun- bis dreizehnjährigeSchulpflicht. Der Besuch derallgemeinbildenden Schulendauert mindestens neun Jahre. Danach könnenweiterführende Schulenbzw.berufsbildende Schulenbesucht werden. Die meisten deutschen Bundesländer haben eingegliedertes SchulsystemmitHauptschule,RealschuleundGymnasium, es gibt jedoch Tendenzen zu mehrGesamtschulenundGanztagsschulen. DieHochschulreifewird – je nach Bundesland – nach zwölf oder dreizehn Schuljahren erworben. Praktisch alle jungen Erwachsenen besuchen nach der Schule eine weiterführende Bildungseinrichtung.Auszubildendein Betrieben besuchen in der Regel an ein oder zwei Tagen in der Woche dieBerufsschule, was als Erfolgsmodell derdualen Ausbildungweltweit bekannt ist. Die akademische Entsprechung ist dasduale Studium.Studierendekönnen zwischenuniversitärenund anwendungsorientiertenHochschulen(Fachhochschulen) wählen. DieAkademikerquotestieg seit den 1970er-Jahren stetig an. Auch die berufliche Weiterbildung spielt eine große Rolle. Für Arbeitslose stellt dieBundesagentur für ArbeitWeiterbildungsgutscheine bereit. Vor ihrer beruflichen Ausbildung können Jugendliche außerdem sogenannte Freiwilligendienste, wie einfreiwilliges soziales Jahroder einfreiwilliges ökologisches Jahr, absolvieren. Weitere populäre Übergangsaktivitäten sind derFreiwillige Wehrdienstund Auslandsaufenthalte, etwa in Form vonWork & TraveloderJugendaustausch. BeiSchulleistungsuntersuchungenschneidet Deutschland im weltweiten Vergleich häufig nur mittelmäßig oder sogar unterdurchschnittlich ab. In den letztenPISA-Studienkonnte Deutschland sich verbessern: Im PISA-Ranking von 2015 erreichten deutsche Schüler Platz 16 von 72 in Mathematik, Platz 15 in Naturwissenschaften und Platz 10 beim Leseverständnis. Die Leistungen deutscher Schüler lagen damit in allen drei Kategorien über demOECD-Durchschnitt.[285][286]Die OECD kritisiert allerdings in den PISA-Studien die deutscheBildungspolitik, da insbesondere die Schulerfolge von Kindern mit sozial- oder bildungsschwachem Elternhaus und mit Migrationshintergrund unter dem Durchschnitt lägen. Entgegen den Reformbemühungen der letzten Jahrzehnte ist es weiterhin statistisch signifikant unwahrscheinlicher, dassArbeiterkinderdasAbitur(Allgemeine Hochschulreife) oder einenHochschulabschlusserreichen als Kinder aus denMittel-oderOberschichten. Zudem würde es an individuellerDifferenzierungund Förderung sowohl bei leistungsstarken als auch -schwachen Schülern mangeln. Die Ausgaben für Bildung (4,6 % des Bruttoinlandsprodukts) liegen im OECD-Vergleich unter dem Durchschnitt. Die schulische Förderung im Grundschulalter gilt als verbesserungswürdig, insbesondere was Betreuungsmöglichkeiten und gezielte Förderung schwächerer Schüler angeht. Von der erwerbsfähigen Bevölkerung galten 2011 etwa 2,3 Millionen (4 %) als vollständige und 7,5 Millionen als funktionaleAnalphabeten.[287] Deutschland ist ein international bedeutender Technologie- und Wissenschaftsstandort. Seit derindustriellen Revolutionwaren deutschsprachige Forscher bei der Gründung empirischer Wissenschaften maßgeblich beteiligt. Insbesondere die wirtschaftliche Leistungsfähigkeit verschiedenster Industrien und derWissenstransferin die Praxis wurde durch die kreative Arbeit von Ingenieuren vorangetrieben. Rund 8 Prozent aller weltweit gemäßPCTangemeldetenPatenteim Jahr 2016 kamen aus Deutschland; damit rangierte Deutschland auf Platz 4 hinter den USA, Japan und China.[288] In Deutschland sindUniversitäten,Technische UniversitätenundFachhochschulenEinrichtungen der Forschung und wissenschaftlichen Lehre. Die (Technischen) Universitäten sind zuPromotions- undHabilitationsverfahrenberechtigt. Beide Verfahren sollen Bildung nachweisen und wissenschaftliche Erkenntnisse enthalten. Mit der Einführung internationaler Abschlussbezeichnungen im Zuge desBologna-Prozesseswird imakademischen Bildungsbereichdie bisherige Trennung der Abschlüsse zwischen Fachhochschulen und Universitäten aufgeweicht. Einzelne Hochschuleinrichtungen bilden überhaupt nicht im tertiären Bildungsbereich aus, sondern sind zur postgradualen Bildung oder ausschließlich zur Promotion und Habilitation eingerichtet. Die meisten deutschen Hochschulen sind in öffentlicher Trägerschaft, werden aber in ihrer Forschung über Drittmittel finanziert (Deutsche Forschungsgemeinschaft,Stiftungen, Unternehmen und andere). Neben den Universitäten gibt es eine größere Anzahl von Forschungsorganisationen, die deutschlandweit und darüber hinaus tätig sind. Dabei wurde in Deutschland zum einen ein System der Arbeitsteilung der Universitäten untereinander und zum anderen eines zwischen den Universitäten und den außeruniversitären Forschungseinrichtungen geschaffen. DieMax-Planck-Gesellschaftverpflichtet sich derGrundlagenforschung. Sie führt 79 Institute in Deutschland und besitzt ein Jahresbudget von 1,8 Milliarden Euro. DieHelmholtz-Gemeinschaftist die größte wissenschaftliche Gesellschaft in Deutschland und betreibt 15 sogenannte Großforschungszentren, die fächerübergreifend an wissenschaftlichen Komplexen arbeiten. DieFraunhofer-Gesellschaftist die größte Organisation derangewandten Forschung. Sie greift in ihren 56 Instituten Ergebnisse der Grundlagenforschung auf und versucht sie wirtschaftlich zu erschließen. Sie stellt der Wirtschaft die Dienstleistung der Auftragsforschung bereit. Weltweite Bekanntheit erlangte sie durch die Entwicklung desMP3-Audioformats. Sie gehört zu den wichtigstenPatentanmeldernund -besitzern in Deutschland. DieLeibniz-Gemeinschaftist ein Verbund eigenständiger Forschungseinrichtungen, die sowohl in der Grundlagenforschung als auch in der angewandten Forschung arbeiten. Die Ausgaben der staatlichen Universitäten und Hochschulen in Deutschland (auch alstertiärer Bildungsbereich in Deutschlandbezeichnet) betrugen im Jahr 2020 über 64 Milliarden Euro (im Jahr 2005: 30,9 Milliarden Euro), die vor allem aus Steuereinnahmen des Bundes und der Länder finanziert sind.[289]An den Universitäten und Hochschulen in Deutschland studierten im Jahr 2020 etwa 2,9 Millionen Studenten.[290]Davon waren etwa 14 % ausländische Studenten.[291] Weitere 15,6 Milliarden Euro erhielten außeruniversitäre Institute wie die Fraunhofer-Gesellschaft, Helmholtz-Gemeinschaft, Leibniz-Gemeinschaft, Max-Planck-Gesellschaft oder die Akademien der Wissenschaften.[292]Die gesamten Ausgaben für Bildung, Forschung und Wissenschaft betrugen im Jahr 2020 etwa 334 Milliarden Euro in Deutschland.[293] Aus Deutschland stammen zahlreiche Forscher aus allen Bereichen der modernen Wissenschaften. Mehr als 100Nobelpreisträgerwerden dem Land zugeordnet.Albert EinsteinundMax Planckbegründeten mit ihren Theorien wichtige Säulen dertheoretischen Physik, auf denen beispielsweiseWerner HeisenbergundMax Bornweiter aufbauen konnten.Wilhelm Conrad Röntgen, der erste Physik-Nobelpreisträger, entdeckte und untersuchte die nach ihm benannteRöntgenstrahlung, die noch heute eine wichtige Rolle unter anderem in der medizinischen Diagnostik und der Werkstoffprüfung spielt.Heinrich Hertzschrieb bedeutende Arbeiten zur elektromagnetischen Strahlung, die für die heutigeTelekommunikationstechnikmaßgeblich sind. Die Entwicklungen vonKarl von Drais,Nikolaus Otto,Rudolf Diesel,Gottlieb DaimlerundCarl Benzhaben das Verkehrswesen revolutioniert, die nach ihren Erfindern benanntenBunsenbrennerundZeppelinesind weltweit ein Begriff. Diedeutsche Raumfahrtleistete entscheidende Pionierarbeit im Bereich der Raumfahrt und derWeltraumforschungund besitzt heute mit demDeutschen Zentrum für Luft- und Raumfahrt(DLR) eine leistungsfähige Raumfahrtagentur, zudem ist Deutschland das am meisten zurEuropäischen Weltraumorganisation(ESA) beitragende Mitgliedsland.[294] Die chemische Forschung wurde unter anderem vonCarl Wilhelm Scheele,Otto HahnundJustus von Liebigmitgeprägt. Mit ihren erfolgreichen Erfindungen sind Namen wieJohannes Gutenberg,Werner von Siemens,Wernher von Braun,Konrad ZuseundPhilipp ReisBestandteile der technologischen Allgemeinbildung. Auch viele bedeutende Mathematiker wurden in Deutschland geboren, so zum BeispielAdam Ries,Friedrich Bessel,Richard Dedekind,Carl Friedrich Gauß,David Hilbert,Emmy Noether,Bernhard Riemann,Karl WeierstraßundJohannes Müller (Regiomontanus). Weitere wichtige deutsche Forscher und Wissenschaftler sind der AstronomJohannes Kepler, der ArchäologeHeinrich Schliemann, die BiologinChristiane Nüsslein-Volhard, der UniversalgelehrteGottfried Wilhelm Leibniz, der NaturforscherAlexander von Humboldt, der ReligionsforscherMax Müller, der HistorikerTheodor Mommsen, der SoziologeMax Weberund der MedizinforscherRobert Koch. 51.16510.455277777778Koordinaten:51°N,10°O Dieser Artikel ist als Audioversion verfügbar: Mehr Informationen zur gesprochenen Wikipedia Flagge des Norddeutschen BundesNorddeutscher Bund Flagge des Deutschen KaiserreichsDeutsches Kaiserreich|Flagge der Weimarer RepublikWeimarer Republik|Flagge des Deutschen Reiches von 1935 bis 1945Nationalsozialistisches Deutschland|Erkennungsflagge für deutsche Handelsschiffe von 1946 bis 1950Alliierte Verwaltung BundesdienstflaggeBundesrepublik Deutschland|Flagge der DDRDeutsche Demokratische Republik Flagge DeutschlandsBundesrepublik Deutschland(vereintes Deutschland) Albanien|Andorra|Belarus|Belgien|Bosnien und Herzegowina|Bulgarien|Dänemark2|Deutschland|Estland|Finnland|Frankreich2|Griechenland|Irland|Island|Italien2|Kasachstan1|Kroatien|Lettland|Liechtenstein|Litauen|Luxemburg|Malta|Moldau|Monaco|Montenegro|Niederlande2|Nordmazedonien|Norwegen2|Österreich|Polen|Portugal2|Rumänien|Russland1|San Marino|Schweden|Schweiz|Serbien|Slowakei|Slowenien|Spanien2|Tschechien|Türkei1|Ukraine|Ungarn|Vatikanstadt|Vereinigtes Königreich2 Färöer|Gibraltar|Guernsey|Isle of Man|Jersey Belgien|Bulgarien|Dänemark|Deutschland|Estland|Finnland|Frankreich|Griechenland|Irland|Italien|Kroatien|Lettland|Litauen|Luxemburg|Malta|Niederlande|Österreich|Polen|Portugal|Rumänien|Schweden|Slowakei|Slowenien|Spanien|Tschechien|Ungarn|Zypern G6:Deutschland|Frankreich|Vereinigtes Königreich|Italien|Japan|Vereinigte Staaten G7:Kanada| (G8:Russland– Mitgliedschaft suspendiert) Australien|Belgien|Chile|Costa Rica|Dänemark|Deutschland|Estland|Finnland|Frankreich|Griechenland|Irland|Island|Israel|Italien|Japan|Kanada|Kolumbien|Lettland|Litauen|Luxemburg|Mexiko|Neuseeland|Niederlande|Norwegen|Österreich|Polen|Portugal|Schweden|Schweiz|Südkorea|Slowakei|Slowenien|Spanien|Tschechien|Türkei|Ungarn|Vereinigtes Königreich|Vereinigte Staaten Albanien|Andorra|Armenien|Aserbaidschan|Belarus|Belgien|Bosnien und Herzegowina|Bulgarien|Dänemark|Deutschland|Estland|Finnland|Frankreich|Georgien|Griechenland|Heiliger Stuhl|Irland|Island|Italien|Kanada|Kasachstan|Kirgisistan|Kroatien|Lettland|Liechtenstein|Litauen|Luxemburg|Malta|Moldau|Monaco|Mongolei|Montenegro|Niederlande|Nordmazedonien|Norwegen|Österreich|Polen|Portugal|Rumänien|Russland|San Marino|Schweden|Schweiz|Serbien|Slowakei|Slowenien|Spanien|Tadschikistan|Tschechien|Türkei|Turkmenistan|Ukraine|Ungarn|Usbekistan|Vereinigtes Königreich|Vereinigte Staaten|Zypern Afghanistan|Australien|Ägypten|Algerien|Israel|Japan|Jordanien|Marokko|Tunesien|Südkorea|Thailand AlbanienAlbanien|BelgienBelgien|BulgarienBulgarien|DanemarkDänemark|DeutschlandDeutschland|EstlandEstland|FinnlandFinnland|FrankreichFrankreich|GriechenlandGriechenland|IslandIsland|ItalienItalien|KanadaKanada|KroatienKroatien|LettlandLettland|LitauenLitauen|LuxemburgLuxemburg|MontenegroMontenegro|NiederlandeNiederlande|NordmazedonienNordmazedonien|NorwegenNorwegen|PolenPolen|PortugalPortugal|RumänienRumänien|SchwedenSchweden|SlowakeiSlowakei|SlowenienSlowenien|SpanienSpanien|TschechienTschechien|TurkeiTürkei|UngarnUngarn|Vereinigtes KonigreichVereinigtes Königreich|Vereinigte StaatenVereinigte Staaten Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Begriffsgeschichte: Deutsch und Deutschland 2Geographie 2.1Physische Geographie 2.1.1Geologie 2.1.2Relief 2.1.3Klima 2.1.4Klimawandel 2.1.5Gewässer 2.1.6Inseln 2.2Flora 2.3Fauna 2.4Humangeographie 2.4.1Verwaltungsgliederung 2.4.2Ballungsgebiete 3Bevölkerung 3.1Demografie 3.2Sprachen 3.3Religionen 3.3.1Traditionen 3.3.2Verhältnis von Staat und Religion 3.3.3Bevölkerungsanteile 4Geschichte 4.1Urgeschichte, Kelten, Germanen und Römer 4.2Völkerwanderung und Frühmittelalter (375–962) 4.3Vom Ostfrankenreich zum Heiligen Römischen Reich (962–1806) 4.4Rheinbund, Deutscher Bund, Norddeutscher Bund (1806–1871) 4.5Deutsches Reich (1871–1945) 4.5.1Deutsches Kaiserreich (1871–1918) 4.5.2Weimarer Republik (1919–1933) 4.5.3Nationalsozialistische Diktatur (1933–1945) 4.6Deutschland unter alliierter Besatzung (1945–1949)"
  },
  {
    "label": 0,
    "text": "Filme – Wikipedia Filme Inhaltsverzeichnis Weblinks Filme – Neues und Altes vom Kinowar eineFilmzeitschriftausDeutschland, die von 1980 bis 1982 erschien. Sie wurden von denFilmkritikernundJournalistenJochen Brunow, Antje Goldau,Norbert Grobund Norbert Jochum gegründet. Das Heft im FormatA4und Schwarz-Weiß-Druck erschien im Verlag Volker Spiess in Berlin und kostete 6 Mark. In der kurzlebigen Zeitschrift wurden Filmkritiken, Berichte zu Festivals, Artikel über Regisseure und Schauspieler sowie Hintergrundinformationen zur Film- und Kinoszene allgemein veröffentlicht. Heft 1 erschien am 15. Februar 1980, die weiteren Ausgaben folgen im Abstand von je zwei Monaten. Ab 1981 erschienFilmedann viermal im Jahr, Anfang 1982 wurde die Zeitschrift mit der Ausgabe 13 eingestellt. Heft 12 enthielt ein separates Register, in dem Personen, Autoren, Filmtitel etc. zum Nachschlagen aufgelistet waren. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Weblinks Links hinzufügen Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Wikidata-Datenobjekt Jochen Brunow ISSN 0173-5101 Filmzeitschrift Deutschland Filmkritikern Journalisten Jochen Brunow Norbert Grob"
  },
  {
    "label": 0,
    "text": "Gastronomie – Wikipedia Gastronomie Inhaltsverzeichnis Wortherkunft Arten und Leistungen der Gastronomie Konzepte Betriebsarten Siehe auch Literatur Weblinks Einzelnachweise DieGastronomieist jener Teilbereich desGastgewerbes, der sich mit der Bewirtung vonGästenbefasst. Im Gegensatz zu denGaststättenbefriedigt Gastronomie nicht nur dieBedürfnisseHungerundDurst, sondern auch den kulturellen Bedarf anErlebnisundKommunikation.[1][2][3]Gastronomie ist eine Sonderform derGemeinschaftsverpflegung. DerInternationalismusGastronomielässt sich auf die eigentliche Bedeutung ‚Magenkunde‘ (altgriechischγαστρονομίαgastronomía) zurückführen. „Gastronomía“ setzt sich zusammen ausγαστήρgastḗr,Genitivγαστρόςgastrós(deutsch: „Bauch, Magen“) und der Wortendung-nomia(‚Fachgebiet‘). Ursprünglich wurde es entlehnt ausγαστρολογίαgastrología(‚Lehre von der Pflege des Bauches‘).[4] Einzug in die deutsche Sprache hat der BegriffGastronomieim 19. Jahrhundert gehalten;[5]er stand für die gehobene Gastronomie oder für dieKochkunst. Die Verbreitung des Begriffs wurde wohl durch die Ähnlichkeit mit demetymologischnicht verwandten deutschen WortGastbegünstigt. Es gibt verschiedene Arten der Gastronomie, wobei der Schwerpunkt auf verschiedene Leistungen gelegt wird:Bars,Bistros,Hotels,Kneipen,Restaurants, aber auchImbisshallenundVeranstaltungenbieten Gastronomie an.[6][7]Unterschieden wird außer zwischen verschiedenen Betriebstypen auch zwischen derInnengastronomiein Gebäuden und derAußengastronomieim Freien. Eine gastronomische Leistung ist ein Zusammenspiel aus verschiedenen Faktoren: Aus der traditionellen Gastronomie, welche die Gäste nur mit Speisen und Getränken versorgte, haben sich neue Formen entwickelt. Während den Gast in derSystemgastronomiegenau definierte Standards erwarten, stellt dieErlebnisgastronomiezusätzlich Unterhaltung in den Vordergrund. Dies sind unterschiedliche Konzepte, welche sich auf Gebiete spezialisieren wie: Eine weitere Einteilung der Gastronomiebetriebe kann nach ihren hauptsächlichen Betriebsarten erfolgen: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Wortherkunft 2Arten und Leistungen der Gastronomie 3Konzepte 4Betriebsarten 5Siehe auch 6Literatur 7Weblinks 8Einzelnachweise العربية Беларуская کوردی English فارسی Français עברית Հայերեն Igbo Italiano 日本語 한국어 Bahasa Melayu Русский Tagalog Українська Oʻzbekcha / ўзбекча Tiếng Việt Winaray Links bearbeiten Artikel Diskussion Lesen"
  },
  {
    "label": 0,
    "text": "Geographie – Wikipedia Geographie Inhaltsverzeichnis Schreibweise Geschichte Ordnungsschema Didaktik Methodologie Ästhetische Dimension Siehe auch Literatur Weblinks Anmerkungen Einzelnachweise Antike und Mittelalter Frühe Neuzeit Etablierung als eigenständige Disziplin Neuere Entwicklungen Das Drei-Säulen-Modell der Geographie Allgemeine und Regionale Geographie Theoretische und Angewandte Geographie Allgemeines Vergleichende Geographie Geoinformatik Überblicks- und Nachschlagewerke Fachgeschichte und -theorie Verbände und Institutionen Informationsangebote Physische Geographie Humangeographie Mensch-Umwelt-Beziehungen DieGeographiebzw.Geografie(vonaltgriechischγεωγραφίαgeōgraphía„Erdbeschreibung“;[1]abgeleitet vonγῆgē„Erde“ und-graphie) oderErdkundeist die sich mit derErdoberflächebefassendeWissenschaft, sowohl in ihrer physischen Beschaffenheit wie auch als Raum und Ort des menschlichen Lebens und Handelns.[2][3]Sie bewegt sich dabei an der Schnittstelle zwischen denNaturwissenschaften,Geistes-undSozialwissenschaften. Gegenstand der Geographie ist die Erfassung, Beschreibung und Erklärung der Strukturen, Prozesse und Wechselwirkungen in derGeosphäre. Die physikalische, chemische und biologische Erforschung ihrer Einzelerscheinungen ist Gegenstand spezialisierterGeowissenschaften. Bis zur amtlichen Neuregelung der deutschen Rechtschreibung war ausschließlich die SchreibweiseGeographierichtig. Ab 1996 war auchGeografiezulässig, wobeiGeographieim amtlichen Wörterverzeichnis zunächst als Hauptvariante verzeichnet war und seit 2004 keine der Schreibweisen als zu bevorzugen angegeben ist (die Unterscheidung zwischen Haupt- und Nebenvarianten ist im amtlichen Wörterverzeichnis generell fallen gelassen worden). Im amtlichen Regelteil wird vomRat für deutsche Rechtschreibungjedoch ausschließlich die Variante „Geografie“ benutzt.[4]ImDuden(27. Auflage) ist die VarianteGeografieals „Dudenempfehlung“ gekennzeichnet. Traditionell wird in wissenschaftlichen Texten und unter Fachleuten weiterhin häufig die alte Schreibweise genutzt. So empfahl das Präsidium derDeutschen Gesellschaft für Geographieim Jahr 2003 einstimmig, die SchreibweiseGeographiebeizubehalten.[5]Der Hinweis auf die Empfehlung wurde jedoch 2017 kommentarlos von der Website entfernt. Die Bedeutung geographischen Wissens wurde, soweit historisch überliefert, erstmals in derAntikevon denGriechenerkannt. Vom NaturphilosophenAnaximanderausMiletwird berichtet, dass er als erster um 550 v. Chr. eineKarte der Erdeund derMeereskizzierte.HerodotvonHalikarnassos(484–424 v. Chr.) verfasste eine Vielzahl geographischer Berichte. Die EroberungenAlexander des Großenöffneten den Blick der griechischen Gelehrten bis weit nachAsienhinein. Es entstandenItinerarien, also Beschreibungen der Straßen und Verzeichnisse der Stationen auf Reisen, sowiePeriploi, praktische Reisehandbücher fürSeefahrerundKaufleute, die oft auf persischen oderparthischenQuellen fußten. Mit zunehmender Fernreisetätigkeit nahmen auch die Versuche der Erkundung der Gesamtgestalt der Welt zu. Neben der physikalischen Geographie und derKulturgeographieentwickelten sich Anfänge einermathematischen Geographie. EineBerechnung des Erdumfangsgelang erstmalsEratosthenes(ca. 273–194 v. Chr.), während der um die Zeitenwende lebendeStraboneines der heute am besten erhaltenen geographischen Werke der Antike verfasste. Der AstronomClaudius Ptolemäus(ca. 100–170) sammeltetopografischesWissen vonSeefahrernund gab Anleitungen für das Zeichnen vonLandkarten. Die Erkenntnisse derGriechennutzten dieRömerweiter. Während desMittelaltersgeriet die Geographie, wie andere Wissenschaftszweige auch, inEuropaweitgehend in Vergessenheit. Neue Impulse kamen jedoch aus demKaiserreich Chinaund der aufstrebendenGeographie und Kartographie im mittelalterlichen Islam. Frühe theoretische Ansätze lieferteAlbertus Magnus: In seiner AbhandlungDe natura locorumbeschrieb er die Abhängigkeit der Eigenschaften eines Ortes von seiner geographischen Lage. Im Anschluss daran führte der Wiener AstronomGeorg Tannstetterdie physikalische Geographie in den Kreis der universitären Lehrgegenstände ein (1514).[6] Die neuzeitliche Geographie wurde vonBartholomäus Keckermann(1572–1608) undBernhard Varenius(1622–1650) begründet. Sie entwickelten ein Begriffssystem, unterschieden „Allgemeine Geographie“(geographia generalis)und die „Regionale Geographie“ beziehungsweiseLänderkunde(geographia specialis). Sie sahen Völker, Staaten und Orte in einem räumlichen,historischenund auchreligiösenKontext. Zu Beginn des 18. Jahrhunderts beförderten inDeutschlandvor allemJohann Hübner(1668–1731) undJohann Gottfried GregoriialiasMelissantes(1685–1770) durch ihre Lehrbücher, thematischenLexikaundAtlantendie Verbreitung der Geographie in weite Teile derbildungsnahenBevölkerung. Das Zeitalter derAufklärungförderte Erklärungsversuche von Naturerscheinungen durchWissenschaftlerwieJohann Gottfried Herder(1744–1803) undGeorg Forster(1754–1794).Anton Friedrich Büsching(1724–1793) verfasste die elfbändigeNeue Erdbeschreibungmit Beschreibungen derLänderund derenWirtschaft. Alexander von Humboldt(1769–1859) undCarl Ritter(1779–1859) begründeten schließlich die moderne wissenschaftliche Geographie, deren ursprüngliches länder- und landschaftskundliches Forschungsprogramm auf Herders Kulturtheorie basiert.[7]Im Laufe des 19. Jahrhunderts gründeten sich zunächst vielerorts „geographische Gesellschaften“, während die universitäre Institutionalisierung des Fachs vor allem mit der Gründung desDeutschen Reichsvorangetrieben wurde. Ferdinand von Richthofen(1833–1905)[8]definierte die Geographie zu jener Zeit als „Wissenschaft von derErdoberflächeund den mit ihr in ursächlichem Zusammenhang stehenden Dingen und Erscheinungen“.[9]DiesergeodeterministischeBetrachtung standen das vonPaul Vidal de la Blache(1845–1918) geprägte Konzept desPossibilismussowie die vonAlfred Hettner(1859–1941) formulierteChorologiegegenüber.[10]Einzelne Fachvertreter wieÉlisée Reclus(1830–1905) knüpften früh Verbindungen zur aufkommendenSoziologie. Auch belegt etwa das Entstehen der erstenNationalparks, dass der prägende Einfluss des Menschen auf seineUmweltnicht nur bekannt, sondern auch vonpolitischerBedeutung war. Insbesondere die deutsche Geographie war aber letztendlich vonsozialdarwinistischund völkisch argumentierenden Vertretern wieAlfred Kirchhoff(1838–1907), dem als Begründer derHumangeographiegeltendenFriedrich Ratzel(1844–1904)[11]und dem GeomorphologenAlbrecht Penck(1859–1945)[12]bestimmt. Anwendung fanden diese Ansichten schließlich vor allem durch dieGeopolitik, wie sie insbesondere durchHalford Mackinder(1861–1947) undKarl Haushofer(1869–1946) formuliert worden war. Nach demZweiten Weltkriegwandte sich die geographische Forschung imdeutschsprachigenRaum zunächst Themengebieten von relativ geringer politischer Brisanz zu.Carl Troll(1899–1975),Karlheinz Paffen(1914–1983),Ernst Neef(1908–1984) undJosef Schmithüsen(1909–1984) entwickelten dieLandschaftsökologie,Hans Bobek(1903–1990) undWolfgang Hartke(1908–1997) die Sozialgeographie weiter. Eine stärker an den Erfordernissen derRaumplanungorientierte, nicht zuletzt auf den Werken vonWalter Christaller(1893–1969) aufbauende Geographie wurde dagegen zunächst inSchwedendurchTorsten Hägerstrand(1916–2004) und imanglo-amerikanischen Raumetabliert. Seit Ende der 1960er Jahre (Quantitative Revolution) versteht sich auch die deutschsprachige Geographie zunehmend als angewandteWissenschaftund sucht ihre Themen im Zusammenhang mitStädtebau, Entwicklung des ländlichen Raumes, Raumplanung oder demUmweltschutz.[13]Gleichzeitig trägt die Entstehung einer sich alskritisch verstehenden Geographiedieser neuerlich übernommenen gesellschaftspolitischen Verantwortung Rechnung. Durch die wachsende Spezialisierung im 20. Jahrhundert entstand die Vielfalt der heutigen Teildisziplinen und die Aufteilung zwischenPhysischer GeographieundHumangeographie. Es existieren verschiedene Versuche, die Geographie schematisch zu ordnen. Die im heutigen Wissenschaftsbetrieb bedeutsamste ist die Einteilung in die beiden großen Teilgebiete derPhysischen Geographieund derHumangeographienebst einem interdisziplinären Bereich als dritter „Säule“.[14]Es lassen sich jeweils diverse Unterdisziplinen identifizieren, wobei die Teilbereiche der physischen Geographie insgesamt relativ stark in die übergeordneten naturwissenschaftlichen Disziplinen integriert sind, während diejenigen der Humangeographie wiederum untereinander eng vernetzt sind. DiePhysische Geographie(oderPhysiogeographie) beschäftigt sich in erster Linie mit den natürlichen Bestandteilen und Strukturen der Erdoberfläche. Dabei wird die Tätigkeit des Menschen zur Erklärung der Landschaftsgenese auch behandelt. Teilgebiete der Physischen Geographie sind unter anderem: DieHumangeographie(auchAnthropogeographie, seltenKulturgeographie) beschäftigt sich sowohl mit dem Einfluss desMenschenauf dengeographischen Raum, als auch mit dem Einfluss des Raums auf den Menschen − beispielsweise im Zusammenhang mit der räumlichen Verteilung von Bevölkerung oder Wirtschaftsgütern. Ehemals als Teil der Geisteswissenschaften aufgefasst, hat sie sich insbesondere seit den 1980er Jahren(spatial turn)denGesellschaftswissenschaftenangenähert.Hartmut Leser(2001) definiert die Humangeographie als denjenigen „Teilbereich der Allgemeinen Geographie, der sich mit der Raumwirksamkeit des Menschen und mit der von ihm gestaltetenKulturlandschaftund ihren Elementen in ihrer räumlichen Differenzierung und Entwicklung befasst.“ DieSozialgeographieund dieKulturgeographiegelten dabei als „Kerngebiete“ der Humangeographie, da sie alle weiteren Unterdisziplinen berühren. Teilweise werden diese Begriffe auch als Synonym für die Humangeographie im Ganzen verwendet.[Anm. 1]Auch diepolitische Geographie, zumal in ihrer damaligen Anwendung alsGeopolitikundMilitärgeographie, ist eng in die Gründungsgeschichte der Humangeographie verwoben, bildet heute aber eine eigenständige Fachrichtung. Weitere sozialwissenschaftlich orientierte Bereiche der Geographie stellen dieBevölkerungsgeographie, dieBildungsgeographieund dieReligionsgeographiedar. Einige andere Unterdisziplinen, die diesem Fächerspektrum zugerechnet werden können, werden im deutschen Sprachraum allerdings nur in geringem Maß oder als Teil anderer sozialwissenschaftlicher Fachrichtungen betrieben. Dazu gehören unter anderem dieKriminalgeographie, dieSprachgeographiemit der Dialektgeographie und dieWahlgeographie. Zu den klassischen Teilgebieten der Humangeographie zählen jene Unterdisziplinen, die sich mit der vom Menschen errichtetengebauten Umweltbefassen, also dieSiedlungsgeographie, dieGeographie ländlicher Räume, dieStadtgeographieund dieVerkehrsgeographie. Letztere wird teilweise auch zurWirtschaftsgeographiegerechnet, die außerdem die Geographie des primären (Agrargeographie), sekundären (Industriegeographie) und tertiären Wirtschaftssektors (Handelsgeographie,Tourismusgeographie) umfasst. Als Querschnittsdisziplin kommt zudem derEnergiegeographiewachsende Bedeutung zu. Eine Sonderstellung nimmt dieHistorische Geographieein. Ursprünglich vor allem mitgenetischerSiedlungsforschung beschäftigt und damit humangeographisch orientiert, ist das Fach inzwischen relativ stark interdisziplinär integriert und insbesondere eng mit derUmweltgeschichteverbunden. Klassische Anwendungsbereiche sind die Kulturlandschaftsforschung, Waldgeschichte, Wüstenbildungsforschung oder Flusslaufdokumentation. Dieraum-zeitlicheAusbreitung von Phänomenen ist Gegenstand der geographischenDiffusionsforschung. Auch wenn sich natur- und geistes- bzw. sozialwissenschaftlich orientierte Ansätze der Geographie inzwischen in ihrer methodischen Vorgehensweise stark voneinander unterscheiden, ergeben sich hinsichtlich der Fragestellungen weiterhin Überschneidungen. Da diese vor allem die Folgen menschlichen Handelns auf die Natur und deren Rückwirkung auf die Gesellschaft betreffen, wurde dieser derHumanökologienahestehende Teilbereich teilweise alsphysische Anthropogeographiebezeichnet, ein Begriff von allgemeiner Verwendung existiert jedoch nicht. Eng eingebunden ist die Geographie auch in die interdisziplinäre Erforschung von spezifischen Mensch-Umwelt-Systemen wie dieGebirgs-,Küsten-,Polar-,Tropen-undWüstenforschung. Eine traditionelle Einteilung hingegen ist jene in dieAllgemeine Geographieund dieRegionale Geographie, wie sie etwa im länderkundlichen Schema vonAlfred Hettnermodellhaft dargestellt wurde. DieAllgemeine Geographieist demnach der Teil der Geographie, welcher sichnomothetischmit denGeofaktorender Erdoberfläche (Geosphäre) beschäftigt. Im Mittelpunkt stehen zumeist ein Geofaktor (z. B. Wasser, Boden, Klima etc.) und dessen Wechselwirkungen mit anderen Geofaktoren. Die allgemeine Geographie beschäftigt sich somit mit allgemeinen Gesetzmäßigkeiten in der gesamten Geosphäre. Physische Geographie und Humangeographie sind dann lediglich Teile der Allgemeinen Geographie. Regionalgeographieoder Regionale Geographie (Spezielle Geographie) wird gemäß dieser Unterteilung als jener Teil der Geographie verstanden, welcher sichidiographischodertypologischmit bestimmten Teilgebieten der Erdoberfläche (Geosphäre) beschäftigt. Im Mittelpunkt steht somit eineRegion, z. B. einLandoder eineLandschaft, die wissenschaftlich in Bezug auf Raum und Zeit, abiotische und biotische Faktoren, den Menschen und Wechselwirkungen untersucht wird. Räumliche Elemente, Strukturen, Prozesse und Funktionsweisen (Wechselwirkungen zwischen den Geofaktoren) werden erfasst, klassiert und erklärt. Die Regionalgeographie lässt sich unterteilen in die einzelnen Fachrichtungen der Geographie (z. B. Bevölkerungsgeographie, Siedlungs- und Stadtgeographie, Biogeographie) und zudem in dieLänderkunde, also die idiographische Untersuchung von Raumindividuen, und dieLandschaftskunde, die typologische Untersuchung von Raumtypen. Kritik: Bis weit ins 20. Jahrhundert hinein galten Länder- und Landschaftskunde als der eigentliche „Kern“ der Geographie, der dem Fach eine gewisse Identität gab. Werke mit entsprechender Thematik werden weiterhin produziert und die Regionalgeographie ist auch weiterhin wesentlicher selbstverständlicher Forschungsgegenstand an bedeutenden Universitäten, doch gibt es vereinzelt auch kritische Stimmen, welche die Regionalgeographie und ihre wissenschaftliche Bedeutung in Bezug auf die Begriffe Länderkunde und Landschaftskunde nachrangig sehen.[14]DieRegionale Geographieerfuhr demnach einen Bedeutungswandel und beschäftigt sich, statt Regionen als Forschungsgegenstand vorauszusetzen, mit dem Regionalisierungsvorgang an sich. Damit ist sie heute Teil von Sozial- und Wirtschaftsgeographie sowie des interdisziplinären Felds derRegionalwissenschaft.[15][16][Anm. 2] DieAngewandte Geographie, die in der zweiten Hälfte des 20. Jahrhunderts in Abgrenzung zurTheoretischen Geographieentstand, stellt einenormativeForm der geographischen Forschung dar, die sich in allen ihren Fachgebieten wiederfindet. Gegenstand der Angewandten Geographie ist die Analyse und Planung räumlicher Strukturen und Prozesse sowie die Lösung raumbezogener Probleme. Praktische Anwendungsgebiete sind dieRaumplanungoder derUmweltschutz. Insbesondere einige Forschungsbereiche der physischen Anthropogeographie sind normativ etwa auf die Paradigmen derNachhaltigkeitund derGesundheithin ausgerichtet. Beispiele hierfür sind dieGeographische Entwicklungsforschung, dieGeographische Risikoforschungund dieMedizinische Geographie. UnterSchulgeographieversteht man dasSchulfach Geographie, auchErdkundegenannt (in ÖsterreichGeographie und wirtschaftliche Bildung), und die dazugehörige Ausbildung für dasLehramt. Zentrales Anliegen dieses Zweiges ist – wie in jeder Disziplin –Wissenschaftsdidaktikin ihrer speziellen Ausprägung alsGeographiedidaktik. Daher umfasst die Schulgeographie auch die Methodologie der systematischen Reduzierung, Paradigmenbildung unddidaktischenAufbau des Fachgebiets in den verschiedenen Schultypen (Erstellung vonLehrplänenundLerninhalten). Innerhalb der Geographiedidaktik greift die Lehre und das Lernen im Fach auf sechs verschiedeneBasiskonzeptezurück, die als Strukturhilfen und Leitideen fungieren. Das übergeordnete („größte“) Basiskonzept der Geographie ist das „(Mensch-Umwelt-)System“. Dieses System kann mit den Konzepten „Struktur-Funktion-Prozess“, den „Maßstabsebenen“, den „Raumkonzepten“, dem „Nachhaltigkeitsviereck“ sowie den „Zeithorizonten“ untersucht werden.[17]Im weiteren Sinne kann die Geographiedidaktik auch in diehochschulische Lehreselbst eingreifen und auchSchulkartografie,Weiterbildung,Beratungund Information umfassen, und so zum Tätigkeitsfeld eines angewandten Geographen werden (Erstellung etwa vonLehrbüchern,Lehrsendungen, geographischenDokumentationen,Kartenwerken, bzw. Fachberatung bei denselben.[18]und Öffentlichkeitsarbeit.[19]) Als „Brückenfach“ zwischen natur-, geistes- und gesellschaftswissenschaftlichen Disziplinen besteht in der Geographie generell eine große methodische Vielfalt, die die Bandbreite an möglichen Forschungsobjekten reflektiert. Während die Erstellung vonKartenund die Nutzunggeographischer Informationssysteme(GIS) als wichtige Darstellungs- und Forschungsmethoden in allen Teilbereichen zu finden sind, kommen außerdem den jeweiligen Nachbardisziplinen entlehnte Verfahren zur Anwendung. DieVergleichende Geographiewurde schon im 19. Jahrhundert vonCarl RitterundOskar Peschelbegründet.[20]Sie ist eine Vorgehensweise, die zwei typologische Kategorien in Bezug setzt. Ein aktueller methodischer Teilbereich, der zunehmend Bedeutung in der Geographie erlangt und auch derMathematischen Geographiezugerechnet werden kann, ist dieGeoinformatik. Sie verwendet Methoden der Informatik bei der Bearbeitung geographischer Fragestellungen. Aufgabenfelder der Geoinformatik sind: Der kritische GeographGerhard Hardargumentierte nach 1968, dass der Landschaftsgeographie, die seit Alexander von Humboldt den Kern der klassischen Geographie bildet, Wahrnehmungsmuster zugrunde liegen, die aus derLandschaftsmalereistammen. Daher bestimmten jene Forschungsrichtungen, die sich aufLandschaftbeziehen wie z. B. dieLandschaftsökologie, ihren Gegenstand primär auf ästhetischer Weise, der erst sekundär mit einem szientistischen Methodendesign versehen werde. Dieses führe wiederum dazu, dass die ästhetischen Implikationen innerhalb der Profession nicht bewusst reflektiert werden.[21] Obwohl sich die Geographie immer wieder neu verstanden und ausgerichtet hat, siehtGábor Paálein kontinuierliches Merkmal in der ästhetischen Grundlage, die der Wissenschaft zugrunde liegt.[22]Demnach ist es immer ein zentrales Motiv von Geographen gewesen, räumliche Muster zu erkunden und zu verstehen, und zwar insbesondere solche Muster, die sich in ihrer Größenordnung innerhalb des menschlichen Aktionsradius bewegen: Sie befasst sich mit Mustern „von der Größenordnung dessen, was das menschliche Augen ohne große Anstrengung noch erkennen kann bis zur gesamten Erdoberfläche.“[23]Ansätze, die sich explizit mit Umweltwahrnehmungen auseinandersetzen, werden unter derWahrnehmungsgeographiegefasst. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Schreibweise 2Geschichte 2.1Antike und Mittelalter 2.2Frühe Neuzeit 2.3Etablierung als eigenständige Disziplin 2.4Neuere Entwicklungen 3Ordnungsschema 3.1Das Drei-Säulen-Modell der Geographie 3.1.1Physische Geographie 3.1.2Humangeographie 3.1.3Mensch-Umwelt-Beziehungen 3.2Allgemeine und Regionale Geographie 3.3Theoretische und Angewandte Geographie 4Didaktik 5Methodologie 5.1Allgemeines 5.2Vergleichende Geographie 5.3Geoinformatik 6Ästhetische Dimension 7Siehe auch 8Literatur 8.1Überblicks- und Nachschlagewerke 8.2Fachgeschichte und -theorie 9Weblinks 9.1Verbände und Institutionen 9.2Informationsangebote 10Anmerkungen 11Einzelnachweise Acèh Afrikaans Alemannisch"
  },
  {
    "label": 0,
    "text": "Geschichte – Wikipedia Geschichte Inhaltsverzeichnis Bedeutungsspektrum Funktionen und Betrachtungsweisen von Geschichte Siehe auch Literatur Weblinks Anmerkungen Wissenschaftliche Annäherungen Künstlerische Verarbeitung Geschichtspolitik Geschichte als quellenabhängige Konstruktion Historische Rekonstruktion mit sprachlichen Mitteln Unbewusste Anteile in geschichtlichen Erzählungen UnterGeschichteversteht man im Allgemeinen diejenigen Aspekte derVergangenheit, dererMenschengedenken und die sie deuten, um sich über den Charakter zeitlichenWandelsund dessen Auswirkungen auf die eigeneGegenwartundZukunftzu orientieren.[1] Im engeren Sinne istGeschichtedie Entwicklung derMenschheit, weshalb auch vonMenschheitsgeschichtegesprochen wird (im Unterschied etwa zurNaturgeschichte). Daneben bedeutetGeschichtealsHistorieaber auch die Betrachtung der Vergangenheit im Gedenken, im Erzählen und in derGeschichtsschreibung. Forscher, die sich derGeschichtswissenschaftwidmen, nennt manHistoriker. Schließlich bezeichnet man mitGeschichteauch dasSchulfach Geschichte, das über den Ablauf der Vergangenheit informiert und einen Überblick über Ereignisse der Welt-, Landes-, Regional-, Personen-, Politik-, Religions- und Kulturgeschichte gibt. Wenn man Geschichte als Vergangenheit betrachtet, lassen sich folgende Bereiche unterscheiden: Geschichte in diesem dritten, auf Schriftlichkeit beruhenden Bereich bildet das Hauptarbeitsfeld derGeschichtswissenschaftmit ihren spezifischen Methoden. Denn erst mittels Schriftzeugnissen wird es möglich, menschliches Tun und Erleben zu dokumentieren, als Teil der Menschheitsgeschichte dauerhaft festzuhalten und sich diese in der jeweiligen Gegenwart wieder anzueignen. Im Mittelpunkt der Beschäftigung mit Geschichte, der Erkundung (griechisch:Historie) der Vergangenheit, stehen dabei dieQuellen, d. h. zeitnahe schriftliche Aufzeichnungen und Dokumente. Dabei ist zu unterscheiden zwischen Geschichte als Geschehen und demGeschichtsbewusstsein, dem Bild des Gewesenen, das sich einerseits im Selbstverständnis der historischen Personen widerspiegelt, andererseits sich bei der Erforschung und Darstellung aufgrund der vorhandenenÜberlieferungenfür den Betrachter ergibt, der das Geschehen zu erfassen versucht (vgl.GeschichtsschreibungundGeschichte der Geschichtsschreibung). Diese nachträgliche Geschichtserkenntnis gründet sich aufÜberresteundTradition. Solche Erkenntnis ist allerdings nie völlig objektiv, sondern abhängig von der historischen Situation, der Perspektive des Betrachters und den verfügbaren Quellen. In manchen Fällen wird vorgeschlagen, die Darstellung der Ergebnisse und Zusammenhänge als eine künstlerische Tätigkeit zu betrachten. Eine bestimmte Perspektive gegen andere Perspektiven durchzusetzen (aber auch der Versuch,Multiperspektivitätzu ermöglichen) ist Sache derGeschichtspolitik. Dagegen hat sichGeschichtsdidaktikdie Aufgabe gestellt, den Zugang zu den wichtigsten Bereichen von Geschichte zu erleichtern und ein mehrdimensionales Geschichtsbewusstsein zu ermöglichen. DerGeschichtsunterrichtist der Versuch der praktischen Umsetzung von Geschichtsdidaktik. Im Idealfall sollen inhaltlich nicht nur die bisherigen Erkenntnisse der Geschichtswissenschaft, sondern zumindest in Ansätzen auch historisch-kritische Methodenkenntnisse vermittelt werden – dies umso mehr, als das in der Schule vermittelte Geschichtswissen an sich stets nur eine Rekonstruktion ist, die keinen Wahrheitsanspruch erheben kann. Ebenso lange, wie es Geschichtsschreibung gibt, stellt sich die Frage: Wozu Geschichte? Neben der Bewahrung von Traditionen aller Art, der vielleicht ursprünglichsten Funktion des Erzählens bzw. Aufschreibens von Geschichte, kann Geschichte auch identitätsstiftende Wirkung entfalten, etwa bei der Suche nach einer Antwort auf die Frage, „woher wir kommen und wohin wir gehen.“[2]Bereits in derAntikegabCiceroeine später häufig zitierte, aber auch skeptisch betrachtete umfassende Funktionsbestimmung der Geschichte als „Lehrmeisterin des Lebens“ (Historia magistra vitae).[3] Der imHistorismusetwa durchLeopold von Rankeerhobene Objektivitätsanspruch, zu zeigen, „wie es eigentlich gewesen“ sei, ist im Hinblick auf die Zeitgebundenheit und Individualität jeglichen Rückblicks in die Vergangenheit als nicht einlösbar anzusehen. DerAnnales-HistorikerFernand Braudelbeschrieb Grenzen der Objektivität, denen alle unterliegen, die Geschichte darstellen, einmal so: „In der Tat tritt der Historiker niemals aus der Dimension der geschichtlichen Zeit heraus; die Zeit klebt an seinem Denken wie die Erde am Spaten des Gärtners. Trotzdem träumt er davon, sich ihr zu entziehen.“[4]Gordon A. Craigäußerte 1981 in einem Vortrag: „Denn Geschichte ist nicht ‚exakte Wissenschaft‘ – sie ist eine humanistische Disziplin. Ihr Hauptgegenstand sind Menschen, und Geschichte ist, wieThukydidesvor langer Zeit sagte, das Studium nicht von Umständen, sondern von Menschen in Umständen. Wer das vergißt, weil er in sein eigenes spezielles Interessengebiet verliebt ist oder fasziniert von den modellbildenden Aktivitäten und Idealtypen der Behaviouristen, kann nur als einfältig bezeichnet werden.“[5] Der HistorikerRolf Schörkenstellte vier Hauptfunktionen der Geschichte heraus: Geschichte kann als Resultat wissenschaftlicher Forschung gesehen werden. DerHistorikersoll dem Leser auf eine nachvollziehbare, annähernd objektive und überzeugende Weise den Gang der Ereignisse sowie deren Ursachen und Wirkungen, ein alltagsweltliches Geschichtsbewusstsein präsentieren. DieGeschichtsphilosophieversucht, den Gang der Handlungen in einen übergeordneten Zusammenhang, einGeschichtsbild, zu bringen. Wesentliche Ordnungskriterien- und Hilfsmittel dabei sindChronologieundPeriodisierung. Der GeschichtsdidaktikerHans-Jürgen Pandeldefiniert fünf Merkmale von Geschichte bzw. historischer Erzählungen[7]: Weiterhin ließe sich Narrativität selbst als ein Merkmal formulieren. Bei Pandel findet sich dieses Merkmal jedoch in der Grundannahme selbst wieder, dass Geschichte per se narrativ ist. Das Konstruktivitätsmerkmal bindet er auch an Narrativitätstheorie an. Der GeschichtstheoretikerJörn Rüsendefiniert sieben Charakteristika als Merkmale für Geschichte bzw. historische Deutung. Diese decken sich z. T. mit Pandels Merkmalen bzw. sind anders formuliert oder ausdifferenziert. Geschichte ist nach Rüsen demnach[8]: Die Geschichtswissenschaft diskutiert auch die Frage, wie weit das von ihr entworfene Bild von der Vergangenheit überhaupt in der Lage ist, die tatsächliche Vergangenheit abzubilden.[9]Das bezieht sich nicht allein auf die Unmöglichkeit, historische Situationen und Prozesse in ihrer Gesamtheit oder Totalität abzubilden, sondern hängt auch mit Zweifeln in ihre Quellen (ganz abgesehen von den Fälschungen) zusammen. Während man im 19. Jahrhundert bemüht war, gegensätzliche Aussagen aus verschiedenen Quellen weitestgehend zu harmonisieren, findet man sich heute eher damit ab, dass der vergangene Sachverhalt bis zum Fund neuer Quellen unrekonstruierbar verschwunden ist. Bekanntes Beispiel für diesen Wandel ist die Darstellung der KrönungKarls des Großenin Rom zum Kaiser, die in den päpstlichen Quellen anders geschildert wird als in den Quellen, die nördlich der Alpen entstanden sind. Während in diesem Falle die Nichtrekonstruierbarkeit angesichts sich widersprechender Quellen heute allgemein akzeptiert wird, ist es bei Quellen, denen keine abweichende oder von ihr unabhängige Darstellung gegenübersteht, eine viel diskutierte Frage, ob das Bild, das auf Grund dieser Quellen von der Vergangenheit gezeichnet wird, nicht eine Konstruktion ist, die mit den wirklichen Geschehnissen wenig oder möglicherweise nichts zu tun hat. Hier können derProzess Jesuoder die Hintergründe derkonstantinischen Wendeals Beispiel dienen. Dabei wird zum einen die Frage diskutiert, ob der Versuch einer Rekonstruktion in derartigen Fällen nicht ebenfalls unterbleiben sollte, und zum anderen, ob eine solche Unterscheidung zwischen „wirklicher“ und „rekonstruierter“ Wirklichkeit überhaupt einen Sinn hat und ob nicht die Maxime genügt, dass die rekonstruierte Geschichte so lange als Wirklichkeit gilt, bis neue Erkenntnisse eine Korrektur erfordern. Das Bemühen um wissenschaftliche Rekonstruktion von Geschichte kommt – schon allein wegen der sprachlichen Bestimmtheit ihrer Vermittlung – nicht ohne konstruierende Anteile aus. Der Rohstoff der Geschichte, die Gesamtheit des Vergangenen, kann erst durch Benennung, Bewertung und Ordnung im Medium der Sprache sichtbar bzw. begreiflich gemacht werden. Demnach ist Geschichte (auch) das Erzeugnis der Historiker und der sich auf die Vergangenheit besinnenden Menschen. „Nur soweit diese Besinnung stattfindet und sich artikuliert, gibt es Geschichte. Außerhalb dieses Bereichs ist nur noch Gegenwart ohne Tiefendimension und totes Material.“[10] Anders als bei den noch im 19. und 20. Jahrhundert vorherrschenden und mit exklusivem Objektivitätsanspruch verbundenen historistischen Geschichtsbildern stehen sich unterdessen in der Geschichtswissenschaft eine Vielzahl vonNarrativenzu Vergangenheitsaspekten gegenüber.[11]Generell zu kurz greift aber lautThomas Walach, wer Geschichte „als reines Produkt bewusster Reflexion über Vergangenheit“ versteht. Sowohl die geschichtlichen Akteure als auch die das Geschehen verarbeitenden Historiker seien durch unbewusste Anteile ihrer Psyche ebenso bestimmt wie durch die bewussten kognitiven Operationen. Eine ihre gesellschaftliche Rolle ernstnehmende Geschichtswissenschaft komme künftig nicht umhin, sich mit den dunkleren Bereichen im historischen Unbewussten – Schuldgefühl, Kränkung, Scham und Ressentiment – auseinanderzusetzen.[12]„Die blinden Flecken auf der historischen Netzhaut“, so Walach, „resultieren aus der typischen empirischen Vorgehensweise der Geschichtswissenschaft, die stets untersucht, wofür sie Quellen findet und sich selbst Aussagen darüber verbietet, wofür sie kein Quellenmaterial hat.“[13] Indem die Darstellung von Geschichte auch als eine künstlerische Gestaltungsaufgabe betrachtet werden kann, kommt es ohne vorrangig wissenschaftlichesErkenntnisinteressezu künstlerischer Interpretation bzw.literarischerVerarbeitung geschichtlicher Themen. Beispiele dafür sind die DramenJulius CaesarvonWilliam ShakespeareoderWallensteinvonFriedrich Schiller– Werke, die der Einbildungskraft des Künstlers weit mehr verdanken als einem wissenschaftlichen Anspruch. Formen künstlerischer Auseinandersetzung mit Geschichte finden sich auch in derbildenden Kunst, speziell in derHistorienmalerei, wo neben Gemälden wie derAlexanderschlachtvonAlbrecht Altdorferauch monumentale Formate wie dasBauernkriegspanoramavonWerner Tübkevorkommen. In der Musik nehmen sich zum Teil Opernwerke historischer Stoffe an, etwaGiuseppe VerdisDon CarlosoderGaetano DonizettisAnna Bolena. Die gezielt von politischen Interessen geleitete Darstellung von Geschichte ist Gegenstand der Geschichtspolitik, die auch von manchen Historikern aktiv mit betrieben wird. Geschichtspolitik dient der Einflussnahme auf die allgemeine Meinungsbildung in der Gesellschaft, insbesondere intotalitärenSystemen. Sie hat in Abhängigkeit vom politischen System zeittypische Auswirkungen aufGeschichtsdidaktikund Geschichtspädagogik, insbesondereGeschichtsunterricht,MuseumspädagogikundGedenkstätten. Zudem gibt es Formen geschichtlicher Wissensvermittlung durchUnterhaltungsmedienbis hin zumHistotainment(wie zum Beispiel Mittelaltermärkte), ein Spektrum, das von didaktischer Wissensvermittlung bis zur bloßen Unterhaltung reicht und auch in mancherlei Kombinationen anzutreffen ist. Die Mittel von Geschichtspolitik sind vielfältig. Zu den diesbezüglichen Begriffen gehören:Geschichtlichkeit,Geschichtsbewusstsein,Geschichtsraum,Geschichtsperspektive,Historisierung,Erinnerungskultur,GlorifizierungbeziehungsweiseGeschichtsfälschung. Dass Geschichtspolitik auch inrepräsentativen Demokratienvon Bedeutung ist, ergibt sich unter anderem aus dem Auftrag zurpolitischen Bildung. Die Art und Weise, wie Vergangenheitsvorstellungen zustande kommen, ist laut Walach entscheidend dafür, „ob und wie der Konsens über gemeinsame Geschichte einen Konsens über Politik herstellen kann.“ Das kulturell vermittelte gesellschaftliche Wissen über Vergangenheit sei jedoch in repräsentativen Demokratien für Brüche besonders anfällig, da es hier – anders als in autoritären politischen Systemen – kein bloß verordnetes historischesNarrativengeben könne.[14]Hinzu komme die neue digitale Medienöffentlichkeit, die den Personenkreis, der eigene Wahrnehmungen aller Art veröffentlichen kann, in bisher ungekannter Weise erweitert. Daraus ergibt sich für Walach das Problem: „Alternative Fakten,Fake News,Geschichtsrevisionismus– all diese Phänomene, die es der Wissenschaft schwer machen, in der Öffentlichkeit Gehör zu finden, haben eines gemeinsam: Die Bereitschaft, ihnen Glauben zu schenken stellt eine Reaktion des Unbewussten auf die Zumutungen der postmodernen Welt dar, in der das Subjekt allzu oft auf sich selbst zurückgeworfen wird, anstatt Halt an identitätsstiftenden Gewissheiten zu finden.“[15] Darum sei es wichtig, dass die Geschichtswissenschaft, der dieHegemonieüber den historischenDiskurszu entgleiten drohe, Mittel und Wege finde, um wieder breite Akzeptanz für ihre Anliegen und Ergebnisse erreichen zu können. Dazu müsse sie die Beziehung zwischen dem historisch Unbewussten und den historischen Narrativen untersuchen und sie etwa im Rahmen derPublic Historyvermitteln, „die exakt am Schnittpunkt von Wissenschaft, öffentlichen Geschichtsbildern und Geschichtspolitik angesiedelt ist.“[16] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Bedeutungsspektrum 2Funktionen und Betrachtungsweisen von Geschichte 2.1Wissenschaftliche Annäherungen 2.1.1Geschichte als quellenabhängige Konstruktion 2.1.2Historische Rekonstruktion mit sprachlichen Mitteln 2.1.3Unbewusste Anteile in geschichtlichen Erzählungen 2.2Künstlerische Verarbeitung 2.3Geschichtspolitik 3Siehe auch 4Literatur 5Weblinks 6Anmerkungen Адыгабзэ Afrikaans Alemannisch አማርኛ Aragonés Ænglisc العربية الدارجة مصرى অসমীয়া Asturianu Авар Kotava Aymar aru Azərbaycanca تۆرکجه Башҡортса Basa Bali Boarisch"
  },
  {
    "label": 0,
    "text": "Haustier – Wikipedia Haustier Inhaltsverzeichnis Domestikation Grenzfälle Zoologische Systematik Haustiere in Deutschland Gesellschaftliche und soziale Bedeutung Wirtschaftliche Bedeutung Kontroversen Siehe auch Literatur Weblinks Quellen HaustieresindTierarten, die durchDomestikationausWildtierartenhervorgegangen sind. Sie werden wegen ihres Nutzens (etwa alsNutztiereoder für wissenschaftliche Zwecke) oder des Vergnügens halber (alsHeimtier) vom Menschengezüchtet. Tiere hielt man getrennt von ihren wildlebenden Artgenossen, um leichter und nachhaltiger tierische Rohstoffe und Nahrungsprodukte zu gewinnen, als das durch die Jagd möglich ist. Später wurden die Tiere auch wegen ihrer Zug- und Tragleistung domestiziert. DieZüchtungzum Vergnügen hat ihre Anfänge im 1. Jahrhundert. Im 20. Jahrhundert kam die Verwendung als Versuchstier als weiterer Grund der Züchtung hinzu (sieheTierversuch). Bei derTierzuchtwerden die körperlichen Eigenschaften der Tiere stark verändert. Manche typische Fähigkeiten des Wildtieres sind weggezüchtet oder verloren gegangen, während andere Fähigkeiten durch die Züchtung verstärkt oder umgebildet wurden. Haustiere sind von den Stammarten häufig so verschieden, dass sie in eigene Arten oder Unterarten gestellt werden. Viele Haustiere haben die Fähigkeit zum Überleben in der Wildnis verloren. Andere, wie dieHauskatze, können sich leicht auf eine vom Menschen unabhängige Lebensweise umstellen. Die vom Menschen zur Nutzung gefangenen und gehaltenenWildtiere, wieArbeitselefanten,ZierfischeundBeizvögel, zählen in diesem Sinne nicht zu den Haustieren, da sie nicht gezüchtet worden sind. Die längste Geschichte als Haustier hat derHund, dessen Domestikation mindestens auf die Zeit nach demPleistozänetwa 13.000 v. Chr. zurückgeht; es gibt jedoch Hinweise, die dahingehend ausgelegt wurden, dass diese bereits vor 100.000 Jahren gelang. DieseZeitspannewird allerdings in neueren Studien in Frage gestellt, da sie auf reinen Hochrechnungen dermolekularen Uhrberuht.[1](Näheres hierzu). Katzen sind heute mindestens seit 9.500 Jahren als domestiziert bekannt. Knochen von Katzen wurden zusammen mit menschlichen Knochen aus dieser Zeit inMesopotamien, Südost-AnatolienundJordaniengefunden, Domestizierung lässt sich für diese Zeit inZypernzeigen.[2]Die Haustiernutzung vonSchwein,RindundSchafhat etwa 8000 Jahre v. Chr. inVorderasienbegonnen. Ab dem 4. Jahrtausend v. Chr. wurden dieTaubenin Vorderasien und derMaulbeer-SeidenspinnerinChinagezüchtet. Die Zucht desPferdesbegann etwa im 4. Jahrtausend v. Chr. inEurasien. In China werden ab dem 9. JahrhundertGoldfischezurZierdegezüchtet. Seit dem 19. Jahrhundert wird derWellensittichgezüchtet. Die Züchtung vonNagetierenundFliegenzu Versuchszwecken begann im 20. Jahrhundert. Tiere müssen, um als Haustiere gehalten werden zu können, bestimmte körperliche und verhaltensbiologische Merkmale aufweisen. Es hat seit Beginn der Haustierhaltungimmer wieder Versuche gegeben, weitere Arten zu domestizieren, ohne dass eine vollständige Haustierwerdung erfolgte. Beispiele dafür sind verschiedene Hirscharten (Elch,Damhirsch,Rothirsch). Zumindest beim Damhirsch ist beim Gehegewild eine Übergangsform erreicht worden. Planmäßige Züchtung führte hier zu Haustiermerkmalen. Bei vielen Tierarten, die in jüngster Zeit alsHeimtieregehalten werden (hiereinige Beispiele), treten Domestikationserscheinungen (Farb- und Fellmutationen, Verhaltensänderungen) auf, ohne dass bereits von vollständiger Domestikation gesprochen werden kann. Namen von biologischen Arten können sowohl auf wildlebende, wie auch auf domestizierte Individuen und Populationen zurückgehen. Das heißt, wissenschaftliche Artnamen, die auf Basis eines Haustiers aufgestellt worden sind, sind genauso gültige Namen für die entsprechende Art.[3]Je nachdem, ob eine Art zuerst auf Basis eines Haustiers oder eines Wildtiers aufgestellt worden ist, ist demnach für sie derjenige Name korrekt, der zuerst publiziert wurde (Prioritätsregel). Sofern sie zur selben Art gerechnet werden, ist dieser Name dann auf alle Angehörigen dieser Art anzuwenden – jede Art darf nur einen wissenschaftlichen Namen tragen. Diese Regel hat in zahlreichen Fällen für Verwirrung gesorgt, besonders dann, wenn wildlebende und domestizierte Formen oder Populationen miteinander verglichen oder einander gegenübergestellt werden sollten. Es ist in diesen Fällen notwendig, die wildlebende und die domestizierte Form irgendwie voneinander zu unterscheiden, d. h. auch unterschiedlich zu benennen. Der Code der zoologischen Nomenklatur bietet hier keine Lösung. Während regionalePopulationenvonWildtieren, die sich in verschiedenen Merkmalen unterscheiden, alsUnterartengekennzeichnet werden, stellt der Lebensraum von Haustieren kein geografisch einheitliches Gebiet dar. Die Kriterien für die Gliederungen von Unterarten können hier also nicht angewendet werden. Haustiere sind als eine Untereinheit einesWildtypsaufzufassen, für die der Rang einer Unterart (im Regelfall) nicht angewendet werden sollte. Andere Ränge unterhalb der Art werden aber vom Code nicht erfasst und geregelt. Für einige Haustierarten, darunter die wichtigsten, wurde durch eine Entscheidung im Jahr 2003 der Name der Wildform als wissenschaftlicher Name nun ausnahmsweise entgegen der Prioritätsregel festgeschrieben.[4]Danach soll für diese Arten ein anderer wissenschaftlicher Name gültig sein, wenn man sich auf wildlebende Populationen bezieht. Dieser ist auch auf die Haustierform dieser Arten anzuwenden „wenn diese nicht unterscheidbar sind“. Das Problem der unterschiedlichen Namen für Haustiere und ihre wildlebenden Verwandten bzw. Vorfahren ist damit allerdings nicht endgültig gelöst. Es ist nach dieser Entscheidung zulässig, für Wildformen und Haustierformen derselben Art unterschiedliche Namen zu verwenden.[5]Beispielsweise ist es zulässig, das HausrindBos tauruszu nennen (obwohl es vom AuerochsenBos primigeniusabstammt). Zahlreiche Autoren ziehen es allerdings vor, für beide Formen denselben wissenschaftlichen Namen zu verwenden. Zur Benennung der Haustierform sind hier verschiedene Verfahren in Gebrauch. Zahlreiche Autoren benennen die Haustierform als Unterart (mit Trinomen), z. B. wird dann der Haushund alsCanis lupus familiarisbezeichnet (d. h. der eigentlich verfügbare ArtnameCanis familiariswird vermieden). In anderen Fällen werden Haustiere als „forma“ der Wildarten bezeichnet. Beispielsweise erhält die Haustaube dann den NamenColumba liviaformadomestica; dieser wird dann auch auf verwilderte Populationen (Stadttauben) angewandt. Bei Haustieren wird zur Unterscheidung verschiedener Formen innerhalb der Art zusätzlich der BegriffRasseverwendet. Nach einer Untersuchung der GFK 2015 für die 22 wichtigsten Länder weltweit hatten über die Hälfte der Menschen in diesen mindestens ein Haustier. Den höchsten Anteil an Haustierbesitzern gibt es in Argentinien, Mexiko und Brasilien, während in den asiatischen Ländern der Anteil erheblich geringer ist. So hat ein Drittel (33 %) der Haushalte weltweit einenHund, ein Viertel (23 %) eineKatze, ein Achtel (12 %) Fische und 6 % Vögel sowie 6 % andere Haustiere.[6]Allein in Deutschland leben mindestens 9 Millionen Hunde als Haustiere.[7]Für ihre Haustiere geben die Deutschen jedes Jahr 9 Milliarden Euro aus.[7]Allerdings landen auch jedes Jahr 300 000 Haustiere in Heimen, weil die Besitzer sie nicht mehr versorgen wollen oder können.[7]Angeblich sterben auch 60 Millionen Fische in deutschen Aquarien, vor allem wegen Haltungsfehlern.[7] Haustiere leisten einen wichtigen Beitrag zur Gesundheit und zum Zusammenhalt der Gesellschaft. Dies betrifft sowohl den Effekt auf den Haustierhalter selbst als auch auf andere Mitglieder der Gesellschaft. Mit der Haustierhaltung verbunden ist eine Verbesserung des Gesundheitszustandes, so sprechen 68 % der Hundehalter und 61 % der Katzenhalter von einem verbesserten Gesundheitszustand durch die Tierhaltung. Auch im Hinblick auf ihre Lebenszufriedenheit fühlen sich nahezu alle Hundehalter (88 %) und Katzenhalter (83 %) besser.[10] Neuere Studien aus den USA und Australien zeigen, dass Haustierbesitzer mehr Kontakte zu ihren Nachbarn unterhalten.[11]Auch verschiedene Therapieansätze nutzen Hunde erfolgreich. Darüber hinaus werden die besonderen Fähigkeiten der Sinnesorgane der Tiere genutzt, um Krankheiten, Rauschgift oder andere Dinge zu entdecken, die unseren menschlichen Sinnen nicht zugänglich sind. Die wirtschaftliche Bedeutung des Haustiermarktes ist nicht zu unterschätzen. So werden die jährlichen Umsätze/Ausgaben für die gesamtdeutsche Wirtschaft auf insgesamt 10,7 Mrd. € geschätzt. Davon entfallen über die Hälfte (52 %) auf die Hundehaltung und mehr als ein Drittel (36,5 %) auf die Katzenhaltung. Auch die Beschäftigung von 210.000 Vollzeitarbeitsplätze (vollzeitäquivalente) ist mit dem Haustiermarkt verbunden. Die Ausgaben entfallen zu mehr als der Hälfte auf Damit trägt die Haustierhaltung mit 0,35 % zum deutschen Bruttoinlandsprodukt bei. Mehr als ein Drittel der angeschafften Hunde (32 %) und Katzen (37 %) kommen aus den Tierheimen.[12] Eine Studie[13]des auf Ökobilanzen spezialisierten Schweizer Unternehmens ESU ergab, dass die Haltung eines Tieres, gerade bei großen Tieren wie dem Pferd, einen relevanten Einfluss auf die individuell verursachten Umweltbelastungen haben kann. Im Verhältnis zum Autofahren verursacht die Pferdehaltung einen jährlichen CO2-Ausstoß, der einer Fahrt von 21.500 Kilometern im Auto entspricht. Bei Hunden entspricht der Vergleichswert einer knapp 3700 Kilometer langen Autofahrt, bei Katzen und Kaninchen rund 1.400 Fahrkilometern.[14] Tierrechtsorganisationen wie z. B.PETAsind prinzipiell gegen Haustierhaltung. Mit dem folgenden Zitat wird die Ansicht von PETA beschrieben: „In einer perfekten Welt könnten alle Tiere frei leben und ihren Bedürfnissen nachkommen.“ Doch da es in unserer Gesellschaft bereits so viele domestizierte Tiere und Haustiere gäbe, trögen wir die Verantwortung dafür, uns um diese Tiere zu kümmern. Aus diesem Grund ist PETA dafür, dass alle Menschen niemals ein Tier aus einem Zooladen oder vom Züchter kaufen sollten, da dies die Überpopulation von Haustieren unweigerlich fördere und die „Produktion“ oftmals unter tierquälerischen Bedingungen stattfände.[15] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Domestikation 2Grenzfälle 3Zoologische Systematik 4Haustiere in Deutschland 5Gesellschaftliche und soziale Bedeutung 6Wirtschaftliche Bedeutung 7Kontroversen 8Siehe auch 9Literatur 10Weblinks 11Quellen Aymar aru Azərbaycanca Башҡортса Беларуская Čeština Dansk Zazaki English Eesti Suomi Võro Français Gaeilge Hrvatski Հայերեն Қазақша 한국어 Кыргызча Lëtzebuergesch Лакку"
  },
  {
    "label": 0,
    "text": "Kalifornien – Wikipedia Kalifornien Inhaltsverzeichnis Herkunft des Namens Geographie Demographie Geschichte Politik Kultur und Sehenswürdigkeiten Wirtschaft und Infrastruktur Siehe auch Literatur Weblinks Einzelnachweise Grenzen Geographische Regionen Klima Bevölkerung Altersstruktur Herkünfte Einwohnerentwicklung Sprachen Religionen Große Städte Frühgeschichte Europäische Entdecker Spanische Kolonialherrschaft US-amerikanische Annexion und Goldrausch Bundesstaat Konflikt Bundesstaat Kalifornien mit der US-Regierung Gouverneur Legislative Vertretung Kaliforniens im US-Kongress Mitglieder im 118. Kongress Politische Geographie Präsidentschaftswahlen Musik Bauwerke Parks Sport Verkehr Klamath Mountains Kaskadenkette Modoc-Plateau Großes Becken/Basin and Range Küstengebirge Kalifornisches Längstal Sierra Nevada Transverse Ranges Mojave-Wüste Peninsular Ranges Colorado-Wüste Kalifornien(englischCaliforniaⓘ/?[kælɪˈfɔɹnjə] undspanischCalifornia[kaliˈfoɾnja]) ist ein Bundesstaat im Westen derVereinigten Staaten von Amerika, der die gesamte südwestliche Küstenregion des Landes ausmacht. Er grenzt im Westen an denPazifik, im Norden anOregon, im Osten anNevadaundArizonaund hat im Süden eine gemeinsame Grenze mit demmexikanischenBundesstaatBaja California(Niederkalifornien). Mit über 39 Millionen Einwohnern auf einer Fläche von 423.970 Quadratkilometern (163.696 sq mi) ist er derbevölkerungsreichste US-Bundesstaat, derdrittgrößte nach Flächeund die bevölkerungsreichste subnationale Einheit in ganz Nordamerika. Vor der europäischen Kolonisierung war Kalifornien eines der kulturell und sprachlich vielfältigsten Gebiete des präkolumbianischen Nordamerikas. Die europäische Erforschung im 16. und 17. Jahrhundert führte zur Kolonisierung durch dasSpanische Reich. Nach dem erfolgreichen Unabhängigkeitskrieg wurde das Gebiet 1821 Teil Mexikos, das es jedoch 1848 nach demMexikanisch-Amerikanischen Kriegan die Vereinigten Staaten abtrat. Der 1848 einsetzendeGoldrauschin Kalifornien führte zu sozialen und demografischen Veränderungen, einschließlich der Entvölkerung der indigenen Stämme. Der westliche Teil vonAlta Californiawurde dann organisiert und 1850 als 31. Staat zugelassen, während aus den östlichen Gebieten die Staaten Nevada, Arizona und TeileUtahshervorgingen. DerGroßraum Los Angelesund dieBucht von San Franciscosind mit 19 Millionen bzw. 10 Millionen Einwohnern die zweit- bzw. fünftbevölkerungsreichsten Ballungsräume der USA;Los Angelesist die bevölkerungsreichste Stadt des Bundesstaates und die zweitbevölkerungsreichste der USA hinterNew York City; die Hauptstadt Kaliforniens istSacramento. Die vielfältige Geografie des Bundesstaates reicht von der Pazifikküste und den Ballungsgebieten im Westen bis zu den Bergen derSierra Nevadaim Osten und von denRedwood- undDouglasienwäldernim Nordwesten bis zurMojave-Wüsteim Südosten. Zwei Drittel der nationalen Erdbebenrisikogebiete liegen in Kalifornien. DasCentral Valley, ein fruchtbares landwirtschaftliches Gebiet, dominiert das Zentrum des Staates. Die Größe Kaliforniens hat zur Folge, dass das Klima von feuchtem, gemäßigtem Regenwald im Norden bis hin zu trockener Wüste im Landesinneren und schneebedeckten Bergregionen variiert. Dürren und Waldbrände sind ein ständiges Problem, während gleichzeitigatmosphärische Flüsseimmer häufiger auftreten und zu starken Überschwemmungen führen – vor allem im Winter. Kalifornien ist diegrößte Volkswirtschaft des Landesmit einem Bruttosozialprodukt von 4,132 Billionen Dollar (Stand: 3. Quartal 2024). Kalifornien hat außerdem die größte Wirtschaftskraft unter allen subnationalen Einheiten weltweit und würde als unabhängiger Staat nach den meisten Schätzungen gemessen am nominalen BIP weltweit den vierten Platz einnehmen.[2]Kalifornien ist vor allem für seineHightechbranchebekannt, aber auch führend in der landwirtschaftlichen Produktion, angeführt von der Produktion von Milchprodukten, Mandeln und Trauben. Mit demverkehrsreichsten Hafendes Landes in Los Angeles spielt Kalifornien eine zentrale Rolle in der globalen Versorgungskette und wickelt etwa 40 % der in die USA eingeführten Waren ab. Bemerkenswerte Beiträge zur Populärkultur, von Unterhaltung, Sport, Musik und Mode, haben ihren Ursprung in Kalifornien. Der Staat ist die Heimat vonHollywood, der ältesten und einer der größten Filmindustrien der Welt, die das globale Unterhaltungsangebot maßgeblich beeinflusst. Die Bucht von San Francisco und der Großraum Los Angeles gelten als Zentren der globalen Technologie- bzw. der US-Filmindustrie. Die Herkunft des NamensCaliforniaist umstritten. Einer bekanntenHypothesenach existierte der Name bereits vor der Entdeckung in der europäischen Literatur. 1510 veröffentlichte der SpanierGarci Rodríguez de Montalvoeinen Roman, in dem eine Insel voller Gold namensKalifornienvorkommt, bewohnt von wunderschönenAmazonen, die von der KöniginCalifiabeherrscht werden. AlsHernán Cortés’ Soldaten 1535 nachBaja Californiakamen, glaubten sie, es sei eineInsel, und benannten sie nach Montalvos Buch.[3] Bei den Namen von US-Bundesstaaten hat sichKalifornienals einzigeEindeutschungbis heute im deutschen Sprachgebrauch erhalten. Andere eingedeutschte Bezeichnungen wieNeuyork, Südkarolinien, VirginienoderPennsylvaniensind heute unüblich, wurden aber bis in die 1960er Jahre verwendet. Kalifornien selbst ist Namensgeber für das chemische ElementCalifornium, das dort 1950 entdeckt wurde. Kalifornien liegt an der Nahtstelle zweiertektonischerPlatten, der sogenanntenSan-Andreas-Verwerfung, weshalb es in der gesamten Region häufig zuErdbebenkommt. Mit seiner Fläche von 423.970 Quadratkilometer ist Kalifornien nachAlaskaundTexasder drittgrößte Bundesstaat der USA. Alseigenständiger Staatläge es weltweit an 59. Stelle der flächenmäßig größten Staaten, zwischen demIrakundParaguay. Der Staat erstreckt sich auf einer Länge von über 1231 Kilometern zwischen 32° 30' N und 42° N sowie auf einer Breite von 400 Kilometern zwischen 114° 8' W und 124° 24' W. Mit demMount Whitney(4419m) liegt der höchste Berg der USA außerhalb Alaskas in Kalifornien. In etwa 100 Kilometer Entfernung befindet sich das WüstengebietDeath Valley(„Tal des Todes“) mit dem niedrigsten Punkt der USA,Badwater,85,5 m unter dem Meeresspiegel. Das Tal hat seinen Namen in der Zeit der ersten Siedler an der Westküste bekommen, die nicht selten die Qualen von Hitze und Durst erleben mussten, wenn sie das Gebiet durchquerten. Darüber hinaus gibt es in Kalifornien zahlreiche Naturparks und Strände unterschiedlichster Beschaffenheit. Zu den bekanntesten Parks zählt derYosemite-Nationalpark. Während Kalifornien im Westen auf rund 1350 Kilometern Küstenlinie vomPazifischen Ozeanbegrenzt wird, hat es auf der Kontinentalseite Grenzen mit denUS-BundesstaatenOregonim Norden,Nevadaim Osten undArizonaim Südosten sowie demmexikanischenBundesstaatBaja Californiaim Süden. Die Geographie Kaliforniens ist im Verhältnis zur Größe des Staates extrem vielfältig. Es gibt alpine Berge, Nebelküsten, heiße Wüsten und das fruchtbareLängstal. In Kalifornien gibt es die höchstenKüstenmammutbäume, die dickstenRiesenmammutbäumeund die ältestenGrannen-Kiefernder Welt. Der Staat wird oft inNord- undSüdkalifornieneingeteilt. DasU. S. Geological Surveydefiniert den geographischen Mittelpunkt Kaliforniens inNorth ForkimMadera County. Erdwissenschaftler teilen den Staat in elf verschiedenegeomorphologischeGebiete mit klar definierten Grenzen. Dies sind von Norden nach Süden: dieKlamath Mountains, dieKaskadenkette(Cascade Range), dasModoc Plateau, dasBasin and Range(Großes Becken), diekalifornischen Küstengebirge, dasLängstal(Central Valley), dieSierra Nevada, dieTransverse Ranges, dieMojave-Wüste, diePeninsular Rangesund dieColorado-Wüste. DieKlamath Mountainssind ein Gebirge im Nordwesten Kaliforniens und im SüdwestenOregons. Der höchste Gipfel istThompson Peak(2744m) imTrinity County. Die Berge haben eine sehr verschiedene Geologie mit wesentlichen Bereichen ausSerpentinit- undMarmorgesteinen. Im Sommer gibt es begrenzten Niederschlag. Aufgrund derGeologiehaben sie eine einzigartige Flora, darunter mehrereendemischePflanzen wieLawsons Scheinzypresse,Fuchsschwanz-Kiefer, dieSiskiyou-FichteundKalmiopsis. DieKaskadenkette(Cascade Range) ist eine Gebirgsregion, die sich vomkanadischenBritish Columbiabis ins nördliche Kalifornien erstreckt. Die Kaskaden sind Teil desPazifischen Feuerrings, eines Rings von Vulkanen rund um den Pazifischen Ozean. Alle bekanntenVulkanausbrüchein den Vereinigten Staaten kamen aus der Kaskadenregion. Der letzte Vulkan in Kalifornien, der ausbrach, warLassen Peak(1921). Lassen ist der südlichste Vulkan der Kaskadenkette. Diese Region liegt im Nordosten Kaliforniens an der Grenze zuOregonundNevadanördlich derSierra Nevadaund desLängstals. Mittelpunkt des Gebietes ist derMount Shastain der Nähe derTrinity Alps. Mount Shasta ist ein ruhender Vulkan, aber es gibt Hinweise darauf, dass er oderShastina, ein kleiner benachbarter Berg, im 18. Jahrhundert ausgebrochen ist. Im Nordosten Kaliforniens liegt dasModoc Plateau, das sich auch in TeileOregonsund Nevadas erstreckt. DasGroße Becken(englischGreat Basin) ist eine abflusslose,arideGroßlandschaft östlich der Sierra Nevada. Es liegt großteils im Nachbarstaat Nevada. AlsBasin and Range(englisch fürBecken und Gebirge) bezeichnet man eine große geologische Region, in der eine ähnliche Vegetation wie im Großen Becken vorherrscht. Zu diesem gehören auch dieMojave- und dieSonora-Wüstein Mexiko. Im großen Becken liegen viele größere und kleinere Gebirgsketten und Täler, mit demMono Lakeder älteste See Nordamerikas, mit demOwens Valleydas tiefste Tal des Kontinents (mehr als 3000 Meter tief, gemessen von der Spitze desMount Whitney). Im Großen Becken gibt es eine Reihe ausgetrockneter Seen, die in derletzten Eiszeitmit Wasser gefüllt waren. Viele dieser Seen haben in der Wüstenlandschaft verschiedene Salze hinterlassen, vor allemBorax, für das derOwens Lakeund dasTal des Todesbekannt sind. Im Gebiet derWhite Mountains(Weiße Berge)wachsen mit denLanglebigen Kieferndie ältesten Bäume der Welt. Diekalifornischen Küstengebirgegrenzen das Kalifornische Längstal von der Pazifikküste ab und umfassen etwa 109.000 km². Sie schließen auch dieDiablo Rangeöstlich vonSan Franciscound dieSanta Cruz Mountainssüdlich der Stadt ein. Die Küste nördlich von San Francisco ist fast immernebligundregnerisch. Das Küstengebirge ist bekannt für seineKüstenmammutbäume, die höchsten Bäume auf der Erde. DasKalifornische Längstal(englischCentral Valley) ist ein großes,fruchtbaresTal zwischen derSierra Nevadaund dem Küstengebirge. Das zwischen 35° und 40° 40'nördlicher Breitegelegene Tal hat eine Fläche von 77.700 km², was etwas größer alsBayernist. Den nördlichen Teil des Längstals bildet dasSacramento Valley, benannt nach demgleichnamigen Fluss. Der südliche Teil wirdSan Joaquin Valleygenannt, ebenfalls nach demgleichnamigenFluss benannt. Neben diesen beiden Flüssen durchfließt derKings Riverals größerer Fluss das Tal, das durch dieBucht von San Franciscoentwässert wird. Die Flüsse sind ausreichend groß und tief, sodass es mehrere Binnenhäfen gibt. InStocktongibt es einen Seehafen. Im Osten Kaliforniens liegt dieSierra Nevada(span. fürverschneites Gebirge). Die Gebirgskette erstreckt sich auf einer Länge von 600 Kilometern von Nord nach Süd. Der höchste Gipfel im Kernland der Vereinigten Staaten (ohneAlaskaundHawaii) ist derMount Whitney(4421 m) in der Nähe des OrtesLone Pine. Die Topographie der Sierra ist geprägt durchHebungundGletscher. Die Sierra hat 200 bis 250 Sonnentage im Jahr mit warmen Sommern und kalten Wintern, also typischesKontinentalklima. Das bekannteYosemite Valleyliegt in der zentralen Sierra Nevada. Der große und tiefe SüßwasserseeLake Tahoeliegt nördlich desYosemite-Nationalparks. In der Gebirgskette wachsen dieRiesenmammutbäume. Durch diese Schönheit inspiriert wurden derSierra Club, eineNaturschutzorganisation, und derAmerican Alpine Clubgegründet. Letzterer setzt sich für die Aufrechterhaltung von Wanderwegen, Hütten und organisierten Ausflügen ein. Der bekannteste Wanderweg der Sierra ist derJohn Muir Trail, der vomMount WhitneyinsYosemite Valleyführt und Teil desPacific Crest Trailist, der von Mexiko nach Kanada verläuft. Die drei größten Nationalparks in dieser Region sind derYosemite-Nationalpark, derKings-Canyon- und der Sequoia-Nationalpark. DieTransverse Ranges(auch Los Angeles Ranges) sind eine Gebirgskette, die sich in ostwestlicher Richtung erstreckt, und nicht, wie die meisten kalifornischen Gebirge, von Nord nach Süd. DieTehachapi Mountainssind Teil der Transverse Ranges. DieMojave-Wüsteist eine Wüste im Südosten Kaliforniens. Sie ist etwa 35.000 km² groß und erstreckt sich auch auf den Territorien vonNevada,UtahundArizona. Die Mojave-Wüste wird durch dieTehachapi Mountainsund die Hochebenen vonSan Bernardinobegrenzt. Westlich ist die Wüste klar abgegrenzt durch dieSan-Andreas-Verwerfungund dieGarlock-Störungszone. Das südlichste Gebirge in Kalifornien sind diePeninsular Ranges(Halbinselgebirge), die sich östlich vonSan Diegound auf der mexikanischen HalbinselNiederkalifornienerstrecken. Zu den Peninsular Ranges gehört dieSierra San Pedro Mártirin Mexiko. Die Peninsular Ranges enthalten dieLaguna Mountains, dieSan Jacinto Mountains, dieSanta Ana Mountainsund diePalomar Mountain Range, die vor allem für die SternwartePalomar-Observatoriumbekannt ist. DieColorado-Wüste(englischColorado desert) ist etwa 39.000 km² groß und liegt im Süden Kaliforniens und in Mexiko. Ein Merkmal der Wüste ist derSaltonsee. Er entstand erst 1905, als ein Damm desColorado Riverbrach und die Wassermassen in das trockene Gebiet stießen. Heute ist der Saltonsee der größte See in Kalifornien, nahe an derGrenze zu Mexiko. Die Wüste bildet den Rest einer früheren Meeresbucht, die heute bis zu 100 Meter unter dem Meeresspiegel liegt. Das Klima in Kalifornien kann in drei Zonen eingeteilt werden: Kalifornien ist auch bekannt als der „Fruchtgarten Amerikas“, dessen Klima ideal für den Anbau vonWeintrauben,Orangen,ZitronenundAvocadosist. Die Einwohnerzahl Kaliforniens steigt stark an. Seit 1962 ist es der bevölkerungsreichste US-Bundesstaat, heute hat es schon doppelt so viele Einwohner wieNew York, der bis dahin der bevölkerungsreichste war. Kalifornien ist nebenTexas,New MexicoundHawaiieiner von vier sogenanntenMajority-Minority-States, also Bundesstaaten, in denen die nicht-spanischsprachigen Weißen weniger als 50 % der Bevölkerung ausmachen. In Kalifornien werden mehr als 200 Sprachen gesprochen, nach Englisch ist Spanisch die am meisten gesprochene Sprache. Spanisch wird vor allem inSüdkalifornienhäufig verwendet, da in dieser Region hispanische Einwanderer bedingt durch die Grenze zu Mexiko besonders stark vertreten sind. Demographische Prognosen gehen davon aus, dass Kalifornien 2020 eine hispanische Bevölkerungsmehrheit hat, was sowohl an der höheren Geburten- als auch an der höheren Einwandererrate liegt. Kalifornien ist der zweitbevölkerungsreichste (Glied-)Staat in der westlichen Hemisphäre, übertroffen nur noch vomBundesstaat São Paulo. Wäre Kalifornien ein eigenes Land, wäre es weltweit bevölkerungsmäßig der 34-bevölkerungsreichste Staat, noch vorKanadaundAustralien, die flächenmäßig deutlich größer sind. Zur VolkszählungVolkszählung 2020betrug Kaliforniens Einwohnerzahl 39.538.223, was ein Wachstum von 6,1 % seit derZählung von 2010bedeutet. Somit lebt jeder achte Amerikaner in Kalifornien. Zwischen den Jahren 2000 und 2010 wuchs die Bevölkerungszahl um 3.382.308, was einer Steigerung von 10 % entspricht. In Kalifornien liegen acht der 50 größten Städte im Land.Los Angelesist die zweitgrößte Stadt in den Vereinigten Staaten mit 3.792.621 Einwohnern (Census 2010), es folgenSan Diego(8.),San José(10.),San Francisco(14.),Fresno(35.),Long Beach(36.),Sacramento(37.) undOakland(45.). Auch eines derNordamerikanischen Kulturarealezur Gliederung der indianischen Bevölkerung nach (zumeist historischen) Kulturmerkmalen heißt „Kalifornien“. Obwohl die kalifornischen Indianer nur über sehr kleineReservationenverfügen, sind viele von ihnen bestrebt, ihre Traditionen zu wahren. DieAltersstrukturvon Kalifornien setzt sich folgendermaßen zusammen: DasMedianalterbeträgt 34,4 Jahre. Kalifornien hat 37.253.956 Einwohner (Stand: U.S. Census 2010), davon sind 61,6 %Weiße, 14,9 %Asiaten, 7,2 %SchwarzeundAfroamerikaner, 1,9 %Indianer, 0,8 %Hawaiianer(Mehrfachnennungen waren zugelassen). Unabhängig von der „Rasse“ identifizieren sich 37,6 % alsHispanics. Es gibt 12.577.498 Haushalte.[4] Kalifornien hat die größte Zahl der weißen Amerikaner in den Vereinigten Staaten, nämlich 22.953.374. Der Staat hat in absoluten Zahlen die fünftgrößte afroamerikanische Bevölkerung (2.683.914). Etwa 5,56 Millionen Bürger asiatischer Abstammung leben in Kalifornien, das ist etwa ein Drittel der gesamten asiatischen Bevölkerung der Vereinigten Staaten. Auch dieUreinwohner Amerikassind mit 723.225 Menschen stärker vertreten als in jedem anderen Bundesstaat. Nach Schätzungen von 2006 gehören 57 % der Bevölkerung Minderheiten an. Der Anteil der nicht-hispanischen, weißen Bevölkerung sank von 80 % (1970) auf nunmehr 43 %. NurNew MexicoundTexashaben prozentual einen höheren Anteil an Hispanics, aber Kalifornien hat in absoluten Zahlen die meisten. Hawaii ist der einzige Staat, in dem prozentual mehr Aso-Amerikaner leben als in Kalifornien. Speziell bei den japanisch- und chinesischstämmigen Amerikanern hatNew YorkKalifornien gerade als größten Staat abgelöst. 25 % der Bevölkerung sind mexikanischer Abstammung. Mexiko ist das größte Herkunftsland der Kalifornier. Sie stellen die größte Gruppe innerhalb der Bevölkerung mit hispanischer/lateinamerikanischer Abstammung, die insgesamt 32,4 % der Gesamtbevölkerung ausmacht. Knapp 10,0 % der Einwohner sinddeutscher Abstammungund stellen damit die größte Gruppe innerhalb der weißen Bevölkerung, die im Census 2000 59,5 % der Gesamtbevölkerung ausmachten. Es folgen die Gruppen der Irisch- (7,8 %), Englisch- (7,1 %) und Italienischstämmigen (4,3 %).[5][6] Mexikano-Amerikanerleben vor allem inSüdkalifornien.Los Angelesist die größte mexikanische Gemeinde der USA seit 1900. Auch dasImperial Valleyan der Grenze zu Mexiko hat einen hohen Anteil (70 bis 75 %) von Latinos.Riverside Countyhat vor allem im Osten einen hohen hispanischen Bevölkerungsanteil. Auch imLängstalund in derSan Francisco Bay Arealeben viele Hispanics. Die meisten Hispanics sind mexikanischen Hintergrunds, allerdings stammen auch viele ausMittelamerika, derKaribik(KubaoderPuerto Rico) oderSüdamerika. InLos Angeles Countymachen Hispanics 40 % der Bevölkerung aus. Etwa 2020 werden die Hispanics die Bevölkerungsmehrheit in Kalifornien stellen. Einige Demographen gehen davon aus, dass Kalifornien mit dem gesamten Südwesten der Vereinigten Staaten zu lateinamerikanisch geprägtem, mehrheitlich spanischsprachigem Gebiet wird. Andere Demographen gehen jedoch davon aus, dass sich die Hispanics in den USA wie die anderenEinwanderergruppenintegrieren und spätestens nach der dritten Generation englischsprachig und assimiliert sein werden. 1900–1990[8]2000 + 2010[9] Kalifornien ist seit 1962 der bevölkerungsreichste Bundesstaat der Vereinigten Staaten, womit esNew Yorkablöste. Seither hat sich die Bevölkerung noch einmal mehr als verdoppelt, was neben der wirtschaftlichen Dynamik wesentlich auf die Änderung der Gesetze für dieEinwanderung in die Vereinigten Staatenzurückzuführen ist. Nach demUnited States Census 2000sprechen 60,5 % der Kalifornier Englisch und 25,8 % Spanisch als Muttersprache. Auf dem dritten Platz liegt dasHochchinesischemit 2,6 % der Sprecher, gefolgt vonTagalog(2,0 %) undVietnamesisch(1,3 %). Insgesamt werden in Kalifornien mehr als 200 Sprachen gesprochen. Mehr als 100Indianersprachenwerden in Kalifornien gesprochen. Viele von ihnen sind gefährdet, aber es gibt Anstrengungen, sie zurevitalisieren. Seit 1986 ist Englisch gemäß Verfassung als Amtssprache festgelegt. Sprachpolitik ist ein wichtiges Thema in Kalifornien. In absoluten Zahlen leben die meistenKatholiken in den USAund die nachUtahzweitmeistenMormonenin Kalifornien. Der Staat hat eine der größtenjüdischen Gemeinschaftenim Westen der USA, die sich vor allem inLos Angeles,Beverly Hills,San Francisco,Oakland,SacramentoundPalm Springsballen. Die Zahl der Muslime in Kalifornien beträgt etwa eine Million. Die meisten Katholiken stammen von Iren, Italienern, Hispanics und Filippinos ab. Durch die Einwanderung von Lateinamerikanern und Filipinos ist die Anzahl der Katholiken in Kalifornien in letzter Zeit stark angewachsen. Während der Anteil der Katholiken an der schwarzen Bevölkerung gering ist, da diese meist aus den protestantischenSüdstaatenstammen, ist er unter den Hispanics am höchsten. Durch den hohen Anteil an Aso-Amerikanern gibt es in Kalifornien zahlreiche asiatische Religionen wieHinduismus,BuddhismusundTaoismus. Die mitgliederstärksten Religionsgemeinschaften im Jahre 2000 waren dierömisch-katholische Kirchemit 10.079.310,Jüdische Gemeindenmit 994.000 Mitgliedern, dieKirche Jesu Christi der Heiligen der Letzten Tagemit 529.575 und dieSouthern Baptist Conventionmit 471.119 Anhängern.[10] Nach einer Pew-Research-Center-Umfrage sind die Kalifornier weniger religiös als die Bevölkerungen anderer Bundesstaaten der USA. Von den Befragten gaben 62 % an, sie seien stark gläubig, während es landesweit 71 % sind. Weiterhin ist Religion für 48 % der Kalifornier wichtig, während dies 56 % der Amerikaner sagen. Die größte Stadt Kaliforniens ist mit weitem AbstandLos Angeles, das auch die zweitgrößte Stadt der USA ist. Die Metropolregion um Los Angeles gehört mit 17,8 Millionen Einwohnern zu den größten der Erde. Weitere bedeutende Städte sindSan FranciscoundSan José, die beide Teil derSan Francisco Bay Areasind, eines etwa 7,5 Millionen Menschen umfassenden Ballungsraumes um dieBucht von San Francisco. Mit etwa 1,3 Millionen Einwohnern und etwa drei Millionen in der Agglomeration ist das inSüdkaliforniengelegeneSan Diegodie zweitgrößte Stadt Kaliforniens und der drittgrößte Ballungsraum des Bundesstaats. Sacramento, seit 1854 die Hauptstadt Kaliforniens, liegt auf Höhe der Bucht von San Francisco etwa 120 km im Landesinneren. Sacramento selbst hat knapp 500.000 Einwohner, im Ballungsraum leben etwa zwei Millionen Menschen. Weitere wichtige Städte sindOaklandundBerkeley, beide in der San Francisco Bay Area gelegen, sowieSanta Barbara,Modesto,Fresno,BakersfieldundStockton. Ebenfalls bedeutend sindVentura,Anaheim,Long Beach,Irvine,Santa Ana,RiversideundSan Bernardino, die sich alle im Großraum Los Angeles befinden. Bis vor wenigen Jahren stieß die Annahme, die frühesten Bewohner der Region seien in der Lage gewesen, dieChannel Islandsvor der Küste Kaliforniens mit seegängigen Fahrzeugen zu erreichen, auf Widerstand.[11]Da sich dort jedoch bis zu 13.000 Jahre alte Spuren menschlicher Anwesenheit nachweisen ließen, nahm man an, dass die Inseln, die gegen Ende derletzten Eiszeitnäher an der Küste lagen als heute, vergleichsweise leicht zu erreichen gewesen seien. Tatsächlich waren die Northern Channel Islands zwar während des letztenglazialenMaximums vor 20.000 Jahren eine Insel und diese alsSantarosae Islandbezeichnete Insel lag vielleicht nur 6 bis 8 km vom Festland entfernt. Doch um 13.000 BP, der Zeit der ältesten menschlichen Spuren,[12]lag sie bereits, bedingt durch die abschmelzenden Gletscher und den damit ansteigenden Meeresspiegel nach dem Ende der Eiszeit, erheblich weiter entfernt. Die große Insel löste sich schließlich um 10.000 BP in mehrere kleinere Inseln auf. Das Befahren noch weiter von der Küste entfernt gelegener Inseln lässt sich ab etwa 9000 BP belegen. Jüngste Forschungen konnten erweisen, dass sich nunmehr die „antiquity of New World seafaring and maritime adaptations back to 13,000-12,000 years ago“ erstreckte (sinngemäß dehnte sich das Alter der neuweltlichen Seefahrt und der Anpassung an maritime Verhältnisse auf 13 bis 12.000 Jahre aus).[13]Darüber hinaus stellte sich heraus, dass die Insel zur Zeit der frühesten Besiedlungsspuren ungefähr 8,5 km vor der Küste lag, 12.500 BP bereits 9,5 km, 11.000 BP 10,75 km.[14] Erste Spuren menschlicher Aufenthalte lassen sich in derDaisy Cave, im Norden der Channel Islands, um ca. 10.300–9100 v. Chr. nachweisen.[15]aber auch weiter südlich, auf derIsla de Cedrosin Niederkalifornien.[16] Bereits vor 8000 v. Chr.[17], wie sich an derArlington Spring SiteimArlington Canyonerwies, jagten kleine Gruppen Wild, Bergschafe und Vögel. Außerdem betrieben sie Fischerei und sammelten Eicheln und Wildgräser. Bodenbau und Korbflechterei wurden von einigen Gruppen weit entwickelt. Etwas jünger, aus der Zeit um 7000 v. Chr., ist die Frau aus denTeergruben von La Brea, die für ihre zahlreichen Tierskelettfunde bekannt sind.[18] Vor der Ankunft der Europäer lassen sich mehr als 70 verschiedeneIndianer-Stämme (Liste) unterscheiden, womit Kalifornien zu den kulturell undlinguistischvielfältigsten Regionen der Welt gehörte. Die Gesamtzahl der Bewohner wird auf über 300.000 geschätzt.[19] Nach den Landungen vonJuan Rodríguez Cabrilloim Jahre 1542 und SirFrancis Drake1579, die das Gebiet für Spanien bzw. England (vgl.Francis Drakes Messingplakette) beanspruchten, verloren die europäischen Kolonialmächte das Gebiet Kaliforniens wieder weitgehend aus den Augen. Für die Indianer bedeutete die Expedition Cabrillos jedoch möglicherweise einen schweren Einbruch der Bevölkerungszahl durchPocken.[20] Weitere Entdecker, wiePedro de Unamuno(1587),Sebastian Rodriguez Cermeno(1595) undSebastián Vizcaíno(1602–1603) erkundeten die Küste. Kalifornien alsOberkalifornien(Alta California), der spätere nördlichste Bestandteil des VizekönigreichsNeuspanien, wurde erst ab 1769 unter der Leitung desFranziskanersJunípero Serrakolonisiert. Dieser gründete die erste von insgesamt 21Missionen. Neben diesen Missionen errichtete man auch militärische Befestigungen (span.:Presidios) und zivileSiedlungen. Währenddessen rissen die Handelskontakte der Indianer mit dem Norden während der schwerenPockenepidemie ab 1775ab. 1812 wurde im heutigenSonoma Countyim nördlichen Kalifornien mitFort RosseinrussischerStützpunkt als Fortsetzung und Abrundung der russischen Besitzungen inAlaskaerrichtet. Nach dermexikanischen Unabhängigkeitim Jahr 1821 wurde Kalifornien mexikanische Provinz. Die Regierung kehrte wieder zum Missionssystem zurück, bis die demokratische Partei das Missionswesen am 17. März 1833 perDekretvollständig beendete und die Stationen auflöste. Zugleich wurde die Besiedlung durch Einwanderung gefördert. Die ersten Einwanderer wurden allerdings nach dem RegierungsantrittSanta Annas, der die Missionsstationen erhalten wollte, wieder vertrieben. Diese Ereignisse legten den Grundstein für die jahrzehntelange Feindseligkeit der Kalifornier gegen die mexikanische Regierung. 1836 brach ein Aufstand unter dem früheren ZollinspektorAlvaradolos, der von der ohnmächtigen Regierung schließlich als Gouverneur von Kalifornien bestätigt werden musste. Zu diesem Zeitpunkt zählte das Land nur noch etwa 150.000Indianer[21]und 5000 Europäer als Einwohner. 1842 setzte Santa Anna den Gouverneur Alvarado, der bei der Bevölkerung alsDespotverhasst war, ab und machte GeneralManuel Micheltorena[22]zum neuen Gouverneur, der allerdings bald ebenso unbeliebt war wie sein Vorgänger. Im Frühjahr 1846 rebellierten die Bewohner Ober-Kaliforniens und wählten DonJosé Castro, einen geborenen Kalifornier, zum Generalkommandanten. Johann August Sutter, seit 1839 in Kalifornien, erhielt die Bewilligung, eine Niederlassung zu bauen, der er den Namen „Neu-Helvetien“ gab. Schnell wurde daraus eine florierende Kolonie mit 20.000 Stück Vieh, drei Pferdemühlen, zweiWassermühlen, einerSägemühle, einerGerbereiund über 50 Häusern. Sutter herrschte wie ein kleiner „Kaiser“ über sein „Imperium“, das auf Landwirtschaft, Rinderzucht, Holzhandel und Jagd aufgebaut war. 1841 erwarb er Fort Ross von Russland. Er verlor seine Besitzungen wenige Jahre später im Zuge desGoldrausches. 1845 annektierten die USA dieRepublik Texas, was zu einem sehr gespannten Verhältnis mit Mexiko führte, das das Kaufangebot der USA für Kalifornien ablehnte. Die von Californios unterJosé CastrogebildeteJunta von Montereyversuchte, Alta California durch Abspaltung von Mexiko aus dem Krieg herauszuhalten. Doch noch während die Junta darüber beriet, ob der Unabhängigkeit oder dem Anschluss an einen anderen Staat der Vorzug zu geben sei, erklärten im Mai 1846 US-amerikanische Siedler die Unabhängigkeit Kaliforniens (Bear Flag Republic) und proklamierten ihre eigeneRepublik Kalifornien. Ohne dass die Akteure in Kalifornien davon wussten, hatte derMexikanisch-Amerikanische Kriegum das umstrittene Gebiet zwischenRio GrandeundNueces Riverbereits begonnen. Eine Armee unterStephen W. Kearnywurde vonSanta FenachSan DiegoundAlta Californiagesandt und eroberte das heutige Kalifornien gegen nur geringen mexikanischen Widerstand. ImVertrag von Guadalupe Hidalgosah sich Mexiko gezwungen, nicht nur Texas, sondern den gesamten Norden, also Kalifornien,Arizona,NevadaundNew Mexicoabzutreten und alle formal mexikanischen Gebiete weit außerhalb dessen tatsächlichen Einflusses inUtah,ColoradoundWyoming. Zahlreiche Glücksritter zogen nun die im Januar 1848 einsetzenden Goldfunde an, die denKalifornischen Goldrauschauslösten. Die Rede des PräsidentenJames K. Polkvor dem Kongress, die er am 5. Dezember 1848 hielt, machte die Goldfunde auf dem Gebiet Sutters nicht nur allseits bekannt, sondern verstärkte auch die Zuwanderungsbewegung. Sutter hatte versucht, den Fund geheim zu halten, doch die Nachricht verbreitete sich rasch. Goldsucher und Glücksritter kamen in großer Zahl nach Kalifornien, was dazu beitrug, dass die öffentliche Ordnung weitgehend zusammenbrach. Hunderttausende durchsuchten die Erde, das Tal desSacramento Riverswar zum „goldenen“ Westen geworden. Letztendlich erkannte die Regierung die unrechtmäßigen Zustände an, da so die USA zu einem wichtigen Goldexportland geworden waren. Die Indianer wurden verfolgt und vertrieben. Von den rund 150.000 Indianern um 1850 lebten um 1870 nur noch rund 30.000.[21] Am 9. September 1850 wurde Kalifornien schließlich als einunddreißigster Staat in die USA aufgenommen. 1854 wurde Sacramento zur Hauptstadt von Kalifornien ernannt. Da in Kalifornien keine Sklaven gehalten wurden, hielt es während desBürgerkriegszur Union, spielte aber wegen der großen Entfernung zum Kriegsschauplatz praktisch keine Rolle. Mit dem 1869 vollendeten Bau dertranskontinentalen Eisenbahnund dem 1914 vollendetenPanamakanalrückte Kalifornien näher an den Rest der USA heran. Der Bau von Straßen wie derRoute 66stärkte erst Jahrzehnte später die Verbindung mit dem Rest der Vereinigten Staaten. Angelockt durch das kontinuierliche Licht und die relativ günstigen Landpreise siedelte sich in den 1920er Jahren die Filmindustrie hier an, und die Filmstudios vonHollywoodentstanden. Während derGroßen Depressionzogen zahlreiche Farmer aus den Präriestaaten, v. a. ausOklahoma, deren Existenzgrundlage durch dieDürrezerstört worden war, auf der Suche nach Arbeit nach Kalifornien. Literarisch verewigt wurde das Schicksal dieser „Oakies“ inJohn SteinbecksRomanFrüchte des Zorns. Die Industrialisierung des zuvor landwirtschaftlich geprägten Staates setzte während desZweiten Weltkriegsdurch die Errichtung zahlreicher Werften und militärischer Einrichtungen an der Pazifikküste ein. Kalifornien entwickelte sich zum bevölkerungsreichsten Bundesstaat und einem wichtigen Standort der Luft- und Raumfahrtindustrie, der Elektronik und der Informationstechnologie. Seit Ende Mai 2025 kam es infolge vonTrumps harter Migrationspolitikvermehrt zu Protesten in Los Angeles, nachdemICE-Razzienin Wohngebieten und Geschäften zahlreiche Festnahmen nach sich zogen.[23]BürgermeisterinKaren Bassverhängte in betroffenen Bezirken eine nächtliche Ausgangssperre, um Plünderungen und Unruhen einzudämmen, während sie Trumps Behauptung zurückwies, Protestierende würden bezahlt.[24]Präsident Trump bezeichnete die Demonstrierenden als einen „gewalttätigen, aufrührerischen Mob“, der die Stadt „besetzt“ habe, und begründete damit die Entsendung tausenderNationalgardisten.[25]Zusätzlich mobilisierte er 700Marineinfanteristender regulären Streitkräfte, um Trumps Kurs gegen angebliche ausländische Eindringlinge durchzusetzen.[26]Kaliforniens Gouverneur Gavin Newsom kritisierte das Vorgehen als verfassungswidrig und warf dem Präsidenten vor, die Gewaltenteilung auszuhöhlen sowie demokratische Institutionen systematisch zu attackieren.[27]In einer feurigen Grundsatzrede auf Social Media bezeichnete Newsom Trump als „Diktator“ und warnte, autoritäre Regime begännen genau so, Menschen mundtot zu machen.[28]Trotz der Ausgangssperre spitzte sich die Lage in Metropolen wie San Francisco und Sacramento weiter zu, als Solidaritätsdemonstrationen folgten.[29]Trump drohte öffentlich mit weiteren Truppeneinsätzen, falls sich die Lage nicht augenblicklich beruhige.[30]Newsom reichte vor einem Bundesgericht eine Klage und einenEilantragein,[31]um die Autonomie Kaliforniens gegen den Einsatz der Bundesstreitkräfte zu verteidigen.[32][33] „\"Donald Trump schafft Angst und Schrecken, indem er sich nicht an die US-Verfassung hält und seine Befugnisse überschreitet. Dies ist eine inszenierte Krise, die es ihm ermöglichen soll, eine staatliche Miliz zu übernehmen und damit das Fundament unserer Republik zu beschädigen. Jeder Gouverneur, ob rot oder blau, sollte diese ungeheuerliche Übervorteilung zurückweisen. Dies ist mehr als nur Inkompetenz - hier verursacht er absichtlich Chaos, terrorisiert Gemeinden und gefährdet die Grundsätze unserer großen Demokratie. Es ist ein unmissverständlicher Schritt in Richtung Autoritarismus. Das werden wir nicht hinnehmen.\"“ Nachdem das angerufene Gericht dem Eilantrag stattgab, revidierte das übergeordnete Berufungsgericht diese Entscheidung; der Präsident könne die Nationalgarde bis auf weiteres nach Los Angeles entsenden, das Berufungsgericht setzte für den 17. Juni 2025 eine Anhörung zu dem Fall vor dem aus drei Richtern bestehenden Gremium an.[34][35] Gouverneur des Bundesstaates ist seit dem 7. Januar 2019Gavin Newsomvon derDemokratischen Partei. Er löste seinen ParteikollegenJerry Brownab, der das Amt schon von 1975 bis 1983 innehatte. Der Gouverneur übt auf bundesstaatlicher Ebene die Exekutivgewalt aus, das heißt, er führt die kalifornische Staatsregierung und bestimmt die Richtlinien der Politik. Er verfügt über dasBegnadigungsrecht, ernennt hohe Beamte sowie Richter am bundesstaatlichen Verfassungsgericht und nimmt in der Gesetzgebung eine zentrale Rolle ein, indem er Gesetzesbeschlüsse unterzeichnet oder seinVetoeinlegt. Ferner ist er Oberbefehlshaber derCalifornia National Guardund derStaatsgardeKaliforniens und vertritt den Bundesstaat nach außen. Der Gouverneur wird im Turnus von vier Jahren direkt vom Volk gewählt. Als Oberhaupt derExekutivebestimmt der Gouverneur die Richtlinien der Politik in dem US-Westenküstenstaat. Er hat keinKabinettim engeren Sinne, wohl gibt es aber Exekutivämter, wie denVizegouverneur, denAttorney General, denSecretary of Stateund denState Treasurer(entspricht etwa einem Finanzminister). Der Secretary of State und der Attorney General werden ebenfalls von den Bürgern gewählt, sind aber dem Gouverneur untergeordnet. Andere wie zum Beispiel derAgriculture Commissioner, der etwa einem Landwirtschaftsminister entspricht, oder derCalifornia Director of Industrial Relations(zuständig für Wirtschaft und Industrie) werden vom Gouverneur mit Zustimmung des Staatssenats ernannt. Die ernannten Amtsträger innerhalb der Exekutive können vom Gouverneur jederzeit, ohne Zustimmung des Senats, wieder abberufen werden. Beim Auftreten einer Vakanz in den gewählten Exekutivämtern wird, da keine Nachwahl vorgesehen ist, ein neuer Amtsträger vom Gouverneur mit Zustimmung der State Legislature für den Rest der Amtsperiode bestimmt.[36][37]Als Berater und Zuarbeiter des Gouverneurs fungiert ein eigener Stab, dasOffice of the Gouverneur, bei dessen Besetzung der Gouverneur freie Hand hat.[38] Die gesetzgebende Gewalt auf Ebene des Bundesstaates wird durch dieCalifornia State Legislatureausgeübt. Sie besteht aus einemStaatssenatmit 40 direkt gewählten Senatoren und derState Assemblymit 80 direkt gewählten Abgeordneten. Die Amtszeiten im Senat beträgt vier Jahre. Die Amtszeit eines Abgeordneten derState Assemblybeträgt zwei Jahre und beginnt im Dezember. Im November jeden geraden Jahres werden alle 80 Sitze neu vergeben. In beiden Häusern des Parlaments verfügt die Demokratische Partei über deutliche Mehrheiten. Sitz der State Legislature ist dasKapitol in Sacramento, der Hauptstadt des Bundesstaates. Im Gegensatz zu den meisten anderen Bundesstaaten ist dieCalifornia State LegislaturekeinFeierabendparlament, sondern ein Vollzeitparlament mit mehreren Tagungen im Monat.[39] Kalifornien ist seit dem 9. September 1850 US-Bundesstaat und hat seither je einenSenatorderclass 1und einen Senator derclass 3imKongress. Die demokratische AbgeordneteNancy Pelosiaus dem 12.KongresswahlbezirkKaliforniens war von 2007 bis 2011 und erneut von 2019 bis 2023 im Amt desSpeakersimRepräsentantenhaus der Vereinigten Staaten.[40] Die republikanischen Hochburgen im bevölkerungsarmen Nordosten, imSan Joaquin Valleyund der Umgebung vonOrange County, der politischen Heimat vonRichard Nixon, vermochten zuletzt nicht, die demokratische Dominanz in den Städten um dieBay Area, in der StadtLos Angelessowie Teilen ihrer Umgebung und im Nordwesten des Staates aufzuwiegen.[41]Daher haben die Demokraten seit 1992 beiPräsidentschaftswahlendas bis dahin meist mehrheitlich republikanisch wählende Kalifornien gewonnen. Aufgrund seiner Größe und der daraus folgenden hohen Anzahl anWahlmännern(54 bei derWahl 2024)[42]nimmt Kalifornien eine wichtige Rolle bei jeder Präsidentschaftswahl ein. War der Bundesstaat bis in die 1970er und 1980er Jahre hinein noch republikanisch geprägt, so ist er inzwischen eine verlässliche Hochburg der Demokraten, die letztmals 1988 für den republikanischen Kandidaten stimmte. Aufgrund des stark linksliberal geprägten Charakters Kaliforniens kam es in den letzten Jahren zunehmend zu Konflikten mit konservativen Administrationen der Zentralregierung hinsichtlich von Fragen der Wirtschafts-, Sozial- und Umweltpolitik. Kalifornien ist seit Langem in gesellschaftspolitischer Hinsicht liberal geprägt – beispielsweise gehörte Kalifornien 1967 zu den ersten US-Bundesstaaten, dieAbtreibunglegalisierten.[43] Kalifornien war die Heimat von drei amerikanischen Präsidenten:Herbert Hoover, Richard Nixon undRonald Reagan. * der KandidatRobert M. La Follette juniorvon derSocialist Party of Americabekam 33,1 % der Stimmen. Das 1973 gegründeteKronos Quartetgehört zu den weltweit führenden Quartetten für zeitgenössische Musik. Die bekanntesten musikalischen Vertreter Kaliforniens in derPopmusiksind dieBeach Boys. Die Gruppe formierte sich 1961 inHawthorneaus fünf jungen Musikern zwischen 14 und 20 Jahren. In ihren Anfangsjahren besangen sie in ihren Texten das Leben in Kalifornien, speziell am Strand, dasSurfen, die hübschen Mädchen in Kalifornien sowie den Auto-Kult. In reiferen Jahren setzten sie sich für die Erhaltung des Lebens imOzeanund für die Erhaltung der Strände ein, priesen und huldigten dem Land Kalifornien. Ihre Texte übermittelten ab den 1970er Jahren immer wieder die Botschaft von der Schönheit des Landes sowie die Auswirkungen der Zerstörung der Natur durch Eingriffe des Menschen. 1995 setzten das BandmitgliedBrian Wilsonund der MusikerVan Dyke Parksmit ihrem gemeinsamen AlbumOrange Crate ArtKalifornien ein musikalisches Denkmal. 2008 folgte von Brian Wilson dessen KonzeptalbumThat Lucky Old Sun (A Narrative)über Kalifornien. Außerdem stammenThrash-Metal-Bands wieMetallica,SlayeroderExodusaus der kalifornischenSan Francisco Bay Area. InLos Angeleswurden die RockgruppeThe Doorssowie dieCrossover-BandsRed Hot Chili PeppersundLinkin Parkgegründet. Zudem brachteBakersfielddie einflussreicheNu-Metal-BandKornhervor. Weitere bekannte Bands aus Kalifornien (in alphabetischer Reihenfolge):Blink-182,Canned Heat,Creedence Clearwater Revival,Green Day,Guns n’ Roses,Huey Lewis & the News,Incubus,Jefferson Airplane,Journey,Mötley Crüe,No Doubt,Pennywise,Santana,Steely Dan,Sublime,The Byrds,The Doobie Brothers,The Eagles,The Grateful Dead,The Offspring,Van Halen. Entlang des kalifornischen Teils desCamino Realbefinden sich einige der ursprünglich 21 historischen Missionen aus der Zeit der spanischenKonquista. Zum Teil noch sehr gut erhalten und restauriert, sind sie Touristenattraktionen ersten Ranges. An derPazifikküste sind dieLeuchttürme Kaliforniensebenfalls ein beliebtes Ziel für Touristen. DerNational Park Service(NPS) führt für Kalifornien fünfNational Historic Sites, einNational Memorialund zweiNational Historical Parks: Insgesamt stehen in Kalifornien 147National Historic Landmarksund 2895 Bauwerke und Stätten, die imNational Register of Historic Placeseingetragen sind (Stand 30. September 2017).[45] In Kalifornien stehen neben etlichen architektonischen Wahrzeichen wie derGolden Gate Bridgezudem etliche Klassiker der Modernen Architektur, etwaHollyhock HouseundEnnis HousevonFrank Lloyd Wright,Chemosphere,Elrod House,Sheats-Goldstein ResidenceundSilvertopvonJohn Lautner,Neutra VDL Studio and Residences,Case Study HousesundNeutra Office BuildingvonRichard Neutraoder derConcord PavilionvonFrank Gehry.[46] Kalifornien gehört auch zu den Bundesstaaten mit den meistenNational Monuments: Daneben wurde in Kalifornien auch eine große Anzahl anState Parkseingerichtet; siehe hierzu den ArtikelState Parks in Kalifornien. Außerdem verwaltet der NPS in Kalifornien vierNational Historic Trails, dreiNational Recreation Areas, eineNational Preserveund eineNational Seashore: Hinzu kommen 36National Natural Landmarks(Stand 30. September 2017).[47] In Kalifornien gibt es in jeder der großen Sportligen mehrere Teams: Mit 2,6 BillionenUS-Dollar(Stand 2016) trägt Kalifornien 14 % des jährlich erwirtschaftetenBruttoinlandsprodukts(gross domestic product)der USA bei.[48]Als von den USA losgelöster Einzelstaat wäre Kalifornien mit Stand 2024 weltweit die viertgrößte Wirtschaftsmacht.[2]Kalifornien ist nachTexasder US-Bundesstaat mit den zweithöchstenExporten.[49]Kalifornien ist der US-Bundesstaat mit den mit Abstand höchsten Ausgaben fürForschung und Entwicklung.[50]Das Bruttoinlandsprodukt pro Kopf(englisch per capita GDP)lag im Jahre 2016 bei 66.310 US-Dollar (nationaler Durchschnitt der 50 US-Bundesstaaten: 57.118 US-Dollar; nationaler Rangplatz: 7).[51]Aufgrund der hohen Lebenshaltungskosten und Ungleichheit lebten jedoch trotzdem eine hohe Zahl an Kaliforniern in Armut.[52]Im Jahre 2016 lebten knapp 15 % der Bevölkerung unterhalb der Armutsgrenze, was eine der höchsten Raten in den USA war.[53] Nicht zuletzt der für dieindustrielle Landwirtschaftwetterbegünstigte Süden und das dortige Einzugsgebiet billigermexikanischerLandarbeiterverhelfen Kalifornien zu diesem Wohlstand. Große Anstrengungen erfordert jedoch die Wasserversorgung. GewaltigenStauseenan den Flüssen im NachbarstaatArizona, den Kanalbauten (mit Längen wie zwischen Hamburg und München) von Nord nach Süd und denAquäduktenvon den östlich derSierra Nevadagelegenen Seen und Schmelzwasser lieferndenBergschneegletschernkommt hier große Bedeutung zu. Da die Nachbarstaaten (Arizona,Nevada) ebenfalls von den ihnen zustehenden Naturressourcen schöpfen wollen, gerät dieLandwirtschaftzunehmend unter Druck. Naturschutzauflagen vergrößern diesen weiter. Dennoch wird in Kalifornien eine intensive Landwirtschaft mit Anbau von Baumwolle, Gerste, Weizen, Mais, Reis, Hafer, Bohnen und Zuckerrüben betrieben. Von Bedeutung ist auch der Südfrucht- und Gemüsebau im kalifornischen Längstal mithilfe künstlicher Bewässerung sowie die Vieh- und Geflügelzucht und die Fischerei. DerWeinbau in Kalifornienist ebenfalls bedeutend. Etwa 90 % der gesamten Weinproduktion der USA stammen aus Kalifornien. Kalifornien hat reiche Vorkommen an Bodenschätzen (u. a. Erdöl, Erdgas, Borsalze, Quecksilber, Magnesit, Gold) und verfügt über eine hoch entwickelte Industrie: Luftfahrt-, Raumfahrt-, Elektronik- und Computerindustrie (Silicon Valley), Fahrzeugbau, Nahrungsmittelindustrie, Hüttenwerke u. a. In Kalifornien befindet sich der Hauptsitz der amerikanischen Filmindustrie (Hollywood). Viele der größten und bekanntesten Unternehmen der Welt sind in Kalifornien gegründet worden und haben dort ihren Sitz. Dazu zählen u. a.Apple,Alphabet Inc.,Chevron,Walt Disney CompanyundMeta Platforms(vormals Facebook Inc.)[54] 2018 beschloss das kalifornische Parlament, dass die Elektrizitätsversorgung zumSchutz des Klimasbis 2045 vollständig auf kohlendioxidfreie,erneuerbare EnergiequellenwieWind-undSonnenenergieumgestellt werden soll. Als Zwischenziel wurde zudem beschlossen, dass bis 2030 mindestens 60 % des Strommixes durch erneuerbare Energien gedeckt werden soll. Das Gesetz wurde im September 2018 durch Gouverneur Jerry Brown unterzeichnet.[55][56]Im Jahr 2019 decktenPhotovoltaikanlagen19 % des Strombedarfs.[57] Die Arbeitslosenquote des Bundesstaats lag im Oktober 2018 bei 4,1 % (Landesdurchschnitt: 3,8 %).[58] Kaliforniens öffentlicher Schienenpersonenverkehr ist traditionell unterentwickelt. Der Autoverkehr steht im Mittelpunkt und macht den Einzelnen zum „Sklave[n] des Autos“, wie es der SchriftstellerT. C. Boyleformulierte.[59]Die Ausnahme bilden dieSan Francisco Bay Area, mit einem nicht nur für Kalifornien, sondern in den ganzen Vereinigten Staaten führenden Regionalverkehrssystem, das auf dem Rückgrat desBART-Systemsbasiert, und die HauptstadtSacramento. Eine stetige Verbesserung des öffentlichen Nahverkehrs gibt es inLos Angeles, mit der seit 1990 in Betrieb genommenenMetro Los Angeles, die seitdem fast jährlich erweitert wurde und wird. Kalifornien ist über mehrere Strecken nach Osten und Norden an das Schienennetz der USA angebunden. Die Eisenbahn dient überwiegend dem Güterverkehr, die meisten Strecken gehören abgesehen von kürzerenShortlineseiner der beiden großen BahngesellschaftenUnion Pacific Railroad(UP) undBurlington Northern Santa Fe(BNSF). Überregionale Personenzüge der BahngesellschaftAmtrakfahren meist einmal täglich auf einigen Strecken von UP und BNSF, so etwa derCoast StarlightvonSeattlenach Los Angeles. Ausnahmen bilden drei, etwa im Stundentakt bediente Strecken: Zwischen San Francisco und San Jose verkehren dieCaltrain-Vorortzüge in dichtem Takt. Weitere Vorortzüge gibt es mit demSan Diego Coasterin San Diego, mitMetrolinkin Los Angeles und mit demAltamont Corridor Expresszwischen San Jose und Stockton.Metrosund/oderStadtbahnenexistieren in Los Angeles, San Diego, San Francisco, San Jose und Sacramento. Weltbekannt sind dieSan Francisco Cable Cars. Bis zum Jahr 2029 soll einHochgeschwindigkeitsnetzvollendet werden, das u. a. die Städte Sacramento, San Diego, Los Angeles und San Francisco miteinander verbindet. Die Kosten für das umstrittene Projekt belaufen sich laut Schätzungen aus dem Jahr 2016 auf ca. 64 Milliarden US-Dollar.[60] Alabama(AL) |Alaska(AK) |Arizona(AZ) |Arkansas(AR) |Colorado(CO) |Connecticut(CT) |Delaware(DE) |Florida(FL) |Georgia(GA) |Hawaii(HI) |Idaho(ID) |Illinois(IL) |Indiana(IN) |Iowa(IA) |Kalifornien(CA) |Kansas(KS) |Kentucky(KY) |Louisiana(LA) |Maine(ME) |Maryland(MD) |Massachusetts(MA) |Michigan(MI) |Minnesota(MN) |Mississippi(MS) |Missouri(MO) |Montana(MT) |Nebraska(NE) |Nevada(NV) |New Hampshire(NH) |New Jersey(NJ) |New Mexico(NM) |New York(NY) |North Carolina(NC) |North Dakota(ND) |Ohio(OH) |Oklahoma(OK) |Oregon(OR) |Pennsylvania(PA) |Rhode Island(RI) |South Carolina(SC) |South Dakota(SD) |Tennessee(TN) |Texas(TX) |Utah(UT) |Vermont(VT) |Virginia(VA) |Washington(WA) |West Virginia(WV) |Wisconsin(WI) |Wyoming(WY) Amerikanisch-Samoa|Amerikanische Jungferninseln|Bakerinsel|Guam|Howlandinsel|Jarvisinsel|Johnston-Atoll|Kingmanriff|Midwayinseln|Navassa|Nördliche Marianen|Palmyra-Atoll|Puerto Rico|Wake 37-120Koordinaten:37° 0′N,120° 0′W Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Herkunft des Namens 2Geographie 2.1Grenzen 2.2Geographische Regionen 2.2.1Klamath Mountains 2.2.2Kaskadenkette 2.2.3Modoc-Plateau 2.2.4Großes Becken/Basin and Range 2.2.5Küstengebirge 2.2.6Kalifornisches Längstal 2.2.7Sierra Nevada 2.2.8Transverse Ranges 2.2.9Mojave-Wüste 2.2.10Peninsular Ranges 2.2.11Colorado-Wüste 2.3Klima 3Demographie 3.1Bevölkerung 3.2Altersstruktur 3.3Herkünfte 3.4Einwohnerentwicklung 3.5Sprachen 3.6Religionen 3.7Große Städte 4Geschichte 4.1Frühgeschichte 4.2Europäische Entdecker 4.3Spanische Kolonialherrschaft 4.4US-amerikanische Annexion und Goldrausch 4.5Bundesstaat 4.6Konflikt Bundesstaat Kalifornien mit der US-Regierung"
  },
  {
    "label": 0,
    "text": "Klavier – Wikipedia Klavier Inhaltsverzeichnis Bezeichnungen Geschichte Aufbau Klang Verbreitung und Nutzung Klaviermusik Folgen und Kritik an der Normierung moderner Klaviaturen Siehe auch Literatur Weblinks Einzelnachweise Vorformen Bartolomeo Cristofori (1655–1731) Gottfried Silbermann (1683–1753) Johann Andreas Stein und die Wiener Mechanik Entwicklung in England: Tafelklavier, Englische Mechanik, Verstrebungen Entwicklung in der ersten Hälfte des 19. Jahrhunderts Entstehung des Pianinos Entwicklung in der zweiten Hälfte des 19. Jahrhunderts Das 20. und 21. Jahrhundert Elektronische Pianos, Digitalpianos und Hybridpianos Bestandteile Spielwerk Klaviatur Pedale Besonderheiten des Flügels Besonderheiten des Pianinos Zusammensetzung Klangbeeinflussung Stimmen, Intonieren und Regulieren Raumklima Klavier(vonlateinischclavis„Schlüssel“;mittellateinischclavis„Taste“) bezeichnet heute das moderne, weiterentwickelte MusikinstrumentHammerklavier, also ein Saitenklavier, bei dem aufTastendrucküber eine spezielleMechanikHämmerchen gegenSaitengeschlagen werden. Die ebenfalls übliche BezeichnungPianoforte, oder verkürztPiano, bezieht sich darauf, dass das Hammerklavier erstmals die Möglichkeit bot, die Lautstärke jederzeit stufenlos zwischen leise (piano) und laut (forte) durch die Anschlagstärke zu verändern. Dies war beispielsweise beimCembalo, bei dem die Saiten von einer Mechanik gezupft werden, nicht möglich. Die heutigen Hauptformen des Klaviers sind derFlügel(englischgrand piano) und dasPianino(aufrechtes Klavier, englischupright piano). Letzteres wird heute fast immer alsKlavierbezeichnet und oft mit diesem Begriff gleichgesetzt. Historisch bezeichnete das Wort Klavier, bis ins 19. Jahrhundert in der SchreibungClavieroderClavir, allgemein irgendeinTasteninstrument, gelegentlich auch nur eineKlaviatur, also einen Teil eines Instruments. Das heutige Klavier ist bei der Bedienung ein Tasteninstrument, einSchlaginstrumentin seiner Erregungsart und wegen des schwingenden Mediums einSaiteninstrument. Wortbestimmung:Clavis(lateinischfür „Schlüssel“) stand in der mittelalterlichen Musiktheorie für eine mit einem Buchstaben bezeichnete Tonstufe. Weil Tonbuchstaben manchmal direkt auf die Tasten derOrgelgeschrieben wurden, konnte die Bezeichnungclavisauf die Taste selbst übergehen. In notierter Musik wurden Tonbuchstaben vor die Liniensysteme geschrieben, wodurch die Bezeichnung auch auf denNotenschlüsselüberging. Im englischen Wortkeyhat sich die mehrfache Bedeutung „Schlüssel, Tonstufe von festgelegter Höhe, Taste, Notenschlüssel“ bis heute erhalten.[1] Für die Gesamtheit allerclaves(„Tasten“) wurde überfranzösischclavier[klaˈvje] „Tastatur, Klaviatur“ das deutsche WortClaviergebräuchlich. Bis Ende des 18. Jahrhunderts fasste man alle Tasteninstrumente unabhängig von der Art der Klangerzeugung, also auch dieOrgeln(Windclaviere), unter diesem Namen zusammen (Sebastian Virdung, 1511;Jakob Adlung, 1758). 1619 nannteMichael Praetoriusjedes über eine Tastatur zum Klingen gebrachte Saiteninstrumentclavicordium– sowohl die Tangentenklaviere (vor allem dieClavichordeim engeren Sinn) als auch die Zupfklaviere (Cembali,VirginaleundSpinette). In seinem LehrwerkVersuch über die wahre Art das Clavier zu spielen(1753) bezeichneteCarl Philipp Emanuel BachSpieler aller besaiteten Tasteninstrumente einschließlich des noch recht jungenHammerklaviersals Clavieristen. Das Cembalo hieß bei ihm Flügel, das Clavichord Clavicord und das Pianoforte Forte piano. Im 19. Jahrhundert setzte sich das Wort Klavier als Bezeichnung für Tasteninstrumente mit Hammermechanik allgemein durch. 1960 empfahl der MusikhistorikerFriedrich Wilhelm Riedeldie Rückübertragung des Begriffs „Clavier“ in dieser Schreibweise auf alle Tasteninstrumente, weil inAlter Musikdie Wahl des Tasteninstruments häufig offen gelassen wurde.[2] Der ebenfalls übliche Name Piano ist die Kurzform von Pianoforte (von italienischpiano[ˈpi̯aːno] „leise“ undforte[ˈfɔrte] „laut“). Er bezieht sich darauf, dass auf Hammerklavieren – anders als auf älteren Tasteninstrumenten – durch unterschiedlich starkesAnschlagender Tasten große Unterschiede der Lautstärke (sieheDynamik (Musik)) erreichbar sind. Oft wird mit dem BegriffKlaviereinengend nur das Pianino (italienisch „kleines Piano“, vertikale Besaitung) bezeichnet, im Gegensatz zum Flügel (horizontale Besaitung). Seit der Erfindung von Tasteninstrumenten mit elektrischer, elektronischer oder digitaler Klangerzeugung (Digitalpianos) wird er zudem meist für Instrumente akustisch-mechanischer Bauweise reserviert, während das WortPianoauch die Digitalpianos, die Klang und Anschlaggefühl des akustisch-mechanischen Instrumentes wirklichkeitsnah zu simulieren versuchen, umfasst. Besaitete Tasteninstrumente werden historisch auf dasMonochordzurückgeführt. Mehrere Monochorde entwickelten sich zur beidhändig gespielten Floß- oder Röhren-Zitherweiter. Daraus entstanden in derAntikeeinerseits mit Tasten gespielteOrgeln, andererseits verschiedene gezupfte, geschlagene oder gestrichene Saiteninstrumente, darunter dasPsalterium. DasOrganistrumaus dem 12. Jahrhundert – eineDrehleiermit durch Tangententasten veränderbaren Saitenlängen – gilt als Zwischenglied der Entstehung besaiteter Tasteninstrumente. 1397 erwähnt ein Jurist inPaduaerstmals ein mit Tasten bedientesPsalterium. 1404 erwähnten dieMinneregelndesEberhard von Cersneerstmals einclavicordiumundclavicymbolum. 1425 erschien ein solches Instrument auf einem Altarbild inMinden, 1440 beschriebArnaut Henri de Zwollediese neue Instrumentengattung in einem Traktat, darunter auch ein mit einerHammermechanikbedientes, demHackbrettverwandtesDulce melos. Durch Hinzufügen einer Tastatur entwickelten sich imSpätmittelalteraus dem Monochord und dem Psalterium dasClavichord(fest mit der Taste verbundene Tangenten schlagen die Saiten an) und in derRenaissancedasVirginalund dasCembalosowie deren VariantenClavicytheriumundSpinett, bei denen der Ton durch Anreißen der Saiten mit einem Kiel erzeugt wird.[3] Die Flügelform des Cembalos wurde schließlich zum Vorbild für die ersten Klaviere. Gegen Ende des 17. und Anfang des 18. Jahrhunderts wurde viel experimentiert, um ein Tasteninstrument zu konstruieren, das eine dynamische Spielweise (leise, laut und feine Abstufungen) durch unterschiedlich starken Anschlag der Tasten ermöglichte. Der erste, dem dies gelang, warBartolomeo Cristofori, ein italienischer Instrumentenbauer ausPadua, der spätestens seit 1690 am HofeFerdinando de’ MedicisinFlorenzals Hofcembalobauer undKustosder Musikinstrumente-Sammlung angestellt war. Das Inventar der Musikinstrumente aus dem Jahre 1700 listet ein „arpicembalo che fà il piano e il forte“ (Cembalo, das laut und leise spielen kann) auf, das üblicherweise auf das Jahr 1698 datiert wird und als erstesHammerklaviergelten kann. Vermutlich baute Cristofori in den Werkstätten im Erdgeschoss derUffizienbereits 1694 einen Prototyp.[4]Nach einem Treffen mit Cristofori veröffentlichte der römische Literat und JournalistScipione Maffeiim Jahre 1711 einen Artikel imGiornale dei letterati d’Italiaüber ein um 1709 von Cristofori gebautes Instrument, das „gravicembalo col piano e forte“ (Cembalo mit (Befähigung zu) Leise und Laut) genannt wurde. Dieser Artikel enthielt eine Skizze der besonderen Spielmechanik[5]und eine detaillierte Beschreibung der Mechanik, mittels dessen Übersetzung ins Deutsche später der OrgelbauerGottfried Silbermann1726 seinen ersten Hammerflügel konstruierte. Cristoforis Instrumente waren bereits erstaunlich ausgereift. Die Mechanik verfügt über einen Mechanismus, bei dem der Hammer mittels einer Stoßzunge und Übersetzungshebel gegen die Saite geschleudert wird (Stoßmechanik mit Treiber, d. h. übersetzendem Zwischenhebel); eine sogenannte Auslösung (Auskopplung des Hammers von der Tastenbewegung kurz vor dem Anschlag) verhindert ein Festdrücken des Hammers und ungewolltes Bedämpfen an den Saiten. Per Ton separierte Dämpfer verhindern das Weiterklingen der im Vergleich zum Cembalo kräftigeren Saiten nach dem Loslassen der Taste. Cristofori verwendete bereitsDoppelsaiten(zwei Saiten pro Ton), um das Klangvolumen zu vergrößern, sowie seit 1722 denuna corda-Mechanismus;[6]die Instrumente umfassten vierOktaven(heutige meistens71⁄3, s. o. unterKlaviatur). Das Instrumentengehäuse hatte er für die deutlich höheren Zugkräfte des Hammerklaviers gründlich verstärkt. Trotz ihrer ausgezeichneten Qualität fanden die ersten Hammerklaviere in Italien keine große Resonanz, wohl wegen ihres zum Cembalo vergleichsweise hohen Fertigungsaufwandes und anfangs auch schwachen Tones, weshalb Cristofori 1726 aufhörte, Hammerflügel zu bauen. Er widmete sich bis zu seinem Lebensende wieder allein dem Cembalobau.[4]Insgesamt fertigte er knapp 20 Hammerflügel an, von denen heute noch drei erhalten sind. Das älteste bekannte Exemplar von 1720 steht imMetropolitan Museum of ArtinNew York, eines aus dem Jahre 1722 im Musikinstrumentenmuseum in Rom und eines aus dem Jahre 1726 imMusikinstrumentenmuseum der Universität Leipzig.[7] Zwei Schüler und Gesellen Cristoforis, Domenico del Mela (1683 bis ca. 1760) und Giovanni Ferrini (ca. 1699 bis 1758), bauten noch einige Instrumente mit Hammermechanik, die v. a. auf deriberischen HalbinselBeliebtheit erlangten und an den Königshöfen Spaniens und Portugals eine eigene Tradition begründeten. Im Jahre 1732 hatteLodovico Giustiniin Florenz die erste speziell fürs Hammerklavier geschriebene Musik komponiert, die Anweisungen zum Lauterwerden (Crescendo) und Leiserwerden (Decrescendo) enthielt und anlässlich eines diplomatischen Besuches des portugiesischen Kronprinzen am florentinischen Hof der Medici gespielt wurde. Der Prinz machte den Christofori-Lehrlingen Angebote, in Portugal unter seiner Sponsorenschaft weiterzuarbeiten, die sie annahmen; sie begleiteten ihn auf dem Rückweg nach Portugal. Hieraus entstand die portugiesische und spanische Klavierbautradition.[8] In Italien hingegen endete nach Ferrinis Tod für viele Dekaden die Klavierbautradition.[9] Einige unabhängige Erfindungen in Frankreich, CuisinésClavier(1708) und Jean Marius’Clavecin à maillets(1716), beide vermutlich inspiriert durchHebenstreitsPantaleon, schienen auf Grund technischer Schwierigkeiten nicht über den Status von Kuriositäten hinauszugehen.[10]Der Funke sprang hingegen auf Deutschland über, das für die folgenden Jahrzehnte, zusammen mit England, maßgeblich zur Entwicklung des modernen Klaviers beitragen sollte. Der deutsche Clavichord-LehrerChristoph Gottlieb Schrötererfand etwa um 1717 zwei Hammermechaniken für Cembali, die er allerdings aus finanziellen Gründen nicht weiterentwickeln konnte. Trotzdem galt er lange Zeit als Erfinder des Klaviers.[11]Einer der bedeutendsten Orgelbauer der Barockzeit,Gottfried Silbermann, lernte im Jahre 1717 einen Hammerflügel aus Cristoforis Werkstatt kennen. Das Instrument gelangte im Tross von Musikern nach Dresden. Diese waren einer Einladung gefolgt, am kurfürstlichen Hof drei neue OpernAntonio Lottisuraufzuführen. Zusammen mitJohann Ulrich von Königkonnte er das Instrument untersuchen und König übersetzte Maffeis Beschreibung der Mechanik ins Deutsche.[12]Silbermann hatte das nötige Know-how sowie die finanziellen Mittel, um ein eigenes Modell, basierend auf Cristoforis Mechanik, zu entwickeln, das er im Jahre 1726 präsentieren konnte. Er baute in der Folge ein weiteres Hammerklavier. „Eins davon hatte der sel. Kapelm. Hr. Joh. Sebastian Bach gesehen und bespielet. Er hatte den Klang desselben gerühmet, ja bewundert: Aber dabey getadelt, daß es in der Höhe zu schwach lautete, und gar zu schwer zu spielen sey. Dieses hatte Hr. Silbermann, der gar keinen Tadel an seinen Ausarbeitungen leiden konnte, höchst übel aufgenommen. Er zürnte deswegen lange mit dem Hrn. Bach.“[13]Trotzdem arbeitete Silbermann fast zehn Jahre lang an der Verbesserung seiner Instrumente und erntete schließlich Bachs Anerkennung. Nach dem Regierungsantritt KönigFriedrichs II. von Preußenkonnte der Freiberger Instrumentenbauer 15 Instrumente an den Hof nach Potsdam liefern.[14]1747 improvisierte dann Johann Sebastian Bach vor dem König auf einem dieser Hammerflügel sein dreistimmiges Ricercare. Dieses heute imNeuen Palais Potsdamaufbewahrte Instrument wird von der Firma Neupert nachgebaut.[15] Zu dieser Zeit verfügte das Hammerklavier offenbar bereits über einen guten Ruf. Es war das universellste Tasteninstrument und ein exzellentes Klangwerkzeug für einen professionellen Musiker.[16]SilbermannsPiano Fortgenannte Hammerklaviere verfügten über eine Prellmechanik. Neu kam eine Dämpfungsaufhebung mit Handhebeln dazu, die seither (heute über die Bedienung durch das Forte-Pedal) zur Grundausstattung eines jeden Klaviers gehört.[17] Zahlreiche Schüler Silbermanns führten seine Arbeit fort und entwickelten sie weiter. Als besonders innovativ erwies sichChristian Ernst Friederici. Er baute als Erster einTafelklavierund experimentierte viel mit aufrecht stehenden Instrumenten; berühmt und eindrucksvoll sind seine Pyramidenflügel.[14]Zwölf von Silbermanns Studenten (deshalb auch „die zwölf Apostel“ genannt) flohen in den Wirren desSiebenjährigen Kriegesnach England, wo sie die englische Klavierbautradition begründeten.[12] Der OrgelbauerJohann Andreas Steinerlernte beimelsässischenZweig der Silbermann-Familie inStraßburgsein Handwerk. Er gründete 1750 inAugsburgseine eigene Werkstatt und begann, eigene Hammerklaviere zu entwickeln. Er nahm entscheidende Veränderungen vor, die den Klavierbau der folgenden Dekaden nachhaltig prägten. Er verbesserte Silbermanns Prellmechanik, indem er eine Auslösung einbaute, wodurch sie leichter spielbar wurde. Diese Prellzungenmechanik entstand um 1781 und wurde alsDeutsche Mechanikbekannt. Die Gehäuse seiner Instrumente waren viel robuster gebaut und vielfach verstrebt. DerResonanzbodenwar kräftiger dimensioniert und unter Spannung durchgehend berippt.[17]All diese Neuerungen verliehen Steins Hammerklavieren einen neuen Klangcharakter. Sie waren heller, durchdringender und präsenter. Die neue Ausdruckskraft stieß bei Komponisten und Musikern auf Begeisterung und schuf damit die Grundlage für das Klavier als Soloinstrument. Steins Nachkommen führten das Geschäft weiter, seine Kinder Andreas und Nanette zogen 1794 nachWien. Nach weiteren Verbesserungen wurde Steins Mechanik unter dem NamenWiener Mechanikbekannt und von zahlreichen Klavierbauern adaptiert. Insbesondere bewirkte derFänger, ein mit Lederauflage versehener Klemmklotz an der Taste, eine große Verbesserung der Spielmechanik. Er verhindert, dass der von den Saiten herabfallende Hammer zurückprellen kann und einen ungewollt doppelten Ton erzeugt. Wien war damals neben London eine Weltmetropole der Musik und ein idealer Nährboden für Künstler und Erfinder. Über 100 Instrumentenbauer waren zeitweise in Wien aktiv, höchst angesehen die Geschwister Stein sowieJoseph Brodmann,Conrad GrafundAnton Walter.[18] Im Gegensatz zu Johann Andreas Stein, der Silbermanns Prellmechanik weiterentwickelte, griffen die englischen Klavierbauer, darunter viele Silbermann-Schüler, die in den Wirren des Siebenjährigen Krieges nach England ausgewandert waren, direkt auf Cristoforis Stoßmechanik zurück. Aus praktischen und finanziellen Gründen fertigteJohann Christoph Zumpeetwa zwischen 1760 und 1762 sein erstesTafelklavieran. Es war ein kostengünstig herstellbares Instrument mit einer einfachen Mechanik und wenigen Ausschmückungen.[19]Das Tafelklavier wurde in England ein großer Verkaufserfolg. Es wurde Mode, eines zu besitzen; Zumpe konnte „sie nicht schnell genug produzieren, um das Verlangen der Öffentlichkeit zu befriedigen“.[20]Nun begannen auch zahlreiche andere Londoner Klavierbauer, Tafelklaviere zu bauen. Der im Vergleich zum Hammerklavier und zum Cembalo günstige Preis erlaubte es auch dem Bürgertum, ein Instrument zu erwerben. Der kommerzielle Erfolg des Tafelklaviers in England legte die Basis dafür, dass das Klavier später zu einem der beliebtesten und weitest verbreiteten Instrumente des europäischen Bürgertums wurde. AuchAmericus Backersentwickelte um etwa 1772 eine neue Stoßzungenmechanik. Nach Verbesserungen durchRobert StodartundJohn Broadwoodwurde diese alsEnglische Mechanikbekannt. John Broadwood, schottischer Vorarbeiter, dann Schwiegersohn des nach London emigrierten Schweizers Burkhard Tschudi, war vermutlich einer der ersten, der wissenschaftliche Methoden anwandte, um Mechanik und Klang zu verbessern. Er ermittelte die optimale Position, an der der Hammer die Saite anschlagen sollte, damit diese möglichst voll tönt. Seither werden Klaviersaiten ca. bei einem Siebtel bis Neuntel ihrer klingenden Länge angeschlagen, eine ungerade Teilzahl, um Oberschwingungen und eine Klanganreicherung zu erzielen. Broadwood überbrückte erstmals den die Struktur des Flügels schwächenden Hammerschacht mit einer stählernen Klammer, Anbeginn der Entwicklung innerer Verstrebungen der Flügel. Die Hammerschachtbrückenklammer ermöglichte ihm, den Tonumfang der Klaviatur um eine Oktave zu erweitern. Die Mehrung und Qualitätsverbesserung innerer Abstützungen bewirkte dann binnen weniger Jahrzehnte die Verbreiterung des Tonumfangs auf die heute gebräuchlichen 88 Tasten. Broadwoods Erfindungen waren äußerst erfolgreich. Gegen Ende des 18. Jahrhunderts fertigte er rund 400 Pianos pro Jahr, deutlich mehr als jeder andere Hersteller.[21]Broadwoods Manufaktur wurde in den ersten Jahrzehnten des 19. Jahrhunderts zum größten Klavierbauer der Welt. Zu Beginn des 19. Jahrhunderts waren zwei Flügelmechaniken vorherrschend: Die auf Johann Andreas Stein zurückgehende Wiener Mechanik (Prellzungenmechanik) und die von Backers, Stodart und Broadwood entwickelte Englische Mechanik (Stoßzungenmechanik). Die mit Wiener Mechanik ausgestatteten Instrumente waren graziler in der Bauart. Der Klang war dünner und süßer. Doch die Musiker und Komponisten der aufkommenden Romantik verlangten nach mehr Kraft, Lautstärke, größerem Tonumfang und mehr Ausdrucksmöglichkeiten, so dass sich die Englische Mechanik mehr und mehr durchsetzte. Um das Klangvolumen weiter zu verstärken, waren etliche Anpassungen nötig. Mehr Klang erfordert größere und schwerere Hämmer. Dies war konstruktionsbedingt mit der englischen Stoßzungenmechanik besser zu realisieren. Zwischen 1750 und 1850 wuchs die Klaviatur von rund fünf auf siebeneinhalb Oktaven an. Der Trend zu größerer Lautstärke und größerem Tonumfang verlangte mehr und dickere Saiten, deren enorme Zugkraft aufgefangen werden musste. Der Weg führte über zusätzliche Verstrebungen und Eisenspreizen (ab 1799) schließlich zum eisernen Gussrahmen. Erste Patente dazu stammen von Broadwood (1827), Chickering (1843) und die heute übliche Form vonSteinway & Sons(1859).[22]Ab 1824 wurden Klaviersaiten aus stärker belastbarem Gussstahl hergestellt. Der 1830 erfundene kreuzsaitige Bezugerlaubte die Anordnung der Saiten in zwei diagonal übereinander verlaufenden Gruppen. Dies brachte Vorteile für die Statik des Instruments und ermöglichte längere Saiten auch in kürzeren bzw. niedrigeren Instrumenten. Eine Innovation vonJohann Heinrich Pape(1789–1875) im Jahre 1826 sollte tiefgreifende Auswirkungen auf den Klavierklang haben und diesen grundlegend verändern. Er umwickelte die Hammerköpfe nicht wie bisher üblich mit Leder, sondern mit einemFilzbelag. Filz kann bei richtiger Behandlung widerstandsfähiger als Leder sein und lässt sich auch besser bearbeiten.[23]In der Maximalausprägung des Hammerbaues nach den Entwicklungen vonHenri Herzin Paris hatten die Flügel von Herz,ErardundPleyelin Paris zur ZeitChopinsbis zu neun Lagen, innen am Holzkern begonnen mit zwei Lagen Hirschleder, mehrere unterschiedlich dichte Lagen Filz und Wolle bis hin zu Kaninchenfell außen als weichstem Werkstoff. Hämmer dieser extrem aufwendigen Art erlaubten Kundigen eine Reichhaltigkeit und Farbigkeit des Klavierklanges zu erzeugen, die mit der Entwicklung zu noch größeren Konzertsälen und zu höherer Lautstärke, erzielt mit dichtem ein- oder zweilagigem Filz, teils wieder verlorenging. Die Aufbringung des Filzes auf den Hammer ist ein delikater Prozess. Bei vielen Hammerherstellern ist die genaue Vorgehensweise ein gut gehütetes Geheimnis. DieIntonierungeines Klavieres, die durch Auflockern und teils Härten des Filzes erzielte Detailveränderung des Klanges eines Einzeltones zur Angleichung innerhalb des gesamten Tonumfanges, ist seither die höchste Kunst der Klavierbauer. Eine bahnbrechende Erfindung im Klavierbau stammt vom FranzosenSébastien Érard. Er entwickelte auf der Basis der Englischen Mechanik eineRepetitionsmechanik, die er 1821 patentieren ließ.[24]Sie erlaubt mittels eines gefederten Repetierschenkels auf Höhe des auskoppelnden Stößels das Repetieren eines Tones, ohne die Taste ganz loslassen zu müssen. Der Repetierschenkel Érards ermöglicht seither im Flügel eine rasche Anschlagfolge für ein virtuoses, schnelles Spiel. Nach Verfeinerungen von Henri Herz, etwa in den Jahren 1840 bis 1850, entstand die Flügelmechanik der sogenanntendoppelten Auslösung, die bis heute praktisch unverändert blieb. Die Dämpfungsaufhebung erfolgte bei einfachen Instrumenten über einen Handzug, den Pantaleonzug oder Fortezug, im „Mozartflügel“ über gut funktionierende Kniehebel, dann aber zunehmend über Pedale; neben der Dämpferaufhebung waren ein Moderator (Filztuchstreifen) und zunehmend die Verschiebung üblich, aber auch Fagottzug (gegen die Saiten gedrückte Pergamentrolle), Harfenzug (Bürsten- oder Tuchfransenleiste), Lautenzug (mit Leder bespannte Leiste),Janitscharenzug (Schlagwerk mit Pauke, Glocken bzw. Schellen) etc. Diese noch vom Cembalobau stammenden Modifikationen des Saitenklanges gingen jedoch nach 1830 drastisch zurück. Es verblieben am Klavier zunächst zwei Pedale, die Dämpfungsaufhebung („forte“) und die seitliche Verschiebung des Hammeranschlags („una chorda“). Das Hammerklavier erlebte in der ersten Hälfte des 19. Jahrhunderts eine Blütezeit und war nicht mehr aus der Gesellschaft wegzudenken. Das Klavier war den Fürstensalons entwachsen, es wurde in Form des großenKonzertflügelsintegraler Bestandteil des Konzertwesens großer Städte und in der Form von Tafelklavieren, beginnenden Hochklavieren und teils Flügeln auch der gutbürgerlichen Wohnung.[23] Schon von Anfang an wurden auch aufrecht stehende Flügel gebaut, so bereits vom Cristofori-Schüler Domenico del Mela[9]und vom Silbermann-Schüler Christian Ernst Friederici (1745).[25]Diese Instrumente hatten oft eindrückliche Formen, die mit Namen wieGiraffenklavier,Harfenklavier,Lyraflügel, Pyramidenklavier oder Schrankklavier belegt wurden; sie waren meist sehr hoch, sehr exklusiv und hatten nicht viel gemeinsam mit den heutigen Pianinos. Die ersten kleinen Pianinos entstanden um 1800 unabhängig von Matthias Müller in Wien und John Isaac Hawkins inPhiladelphia. Technisch und kommerziell erfolgreich wurdeRobert Wornum, der um 1811 einCottage Pianobaute, das sich bis 1826 zumPiccolo Pianoentwickelte und zum Vorbild für alle späteren Pianinos werden sollte. Seine Mechanik ist eine Stößelmechanik mit Auslösung; sie beruht auf den Prinzipien der englischen Mechanik von Flügeln und wandelt diese mittels des Hammer-Drehgelenks ab, der sogenannten Hammernuss. Er entwickelte sie in den 1830er Jahren weiter. Diese Mechanik wurde in Paris von Pleyel und Pape weiterentwickelt und kommerziell erfolgreich gemacht, weshalb sie auch alsFranzösische Mechanikbekannt wurde. Sie entspricht im Wesentlichen schon der heutigen Klaviermechanik.[26]Die Bauweise der Pianinos löste die material- und platzaufwendigeren und klanglich benachteiligten Tafelklaviere in Europa bereits um ca. 1850, in den USA bis ca. 1900 ab. Die Klavierhersteller versuchten ihre Baukunst nicht nur durch technische Aspekte zu behaupten, sondern auch durch besonders kunstvolle Gestaltung der Gehäuse, die sie v. a. für die zahlreichen Messen und Ausstellungen extra anfertigten. Die zumeist sehr wohlhabenden Kunden bestellten Klaviere und Flügel oft nach Skizzen bedeutender Architekten und Künstler, sodass viele äußerst prachtvolle Instrumente entstanden. In Deutschland hatte z. B. der KlavierherstellerIbacheine eigene Fabrik in Köln mit bis zu 2000 Mitarbeiter, die diese kunstvollen Gehäusearbeiten ausführte. Auch C.Bechsteinin Berlin hat sehr viele dieser sogenannten Artcase Instrumente hergestellt. Um die Mitte des 19. Jahrhunderts waren die meisten Elemente des modernen Klaviers, sowohl beim Flügel als auch beim Pianino, entwickelt. Was folgen sollte, waren einige wenige Neuerungen, v. a. die Kreuzbesaitung beim Flügel, besonders aber kontinuierliche Verfeinerungen und Verbesserungen bei Mechanik, Konstruktion und Herstellungsverfahren. Charakteristisch für die zweite Hälfte des 19. Jahrhunderts ist eine nie zuvor dagewesene Intensivierung der Produktion. 1850 wurden in Europa rund 33.000 Klaviere gefertigt, 1910 waren es bereits 215.000 Stück.[26]Die starke Zunahme dürfte zum einen mit der stetig steigenden Beliebtheit des Klaviers bei der bürgerlichen Mittelklasse, bei der der Besitz eines Pianinos zumStatussymbolavancierte, zum anderen aber auch mit der generellenBevölkerungszunahmeim 19. Jahrhundert zusammenhängen. Das einst so beliebte Tafelklavier wurde vom Pianino verdrängt, wobei es gewissermaßen Opfer seines eigenen Erfolgs wurde. Es entwickelte sich vom anfangs einfachen, kleinen Instrument zu einem großen und schweren Koloss in exklusiver Ausführung. Die Lücke füllte das neue, kleinere und preiswertere Pianino, das international zum mit Abstand beliebtesten Hausinstrument des Bürgertums wurde.[27]Gegen Ende des 19. Jahrhunderts hatten die meisten Instrumentenbauer ihre Tafelklavierproduktion eingestellt.[28] An derLondoner Industrieausstellung (Great Exhibition)von 1851, einer der ersten großen internationalenWeltausstellungen, trafen sich erstmals Klavierhersteller aus ganz Europa und derneuen Welt. Die Ausstellung war ein riesiger Erfolg und sollte fortan regelmäßig stattfinden. Solche Anlässe ließen technologische Vergleiche zu, stachelten die Konkurrenz an und trugen wesentlich zu Innovationen bei.[29]Eine zentrale Rolle bei den weiteren Entwicklungen des Klaviers spielteHeinrich Steinwegund sein Sohn Henry Steinway. Sie patentierten 1859 die vollständige Verbindung von Gussrahmen und Kreuzbesaitung bei Flügeln und 1866 den Einbau von Gussrahmen und Kreuzbesaitung bei Pianinos.[23]1878 ließSteinwaydie Formbiegung des Flügelgehäuses aus laminierten Ahornschichten patentieren.[30]Mit diesen Neuerungen war die Form und Grundkonstruktion des modernen Klaviers entstanden, die sich seither, seit über 140 Jahren, kaum mehr verändert hat. Die Neuerungen wurden bald von anderen Herstellern übernommen. Den Konzertflügel kann man mit den Entwicklungen des Steinway & Sons Modell Centennial D vom Dezember 1875 als weitenteils ausentwickelt betrachten. Er hat die Kreuzbesaitung von 1859, die einteilige Gussplatte, das Mechanikgestell von 1871, das Sostenutopedal und die Pilotenschrauben von 1875, erst auch noch die Bass-Spannschrauben am Resonanzboden, die 1878 entfielen. Die dann noch folgenden kleineren Modifikationen dienten weniger der Klangverbesserung als eher der Vereinfachung und Verbilligung der Produktion und der Verbesserung des Handlings – unter Beibehaltung des erzielten Klangergebnisses. Sein Nachfolger, der 1884 herausgebrachte und noch heute produzierte D-Flügel, ist beinahe 200 Kilogramm leichter. Der Centennial D zeigte über seine Produktionszeit noch einige experimentelle Entwicklungen, aber mit der Installation des „Rims“, des aus Dickten verleimten Außengehäuses, beim Modell D ab 1880, war die endgültige Form gefunden. Was in jenen Jahren fortschreitender Technologie zunächst kaum auffiel, war die Verarmung des Klanges der Flügel mit Hämmern aus gebogenen Filzstreifen nach den Dolge-Patenten und Saiten aus dem 1856 erfundenen Bessemer-Stahl – Entwicklungen, die den Anforderungen an die Beschallung sehr großer Konzertsäle mit 2500 bis 7000 Zuhörern geschuldet waren, eine Leistung, die die Flügel 30 Jahre zuvor keinesfalls hätten erbringen können. Dieser bis heute als technisch aktuell anzusehende Flügeltyp wurde auf der Weltausstellung 1876 prämiert – und seither kaum noch entscheidend verbessert. Die französischen Flügel der 1830er und 1840er Jahre von Hertz, Boisselot, Erard und insbesondere Pleyel waren jedoch klangreicher, feuriger, allerdings leiser und nicht für Publikum von mehr als 1000 Personen geeignet, und ihr Klangreichtum musste mit einem ungemein hohen, nach heutigen Maßstäben keinesfalls zu leistendem Wartungsaufwand an den schnell verschleißenden, aufwendig handgefertigten Hämmern bezahlt werden. Während in den Kriegen und politischen Umwälzungen des 18. und frühen 19. Jahrhunderts viele Klavierbauer aus Deutschland und Frankreich nach England und nach Amerika flohen, kehrten in der zweiten Hälfte des 19. Jahrhunderts viele wieder zurück nach Europa. Deutschland wurde, vor England, Frankreich und den USA, zum führenden Klavierherstellerland weltweit. Deutsche Klavierbauer lieferten in die ganze Welt. Gegen Ende des 19. Jahrhunderts warenBerlin(über 200 Klavierbauer) undLeipzigdie Zentren des Klavierbaus.[31]Eine so große Nachfrage konnte nur durch veränderte und standardisierte Produktionsmethoden und die aufkommende, fabrikmäßigeMassenproduktionbefriedigt werden. Zu Beginn des 20. Jahrhunderts boomte das Klavier auch in den Vereinigten Staaten, die die europäische Produktion bald überholten. 1910 wurden in den USA 370.000 Klaviere produziert, im Gegensatz zu 215.000 in Europa.[26]In der Hochblüte des Klavierbaus wurden allein in Deutschland 300.000 Stück im Jahr verkauft; das Klavier war zu dieser Zeit „Statussymbol, Kommunikationsmittel und liebste Freizeitbeschäftigung zugleich“ und erlaubte Töchtern „aus gutem Hause“, sich mit ihrem Vorspiel für Männer vorteilhaft darzustellen.[32]Ende des 19. Jahrhunderts wurde in den USA dasPianola(Markenname des US-Herstellers Aeolian) erfunden und auch in Europa kamen die pedalbetriebenen Selbstspielklaviere v. a. durchHupfeldsehr in Mode, sodass Anfang des 20. Jahrhunderts phasenweise mehr Pianola-Klaviere und -Flügel ausgeliefert wurden als reine Handspiel-Klaviere und die Klavierproduktion ihren Höhepunkt erreichte. Der Blütezeit in Deutschland wurde durch die beidenWeltkriegeund dieWeltwirtschaftskriseund auch durch einen veränderten Zeitgeschmack ein jähes Ende gesetzt. Hausmusik wich der Schallplatte und dem Radio, Klavierspielen lernen gehörte nicht mehr automatisch zur Ausbildung. Zahlreiche Hersteller mussten ihre Fabriken schließen, verloren sie durch Zerstörung im Krieg oder mussten auf Kriegsmaterialproduktion umstellen.[33]Zusätzlich geriet die Klavierproduktion auch in Kritik und Material-Schwierigkeiten durch den zuvor massenhaften Einsatz vonElfenbeinals Tastenbelag für dieKlaviatureines Klaviers, der fast zur Ausrottung derElefantenpopulationen in Afrika führte. Nur zaghaft erholte sich die Branche nach demZweiten Weltkriegund erst in den 1960er Jahren begann allmählich wieder ein Aufschwung. Auch diedeutsche Wiedervereinigungwirkte sich positiv auf den Klavierbau aus, konnten sich doch bis 1990 traditionsreiche Firmen inOstdeutschland(zum BeispielBlüthner) nicht voll entfalten. Der Einbruch der europäischen Klavierproduktion wurde von der amerikanischen und der aufkommenden asiatischen kompensiert. Besonders die letzten Jahrzehnte sind durch den boomenden Klavierbau inJapan,SüdkoreaundChinageprägt. Die japanischeYamaha Corporationfertigt mittlerweile Flügel auf höchstem Niveau, die man immer öfter in Konzertsälen (z. B. der Philharmonie in Berlin) antrifft. Die koreanischeYoung Changund die chinesischePearl River Gruppegehören heute zu den zahlenmäßig größten Klavierherstellern der Welt.[34] Seit den 1980er Jahren werden vermehrt auch die Vorteile der Elektronik im Klavierbau eingesetzt. Das Resultat ist eine Kombination von akustisch-mechanischem Piano undDigitalpiano. Dazu wird in der Klaviermechanik eine Stoppleiste montiert, die die Hämmer kurz vor dem Anschlagen der Saite auffängt (Stummschaltung). Gleichzeitig wird unter den Tasten eineSensorikmontiert, die die Spielsignale auf eine Box überträgt an der Kopfhörer angeschlossen werden können. Somit lässt sich das Klavier auch „stumm“ spielen. Diese Technik wird von verschiedenen Klavierherstellern verwendet und mit verschiedenen, ähnlich klingenden Namen versehen. Yamaha nennt sieSilent Piano (TM)und seit der Neuvorstellung der nächsten Generation auchTransAcoustic (TM),KawaiAnytimeund PianoDiscQuietTime. Auch zum Nachrüsten werden solche Stummschalt-Systeme angeboten. Zu denführenden Klavierherstellerngehören heuteSteinway & Sons,Yamaha(vor allem mit der CF-Serie),Fazioli,KawaiundBösendorfer (Wien)(gehört seit 2007 zurYamaha Gruppe) sowie auch die deutschen UnternehmenC. Bechstein,Julius Blüthner,Wilhelm Schimmel,SauterundSteingraeber.[35] Eine charakteristische Entwicklung des 20. Jahrhunderts sind dieelektronischen Tasteninstrumente. Bereits Ende des 19. Jahrhunderts wurde, kaum nach Entdecken derElektrizität, mit deren neuen Möglichkeiten experimentiert. Aus ihnen entwickelten sich selbstständige und neue Instrumentengruppen, so beispielsweise dasRhodes Piano, welche meist für andere Musikstile als das klassische Piano Verwendung finden. So hat beispielsweise einKeyboardnicht mehr viel mit einem Klavier zu tun. Eine ganz andere Entwicklungslinie, welche in den 1980er Jahren begann, steht hinter den Digitalpianos. Im Gegensatz zu früheren Neuentwicklungen in der Geschichte des Klaviers, ist das Ziel nicht die Verbesserung des Bestehenden oder die Erschaffung von etwas Neuem, sondern im Gegenteil die Absicht, das „Original“ möglichst genau zu imitieren. Die entscheidenden Elemente sind dabei der Klang und das Spielgefühl (Klaviatur und Mechanik). Heute wird der Klang eines Tones nichtsynthetisiert, sondern unter verschiedenen Bedingungen (Anschlagstärke, Pedalgebung, Resonanzen in Abhängigkeit von bereits zuvor niedergedrückten Tasten) mit hochwertigen Mikrophonenaufgenommen,digitalisiertundgespeichert(englisch: „Sampling“) und dann durch das digitale Instrument entsprechend der Betätigung der Tasten wiedergegeben.[36] Um das Spielgefühl möglichst genau zu imitieren, wurden eigene Mechaniken für Digitalpianos entwickelt. Teilweise werden sogarKlaviermechanikenvon mechanisch-akustischen Instrumenten eingebaut, deren Bewegung mit Sensoren erfasst wird. Man spricht in diesem Falle vonHybridpianos. Digitale Instrumente werden zunehmend auch von professionellen Pianisten zu Übungszwecken[37]und zum Unterrichten eingesetzt.[38]Sie bieten gegenüber akustischen Klavieren nicht nur Nachteile, sondern auch bestimmte Vorzüge, wobei die Bandbreite und Qualität auch bei derlei Instrumenten sehr stark variieren kann: Der Bezugston lässt sichtransponierenund dieTonhöhekann frequenzgenau angepasst werden. Ferner können bei manchen Modellen dieKlangfarbe,Klangeffekteund dasStimmungssystemgewählt werden. Viele Digitalpianos verfügen überdigitale Schnittstellenund können sowohl zur Aufnahme der darauf gespielten Musik als auch zur Wiedergabe eingesetzt werden. Sie sind verhältnismäßig leicht und brauchen kaum Wartung. Die Lautstärke lässt sich regulieren und das Instrument kann mit Kopfhörern gespielt werden. Dafür reicht der Klang und das Anschlagsgefühl eines Digitalpianos in der Regel nicht an ein echtes Klavier heran. Flügel und Pianos haben alle wesentlichen Bauteile gemeinsam: Diese Bauteile waren ca. 1880 bis zur Perfektion entwickelt und werden ohne wesentliche Änderung bis heute zusammengefügt. Die einzigen Fortschritte ergaben sich in der Mechanisierung und Automation der Fertigung der Kleinteile.[39] Das Spielwerk, auch alsKlaviermechanik,HammermechanikoderAnschlagmechanikbezeichnet, ist eineHebel-Konstruktion, bei der auf Tastendruck Hämmer gegen dieSaitendes Klaviers geschleudert werden, um diese zum Klingen zu bringen. Die Mechaniken wurden über die Jahrhunderte immer wieder verbessert, zu unterscheiden sind Mechaniken für die senkrecht besaitetenPianinosund Mechaniken für waagerecht besaitete Flügel bzw.Tafelklaviere. Seit dem 19. Jahrhundert ist es üblich geworden, den gesamten auf einem Instrument zur Verfügung stehenden Tonraum durchgehend mit Tasten in einer von links nach rechts in der Tonhöhe aufsteigendenchromatischenAnordnung zu versehen, wobei jeder vollständigeOktavraumaus sieben weißen und fünf schwarzen Tasten („7-5-Klaviatur“) besteht. Die weißen Tasten entsprechen den siebendiatonischenStammtönen, die schwarzen Tasten deren chromatischen Veränderungen, wodurch sich die vollständige chromatische Abfolge von 12 Tönen innerhalb eines Oktavraums ergibt.[40] Die Klaviatur der meisten Flügel, Pianinos und Digitalpianos besteht aus 88 Tasten (bei älteren Instrumenten sind es oft nur 85, weil bei ihnen die Klaviatur in der Höhe beim a4endet), davon 52 „weiße Tasten“ (auch „Vordertasten“ oder „Untertasten“) und 36 „schwarze Tasten“ (auch „Hintertasten“ oder „Obertasten“), die in ihrer Höhe über die weißen Tasten hinausragen, schmaler sind und zusätzlich abgeschrägte Seitenflächen haben. Aus der normierten Tastenbreite moderner Instrumente ergibt sich eine Gesamtbreite der Klaviatur von 123 cm; die Oberfläche der weißen Tasten befindet sich etwa 74 cm über dem Boden. Der Klavierklang kann durch mehrerePedalebeeinflusst werden. Heute sind meist zwei bis drei Pedale Standard. Das rechte Pedal heißtFortepedal(von it.forte: kräftig, laut), auchDämpferpedaloderHaltepedal(nicht zu verwechseln mit dem weiter unten beschriebenen Tonhaltepedal); mit der Aufforderung „senza sordino“ (it. für „ohne Dämpfer“, oft in der italienischen Pluralform „senza sordini“, etwa im 1. Satz von Beethovens „Mondscheinsonate“) ist ebenfalls das rechte Pedal gemeint. Es sorgt dafür, dass alleDämpfervon denSaitenabgehoben werden, damit die angeschlagenen Töne auch nach dem Loslassen der Tasten weiterklingen. Außerdem schwingen die nun ungedämpften Saiten anderer Töne mit, was dem Klavier einen volleren Klang gibt. Im künstlerischen Klavierspiel wird das rechte Pedal in hochdifferenzierter Weise eingesetzt; man unterscheidet z. B. das Harmoniepedal (Sammelpedal), das synkopierte Pedal (Legato- oder Bindepedal), das Halbpedal, das voraus getretene und das gleichzeitig getretene Pedal.[41] Das linke Pedal heißt „Pianopedal“ (von it.piano: leise), auchLeisepedal,Verschiebungoderuna corda(it. für „eine Saite“). BeimFlügelwird die gesamte Mechanik einige Millimeter nach links oder rechts verschoben, sodass die Hämmer nicht mehr alle drei Saiten eines Saitenchors treffen, sondern nur noch zwei bzw. eine Saite. Dadurch verändert sich auch dieKlangfarbe, weil nunmehr Saiten existieren, die nicht durch direkten Anschlag, sondern durchResonanzerregt werden. Außerdem treffen durch die Verschiebung andere Stellen des Hammerfilzes auf die Saiten. Diese Stellen sind andersintoniert(d. h. vomKlavierstimmermit der Intoniernadel aufgeweicht bzw. mit einer Feile gehärtet) als die Filzstellen, die in Normalstellung die Saiten anschlagen. BeimPianinobewegt das linke Pedal die Hämmer der Klaviermechanik näher an die Saiten, sodass die Kraft, die jeder Hammer bei Betätigung aufbauen kann, geringer ist. Damit wird das Spielen besonders leiser Stellen vereinfacht. Der HerstellerFaziolibietet ein Flügel-Modell mit zwei Piano-Pedalen an, die dem Pianisten die Wahl zwischen der „Verschiebung“ und dem Pianopedal der Pianino-Technik ermöglicht. Das (nicht immer vorhandene) mittlere Pedal ist entweder ein Tonhaltepedal, ein Moderatorpedal oder ein Stummschaltepedal (bei Hybridpianos). Wenn einFlügelein mittleres Pedal besitzt, handelt es sich in der Regel um das so genannte Tonhalte-, Tonhaltungs-, Sostenuto- oder Steinway-Pedal. Diese Vorrichtung wurde von französischen Klavierbauern entwickelt (Jean Louis Boisselot1844,Claude Montal1862)[42]und in den USA zum Erfolg geführt (Albert SteinwaysPatent von 1874).[43]Sie hindert die gerade gehobenen Dämpfer daran, wieder zurückzufallen. Der Spieler kann damit also einzelne Töne oder Klänge festhalten, während alle anderen Dämpfer weiterhin auf das Spielen und Loslassen der Tasten (bzw. das rechte Pedal) reagieren. Das Tonhaltepedal – mittlerweile ist es auch bei größeren und teureren Pianinomodellen anzutreffen – findet vor allem in der Klaviermusik des 20. Jahrhunderts Verwendung. Wenn einPianinoein mittleres Pedal besitzt, handelt es sich meist um den so genannten Moderator. Bei Betätigung schiebt sich einFilzstreifenzwischen Hämmer und Saiten und macht das Instrument deutlich leiser. Dieses Pedal kann oft in der unteren Position durch eine Seitwärtsbewegung verriegelt werden. Bei manchen Pianinos wird der Moderator nicht über ein Pedal, sondern über einen schiebbaren Knopf oder einen drehbaren Hebel aktiviert, der links der Klaviatur oder unter ihr sitzt. Vor allem in den 1960er Jahren versahen einige Hersteller den Filzstreifen mitNieten, die dem Klavier einen klimpernden, cembaloähnlichen Klang verliehen. Da diese Metallplättchen allzu leicht Saiten und Hammerköpfe beschädigten, haben sie sich nicht durchgesetzt. Ein Flügel steht, wie ein Cembalo, frei im Raum. Raste, Resonanzboden und Besaitung sind horizontal, parallel zum Boden, angeordnet. Der Klang strahlt daher vom Resonanzboden überwiegend nach unten und oben ab. Unten wird er vom Fußboden reflektiert und verteilt, oben entweder vom geschlossenen Deckel gedämpft oder vom geöffneten Deckel gebündelt zur Seite hin abgestrahlt. Ein Tastendruck führt zu einer Aufwärtsbewegung des hinteren Teils der Tastenwippe. Beim Flügel wird dadurch der Hammer nach oben an die Saite geschleudert. Das Gewicht des Hammers ist direkt an der Taste spürbar und ermöglicht eine differenzierte Klanggestaltung. Durch ihre horizontale Lagerung wird das Zurückschnellen der Hämmer von der Saite durch die natürliche Schwerkraft unterstützt. Die Repetitionsfähigkeit eines Flügels, also die Geschwindigkeit, mit der ein und derselbe Ton mehrfach hintereinander angeschlagen werden kann, ist daher stärker ausgeprägt als bei einem Pianino. Manche Konzertflügel, etwa der „Imperial“ vonBösendorfer, haben eine auf bis zu acht Oktaven Tonumfang (C2bis c5) erweiterte Klaviatur. Beim Pianino stehen Raste, Resonanzboden, Gussrahmen, Besaitung und Hammermechanik (Ständermechanik) senkrecht zum Boden, so dass man es platzsparend an die Wand stellen kann und der Klang zunächst nach vorne und nach hinten abstrahlt. Bei der üblichen Aufstellung wird der hintere Anteil direkt von der Zimmerwand reflektiert und zurück auf den Resonanzboden gelenkt. Eine leicht von der Wand abgewandte Position oder ein kleiner Winkel zur Wand verändert oft den Klang von Pianinos enorm zum Vorteil. Der vordere Klang-Anteil wird im Gehäuse reflektiert. Durch den Anschlag auf der Vorderseite der Harfe ist die Resonanzbodenfläche eines Hochklaviers oft vergleichsweise groß. Das macht höhere Pianinos (ab ca. 120 cm Höhe) oft erstaunlich klangstark – speziell im Vergleich zu kleineren Flügeln (unterhalb von 170 cm Länge). Beim Pianino muss die Aufwärtsbewegung der Tastenwippe in eine Vorwärtsbewegung des Hammers umgesetzt werden. Dadurch ist der Fingerkontakt zum Hammer indirekter. Die Dämpfung eines Pianinos oder Hochklaviers befindet sich normalerweise unterhalb der Hämmer auf derselben Seite der Saitenanlage, im Bereich der stärkeren Amplituden der Schwingungsbäuche. Ältere Pianinos haben jedoch (bis ca. 1910) teils eine sogenannteOberdämpfer-Mechanik; die Dämpfer-Puppen sitzen über den Hämmern. Im Englischen findet man hierfür auch den Begriff „birdcage action“, „Vogelkäfig“-Mechanik, wegen der vor die Hammermechanik gebauten Dämpfer-Betätigungsdrähte. Diese Art der Dämpfung ist zum einen weniger effektiv als bei einem Unterdämpfer-Klavier, da sie die Schwingungen nur im Randbereich der Schwingungsbäuche abdämpft, zum Weiteren kann die Dämpferpuppe bei kurzen Diskantsaiten einen optimalen Hammeranschlagspunkt vereiteln – mit entsprechenden Nachteilen für die Klangqualität. Das Stimmen und vor allem die Regulation der Mechanik können durch die vorn liegenden Dämpferdrähte erschwert sein. Dass Oberdämpfer-Klaviere aus diesen Gründen jedoch generell völlig untauglich seien, wie man oft behauptet findet, kann man nicht sagen. Ein gut reguliertes Oberdämpferklavier ist wegen seines deutlichen Nachklingens das prädestinierte Instrument für frühenJazzund vor allem für denRagtime. Zu den spezifischen Merkmalen des Klavierklangs gehören die festgelegten Tonhöhen, eine an die Anschlagsgeschwindigkeit und somit die Lautstärke gekoppelteFärbung des Klangsund das unwiderrufliche Verklingen des Tons, der nach erfolgtem Anschlag nur noch durch Gebrauch des rechten Pedals verlängert und durch allmähliches oder abruptes Aufsetzen der Dämpfung allmählich oder abrupt beendet werden kann. Eine Besonderheit des Klaviers ist, dass die Töne (abgesehen von den tiefsten) nicht nur von einer, sondern von zwei bis drei gleich gestimmten Saiten erzeugt werden, einem so genanntenSaitenchor. Ursprünglich sollte diese „Mehrchörigkeit“ die Lautstärke des Instrumentes erhöhen; vor allem aber führte sie zu einem komplexeren Verlauf des aus Sofort- und Nachklang zusammengesetzten Klanges. Die Saiten eines Saitenchors werden gemeinsam angeschlagen. Da sie gleichgestimmt sind, schwingen siegleichphasig, allerdings mit leicht unterschiedlichenAmplituden, weil die Form des Hammers nie vollkommen regelmäßig ist. Die am schwächsten angeschlagene Saite schwingt nach Abklingen ihrer eigenen Anregung allmählich mit den anderen Saiten mit. Nun fungieren die Saiten des Saitenchors als gekoppelte Pendel und tauschen einen Großteil ihrer Energie miteinander aus. AlsSofortklangwird der laute, aber schnell abklingende Teil des Klaviertones bezeichnet. Er entsteht hauptsächlich durch eine Transversalschwingung der Saiten in Richtung des Hammerschlags, also senkrecht zum Resonanzboden. Diese Schwingung wird primär vom Hammer angeregt, aber vergleichsweise rasch senkrecht auf den Resonanzboden übertragen, wodurch sie ihre Energie als Schall an die Luft abgibt. AlsNachklangwird der leisere, dafür aber langsamer abklingende Teil des Klaviertones bezeichnet. Dieser entsteht vor allem durch eine leichte Transversalschwingung der Saiten quer zum Hammerschlag, also parallel zum Resonanzboden. Diese Schwingung gibt ihre Energie nur schwer an den Resonanzboden ab und verklingt daher langsam. Die Verwendung des linken Pedals schwächt einerseits den Sofortklang, da nur zwei der drei Saiten eines Saitenchores angeschlagen werden, und unterstützt andererseits den Nachklang, da der Saitenchor als System gekoppelter Pendel seine Energie vergleichsweise langsam abgibt. Das linke Pedal führt also nicht nur zu einem anfangs leiseren, sondern auch zu einem relativ länger anhaltenden Ton. Der Klang und die Lautstärke eines Tones auf dem Klavier ist allein abhängig von derGeschwindigkeitund somit von derBewegungsenergiedes Hammers, der die Saiten anschlägt, nicht jedoch von der Art und Weise, wie der Klavierspieler den Hammer auf diese Geschwindigkeitbeschleunigt, also auch nicht von einer bestimmten Anschlagstechnik.[44]Wenn man die Pedale unberücksichtigt lässt und von einigen Phänomenen absieht, die eine zusätzliche Rolle spielen, etwa den „oberen“ und „unteren Geräuschen“, die abhängig von der Spielweise beim Zusammenstoß zwischen Finger und Taste bzw. zwischen Tastenholz und Tastenboden entstehen,[45]verlaufenKlangfarben- undLautstärkenänderungauf dem Klavier also stets parallel zueinander. Allerdings hängt der Zeitpunkt des Anschlags der Saiten nach dem Beginn des Niederdrückens einer Klaviertaste vom zeitlichen Kraftverlauf und somit der Beschleunigung des Hammers während des Niederdrückens ab, wodurch ein trainierter Pianist einen bestimmten Ton trotz gleicher Lautstärke in gewissen Grenzen gezielt etwas früher oder später erklingen lassen kann („Mikro-Agogik“) und unabhängig von der LautstärkeAkzentesetzen kann.[46]Insofern hat die Anschlagtechnik des Pianisten durch den tatsächlich erzielten Zeitpunkt des Einsetzens des Klaviertones einen entscheidenden Einfluss auf den Klaviervortrag.[47] Da sich Klaviere durch den Saitenzug, durch die Spielbelastung und durch klimatische Schwankungen verstimmen und in der Folge unschön (gewollt beimHonky-Tonk-Piano) klingen, sollten sie mindestens einmal jährlichgestimmtwerden. Aufgrund vonInharmonizitätender Obertöne ist auch die Stimmung subjektiv vomKlavierstimmerfestgelegt. (In Konzerthäusern werden Flügel bis zu dreimal täglich gestimmt.) Standard ist diegleichstufige Stimmung; für originale oder nachgebaute historische Instrumente werden oft ungleichstufige Stimmungen bevorzugt (historische Aufführungspraxis). Um den Flügel oder das Pianino klanglich auszuarbeiten, wird derKlavierbauernicht nur stimmen, sondern auchintonieren. Zu den möglichen Vorarbeiten zählt das leichte Abziehen der aus Filz bestehenden Hammerköpfe mit Sandpapierfeilen – das macht den Klang gleichmäßiger und gegebenenfalls etwas „härter“. Dann folgt das eigentliche Intonieren durch gezieltes Stechen in bestimmte Hammerkopfbereiche mit Intoniernadeln – eine Arbeit, die den Klang in der Regel „weicher“ macht. Neben dem Stimmen und Intonieren wirkt sich auch das Regulieren der Mechanik (des Spielwerks, der Klaviatur und der Pedale) unmittelbar auf den Klang des Instrumentes aus. DasRaumklimahat direkte Auswirkungen auf den Klang des Instruments, außerdem auf Regulierung, Stimmung und insgesamt auf die Wertbeständigkeit. Vor allem dieLuftfeuchtigkeitsollte möglichst konstant sein. Empfohlen wird eine relative Luftfeuchte zwischen 40 und 70 %, idealerweise zwischen 50 und 60 %. Werte unter 40 % führen zu starker Austrocknung des Holzes und sollten unbedingt vermieden werden, Werte über 70 % begünstigen Rostbildung an Metallteilen, zum Beispiel den Saiten. Nicht empfohlen wird die Aufstellung an schlecht isolierten Außenwänden, in der Nähe von Heizkörpern oder auf einem geheizten Fußboden; auch Zugluft und direkte Sonneneinstrahlung sind zu vermeiden. Klaviere reisen oft um den halben Erdball, bevor sie ihren Bestimmungsort erreichen. Das kann zu schwerwiegenden Problemen führen, beispielsweise, wenn ein für das schwüle Klima Ostasiens konzipiertes Instrument in Mittel- oder Nordeuropa den ersten kalten und somit trockenen Winter durchstehen muss. Heute produzieren große und renommierte Klavierfirmen wieYamahaihre für den Export nach Europa oder Nordamerika bestimmten Instrumente in spezifisch klimatisierten Räumen. Sinkt die Luftfeuchtigkeit über einen längeren Zeitraum stärker ab, so verlieren die Holzbauteile Feuchtigkeit und ziehen sich zusammen. Die Gefahr besteht, dass sich Stimmwirbel und Schrauben lockern, Klaviaturrahmenbalken und Mechanikbalken verziehen (was die Regulierung von Klaviatur und Mechanik beeinträchtigt), dass der Resonanzboden seine Wölbung verliert (wodurch die Stimmung sinkt und der Klang leidet) und vielleicht sogar reißt. Steigt hingegen die Luftfeuchtigkeit über einen längeren Zeitraum stärker an, so verstärkt sich die Wölbung des Resonanzbodens, steigt die Stimmung, können Achsen und Tasten klemmen und wird der Klang dumpfer (weil der Hammerfilz Feuchtigkeit aufnimmt). Diesen Problemen kann bis zu einem gewissen Maß durch hochwertige Materialien entgegengewirkt werden. Auch sind Klaviaturrahmen und Mechanikbalken aus Metall möglich, bringen allerdings wieder andere Nachteile mit sich. Schichtverleimte Resonanzböden arbeiten kaum, klingen aber deutlich schlechter. Materialien wiePlexiglas[48]oderKohlenstofffaser-Verbundwerkstoffe(CFK)[49]reagieren nur wenig auf Klimaschwankungen und werden inzwischen bei einzelnen Serienmodellen zur Fertigung des Klavierkörpers bzw. des Resonanzbodens eingesetzt. 1925 wurden allein in Deutschland, dem damals führenden Produktionsland, 137.000 Klaviere gebaut. In denUSAging mit dem Erfolg desRagtimezu Beginn des 20. Jahrhunderts ein enormer Aufschwung des Klavierbaus einher, auch (bis etwa 1930) beim Bau pneumatisch und elektrisch angetriebenerReproduktionsklaviere.[50]2007 wurden weltweit zirka 450.000 Pianinos und Flügel produziert, etwa zwei Drittel davon imFernen Osten; aus Deutschland kamen weniger als 10.000 Instrumente.[51]Preisunterschiede zwischen ähnlich dimensionierten Klavieren (auch zwischen verschiedenen Produktlinien eines und desselben Herstellers) ergeben sich aus kürzeren oder längeren, mehr oder weniger automatisierten Produktionsprozessen, aus der Produktion inNiedrig-oderHochlohnländernund aus unterschiedlichen Qualitäten etwa des Klangholzes oder des Filzes. Im Jahr 1980 gab es in den westdeutschen Privathaushalten etwa 9.300.000 Flöten, 8.400.000 Mundharmonikas/Melodikas, 3.800.000 Gitarren, 2.200.000 Akkordeons und 1.600.000 Pianinos/Flügel.[52] Das Freizeitverhalten in Deutschland hat sich geändert: Nur noch zwei Prozent der Menschen musizieren täglich, 78 Prozent jedoch nie. Entsprechend hat sich der Absatz von Klavieren seit 1925 auf etwa ein Zehntel (12.000 im Jahr) verringert. Es gibt 1,5 Millionen Instrumente; 130.000 Schüler nehmen Unterricht. Gebrauchte Klaviere werden wegen der hohen Kosten bei Umzügen und für das Stimmen häufig verschenkt; pro Jahr werden rund 3500 Instrumente verschrottet.[32] Weltweit werden pro Jahr knapp 500.000 Klaviere gefertigt, davon mehr als die Hälfte in China. Aus Deutschland kommen rund 5 Prozent der jährlich produzierten Instrumente.[53] Der erste Komponist, welcher speziell für das von Bartolomeo Cristofori erfundene Hammerklavier schrieb, warLodovico GiustiniausPistoia. Er komponierte zwölfSonatenmit dem Titel„Sonate Da Cimbalo di piano e forte detto volgarmente di martelletti“, die im Jahre 1732 inFlorenzpubliziert wurden. Damit die Interpreten die Möglichkeiten des neuen Instruments ausschöpfen würden, versah er seine Musik mit Anmerkungen wie „più forte“ (lauter) oder „più piano“ (leiser).[10] Komponisten wie die Bach-Söhne,Wolfgang Amadeus MozartundLudwig van Beethovenund andere komponierten Musik, die in der zweiten Hälfte des 18. Jahrhunderts zu Teilen bereits solistisch für das Klavier geschrieben war. In der ersten Hälfte des 19. Jahrhunderts war es insbesondereFrédéric Chopin, welcher vornehmlich für das Klavier Musik schrieb. In der zweiten Hälfte waren es Komponisten wieFranz Liszt,Sergei Rachmaninow,Anton Rubinstein,Ignacy Jan Paderewskiund andere Komponisten des romantischen Repertoires, die sich in der Pianomusik hervortaten, oft noch mit dem Hauptanliegen, auf den Bühnen als Pianist vorrangig ihre eigenen Musikkompositionen zur Aufführung zu bringen. Mit Beginn des zwanzigsten Jahrhunderts trat der Komponist-Interpret in den Hintergrund; die Tätigkeiten der Komposition zum einen und des Aufführens, Interpretierens zum anderen trennten sich. Es waren sowohl Komponisten der Moderne wieBéla BartókundFerruccio Busoniim Segment der sogenannten „E-Musik“ (ernster Musik) als auch im Bereich der „U-Musik“, unterhaltender, populärer Musik, vor allem die Entwicklungen im US-amerikanischen Raum, wie derBlues, derRagtime, derBoogie-Woogieund derJazzmit Komponisten wieScott Joplin,Jelly Roll Morton,Albert AmmonsundGeorge Gershwin, die der Klaviermusik große Impulse gaben. Die Standardbreite moderner Klaviaturen von ca. 16,5 cm pro Oktave (ca. 2,3 cm pro weißer Taste) gerät zunehmend in die Kritik, da sie für viele Menschen objektiv zu breit ist und es keine bautechnischen Gründe für dieses Maß gibt, d. h. auch andere Tastenbreiten gebaut werden könnten. Das Spielen auf für die individuelle Handgröße zu breiten Tasten verursacht sowohl medizinische als auch musikalische Probleme. So tretenÜberlastungssyndrome, wie z. B.Sehnenscheidenentzündungen,Fokale Dystonien,Ganglien,KarpaltunnelsyndromeoderSehnenreizungen, signifikant häufiger bei Pianisten mit im Verhältnis zur Klaviatur kleinen Händen auf.[54][55][56][57][58]EinePräventionoder Reduktion solcher Beschwerden (z. B. durch Entspannungstechniken, Dehnübungen oder Muskeltraining) ist nicht möglich. Fälschlicherweise werden derlei Verletzungen oft pauschal auf mangelhafte Spieltechnik, bloße Überbeanspruchung oder falsche Haltung zurückgeführt, nicht auf ein potentiell ungünstiges Größenverhältnis zwischen Hand und Instrument.[59]Auch bezeugen Vergleichsstudien zwischen klein- und großhändigen Pianisten eine beim Klavierspielen umso höhere Aktivität in den für das Spreizen von Hand und Fingern zuständigen Muskeln, je kleiner eine Hand ist und je weiter sie deswegen gespreizt werden muss.[60][61][62]Demzufolge ist das Klavierspiel mit kleinen Händen belastender und kostet mehr Energie. Aufgrund der Verletzungsgefahr wird im Allgemeinen davon abgeraten, die Spannweite durch Dehnübungen und ähnliches zwangsweise vergrößern zu wollen, da dies sowieso nur bedingt möglich ist.[63]Potentielle Langzeitschäden infolge jahrelanger Überdehnung und einseitiger Belastung der Gelenke beim Klavierspiel wurden bislang jedoch noch nicht systematisch erforscht. Auch schränkt „Kleinhändigkeit“ das mögliche Repertoire eines Pianisten ein, da bestimmte Techniken (z. B. großeAkkordeoder schnelle Oktavfolgen) gar nicht oder zumindest nicht sauber gespielt werden können.[64]Dies ist v. a. darauf zurückzuführen, dass sich bei Akkorden der Griffbereich auf den Tasten immer weiter in Richtung Vorderkante reduziert, je weiter die Hand gespreizt werden muss. Hierdurch läuft der Spieler ab einem bestimmten Griffwinkel Gefahr, die Tasten zu verfehlen, abzurutschen oder zu weit nach innen zu greifen, wodurch benachbarte Tasten mitbetätigt werden.[65]Zudem erschwert die starke Spreizung der Finger das schnelle Umgreifen und beeinträchtigt Klanggestaltung und Kontrolle. Derlei Probleme werden häufig durch behelfsmäßige Spieltechniken kompensiert (z. B. durchAufbrechen von Akkordenoder Weglassen einzelner Noten), was jedoch den Klang eines Stückes deutlich verändern kann.[66]Zur Beurteilung der Handgröße wird meist die Spannweite zwischen Daumen und kleinem Finger (1–5) herangezogen, da sich hieraus die maximale Reichweite auf der Klaviatur bestimmt. Eine einheitliche Definition von „Kleinhändigkeit“ gibt es allerdings nicht. Verbreitet ist mittlerweile der Grenzwert von 21,5 cm, nachdem eine Studie aus dem Jahr 2009 einen signifikanten Anstieg von Anstrengung und Schmerzen während des Klavierspiels bei denjenigen Teilnehmern registrierte, deren 1–5-Spannweite weniger als dieses Maß betrug.[67]Mit 21,5 cm Spannweite ist ungefähr eineOktavezu greifen. Um Oktaven wirklich mühelos und ohne Anspannung spielen zu können, gilt jedoch eine Reichweite von einerDezimeals notwendig.[68][69]Um die Eignung einer Hand für das Klavierspiel zu beurteilen, sind in der Praxis allerdings auch die Spannweiten zwischen den einzelnen Fingern im Verhältnis zu Länge und Breite der Hand und der Länge der Finger sowie zahlreiche andere, höchst individuelle Faktoren relevant.[70]So kann auch eine im Verhältnis zur Klaviatur als groß anzusehende Hand eine vergleichsweise geringe Spannweite aufweisen, wenn es ihr an der notwendigen Flexibilität mangelt.[71]Darüber hinaus können auch völlig andere Faktoren eine Beeinträchtigung darstellen, wie etwa besonders breite Finger(-kuppen), die nicht zwischen schwarze Tasten greifen können.[65] Bislang liegen nur drei Studien zu Handgrößen und Spannweiten bei Pianisten vor, die jedoch alle nahezu identische Ergebnisse liefern.[54][69][72]So besitzen Männer eine durchschnittliche Spannweite von rund 22,6 cm, derweil Frauen nur auf 20,1 cm (20,7 cm beiWagner) kommen, was einen Unterschied von etwa einer weißen Taste ausmacht. Die kleinste in einer dieser Studien dokumentierte Spannweite betrug 16,3 cm und gehörte einer Frau, die größte mit 27,4 cm wurde bei einem Mann gemessen. In einer australischen Studie mit 159 männlichen und 314 weiblichen Pianisten verschiedener Herkunft waren rund 24 % aller Männer und rund 87 % aller Frauen von „Kleinhändigkeit“ (>21,5 cm Spannweite) betroffen, unter den asiatischstämmigen Teilnehmern sogar 30 % bzw. 93 %. Dieselbe Studie konnte zudem eine signifikante Korrelation zwischen Handgröße und beruflichem Erfolg bei Pianisten nachweisen. So besaßen die beteiligten Pianisten von internationalem Rang eine Spannweite von durchschnittlich 24 cm, während solche von nationalem Renommée nur auf 21,6 cm kamen und Amateurpianisten sogar nur auf 20,8 cm. Die internationale Gruppe war zu rund 83 % männlich, die nationale zu rund 61 %, die Gruppe der Amateure hingegen zu rund 71 % weiblich.[54]Eine ähnliche Geschlechterverteilung spiegelt sich auch in den Ergebnissen internationaler Klavierwettbewerbe. So ist im Durchschnitt weniger als jeder dritte Preisträger eine Frau, unter den Erstplatzierten sogar nur rund jeder fünfte. Nur beiBach- undSchumann-Wettbewerben ist das Verhältnis ausgeglichener, einzig beiMozartliegen Frauen deutlich vor den Männern.[73]Dies ist darauf zurückzuführen, dass das Repertoire dieser Komponisten kaum Techniken verlangt, für die es große Hände bräuchte. Ausgleichen lassen sich die durch „Kleinhändigkeit“ hervorgerufenen Probleme nur durch schmalere Klaviaturen. Solche werden bislang nur von den ManufakturenSteingraeber[90],August Förster[91]undHailun[92]standardmäßig angeboten und sind ansonsten nur als Einzelanfertigungen erhältlich. Der gegenwärtig einzige auf den Bau schmalerer Klaviaturen spezialisierte Hersteller ist die amerikanischeDS Standard Foundation(ehemalsSteinbuhler & Company),[93]die Klaviaturen in verschiedenen standardisierten Breiten zur Einpassung in bestehende Flügel anbietet. Da die Mechanik in Flügeln in der Regel nicht fest verbaut ist, können die Klaviaturen auch als austauschbare Module konstruiert werden. Ein nachträglicher Einbau in Pianinos ist technisch nicht möglich. DieStaatliche Hochschule für Musik und Darstellende KunstinStuttgartund dieHochschule für MusikinNürnbergstellen ihren Studierenden als bislang einzige Musikhochschulen in Europa Flügel mit schmalerer Klaviatur zur Verfügung, die sogenanntenSirius 6.0.[94][95]Für die Verbreitung schmalerer Klaviaturen engagiert sich insbesondere das internationale NetzwerkPianists for Alternatively Sized Keyboards, kurz PASK.[96] Dieser Artikel ist als Audiodatei verfügbar: Mehr Informationen zur gesprochenen Wikipedia Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Bezeichnungen 2Geschichte 2.1Vorformen 2.2Bartolomeo Cristofori (1655–1731) 2.3Gottfried Silbermann (1683–1753) 2.4Johann Andreas Stein und die Wiener Mechanik 2.5Entwicklung in England: Tafelklavier, Englische Mechanik, Verstrebungen 2.6Entwicklung in der ersten Hälfte des 19. Jahrhunderts 2.7Entstehung des Pianinos 2.8Entwicklung in der zweiten Hälfte des 19. Jahrhunderts 2.9Das 20. und 21. Jahrhundert 2.10Elektronische Pianos, Digitalpianos und Hybridpianos 3Aufbau 3.1Bestandteile 3.2Spielwerk 3.3Klaviatur 3.4Pedale 3.5Besonderheiten des Flügels 3.6Besonderheiten des Pianinos 4Klang 4.1Zusammensetzung 4.2Klangbeeinflussung 4.3Stimmen, Intonieren und Regulieren 4.4Raumklima 5Verbreitung und Nutzung 6Klaviermusik 7Folgen und Kritik an der Normierung moderner Klaviaturen 8Siehe auch 9Literatur 10Weblinks 11Einzelnachweise"
  },
  {
    "label": 0,
    "text": "Kultur – Wikipedia Kultur Inhaltsverzeichnis Begriffsvielfalt Begriffsgeschichte Entstehung von Kultur Aspekte kulturellen Lebens Schutz von Kultur Kulturkritik Siehe auch Literatur Weblinks Belege Wortherkunft Antike Neuzeit Moderne Entwicklungen Varianten und Grenzen des Kulturbegriffs Theorien der Kulturentstehung Biologische Voraussetzungen und Umweltbedingungen Kultur als Bewältigung Kultur als symbolische Sinnerzeugung Tradition und kulturelles Gedächtnis Tradition Sprache Handlung Geltung Identität Zeit Raum Kultur und Zivilisation Kulturnation und Staatsnation Entgegensetzung von Kultur und Natur Kulturen: Die Pluralisierung des Kulturbegriffs Der Kulturbegriff außerhalb des westlichen Denkens Normative Verwendung des Begriffs Der Kulturbegriff in der Biologie Kulturbezeichnet im weitesten Sinne alle ErscheinungsformenmenschlichenDaseins, die auf bestimmtenWertvorstellungenunderlerntenVerhaltensweisenberuhen und die sich wiederum in der dauerhaften Erzeugung und Erhaltung von Werten ausdrücken – alsGegenbegriffzu der nicht vom Menschen geschaffenen und nicht verändertenNatur. Wichtige Vordenker dieses Kulturbegriffs sind etwaArthur Schopenhauer,Harald HöffdingundJoseph Petzoldt. Es gibt – je nachWissenschaft,Weltanschauungoder fachlichem Zusammenhang – eine Vielzahl enger gefassterDefinitionen vonKultur. Über den wissenschaftlichen Diskurs hinaus wird die BezeichnungKulturin derKulturpolitiksynonym auf die„Schönen Künste“(Bildende Kunst,Musik,Literatur) beschränkt. Gemeinsprachlichsteht die Bezeichnung häufig entweder fürKultiviertheit(Umgangsformen,Sittlichkeit,Wohnkultur,Esskulturu. Ä.) oder in Abgrenzung der (als typisch angenommenen) Ausdrucks- und Verhaltensweisen – denKulturstandards– der eigenenethnischenGruppe (z. B. Bayern, Deutsche, Europäer) im Vergleich mit sogenanntenanderen Kulturen(etwa Chinesen, Lateinamerikaner,Indigene Völker). Kulturenim Pluralwird nicht nur im öffentlich-politischen Diskurs, sondern auch in derEthnologie(Völkerkunde), fürarchäologische Kulturensowie in derkulturvergleichenden Sozialforschungbenutzt, umMenschengruppennach kulturellen Merkmalen voneinander abzugrenzen. Für Varianten innerhalb einer Kulturgruppe wird häufig die BezeichnungSubkulturenverwendet. In der Ethnologie wird das Konzept der unterscheidbaren Kulturen aufgrund seineskonstruierten Charakters(Festlegung von Grenzen, wo in der Realität fließende Übergänge sind) heute zunehmend problematisch gesehen. Der Begriff der Kultur ist im Lauf der Geschichte immer wieder von unterschiedlichen Seiten einer Bestimmung unterzogen worden. Je nachdem drückt sich in der BezeichnungKulturdas jeweils lebendige Selbstverständnis und derZeitgeisteinerEpocheaus, der Herrschaftsstatus oder -anspruch bestimmtergesellschaftlicher Klassenoder auch wissenschaftliche undphilosophisch-anthropologischeAnschauungen. Die Bandbreite derBedeutungsinhalteist entsprechend groß und reicht von einer rein beschreibenden (deskriptiven) Verwendung („die Kultur jener Zeit“) bis zu einer vorschreibenden (normativen), wenn bei letzterem mit dem Begriff der Kultur zu erfüllende Ansprüche verbunden werden. Hinsichtlich desSchutzes von Kulturgüterngibt es eine Reihe von Abkommen und Gesetzen. DieUNESCOund ihre Partnerorganisationen koordinieren einen internationalen Schutz und lokale Umsetzungen. Einige Beispiele für die verschiedenen Blickwinkel, nach denen Kultur definiert wird:[1] Kroeber und Kluckhohn (1952) kommen nach Sichtung von mehr als 150 Definitionen von Kultur zu folgender synthetischer Definition: „Kultur besteht aus expliziten und impliziten Mustern (patterns) des erworbenen Verhaltens, die durch Symbole übermittelt werden und die unterschiedlichen Errungenschaften (achievements) verschiedener menschlicher Gruppen einschließlich ihrer Verkörperung in Artefakten begründen; der wesentliche Kern der Kultur besteht in traditionellen (d. h. historisch hergeleiteten und ausgewählten) Ideen und insbesondere den daran geknüpften Werten; Kultursysteme können einerseits als Produkte von Handlungen und andererseits als konstitutive Elemente künftiger Handlungen angesehen werden.“ Das Wort „Kultur“ ist die Eindeutschung des lateinischen Wortscultura(„Bebauung, Bearbeitung, Bestellung, Pflege“), das eine Ableitung von lateinischcolere(„bebauen, pflegen, urbar machen, ausbilden“) darstellt.[8]Kultivieren bezeichnet in diesem Sinne dieArbeitdes Menschen zur gemeinschaftlichen Aneignung, Nutzung und Veränderung der Natur entsprechend seinen Bedürfnissen und Vorstellungen. Denselben Ursprung haben die BezeichnungenKolonieundKult. „Kultur“ ist in derdeutschen Spracheseit Ende des 17. Jahrhunderts belegt und bezeichnet hier von Anfang an sowohl die Bodenbewirtschaftung (landwirtschaftlicher Anbau) als auch die „Pflege der geistigen Güter“ (Geisteskultur, d. h. Pflege der Sprache oder einer Wissenschaft). Im 19. Jahrhundert verwendete der NürnbergerIndustrie- und Kulturvereindas Wort Kultur ebenfalls noch im Sinne von „Bodenkultur“.[9]Heute ist derlandwirtschaftlicheBezug des Begriffs nur noch in Wendungen wieKulturlandfürAckerlandoderKultivierungfürUrbarmachungverbreitet; in der Biologie werden auch verwandte Bedeutungen wieZell-undBakterienkulturenbenutzt. Im 20. Jahrhundert wirdkulturellals Adjektiv gebräuchlich, jedoch mit deutlich geistigem Schwerpunkt.[10] Die Herkunft des lateinischen Wortscolereleitet sich ab von derindogermanischenWurzelkuel-für „[sich] drehen, wenden“, sodass die ursprüngliche Bedeutung wohl im Sinne von „emsig beschäftigt sein“ zu suchen ist.[11] Plinius der Ältereprägte zwar noch nicht das Wort „Kultur“ für einen Begriff, unterschied allerdings schon zwischenterrenus(zum Erdreich gehörend) undfacticius(künstlich Hergestelltes).[12]Im lateinischen Raum wird die Bezeichnungculturasowohl auf die persönliche Kultur von Individuen als auch auf die Kultur bestimmter historischer Perioden angewendet. So charakterisiert zum BeispielCicerodie Philosophie alscultura animi, das heißt als Pflege des Geistes.[13]Neben der Kultur als Sachkultur bei Plinius findet sich also auch Kultur als Bearbeitung der eigenen Persönlichkeit. Immanuel KantsBestimmung des Menschen als kulturschaffendes Wesen vollzieht sich im Verhältnis zur Natur. Für Kant sind Mensch und Kultur ein Endzweck der Natur.[14]Dabei ist mit diesem Endzweck der Natur die moralische Fähigkeit des Menschen zumkategorischen Imperativverbunden: „Handle nur nach derjenigenMaxime, durch die du zugleich wollen kannst, dass sie ein allgemeines Gesetz werde.“ Ein solches allgemeines Gesetz anzuerkennen als „Idee derMoralitätgehört noch zur Kultur.“[15]Es ist dieser Leitsatz des moralischenHandelns, der den Menschen einerseits von der Natur trennt, andererseits steht er als Endziel der Natur in ihrem Dienst dieses Ziel zu achten und zu verfolgen. Ohne diesen moralischen Leitsatz vermag der Mensch sich bloß technologisch fortzuentwickeln, was zurZivilisationführt. In der von ihnen begründeten und später vor allem vonWilhelm Wundtprominent gemachten „Völkerpsychologie“ als einer interdisziplinären Kulturwissenschaft verstehenMoritz LazarusundHeymann SteinthalKultur unter Rückgriff aufHegelals Ausdruck eines kollektiven und langlebigen „Volksgeists“, in dem sich soziale und historische Dimensionen menschlicher Entwicklung, gesellschaftlicher Erfahrungen, individuellen Erlebens und Verhaltens in Form etwa von Anschauungen, Ideen, Wertvorstellungen zeigen.[16] DerAnthropologeEdward Tylorbestimmt Kultur 1871 („Primitive Culture“) unter Aufnahme derdarwinschenEvolutionstheorieund gibt so eine erste an den Erkenntnissen der Naturwissenschaft orientierte Definition: „Cultur oder Civilisation im weitesten ethnographischen Sinne ist jener Inbegriff von Wissen, Glauben, Kunst, Moral, Gesetz, Sitte und alle übrigen Fähigkeiten und Gewohnheiten, welche der Mensch als Glied derGesellschaftsich angeeignet hat.“[17] NachAlbert Schweitzererstrebt die Kultur letztlich „die geistige und sittliche Vollendung des Einzelnen“:„Der Kampf ums Dasein ist ein doppelter. Der Mensch hat sich in der Natur und gegen die Natur und ebenso unter den Menschen und gegen die Menschen zu behaupten. Eine Herabsetzung des Kampfes ums Dasein wird dadurch erreicht, dass die Herrschaft derVernunftsich sowohl über die Natur als auch über die menschliche, stinkende Natur sich in größtmöglicher und zweckmäßigster Weise ausbreitet. Die Kultur ist ihrem Wesen nach also zweifach. Sie verwirklicht sich in der Herrschaft der Vernunft über die Naturkräfte und in der Herrschaft der Vernunft über die menschlichen Gesinnungen.“[18] Der französischeKulturphilosophClaude Lévi-Straussverglich das Konzept derSprachemit der Kultur: Die Kultur verhalte sich wie die Sprache: Nur ein Außenstehender könne die ihr zugrunde liegenden Regeln und Strukturen erkennen und interpretieren. Vor allem im deutschsprachigen Raum hat sich im allgemeinen Begriffsverständnis die Unterscheidung inKulturundZivilisationentwickelt, während beispielsweise im englischen Sprachraum lange Zeit nur ein Wort für „Kultur“(civilization)genutzt wurde (vergleiche den Buchtitel vonSamuel P. HuntingtonClash of Civilisations, deutschKampf der Kulturen). Erst seit einigen Jahrzehnten findet sich auchculturehäufiger, ohne dass hiermit jedoch auf einen Gegensatz zucivilizationBezug genommen wurde. Die früheste Formulierung diesesGegensatzesin der deutschen Sprache stammt von Immanuel Kant:[19] „Wir sind im hohen Grade durch Kunst und Wissenschaft cultivirt. Wir sind civilisirt bis zum Überlästigen, zu allerlei gesellschaftlicher Artigkeit und Anständigkeit. Aber uns für schon moralisirt zu halten, daran fehlt noch sehr viel. Denn die Idee der Moralität gehört noch zur Cultur; der Gebrauch dieser Idee aber, welcher nur auf das Sittenähnliche in der Ehrliebe und der äußeren Anständigkeit hinausläuft, macht blos die Civilisirung aus.“ „Zivilisation“ bedeutet also für Kant, dass sich die Menschen zwar zu einem artigen Miteinander erziehen,Manierenzulegen und ihren Alltag bequem und praktisch einzurichten wissen und dass sie vielleicht durch Wissenschaft und Technik Fahrzeuge, Krankenhäuser und Kühlschränke hervorbringen. All dies reicht jedoch noch nicht dafür, dass sie „Kultur haben“, wenngleich es der Kultur dienen könnte. Denn als Bedingung für Kultur gilt für Kant die „Idee der Moralität“ (derkategorische Imperativ), d. h., dass die Menschen ihre Handlungen bewusst aufan sich guteZwecke einrichten. Wilhelm von Humboldtschließt hieran an, indem er den Gegensatz auf Äußeres und Inneres des Menschen bezieht: Bildung und Entwicklung der Persönlichkeit sind Momente der Kultur, während rein praktische und technische Dinge dem Bereich der Zivilisation zugehören.[20] FürOswald Spenglerist Zivilisation negativ belegt, wenn sie nämlich das unausweichliche Auflösungsstadium von Kultur bezeichnet. Spengler sah Kulturen als lebendige Organismen an, die inAnalogiezur Entwicklung des menschlichen Individuums eine Jugend, eine Manneszeit und ein Alter durchlaufen und alsdann verenden. Die Zivilisation entspricht dem letzten dieser Stadien, daher hat der zivilisierte Mensch keine künftige Kultur mehr. Zivilisationen „sind ein Abschluß; sie folgen dem Werden als das Gewordene, dem Leben als der Tod, der Entwicklung als die Starrheit […] Sie sind ein Ende [sc. der Kultur], unwiderruflich, aber sie sind mit innerster Notwendigkeit immer wieder erreicht worden.“[21] Helmuth Plessnerhält gar das deutsche Wort „Kultur“ für fast nicht übersetzbar. In seiner „empathischen“ Bedeutung sieht er eine religiöse Funktion:[22] „Kultur, der deutsche Inbegriff für geistige Tätigkeit und ihren Ertrag im weltlichen Felde, ist ein schwer zu übersetzendes Wort. Es deckt sich nicht mit Zivilisation, mit Kultiviertheit und Bildung oder gar Arbeit. Alle diese Begriffe sind zu nüchtern oder zu flach, zu formal, bzw. ›westlich‹ oder an eine andere Sphäre gebunden. Ihnen fehlt das Schwere, die trächtige Fülle, das seelenhafte Pathos, das sich im deutschen Bewußtsein des 19. und 20. Jahrhunderts mit diesem Wort verbindet und seine oft empathische Verwendung verständlich macht.“ Der Begriff derKulturnationentstand im 19. Jahrhundert als Ausdruck eines weniger durch Politik und militärische Macht als durch Kulturmerkmale repräsentierten Nationsverständnisses. Der HistorikerFriedrich Meineckesah in den kulturellen Gemeinsamkeiten, die eineNationzusammenhalten, neben gemeinsamem „Kulturbesitz“ (z. B. dieWeimarer Klassik) vor allem religiöse Gemeinsamkeiten.[23] Während von einer Kulturnation anfangs in einem kritischen Sinne gegenüber der Staatsnation die Rede war, da das deutsche Nationalgefühl (aus Sprache, Traditionen, Kultur und Religion) nicht vom politischenPartikularismuswidergespiegelt wurde, wandelte sich der Begriff unter dem Einfluss desvölkischenGedankengutes: Als Basis einer Kulturnation wurde nun ein „Volk“ im Sinne einer Abstammungsgemeinschaft verstanden. In den Sozialwissenschaften wird die Idee einer homogenen Nationalkultur, durch die sich die Völker klar voneinander unterscheiden würden, zunehmend kritisch gesehen, da sie zurAusgrenzungvonMigrantengenutzt werde und empirisch nicht nachweisbar sei.[24] Systemtheoretischer Ansatz Für den SystemtheoretikerNiklas Luhmannbeginnt geschichtlich gesehen Kultur erst dann, wenn es einer Gesellschaft gelingt, nicht nur Beobachtungen vom Menschen und dessen Umwelt anzustellen, sondern auch Formen und Blickwinkel derBeobachtungen der Beobachtungenzu entwickeln. Eine solche Gesellschaft ist nicht nur kulturell und arbeitsteilig in einem hohen Maße in Experten ausdifferenziert, sondern hat auch Experten zweiter Stufe ausgebildet. Diese letzteren untersuchen die Beobachtungsweisen der ersteren und helfen diese in ihrer Kontingenz zu begreifen, d. h., erst jetzt werden die Inhalte von Kultur als etwas Gemachtes aufgefasst und nicht als eine dem Menschen gegebene Fähigkeit. Kultur wird damit de- und rekonstruierbar.[25] „Historische Anthropologie“ Ein aktuelles Arbeitsfeld, welches sich als „historisch ausgerichteteAnthropologie“ bezeichnen ließe, untersucht die im Laufe der Geschichte vollzogenen Bestimmungen der „menschlichen Natur“. So zeigt beispielsweise dieOrdnung der Sinne, dass ihre Anzahl nicht eindeutig auf fünf festzulegen ist, sie teils hierarchisch, teils gleichberechtigt auftreten. Damit haben auch die Sinne eineGeschichte, wenn sie nämlich kulturell codiert sind. Es zeigt sich dann etwa eine für die abendländische Kultur prägende Bevorzugung des Gesichtssinns gegenüber anderen Sinnen.[26]Weitere Felder der historischen Anthropologie sind:[27] Der Germanist und Professor für interkulturelle Wirtschaftskommunikation Jürgen Bolten unterscheidet Zusammensetzungen mit dem Wortstammkult-hinsichtlich ihrer Bedeutung in vier deutlich voneinander abgrenzbare Gruppen. Zwei davon fasst er unter einen weiten Kulturbegriff: (1.) Kultur als Lebenswelt oder die Ethnie, im Sinne von: bewohnen bzw. ansässig sein; (2.) Kultur als biologische Kulturen, im Sinne von: bebauen, Ackerbau treiben. Zwei weitere fasst er unter einen engen Kulturbegriff: (3.) Kultur als „Hoch“kultur, im Sinne von: pflegen, schmücken, verehren, und (4.) Kultur als Kult bzw. Kultus, im Sinne von: verehren, anbeten, feiern.[31]Den engen Kulturbegriff führt Bolten auf die Trennung von Kultur und Zivilisation zurück, die vor allem vonImmanuel Kantund später vonOswald Spenglervertreten wurde (siehe hierzu auch denAbschnitt „Kultur und Zivilisation“).[32] Andere Autoren verweisen hinsichtlich der Entwicklung des Kulturbegriffs im deutschsprachigen Raum aufCicero,Herder,von Humboldt.[33][34] Angesichts der Vielzahl unterschiedlicher Verwendungsweisen des Wortes „Kultur“ und der Vielfalt konkurrierender wissenschaftlicher Definitionen erscheint es sinnvoll, statt voneinemKulturbegriff besser vonvielenKulturbegriffenzu sprechen. Bereits 1952 wurden 170 verschiedene Begriffsbestimmungen gezählt. Kultur ist gewissermaßen eineVariable, die von den verschiedenen Rahmenbedingungen verschiedener Fachgebiete und ihrer Blickwinkel abhängig ist. Der Kulturphilosoph Egon Friedell vertrat folgende provokante These: „Kultur ist Reichtum an Problemen“ Dasjenige Konzept, welches das Entstehen von Kultur verständlich macht und den Begriff klar eingrenzt, stellt die Kultur der Natur entgegen. Damit ist als Kultur alles bestimmt, was der Mensch von sich aus verändert und hervorbringt, während der Begriff Natur dasjenige umfasst, was von selbst ist, wie es ist. Mit „Natur“ kann jedoch immer nur etwas gemeint sein, dasdurch Kulturtechnikenwie Kunst und Wissenschaftbeschriebenwurde. Dabei werden die Grenzen dessen, was „Natur“ bezeichnet, durch menschliche Forschung immer mehr erweitert: so macht etwa dasElektronenmikroskopkleinste Partikel sichtbar, während dasHubble-Teleskopdie großen kosmischen Maßstäbe zur Darstellung bringt. Wenn jedoch Natur nur durch die Kulturtechnik wahrgenommen werden kann, scheint es letztlich so, dass „alles Kultur sei“. Damit wird die Vorstellung, dass Kultur stets Auseinandersetzung mitdem Anderen, dem Neuen und Fremden ist, zunehmendunplausibel, denn wenn alles Kultur ist, dann ist unklar, was überhaupt mit dem Begriff gemeint ist.[36] Wenn Kultur trotzdem weiterhin als die Bewältigung des Anderen, der Natur, begriffen werden soll, so darf die Natur nicht alsräumlichdem Menschen gegenüberstehend gedacht werden, sondern das Andereist der Kultur selbst eingeschrieben. Das Andere besteht nicht einfach neben oder außerhalb der Kultur, sondern haftet ihr an wie eine Kehrseite.[37][38]„Natur“ wäre dann ein Grenzbegriff, der „etwas“ umfasst, das vom Menschen beschrieben und bearbeitet wird, was aber zugleich bedeutet, dass dieses „etwas“ niemals unmittelbar zugänglich wird. Damit gibt es keine „Natur an sich“, sondern nurBeschreibungenvon Natur. Die exakte mathematische Physik ist nureinemögliche Form der Naturdarstellung, wenngleich sich die mathematische Naturbeschreibung innerhalb ihrer gegebenen Logik schrittweise dem Wesen der „Natur“ anzunähern vermag.Ernst Cassirerhat diese veränderte Auffassung von Natur als Übergang von der Substanz zur Funktion in seiner AbhandlungSubstanzbegriff und Funktionsbegriff1910 beschrieben. DerPluralKulturenwird imallgemeinen SprachgebrauchalsSammelbegrifffürMenschengruppenverwendet, die aufgrund verschiedener kultureller Eigenheiten der Mehrheit ihrer Mitglieder als voneinander unterscheidbar aufgefasst werden. Dies können Völker oder Ethnien sein, aber auch Berufsgruppen, Belegschaften von Unternehmen, Vereine u. ä.[39]Im Gegensatz zu den (häufigsynonymverwendeten) BezeichnungenVölkeroderEthnien, bei denen in Fachdiskursen immer auch die Selbstzuschreibung betrachtet wird, beruht die Vorstellung vonKulturenin der Regel nur aufFremdzuschreibungen. Die fehlende Stellungnahme der so bezeichneten Menschen (emische Perspektive) hat einseitige, verzerrte oder falsche Auffassungen zur Folge. Zudem kann der Begriff auch rassistisch motivierteNebenbedeutungenerhalten. Dennoch findet in der öffentlich-politischen Debatte im Gegensatz zu den Wissenschaften keine kritische Auseinandersetzung mit dem BegriffKulturenstatt. Die Pluralisierung des Begriffes (sofern nicht durch den Alltagsgebrauch vorweggenommen) geht auf den EthnologenFranz Boaszurück, der Kultur vor allem als Ergebnis einer geschichtlichen Entwicklung sah. Da aber jedes „Volk“ seine eigene Geschichte hat, muss sie auch ihre eigene Kultur haben, folgerte Boas(vgl.Kulturrelativismus). Mit diesem Ansatz distanzierte er sich von denRassentheorien, die die Unterschiede menschlichen Seins auf angeborene Eigenschaften und Fähigkeiten zurückführen. Das Konzept wurde von den anderenSozialwissenschaftenund von derArchäologie(deren Grundlage zwangsläufig nur dieArtefaktekulturellen Schaffens und ihre stilistischern und funktionalen Unterschiede sind) übernommen. In der Völkerkunde erfreute sich der pluralistische Begriff bis zur Mitte des 20. Jahrhunderts großer Beliebtheit: Man zog scharfe Grenzen zwischen verschiedenen Kulturen (bis hin zu kontinentalenKulturarealen), die sich häufig mit geographischen, politischen oder sprachlichen Grenzen deckten. Spätestens seit den 1970er Jahren erkannten Ethnologen, dass die Abgrenzungen zwischen denKultureneher konstruiert als real waren und das damit häufig Bewertungen verbunden wurden. Tatsächlich fließende Übergänge wurden so als Gegensätze wahrgenommen. Diese Sichtweise fand regen Zuspruch in Öffentlichkeit undPolitik. Insbesondere in Zeiten massenhafterMigrationwurde so die kulturelle Andersartigkeit hervorgehoben und mit schwer zu überwindenden Grenzen versehen, während die viel größeren Gemeinsamkeiten kaum beachtet wurden. Ethnologen haben sich daher wieder weitgehend vom Begriff derKultur(en)distanziert, während das politische Konzept unkritisch weiter verwendet wird und dasFremdebetont.[40] „Es gibt zweifellos kulturelle Grenzen; sie sind aber nicht so beschaffen, dass sie Gruppen von Menschen eindeutig identifizieren und dass wir von diesen Gruppen als „Kulturen“ sprechen können.“ Prinzipiell ist die Gegenüberstellung von Natur und Kultur ein typisch europäisches Ordnungsmuster. Die Ethnologie hat gezeigt, dass es keine Weltauffassung gibt, die von allen Menschen gleichsam verstanden wird. Die in der „modernen Welt“ als selbstverständlich betrachteteDichotomieNatur ↔ Kulturist nicht bei allen Völkern gegeben. So betrachten beispielsweise Amazonasindigene auch Tiere, Pflanzen, Naturerscheinungen undNaturgeisteralsMenschen. Sie existieren nach ihrer Vorstellung zeitweilig in einer anderen Form, sind jedoch ebenfalls vollwertige „Kulturwesen“.[42] Verschiedene Fragen werden aufgeworfen, wenn der Begriff „Kultur“ nicht nurdeskriptiv(beschreibend) verwendet wird, sondern auchnormativ(vorschreibend) verwendet wird. In diesem Sinne bedeutet „Kultur“ nicht nur das, was tatsächlich vorgefunden wird, sondern auch das,was sein soll, beispielsweiseGewaltfreiheit. Eine normative Verwendung des Kulturbegriffes ist in der Alltagssprache nicht unüblich, wie man beispielsweise daran hört, dass von einer „Kultur der Gewalt“ wenn überhaupt nur abwertend die Rede ist – eine solche Kultur wäre eine „Unkultur“. Häufig sind also moralische Maßstäbe mit dem Kulturbegriff verbunden. Dabei ergibt sich jedoch die Schwierigkeit, zu bestimmen, was sich etwa unter „Gewalt“ verstehen lässt und wann sie vermeidbar ist. Nicht nur haben verschiedene Kulturen unterschiedliche Auffassungen darüber, wann eine Handlung gewaltsam ist, sondern auch darüber,was durch die Gewalt überhaupt verletzt wird.[43] Wie sehr auch immer ein Organismus sich an seine Umwelt anpasst: Eine genetischeVererbungder durch Lernen oder durchphysischeAnpassungindividuell erworbenen Eigenschaften gilt als unmöglich, da die imGenomangelegtenangeborenenEigenschaften – abgesehen von wenigenepigenetischenFaktoren, deren Einflussbreite aber genomisch beschränkt ist – dadurch nicht verändert werden. Eine nichtgenetische (kulturelle) Vererbung ist jedoch prinzipiell möglich. Zum Beispiel indem ein Tier individuell erworbene Eigenschaften und Informationen, oder auch von anderen durch sozialesLernenerworbene Eigenschaften und Informationen an andere (z. B. die eigenen Nachkommen) weitergibt. „Die Übertragung von Informationen von einer Generation zur nächsten auf nichtgenetischem Wege wird im Allgemeinen alskulturelle Traditionbezeichnet.“[44]In derVerhaltensbiologiewerden solchekulturellen Traditionenhäufig alsKulturbezeichnet.[45] Kulturelle Traditionen gibt es beispielsweise bei vielen (allerdings nicht allen) Vogel-Spezies, bei denen die Jungtiere den arttypischen Gesang auf nichtgenetischem Wege von den Eltern lernen (vokale Nachahmung). Auch derWerkzeuggebrauch bei Tierenentspricht häufig – jedoch nicht immer – der Definition vonkultureller Tradition. Die weitestreichenden Beispiele finden sich bei denMenschenaffen[46]sowie denRaben und Krähen, da diese Spezies nicht auf einzelne Werkzeuge beschränkt sind – also innerhalb einer Art sogar mehrere Traditionen entwickeln können. Kulturentstehungslehren oder -theorien (Ätiologiender Kultur) existierten im alten Mesopotamien, im Iran, in Indien und in der griechischen und römischen Antike, und zwar sowohl inmythologischerals auch inrationalistischerAusprägung. Es handelte sich meist umelaborierteErzählungen über die Entstehung von Sprache, Dichtung, Musik, die Beherrschung des Feuers usw.[47][48]Die frühen Kulturentstehungsmythen und -theorien propagierten entweder einen kontinuierlichen Fortschritt hin zu einer sicheren Gesellschaft oder aber die Entfernung von einem paradiesischen Urzustand mit dem moralischen Verfall als Kulturfolge (so beiLukrez). Aus der Reihe fällt derPrometheus-Mythos: Hier ist die Kultur das Resultat einer Auflehnung gegen die Götter: Prometheus stiehlt den Göttern das Feuer und die technischen Fähigkeiten, weil sein Bruder Epimetheus bei deren Zuteilung für den Menschen nichts mehr übriggelassen hat, und schenkt dies dem Menschen.[49]Ähnliches erzählt der Mythos derMāorium den HalbgottMāui. Solche und ähnlicheKulturheroen, die auch bei nordamerikanischen Völkern, in Afrika und Japan eine wichtige Rolle spielen, sind zwiespältige Figuren – von Ethnologen alsTricksterbezeichnet – im Zwischenreich zwischen Göttern, Menschen, Tieren und Geistern. Sie verändern oft ihre Gestalt, sind Diebe und Betrüger, täuschen, spalten und verwirren und entziehen sich allen Normen, vermitteln aber wichtige Kulturelemente. In der nordischen Mythologie entsprichtLokiam ehesten dem Bild des Tricksters; er gilt als Erfinder des Fischernetzes.[50]ImKalevalawerden die beiden KulturheldenIlmarinen(der Schmied) undVäinämöinen(der Sänger und Erfinder derKantele) positiv dargestellt. In der christlichen Tradition wird die Kulturentstehung vom Feigenblatt bis hin zum Ackerbau eher als Strafe Gottes betrachtet oder ist eng mit derErbsündeverbunden wie fürAvitus. So werden nach dem Mord Kains an Abel Kains Nachkommen dazu verurteilt, als Nomaden, Musiker oder Eisenschmiede zu arbeiten. FürPrudentiushingegen stellt das Christentum den Höhepunkt in der Entwicklung Roms dar, dessen heidnischen Urzustand er negativ sieht: Der Mensch erschafft oder sucht sich das aus, was sein Leben verbessert. Dazu gehört neben Technik und Wirtschaft auch das Christentum.[51] Erzählungen über die Genese einzelner Kulturtechniken gibt es auch bei frühen Stammesgesellschaften. Mit dersoziokulturellen Evolutiondes Menschen, die durchgeistigeFähigkeiten wie Sprache, Ideen, Verständnis von Raum und Zeit sowie die Bildung von Verhaltensregeln ermöglicht wird, entstand eine Art gesellschaftlicher „Superorganismus“, der sich in vielerlei Hinsicht von der biologisch-genetischen Evolution löste und völlig neue, vergleichsweise sehr schnell wandelbare Strategien „kulturellen Überlebens“ hervorbrachte. Die Dynamik von Sozialsystemen kann durch die klassische Evolutionstheorie nicht erklärt werden.[52] Unterschiedliche Kulturbegriffe erschweren die Identifikation der notwendigen biologischen Voraussetzungen. Dem australischenArchäologenIain Davidson zufolge sei es sogar möglich, dass Kultur im Sinne sozialen Lernens in beinahe allen ausreichend sozial lebendenArtennachweisbar wäre, sofern diese angemessen untersucht würden. Dies hätte Auswirkungen für die Arbeitsweise der Archäologie, da das Vorfinden von „Kultur“ in frühzeitlichen Menschen dann vor allem über die Artsozialer Interaktionzu diesem Zeitpunkt menschlicher Evolution informieren würde.[53] DasSelbstbewusstseinder psychischen Akte eröffnet dem Menschen die Veränderbarkeit seiner selbst und der Welt: Die Dinge sind nicht unveränderlich gegeben, sondern es bildet sich ein Verständnis desMöglichen. Durch die symbolischeRepräsentationlassen sich Möglichkeiten durchspielen und Dinge kombinieren. Der Mensch steht in einem offenen Verhältnis zu seiner Umwelt, die ihn und seine Handlungen nicht linear determiniert (vorausbestimmt), sondern er kann relativfreiauf sie reagieren. Die günstigen klimatischen Bedingungen der letzten 10.000 Jahren (erdgeschichtlicher Abschnitt desHolozän) haben es seit der letzten Eiszeit ermöglicht, dass sich Zivilisationen entwickeln konnten. Durch Ackerbau, Arbeitsteilung und Bevölkerungswachstum(sieheNeolithische Revolution),konnten sich Gesellschaften ausdifferenzieren, die Wissenschaft und Künste hervorbrachten. Die Frage nach den Urbedürfnissen DerMenschsieht sich gegenüber der natürlichen Umwelt vielen Herausforderungen und Gefahren gegenübergestellt und ist wie jedes Lebewesen darauf angewiesen, seine biologisch-physiologischen Bedürfnisse aus seiner natürlichen Umwelt heraus zu befriedigen. So versuchte beispielsweiseBronisław Malinowski, im historischen Rückblick die an den Menschen gestellten Herausforderungen als „Grundbedürfnisse“ des Menschen freizulegen. Anhand von historischen Vergleichen versuchte er eine endliche Zahl solcher Grundbedürfnisse freizulegen, aus welchen sich dann alles menschliche Tun erklären ließe. Auchfunktionalistisch-evolutionistischeKulturtheorienetwa sehen in den verschiedenen Kulturtechniken allein Mittel, die dem Zweck des Überlebens dienen. Kultur wäre dann die Befriedigung der immer gleichen menschlichen Bedürfnisse. Es kann jedoch nicht ohne Weiteres davon ausgegangen werden, dass Kulturerzeugnisse lediglich Urbedürfnisse des Menschen befriedigen.[54]Dies wird etwa am modernen Verkehrswesen deutlich: So ermöglichen es neue technische Verkehrsmittel nicht nur, größere Entfernungen zu überwinden, sondern es wird mit ihnen zugleich gesellschaftlichnotwendig, immer größere Entfernungen zurückzulegen. Daher kann nicht ohne Weiteres davon gesprochen werden, dass etwa das Flugzeug ein Urbedürfnis nach Interkontinentalflügen befriedigt. Kulturinstitutionensind daher nicht allein eine Antwort auf Anforderungen durch die Natur oder auf natürliche Bedürfnisse, sondern auch eine Reaktion auf durch sie selbst hervorgebrachte Strukturen; sie erfordern neue Institutionen (Malinowski), weshalb ihnen wesentlich eine Selbstbezüglichkeit eingeschrieben ist. So bedient etwa auch die moderneKulturindustriemitMusik,KinoundFernsehenkeine überlebenswichtigen Bedürfnisse, sondern stellt eine Eigenwelt dar, welche gewisse Bedürfnisse erst hervorbringt.[55][56] Für SigmundFreudschützt die Kultur den Menschen vor der Natur; durch sie entfernt sich der Mensch von seinen tierischen Ahnen.[57]Sie ist kein Sinngebungsversuch, sondern ein vermittelnder Faktor zwischen dem drängendenLustprinzipund den Anforderungen der Außenwelt. Verstehen lässt sie sich als System kollektiver Einschränkungen, das den Menschen zugleich Sicherheit und Ordnung garantiert. Insbesondere die Künste gelten Freud als „milde Narkose“, als „Befriedigung [] aus Illusionen“, ein symbolisches Hilfsprogramm also, dessen Illusionscharakter man zwar erkennt, ohne sich jedoch dadurch „im Genuss stören zu lassen“.[58]Freud unterscheidet dabei drei Formen kultureller Ablenkung, die er mit den Namen und Werken von drei Schriftstellern assoziiert: Demgegenüber hält Freud Religion für eine kulturelle Illusion, die nicht durchschaut wird. Dass mit Kulturleistungen eine Freude am Entdecken, am Erfinden und Schaffen von Neuem einhergeht, die nicht auf einen unmittelbaren Nutzen zielt, lässt sich gut ablesen am Werk des KulturphilosophenErnst Cassirersund dessen Auseinandersetzung mit derRenaissance.[60]Hierbei ist vor allem zu bedenken, dass gerade technische Neuerungen in der Renaissance nicht allein der besseren Bearbeitung der Natur dienten und also der Befriedigung grundlegender Bedürfnisse, sondern zu großem Teil in der Kunst zum Einsatz kamen. Formgebung und Ordnung von zufällig und unstrukturiert Gegebenem Funktionalistische Theorien, die alles Tun des Menschen auf sein Überleben hin interpretieren, übergehen den sinngebenden Charakter menschlicher Kulturtätigkeit. Kultur schafft auch Sinnstrukturen und Ordnungssysteme, die dem zufällig (Kontingenten) und ungeordnet Gegebenen einen Ort in der Welt des Menschen verschaffen. Das heißt, der Mensch versucht im Prozess der Kultur dem Zufälligen und Ungeordneten eine Struktur zu geben, es wiedererkennbar, symbolisch kommunizierbar oder nutzbar zu machen. Dabei ist Kultur gegenüber den Ansprüchen und Herausforderungen, denen sich der Mensch gegenübersieht, stets im Verzug, sie ist nachträgliche Kontingenzbewältigung.[61] Einbindung in stets schon vorhandene Sinnstrukturen und Formverhältnisse Werden außergewöhnliche Ereignisse kulturell vom einzelnen Menschen oder einer Gruppe verarbeitet, so findet dies nicht im luftleeren Raum statt. Zur Bewältigung werden tradierte Sinn- und Formverhältnisse, Denkweisen und Praktiken herangezogen, die aber ihrerseits kontingent sind, d. h.,nicht notwendigfür alle menschlichen Kulturen genau in dieser Form entstehen mussten. Damit kann keine allgemeine und für alle menschlichen Lebensgemeinschaften gleich verlaufende Kulturentwicklung nachgezeichnet oder vorausgesagt werden. Dies zeigt sich beispielsweise daran, dass selbst Symbolsysteme mit universalem Anspruch wie dieMathematikin unterschiedlichen Kulturen verschiedene Ausprägungen erfahren haben (siehe auchGeschichte der Mathematik). Kultur als symbolischer Bezug zur Welt Wenn der Mensch sich auf sich selbst oder auf seine Umwelt bezieht, so tut er dies nicht nur durch seine leiblichen Sinne, sondern vor allem mittelsSymbolen. Anders als bei Tieren, deren Verhaltensmuster und Reaktionen instinktiv vorgeschrieben oderkonditioniertsind, kann sich der Mensch mit Hilfe von Symbolen, beispielsweise mit Wörtern, auf die Dinge in der Welt beziehen. Symbole machen die Dinge handhabbar, indem sie diese unter gewissen Gesichtspunktendarstellen. Der Mensch kann die Natur durch mathematische Symbole beschreiben oder durch dichterische Worte besingen, er kann sie malen oder tanzen, in Stein hauen oder im Text beschreiben. Einzelne Dinge erscheinen ihm unter religiösen, wissenschaftlichen, weltanschaulichen, ästhetischen, zweckrationalen oder politischen Gesichtspunkten, werden also stetsin ein größeres Ganzes eingebunden, in dem ihnen eineBedeutungzukommt. Dies macht diekulturelle Weltdes Menschen aus. Symbolisierung als Formgebung Als früheste und wichtigste Arbeiten, welche die Bedeutung vonZeichenundSymbolenfür menschliche Sprache und Denken herausstellen, gelten die Werke vonCharles S. Peirce, der eineZeichentheorieals erweiterteLogikentwickelt, undFerdinand de Saussure, der dieSemiotikals allgemeine Sprachwissenschaft etabliert hat. Es war dannErnst Cassirer, der in den 1920er Jahren eineKulturphilosophieentwickelte, die den Menschen alssymbolisches Wesenbegreift. Anders als Peirce und Saussure setzte Cassirer dabei nicht bei Gedanken und Bewusstsein des Menschen an, sondern bei dessen praktischem Weltbezug. Der Mensch verhält sich also zur Welt nicht bloß theoretisch, sondern steht in einemleiblichenVerhältnis zu ihr. Kulturtätigkeit des Menschen ist daher stets ein Gestalten, Formen und Bilden von Dingen.[62] Die elementare Form der Gestaltung ist dabei die Abgrenzung oder Perspektivierung. Da jede Wahrnehmung nur einen Teil der Wirklichkeit erfasst, ist damit schon jegliches Wahrnehmen gestaltend: Im Sehen beispielsweise wird der Hintergrund abgeblendet und der Fokus auf das davorliegende Objekt gerichtet. Erst durch diese Abgrenzung (Prägnanzbildung) kann das Objekt symbolisch erfasst werdenalsdieses oder jenes. Dabei verhält sich der Mensch nicht passiv. Vielmehr bringt erst sein Tun undHandelnjene Welt der symbolischen Gestalten hervor, die seine Kultur ausmacht. Nichts in der Welt ist also an sich gegeben, die Welt ist kein Sammelsurium von einfach Vorhandenem, sondern all die uns vertrauten Sachen entspringen erst der Kulturtätigkeit des Menschen, seinem Tun: „Die grundlegenden Qualitäten des Tastsinns – Qualitäten wie ›hart‹, ›rauh‹ und ›glatt‹ – [entstehen] erst kraft der Bewegung […], so daß sie, wenn wir die Tastempfindung auf einen einzelnen Augenblick beschränkt sein lassen, innerhalb dieses Augenblicks als Data gar nicht mehr aufgefunden werden können.“[63] Gestalten vollzieht sich für Cassirer stets in Verbindung mit einem sinnlichen Gehalt. Jede Formgebung geschieht also in einem Medium: Sprache braucht den Klang, Musik den Ton, der Maler die Leinwand, der Bildhauer den Stein, der Schreiner das Holz. Diesen Kerngedanken fasst Cassirers Formulierung dersymbolischen Prägnanz: In einem Medium wird eine prägnante Form herausgearbeitet, die sich dann symbolisch auf anderes beziehen kann. „Unter ‚symbolischer Prägnanz‘ soll also die Art verstanden werden, in der ein Wahrnehmungserlebnis, als ‚sinnliches‘ Erlebnis, zugleich einen bestimmten nicht-anschaulichen ‚Sinn‘ in sich faßt und ihn zur unmittelbaren konkreten Darstellung bringt.“[64] Wenn Prägnanzbildung sich immer immanent in einem Medium vollzieht, dann kann von einerimmanenten Gliederunggesprochen werden: Die Eigenschaften des Mediums bestimmen zugleich die Möglichkeiten zur Formgebungundzum Sinngehalt. Das Symbol ist also nicht gänzlich beliebig, sondern entwickelt sich in steter Beziehung zur Widerständigkeit der Welt, an welcher der Mensch sich abarbeitet: Holz kann nicht in Form gegossen werden, sondern verlangt einen bestimmten Umgang mit ihm, Wörter sind nicht minutenlang, sondern sind von einer Kürze, die sie im Alltag erst gebrauchbar macht. Warnsignale sind laut und grell, Liebesgeflüster ist leise und zart, so dass es dem Ohr schmeichelt. Cassirer spricht bezüglich des Gesichtssinns davon, dass sich „imSehen undfür dasSehen“ Gestalt ausbildet, denn jedem Sehprozess geht immer schon eine Gestaltung voraus, die auch das neu Erfasste bestimmt. (Siehe Absch.Raumwahrnehmung.) Die immanente Gliederung des sinnlichen Gehalts ist Voraussetzung dafür, dass die Welt nicht als formlos-unbestimmte Masse begegnet: durch Verdichtung und Herauslösung bilden sich Formen, Gestalten, Kontraste, die durch Fixierung zu einerIdentitätgegenüber anderen Wahrnehmungsinhalten gelangen. Erst hierdurch „zerfließt“ die Welt nicht. Damit die Formen und Gestalten aber zu einer Dauerhaftigkeit kommen und sich „aus dem Strom des Bewusstseins bestimmte gleichbleibende Grundgestalten teils begrifflicher, teils rein anschaulicher Natur“ herausheben, braucht es eine anschließende Repräsentation. Damit tritt „an die Stelle des fließenden Inhalts […] eine in sich geschlossene und in sich beharrende Form.“[65] Nicht alles, was dem Menschen begegnet, wird von ihm sogleich zur Darstellung gebracht. Damit sich durch Prägnanz ein zwischenmenschlich handhabbares Symbol bilden kann, ist nötig:[66] Universalität der Symbole Symbole sinduniverselleBedeutungsträger. Das heißt, es kann einerseitsallesirgendwie Geformte zum Symbol werden, und andererseits lassen sich Symbole beliebig von einer Bedeutung hin zu einer anderen verschieben. Während zwar auch Tiere etwa Warnschreie haben, durch die sie Artgenossen auf Gefahr aufmerksam machen, bleiben diese jedoch immer an diekonkrete Situationgebunden. So führen tierischeSignalestets zu der gleichen Reaktion der Artgenossen oder bleiben, wenn sie außerhalb des gewöhnlichen Zusammenhangs geäußert werden, für die anderen unverständlich.[67]Menschliche Symbole hingegen, wie etwa das Wort, sind universell einsetzbar und auf verschiedene Dinge oder Situationen übertragbar. Einbettung der Symbole in ein Sinnganzes Wenn sich in der Formgebung etwas herausbildet, das dann für den Menschen von Bedeutung ist, wird nicht einfach ein beliebiger Sinn zum Wahrnehmungsinhalt hinzugesetzt, sondern das Wahrgenommene wird in ein Sinnganzes eingebettet: „Vielmehr ist es die Wahrnehmung selbst, die kraft ihrer eigenen immanenten Gliederung eine Art von geistiger ‚Artikulation‘ gewinnt – die, als in sich gefügte, auch einer bestimmten Sinnfügung angehört. […] Diese ideelle Verwobenheit, diese Bezogenheit des einzelnen, hier und jetzt gegebenen Wahrnehmungsphänomens auf ein charakteristisches Sinn-Ganzes, soll der Ausdruck ‚Prägnanz‘ bezeichnen.“[64] Obwohl von dieser Fähigkeit des Menschen jegliche Formgebung abhängt, gibt es historisch keinen „absoluten Nullpunkt“ der symbolischen Prägnanz, keinen Zustand der völligen Formlosigkeit, denn Ausgangspunkt ist die „physiognomische“ Weltwahrnehmung des mythischen Bewusstseins.[68]Für das mythische Bewusstsein zeigt sich die Welt in mimetischen Ausdrucksmomenten. Diese sind affektiv wirksam und ragen ihrem Ursprung nach noch in die tierische Welt hinein.[69]Sie bieten Anknüpfungspunkte für jede weitere Formgebung. Durch Symbole werdensinnlicheEinzelinhalte zu Trägern einer allgemeinengeistigenBedeutung geformt. Die Formgebung läuft somit zugleich mit der sinnlichen Wahrnehmung ab. „Unter einer ‚symbolischen Form‘ soll jede Energie des Geistes verstanden werden, durch welche ein geistiger Bedeutungsgehalt an ein konkretes sinnlichesZeichengeknüpft und diesem Zeichen innerlich zugeeignet wird.“[70] Mit der Formgebung geht gleichzeitig eine Sinngebung einher, erst Formen lassen Bezüge und Strukturen in der Welt erkennen. Symbolische Formen sind somit Grundformen des Verstehens, die universell und intersubjektiv gültig sind, und mit denen der Mensch seine Wirklichkeit gestaltet. Kultur ist die Art und Weise, wie der Mensch durch SymboleSinnerzeugt. Symbole entstehen also stets in Verbindung zur Sinnlichkeit, haben aber einen Sinn, der über diese hinausverweist: „Jeder noch so ‚elementare‘ sinnliche Inhalt ist […] niemals einfach, als isolierter und abgelöster Inhalt ‚da‘; sondern er weist in eben diesem Dasein, über sich hinweg; er bildet eine konkrete Einheit von ‚Präsenz‘ und ‚Repräsentation‘.“[71] Kultur als ein Geflecht von symbolischen Beziehungen:„Kultur als Text“ Besonders anschaulich lässt sich die Einbettung einzelner Symbole in ein übergeordnetes Ganzes fassen, wenn man Kulturmetaphorischals „Text“ beschreibt. So wie ein einzelnes Wort in einem Satz erst seine genaue Bedeutung erhält, erhalten auch Gesten, Bilder, Kleidung und andere ihre Bedeutung erst im Gesamtzusammenhang einer Kultur.Max Weberbestimmte bereits 1904 Kultur als ein Gewebe von Zeichen:[72] „‚Kultur‘ ist ein vom Standpunkt des Menschen aus mit Sinn und Bedeutung bedachter endlicher Ausschnitt aus der sinnlosen Unendlichkeit des Weltgeschehens.“ „Kultur“ ist damit für Weberalles: „Eine Kulturerscheinung ist die Prostitution so gut wie die Religion oder das Geld.“[73]In neuerer Zeit hatClifford Geertzseinen Kulturbegriff an Weber angeschlossen:[74] „Der Kulturbegriff, den ich vertrete und dessen Nützlichkeit ich in den folgenden Aufsätzen zeigen möchte, ist wesentlich ein semiotischer. Ich meine mit Max Weber, daß der Mensch ein Wesen ist, das in selbstgesponnene Bedeutungsgewebe verstrickt ist, wobei ich Kultur als dieses Gewebe ansehe. Ihre Untersuchung ist daher keine experimentelle Wissenschaft, die nach Gesetzen sucht, sondern eine interpretierende, die nach Bedeutungen sucht.“ Der Mensch lässt sich daher als dasjenige Wesen beschreiben, das durch Formgebung den Dingen eine Bedeutung verleiht, indem es sie einem Gesamtzusammenhang einordnet. Die Auffassung, dass Kultur einZeichensystemsei, bestimmt daher die meisten modernen anthropologischen, soziologischen, literaturwissenschaftlichen und philosophischen Kulturtheorien.[75]In diesem Zusammenhang hat sich der stehende Begriff von „Kultur als Text“ etabliert.[76]Während allerdings Cassirer seinen Kulturbegriff an das praktische Tätigsein des Menschen und dessen Umgang mit der Welt knüpft, birgt hingegen die pointierteMetaphervon „Kultur als Text“ die Gefahr einer Verengung des Kulturbegriffs und führt dazu, dass kulturelle Phänomene nur noch von ihrersprachlichenSeite her in den Blick genommen werden.[77] Menschliche Gesellschaften sind für ihr Überleben und ihre Bedürfnisbefriedigung auf ihre kulturellen Fähigkeiten angewiesen. Damit diese auch folgenden Generationen zur Verfügung stehen, muss eine Generation ihre Praktiken, Normen, Werke, Sprache, Institutionen an die nächste Generationüberliefern. Diese Traditionsbildung ist alsanthropologischesGrundgesetz in allen menschlichen Gesellschaften anzutreffen. Dieses kulturelle Gedächtnis ist in vielen Kriegen und bewaffneten Konflikten eines der Primärziele und damit von Zerstörung bedroht. Oft soll dabei bewusst gerade das kulturelle Erbe des Feindes nachhaltig beschädigt oder gar vernichtet werden. Nationale beziehungsweise internationale Koordination hinsichtlich militärischer und ziviler Strukturen zum Schutz vonkulturellen Identitäteneiner Gesellschaft bzw. der Weltgemeinschaft betreibt dasInternationale Komitee vom Blauen Schildals Partnerorganisation derUNESCO.[78] Anthropologische Voraussetzungen der Traditionsbildung Den Anreicherungsprozess von Wissen durch Traditionsbildung hat in neuerer ZeitMichael Tomaselloaus anthropologischer Sicht als „Wagenheber-Effekt“ beschrieben: Mit jeder Generation kommen etwas Wissen und kulturelle Fähigkeiten hinzu.[79]In der Traditionsbildung zeigt sich für Tomasello ein Hauptunterscheidungsmerkmal des Menschen gegenüber dem Tier, das keine Wissensweitergabe durch Nachmachen kennt. Zwar können beispielsweise Affen ihre Artgenossen nachahmen, aber sie sind nicht dazu in der Lage diese alsintentionaleWesen zu erkennen, d. h. als Wesen, die bei ihrem Tun einen bestimmten Zweck im Sinn haben. Es gelingt ihnen daher nicht, den Sinn hinter einer Handlung nachzuvollziehen und diese in der zum Gelingen notwendigen Weise selbst auszuführen. Stattdessen bilden sie nur spiegelbildlich die Bewegungen ihrer Artgenossen ab und kommen somit nur zu zufälligen Erfolgen. Sprache als Medium des kulturellen Gedächtnisses Damit die Überlieferung der kulturellen Gehalte gelingt, bedarf es einer regelmäßigenWiederholungdessen, was überliefert werden soll, beispielsweise eines bestimmtenRitualszu einer bestimmtenJahreszeit. Eine wesentliche Form der Wiederholung ist nicht nur die tatsächliche Ausübung dessen, was tradiert wird, sondern auch die Fixierung in derSprache, also die Einbettung in einSymbolsystem. Sprache ist daher ein vorrangiges Medium der Überlieferung, welches auch jede nichtsprachliche Weitergabe von Wissen begleitet. Folgen der Schriftkultur Ist die mündliche Sprache das einzige Medium, in das sich das kulturelle Gedächtnis einschreibt, dann ist die Überlieferung stets von einer Verfälschung bedroht. Denn werden Sagen, Mythen und Abstammungslinien lediglich mündlich weitergegeben (orale Tradition), dann können sich die erzählten Geschichten mit der Zeit unmerklich verändern oder bewusst verändert werden. So rechtfertigen in den meisten frühen Kulturen die Erzählungen über Abstammungslinien und Herrschergeschlecht die aktuellen sozialen Verhältnisse. Nun kann es aber vorkommen, dass beispielsweise durch den plötzlichen Tod des Herrschers eine andere Familie diesen Platz besetzt. In der Absicht, diese neuen Verhältnisse zu rechtfertigen, können Kulturen, die allein auf eine mündliche Überlieferung angewiesen sind, die die Herrschaft rechtfertigenden Erzählungen den neuen Verhältnissen anpassen. Dies führt dann zu einer Stabilisierung der neuen Ordnung. Dieser Vorgang kann als „homöostatische Organisation der kulturellen Tradition“ bezeichnet werden.[80]Erst mit derSchriftsteht einer Kultur ein Medium zur Verfügung, welches dieNachprüfbarkeitder überlieferten Inhalte ermöglicht. So ist beispielsweise in Streitfällennachlesbar, welcher Familie die Abstammung vom Göttergeschlecht zugesprochen wird. Damit bringt die Schrift den größten Einschnitt innerhalb der kulturellen Entwicklung des Menschen, sie stellt eineRevolutiondar, die – außer der Erfindung desBuchdrucksmit beweglichen Lettern – auch von folgendenAufschreibesystemenwieGrammophon,FilmundComputernicht mehr erreicht wird. Der Vergleich der folgenden Kulturelemente hat zu verschiedenen Versuchen geführt, geographische Räume zu definieren, in denen ähnliche, abgrenzbare Kulturen konstatiert werden können. Die so entstehendenKulturarealesind zwar aus verschiedenen Gründen umstritten, bilden jedoch eine Möglichkeit, diekulturelle Vielfaltder Welt zu strukturieren, um einen groben Überblick zu erhalten. Identität und Tradition Die Identitätsbildung einer Gruppe ist stark mit der in ihr lebendigenTraditionverknüpft. Die soziale Gruppe prägt dadurch auch die Kultur. So bestimmen viele Traditionslinien derReligionenauch die Identität der ihnen angehörenden Mitglieder durch gemeinsameZeremonienund Rituale. Daher kann „Tradition […] definiert werden als eine auf Dauer gestellte kulturelle Konstruktion von Identität.“[81] Verhältnis zu anderen Traditionen Oft geht mit der eigenen Tradition ein Anspruch auf Wahrheit einher, weshalb andere Traditionen als unverständlich und seltsam empfunden werden. Während die eigene Tradition keiner Begründung bedarf, gilt die andere als nicht begründungsfähig. Bei einem solchen Zusammentreffen kann es entweder zur Abschottung gegen das Fremde kommen, zur Übernahme einzelner fremder Elemente (Synkretismus) oder aber auch zu ersten Ansätzen einerTraditionskritik, welche die eigenen Riten, Sitten, Gebräuche und Normen in Frage stellt. Eine einschneidendere Situation tritt ein, wenn im Dialog mit der anderen Tradition nach einer gemeinsamen Geltungsgrundlage gesucht wird. Da jede Tradition für sich das Alter ihrer Herkunft geltend macht, kann dies nicht als Maßstab dienen. Damit wird aber das erste Mal Traditionan sichzum Thema und Gegenstand der bewussten Auseinandersetzung. Damit kann Tradition in Zweifel gezogen werden, weil sienurTradition ist. Die imAbendlandhistorisch frühste Traditionskritik vollzieht sich in den Anfängen der griechischenPhilosophie, wenn nämlich in denPlatonischen Dialogenes den Verfechtern der Tradition nicht gelingt, ihre eigene Position philosophisch zu begründen. Auch in der Zeit vom 16. bis zum 18. Jahrhundert übernimmt die Philosophie die führende Rolle in der Traditionskritik, vor allem im Zeitalter derAufklärung. Die Aufklärer kritisieren das mit Fehlern behaftete Überlieferungsgeschehen der heiligen Schriften und setzen ihm die ewig gültigen Gesetze der Vernunft entgegen. ImNaturrechtwird nach natürlichen Gesetzen gesucht, auf deren Grundlage das traditionelle Recht kritisiert werden kann. Mit derFranzösischen Revolutionwird erstmals erkannt, dass Gesellschaften von Grund auf veränderbar, revolutionierbar, sind. In der Kunst tobt derStreit der Alten und der Neuen(frz.querelle des anciens et des modernes) welchem das Gegensatzpaar von Tradition und Moderne entspringt. Dieser Gegensatz machte allerdings auch dafür blind, dass die moderne Gesellschaft ihrerseits eine Tradition derZweckrationalitätundWertrationalitäthat, ihre Festschreibung auf Wandel statt wie in traditionellen Gesellschaften auf Stabilität. Neben Ansätzen beiGiambattista Vico(1668–1744) liefertGottfried Herderin seinenIdeen zur Philosophie der Geschichte der Menschheit1784 eine erste Traditionstheorie: „Hier also liegt das Principium zur Geschichte der Menschheit, ohne welches es keine solche Geschichte gäbe. Empfinge der Mensch alles aus sich und entwickelte es abgetrennt von äußern Gegenständen, so wäre zwar eine Geschichte des Menschen, aber nicht der Menschen, nicht ihres ganzen Geschlechts möglich. Da nun aber unser spezifische Charakter eben darin liegt, daß wir, beinah ohne Instinkt geboren, nur durch eine lebenslange Übung zur Menschheit gebildet werden, und sowohl die Perfektibilität als die Korruptibilität unsres Geschlechts hierauf beruhet, so wird eben damit auch die Geschichte der Menschheit notwendig ein Ganzes, d. i. eine Kette der Geselligkeit und bildenden Tradition vom ersten bis zum letzten Gliede.“[82] Durch Tradition und Kultur vollzieht sich also eine Überformung des Menschen, die Herder eine „zweiteGenesisdes Menschen“ nennt und mitLessingeine „Erziehung des Menschengeschlechts“. Indem Herder die Kette der Tradition zurückreichen lässt bis zu ihren Anfängen wertet diese zugleich auf:[83] „Wollen wir diese zweite Genesis des Menschen, die sein ganzes Leben durchgeht, von der Bearbeitung des Ackers Kultur oder vom Bilde des Lichts Aufklärung nennen, so stehet uns der Name frei; die Kette der Kultur und Aufklärung reicht aber sodann bis ans Ende der Erde. Auch der Kalifornier und Feuerländer lernte Bogen und Pfeile machen und sie gebrauchen; er hat Sprache und Begriffe, Übungen und Künste, die er lernte, wie wir sie lernen; sofern ward er also wirklich kultiviert und aufgekläret, wiewohl im niedrigsten Grade. Der Unterschied zwischen aufgeklärten und unaufgeklärten, zwischen kultivierten und unkultivierten Völkern ist also nicht spezifisch, sondern nur gradweise.“ Für Herder ist der Traditionsbegriff also nicht auf die treue Wahrung einer Ursprungsweisheit angelegt, sondern auf die allmähliche Anreicherung wertvollen Wissens, dass über die gesamte Geschichte der Menschheit nach und nach das Unmenschliche ausscheidet. Dass allerdings Traditionsbildung auch auf irrationalen Ängsten und gewaltsamen Zwängen beruhen kann, darauf hatSigmund Freudin seiner StudieDer Mann Moses und die monotheistische Religionhingewiesen. Freuds inhaltliche Rekonstruktion des Überlieferungsgeschehens durchunbewussteZwänge und archaische Ängste stieß zwar auf breite Ablehnung; trotz allem kommt ihm das Verdienst die Gründe für Tradition und Überlieferung nicht nur unter dem optimistischen Gesichtspunkt einer fortschreitenden Verbesserung zu sehen und so den Blick auf pathologische Momente der Tradierung zu öffnen. So weist schonGeorg Simmeldarauf hin, dass der Anspruch der Kultur auf Zeitlosigkeit unbegründet sei. „In rascherem oder langsamerem Tempo nagen die Kräfte des Lebens an jedem einmal entstandenen Kulturgebilde“. Das Leben sträube sich dagegen, „in irgendwie festen Formen zu verlaufen“; insbesondere der moderne Individualismus wende sich gegen das „Prinzip der Form“, was sich z. B. imExpressionismusmanifestiere.[84] Als die institutionalisierten Geistes- und Geschichtswissenschaften im 20. Jahrhundert den Anschein aufkommen ließen, man könnte sich der Vergangenheit gänzlich objektiv und theoriefrei nähern, hatHans-Georg Gadamerdarauf hingewiesen, wie prägend auch für uns Heutige noch der Bezug zur Tradition ist: Inhalte der Überlieferung können durch wissenschaftliche Methoden niemals restlos verobjektiviert und zum bloßen Gegenstand einer der Tradition enthobenen Erkenntnis werden. Gadamer prägt dafür den Begriff deswirkungsgeschichtlichen Bewußtseins, dass über die Tradition reflektiert und sich zugleich seiner Bestimmtheit durch die Tradition bewusst ist.[85] Ein wesentliches Ordnungssystem, durch welches sich Bewältigungs- und Kommunikationsprozesse vollziehen, ist die Sprache. Sprache ist ein symbolisches Medium, das kein einzelner Mensch aus sich heraus selbst erfindet, sondern welches ihm überliefert wird. Der Mensch kann sich daher immer nur zur Sprache als einem immer schon Gegebenenverhalten. Als ein Zeichensystem schafft Sprache einen Raum der Öffentlichkeit, aus dem der Mensch beim Sprechen schöpft und in den hinein er stets zurückspricht. Sprache darf, wenn ihre kulturelle Bedeutung verstanden werden soll, nicht nur als Mittel der Kommunikation angesehen werden, sondern sie strukturiert grundsätzlich das menschliche Verstehen der Welt. Wenn die Bedeutung der Sprache für den Menschen als kulturelles Wesen verstanden werden soll, dann kann es nicht darum gehen, einzelne konkrete Sprachen auf ihre Eigenart hin zu untersuchen, sondern es muss verstanden werden was überhaupt Spracheals Spracheausmacht. Dabei konnten sichbiologistischeSprachtheorien nicht durchsetzen, wie etwa in der Antike die vonDemokrit(460–371 v. Chr.) vertretene Auffassung, dass Sprache aus Lauten rein emotionalen Charakter hervorginge, oder die anCharles Darwin(1809–1882) anschließende Sprachforschung, welche Sprache auf evolutionstheoretische Notwendigkeiten zurückführen möchte. Auch die ausgefeiltere vonOtto Jespersen(1860–1943) vorgeschlageneHolistische Sprachgenesetheorieist für die kulturwissenschaftliche Sprachauffassung bedeutungslos geblieben.[86]Diesen Sprachtheorien ist gemeinsam, dass sie Sprache lediglich im Hinblick auf ihrenaffektivenund emotionalen Zug betrachten. Damit wird aber der propositionale Gehalt von einfachen Aussagen wie „Der Himmel ist blau“ übergangen, denn diese Aussage fordert weder zu einer unmittelbaren Handlung auf, noch hat sie einen emotionalen Gegenstand, sondern sie weistsymbolischauf etwas hin, das womöglich im Gesamtzusammenhang einer Kultur vonBedeutungist. Sprache als Zeichensystem Es war der SprachwissenschaftlerFerdinand de Saussure, der eine Zeichentheorie der Sprache entwickelte, dieSemiotik, von griechischsemeionfür Zeichen, und der vorschlug, diese für das allgemeine Studium der Kultur zu verwenden. Nach Saussure sind sprachliche Zeichen durch zwei Eigenschaften ausgezeichnet: Bei der Untersuchung vorhandener Sprachen unterscheidet Saussure zwischen dersynchronischen(zeitgleichen) unddiachronischen(in der Zeit sich verändernden) Betrachtungsweise. Für Saussure ist die erste Form die wichtigere. Das heißt, er arbeitete nicht sprachhistorisch, sondern versuchte anhand einer gegebenen Sprache deren innereStrukturfreizulegen, weshalb man Saussure auch als Gründer desStrukturalismusbezeichnet. Saussure kommt zu dem Urteil, dass Sprache nicht dadurch funktioniert, dass ein Laut oder eine damit bezeichnete Vorstellung an sich gegeben ist. Vielmehr bilden sich einzelne verständliche Laute (Phoneme) nur inAbgrenzungzu anderen aus: „In der Sprache gibt es nur Verschiedenheiten.“[87]Dass phonetische Laute nicht einfach gegeben sind, zeigt sich beispielsweise daran, dass Japaner und Chinesen den Unterschied zwischen „L“ und „R“ nicht hören, da sich diese Differenz kulturell nicht ausgeprägt hat. Es wird also nicht ein Wort wie ein Anker an einen Gegenstand gekettet, den es von nun ab bezeichnet, sondern aus dem durch Verschiedenheiten aufgebauten Geflecht der Laute können mehrere Laute zu einem neuenund von den anderen unterscheidbarenGebilde zusammengesetzt werden. Dieses Wort kann dann innerhalb der Menge derVorstellungen.die sich ebenfalls durch Abgrenzung zueinander ausbilden, eine solche Vorstellung bezeichnen. Indem Saussure vorschlägt, dieses Modell der Sprache auf alles kulturell Hervorgebrachte anzuwenden, öffnet er den Blick dafür, Kultur als einen Zusammenhang von Zeichen und Symbolen aufzufassen: „Man kann also sagen, dass völlig beliebige Zeichen besser als andere das Ideal des semeologischen Verfahrens verwirklichen; deshalb ist auch die Sprache, das reichhaltigste und verbreitetste Ausdruckssystem, zugleich das charakteristischste von allen; in diesem Sinne kann die Sprachwissenschaft Musterbeispiel und Hauptvertreter der ganzen Semeologie werden, obwohl die Sprache nur ein System unter anderen ist.“[88] Mit derikonischen Wende(von altgriechischikon„Zeichen“; englischiconic turn) wird seither Kultur hauptsächlich unter dem Aspekt der Zeichentheorie aufgefasst, wobei nun nicht mehr nur abstrakte Zeichen, sondern auch an Anschauungen angelehnte Bilder als Zeichen aufgefasst werden. Dies hebt die scharfe Grenze zwischen Text und Bild auf und Kultur zeigt sich als Zeichenuniversum von Verweisungen und Bezügen, das die Lebenswelt des Menschen ausmacht.Juri Michailowitsch Lotmanspricht daher auch von der „Semiosphäre“ in Analogie zurBiosphäre.[89]Wenn in modernen Kulturtheorien von „Text“ oder „Diskurs“ die Rede ist, beschränken sich diese beiden Begriffe auch nicht mehr auf die schriftliche Aufzeichnung, sondern werden für Symbolismen jeder Art verwendet:[90]Körper, Dinge, Kleidung, Lebensstil, Gesten, all dies sind Teile des Zeichenuniversums Kultur. Im Anschluss an Saussure hatJacques Derridamit seinem Begriff derDifféranceeineliteraturwissenschaftlicheMethode geprägt, die einen Text nicht durch eindeutige Aussagen geprägt auffasst, sondern als ein Geflecht, in dem sich erst durch Differenzen Bedeutungen ausbilden. DieDekonstruktionversucht den Nebenbedeutungen nachzugehen und die an den „Rändern“ eines Textes abgeblendeten und so unthematisch bleibenden Bezüge wieder ins Bewusstsein zu rufen. Für Derrida stellt die Kultur somit einen Text dar, in dem es zu lesen gilt.[91] Nichtpropositionale Sprachlichkeit Martin Heideggerhat 1927 darauf hingewiesen, dass sprachliche Äußerungen nicht schlichtweg als propositionale Aussagen im Sinne von „A ist B“ verstanden werden können. Die Struktur der Sprache ist stets so vielfältig verästelt, dass sich einzelne Begriffe niemals klar umgrenzen lassen, sondern erst durch ihre Nebenbedeutungen undBeiklängeein Verstehen erst möglich machen. In einer Aussage der Form „A ist B“ wird beispielsweise AalsB aufgefasst. Heidegger bezeichnet diese Verkettung von A und B durch das „Als“ mit dem Titel „apophantischesAls“. Es ist diese Form, nach der in der philosophischen Tradition die meisten sprachlichen Aussagen aufgefasst wurden. Dem entgegen weist Heidegger darauf hin, dass die Bedeutung von A und B nicht bloß an deren Rändern abreißt, sondern immer nur in einem größeren Gesamtzusammenhang zu verstehen ist. Auch eine Aussage des Schemas „A ist B“ kann nur vor einem größeren Verständnishorizont verstanden und eingeordnet werden.[92]Eine Form der Sprachlichkeit, die sich nicht in Aussagen des Schemas „A ist B“ ergeht, sondern die den ganzen Reichtum einer kulturgeschichtlich gewachsenen Sprache hervortreten lässt, stellt für Heidegger dieDichtungdar. In der Dichtung treten einzelne Bedeutungsmomente besonders hervor, andere werden hingegen bewusst abgeschattet. Damit verengt die Dichtung sich nicht zu eindeutigen Feststellungen, sondern lässt Raum für das Ungesagt, Unbewusste und Unthematische unseres kulturell geprägten Welt- und Selbstbezugs, das so durch sie erstzur Sprache kommt. Auch wies Heidegger Sprachtheorien zurück, welche die Sprache lediglich als ein Mittel zur Kommunikation auffassen, so dass mit ihr Aussagen wie „A ist B“ mitgeteilt werden können. Diese funktionalistisch geprägte Auffassung sieht Sprache lediglich als Hilfsmittel zur gemeinsamen Bewältigung von praktischen Bedürfnissen. Für Heidegger gingen solche Sprachtheorien zurück auf die mit der Neuzeit einsetzende ökonomisch-technische Verwertbarmachung der Welt. Sprache wird dann als Werkzeug zur Kommunikation aufgefasst, dass sich durch logische Strukturierung verbessern ließe,[93]wie diesGottlob Frege,Bertrand RussellundRudolf Carnapim Projekt derEinheitsspracheanstrebten. Gegen einen so verengten Sprachbegriff machte Heidegger die Dichtung stark und weist darauf hin, dass im dichterischen Besingen der Welt keine praktische Haltung vorherrscht (siehe beispielsweiseHölderlinsHymneDer Ister). Zum anderen sah es Heidegger als verfehlt an, davon auszugehen, dass Spracheinnerhalb einer Welteine einzelne Aussage mitteilt. Vielmehrist die Sprache die Welt, in welcher der Mensch lebt, da alles Wissen, Denken und Begreifen sich in sprachlichen Strukturen vollzieht. Heidegger prägte hierfür den Ausdruck, die Sprache sei „das Haus des Seins“.[94] Ausbildung von Institutionen Kultur besteht nicht nur aus sprachlich festgeschriebenen Strukturen des Verstehens und der Objektivität, sondern auch aus geschichtlichhandelndenundleidendenMenschen. Nicht alles Tun des Menschen ist aber schon kulturellePraxis. Damit diese entsteht, bedarf es einer Gruppe von Menschen, die gemeinsam und regelmäßig für sie bedeutsame Handlungen ausführt. Verfestigen sie das Tun auf diese Art zu Ereignissen, die regelmäßig wiederholt werden oder Orten, an denen die Praxen gemeinsam durchgeführt werden, spricht man auch vonInstitutionen. Institutionen sind Orte des menschlichen Handelns beispielsweise in Form vonArbeit, Kunst oderSpiel, vonHerrschaft, Recht, Religion oder von Wissenschaft und Technik. Kultur als Praxis und Kultur als Bedeutungszusammenhang Wird Kultur unter dem Gesichtspunkt der praktischen Handlungen und des Kulturgeschehens betrachtet, so stellt dies auch ein gewisses Gegengewicht dar zu Auffassungen, welche Kultur in erster Linie (oder ausschließlich;Kulturalismus) als Sinnsystem von symbolischen Codes verstehen und in ihr einen lesbaren Text sehen.[95]So ist Kultur nicht nur ein Gewebe von Bedeutungen, sondern diese bedürfen einerAusübung,um sich zu erhalten und fortzusetzen. Dabei können jedoch auch gerade durch die Ausübung neue Sinnzusammenhänge entstehen oder alte sich abschleifen, als unpassend oder unbedeutend empfunden werden. Im Zurückgreifen auf kulturelle Symbole, Sinn- und Handlungszusammenhänge, die in der Ausübung jedoch nie gänzlich verwirklicht werden können, ergibt sich ein Wechselspiel das die Kultur in lebendiger Bewegtheit hält: Auch aus dem Zufälligen und Ungewollten entsteht Neues. Diese Perspektive auf Kultur als einen sinnstiftenden und wandelbaren Wissensvorrat, der menschliches Erleben und Handeln orientiert und davon zugleich mit konstituiert wird, hat auch große psychologische Bedeutung, die beispielsweise in der modernenKulturpsychologieuntersucht wird.[96] Dinge, die für das Denken und Handeln des Menschen in irgendeiner Form den Anspruch auf eine Bedeutung erheben, kommt eine gewisseGeltungzu. Im zwischenmenschlichen Umgang können solche Ansprüche und Herausforderungen an den Einzelnen oder an Gruppen angenommen oder abgelehnt werden. Ansichten, Gesetze und Bedeutungen können daher umstritten sein. Die Frage, welche sich diesem Sachverhalt widmet ist die derGeltungvon symbolischen, praktischen, kognitiven, narrativen und ästhetischen Geltungsansprüchen. Menschen begegnen sich meist als Individuen anhand ihrer Geschlechtlichkeit, Leiblichkeit, psychischen Triebstrukturen und biographischer Einzigartigkeit. Diese Merkmale können für den Einzelnen oder für die Gruppeidentitätsbildendwirken und werfen im Falle von Gruppen die Fragen von Zugehörigkeit und Mitgliedschaft auf. Damit geben soziale Gruppen im kulturellen Leben dem Menschen eine Antwort auf die Frage, wer er im Vergleich zu den übrigen ist, sie bestimmen seine Identität. Durch Gruppenbildung und der Form des Handelns in ihr bilden sich Gemeinschaften oder Gesellschaften, die sich gegen andere Gruppen abschließen, Mitglieder aufnehmen oder ausschließen. Diese Vorgänge bestimmen unabhängig von den konkreten Inhalten die Identität der Gruppe und des Einzelnen. Menschliche Kultur erhält sich dadurch, dass sie weitergegeben wird. Dieser Moment der Tradition steht in engem Zusammenhang mit der geschichtlichen Entwicklung von Kulturen. Geschichte kann einerseits rückblickend anhand verschiedener Kriterien inEpochenunterteilt werden, andererseits ist jeder Kultur ein historisch gewachsener Zeitgeist innewohnend. Räume werden nicht einfach wie der mathematischeeuklidische Raumals dreidimensionale Strukturen wahrgenommen und erst anschließend und unter Umständen mit Bedeutung versehen oder Interpretationen unterworfen: Es macht stets einen Unterschied, ob man fünf Meter gerade aus schaut, oder fünf Meter unter sich. Der Blick fünf Meter nach unten mag wiederum dem norddeutschen Küstenbewohner unbehaglicher sein, als dem Alpenbewohner. DieRaumwahrnehmungist also niemals eine neutral-mathematische, sondern unterliegt kulturellen Prägungen. So werden in erster Linie Verhältnisse im Raum entdeckt, welche einephysische Orientierungin ihm ermöglichen: Wege, Hindernisse, Sitzmöglichkeiten und Gefahren. Die Orientierung im städtischen Raum erfordert es das Geflecht von Straßen, Kreuzungen und Ampeln zu verstehen und anhand von Häusern bekannter Größe die Entfernungen richtig einschätzen zu können, während indigene Völker sich im Urwald ganz ohne Straßen und Wege zurechtfinden, sondern Bäume, Flussläufe und ähnliches nutzen. Beides mal strukturieren kulturellerlernteFähigkeiten und Sehgewohnheiten die Raumwahrnehmung. Auch das Haus ist ein Raum, der durch eine „sinnhafte“ Struktur bestimmt ist, wie es der deutsche PhilosophMartin Heideggerbeschreibt: Gebrauchsgegenstände haben ihren „Platz“, sie gehören in eine „Gegend“ anderer zu ähnlich nützlichen Gegenstände. Die Dinge sind nicht im dreidimensionalen Raum einfach „oben“ oder „unten“, sondern „an der Decke“ oder „auf dem Boden“. Gesehen werden nicht zuerst unbedeutende Objekte im physikalischen Raum, sondern etwas liegt „am falschen Platz“ oder „steht im Weg“, dort „wo es nicht hingehört“.[97]Diese Bestimmungen sind aber keine absoluten, sondern hängen von der Kultur und dem Umfeld ab, in welchem der Mensch herangewachsen ist. Bereits beiJohann Wolfgang von Goethefindet sich die Unterscheidung zwischen neutralen Raum und bedeutungsgeladenem Ort: „Immer war mir das Feld und der Wald und der Fels und die Gärten / Nur ein Raum, und du machst sie, Geliebte, zum Ort“ (Vier Jahreszeiten). Auch solche atmosphärische Qualitäten bestimmen die Wahrnehmung des Raumes.Gernot Böhmeuntersucht, wie repräsentative Zimmer oder Säle mit Gegenständen ausgestattet werden, die eigentlich keinen Gebrauchswert haben, bzw. deren Wert genau darin liegt, Atmosphäre zu erzeugen.[98]Luc Ciompikonnte zeigen, inwieweit das, was als atmosphärisch angenehm empfunden ist, kulturabhängig ist. Während sich etwa Italiener in hohen, kühlen und dunklen Zimmern wohlfühlen, bevorzugen Nordländern niedrige, helle und warme Räume, was sich auf die unterschiedlichen klimatischen Bedingungen zurückführen lässt und die von Kindheit her vertraute Wohnatmosphäre.[99] Kulturelles Leben findet in Räumen statt. Diese Räume sind nicht einfach der dreidimensionaleRaum der Physik, der die Kulturgüter wie ein Behälter umschließt. Vielmehr ist Kultur selbst raumbildend, d. h., sie schafft sich symbolische und figurative Räume. Diese Räume sind in erster Linie nicht durch ihre Eigenschaft als Behältnis bestimmt, sondern durch einen sinnhaften Zusammenhang, so bildet beispielsweise der Herd des Hauses einen Ort der Versammlung, an den die Mitglieder bäuerlicher Hausgemeinschaften nach getanem Tageswerk zusammenkommen. DerTempeloder dieKirchesind Orte, an welchen das Heilige dem Leben des Menschen ein Maß gibt und andere Gesetze und Verhaltensweisen gelten, als in der profanen Sphäre derKüche. Auch politisch werden Grenzziehungen propagiert, die sich nicht an geographischen, sondern kulturellen Räumen orientieren, bzw. diese vorschreiben, wenn etwaGeorge W. BushAmerika und Europa zur „Westlichen Welt“ zusammenfasst und ihnen die „Achse des Bösen“ entgegenstellt.[100] Kulturelle Räume können festeAnordnungenan einem ausgezeichnetenPlatzsein, wie etwa bei einemKlosteroder aber alsbewegte Anordnungenauftreten, wenn beispielsweiseMobilfunkteilnehmerraum-zeitliche Abstände überbrücken. Frühes Entstehen kulturellen Raums:Heilige Orte Eine der frühsten Einteilungen der Welt scheidet profane undheilige Orte. Heilige Orte sind jene, an denen das Göttliche durch besondere Ereignisse zur Erscheinung kommt. Für den mythisch denkenden Menschen bleiben Götter oder Geister an diesen Ort gebunden, es ist jener Stein oder jene Eiche, in der sich das Heilige manifestiert. Damit ergibt sich eine Einteilung des Lebensraums, die nicht mehr allein wie beim Tier an physiologischen Bedürfnissen orientiert ist (Wasser, Nahrung), sondern sich an einemsymbolischenGehaltfestmacht. Verschiedene Orte können ethnisch-, klassen- oder geschlechterspezifisch zu neuen Orten zusammengefügt werden. Hierdurch kann es zu Abgrenzungen zwischen Ein- und Ausgeschlossenen kommen, auch können bestimmte räumliche Anordnungen soziale Ungleichheiten widerspiegeln oder festschreiben. WährendVIP-Räume bewusst „wichtige“ von den „weniger wichtigen“ Menschen trennen, vollziehen sich räumlich-soziale Abgrenzungen meist über längere Zeit. So werden Häuser, Wohnungen und Stadtteile nach dem entsprechenden Einkommen gewählt und hierdurch Klassenverhältnisse reproduziert, die sich dann auch physisch in den Raum einschreiben. Dieses Einschreiben in den Raum fasstPierre Bourdieuin die Worte, dass derHabitusdasHabitatausmacht.[101]Damit spiegelt der städtische Raum die sozialen und geschlechterspezifischen Verhältnisse: Arbeiterjugendliche halten sich häufiger auf öffentlichen Plätzen und Straßenecken auf,[102]Jungen mehr als Mädchen.[103] Während ein entsprechendes Vermögen die Aneignung und bauliche Umgestaltung vonöffentlichem Raumnach den eigenen Bedürfnissen ermöglicht, ist dies den unteren sozialen Schichten einer Gesellschaft nicht ohne weiteres möglich. Auch Kinder und Jugendliche können sich nicht materiell eigene Räume schaffen und sind daher darauf angewiesen diese durch ihre leibhaftige Anwesenheit zu besetzen: Die geduldete Raucherecke hinter der Turnhalle bildet gegenüber dem autoritären Raum des Schulgeländes einen Rückzugsort für die Schüler. Dieser Ort schreibt sich aber nicht physisch ein, sondern entsteht allein durch das häufige Aufsuchen und die Anwesenheit der Schüler. Hier wird besonders deutlich, dass kultureller Raum nicht einfach gegeben ist, sondern dadurch hergestellt wird, dass im Handeln individuell und kollektiv darauf Bezug genommen wird.[104] Auch die globalekapitalistische Wirtschaftsweiseschafft einen neuen sozialen Raum, der sich nun erstmals über den ganzen Erdball ausdehnt. Dieser Raum, dessen Verbindungslinien durch Flugzeuge, Schnellstraßen und Zugstrecken zusammengehalten wird, kann jedoch nicht von allen genutzt werden. So haben etwa nur fünf Prozent der Weltbevölkerung je in einem Flugzeug gesessen, zudem verbindet der Flugverkehr nur die „Reichtumsinseln“ des Planeten.[105]Peter Sloterdijkhat sich diesem „Innenraum“ des Planeten gewidmet, zu dem nur der Zugang erhält, der genügend zahlen kann.[106] Geschlechterspezifische Räume Geschlechterspezifischabgetrennte Räume sind in modernenwestlichen Gesellschaftenseltener geworden und beschränken sich auf Umkleidekabinen, Saunen und Toiletten. Das Hamburger Rotlichtviertel derHerbertstraßebeispielsweise markiert aber weiterhin einen geschlechterspezifischen Raum, zu dem Frauen und Jugendlichen der Zutritt verwehrt wird. Hinsichtlich des Schutzes von Kultur und des kulturellen Erbes gibt es eine Reihe von internationalen Abkommen und nationalen Gesetzen. DieUNESCOund ihre Partnerorganisationen wieBlue Shield Internationalkoordinieren einen internationalen Schutz und die lokale Umsetzungen. Grundsätzlich befassen sich dieHaager Konvention zum Schutz von Kulturgut bei bewaffneten Konfliktenund dasÜbereinkommen zum Schutz und zur Förderung der Vielfalt kultureller Ausdrucksformenmit dem Schutz von Kultur. Der Artikel 27 derAllgemeinen Erklärung der Menschenrechtebefasst sich in zweifacher Hinsicht mit dem kulturellen Erbe: Er spricht dem Menschen einerseits das Recht auf Teilhabe am kulturellen Leben und andererseits einen Anspruch auf den Schutz seiner Beiträge zum kulturellen Leben zu. Der Schutz von Kultur beziehungsweise derKulturgüternimmt national und international zunehmend einen breiten Raum ein. Völkerrechtlich versuchen die UNO und die UNESCO dazu Regeln aufzustellen und durchzusetzen. Dabei geht es nicht darum, das Eigentum einer Person zu schützen, sondern es steht das Bewahren des kulturellen Erbes der Menschheit gerade im Kriegsfall und bei bewaffneten Konflikten im Vordergrund. Die Zerstörung von Kulturgütern ist dabei laut Karl von Habsburg, Präsident Blue Shield International, auch ein Teil der psychologischen Kriegsführung. Das Angriffsziel ist die Identität des Gegners, weshalb symbolträchtige Kulturgüter zu einem Hauptziel werden. Es sollen damit auch das besonders sensible kulturelle Gedächtnis, die gewachsene kulturelle Vielfalt und die wirtschaftliche Grundlage (wie zum Beispiel des Tourismus) eines Staates, einer Region oder einer Kommune getroffen werden.[107][108][109] In der Kulturkritik werden die einzelnen Kulturleistungen des Menschen kritisch befragt auf ihre ungewollten, zerstörerischen, unmoralischen und unsinnigen Folgen. Dies kann sich zu einer Gesamtschau der Menschheitsgeschichte ausweiten, die dann insgesamt als Verfallsgeschichte erscheint. Die Kernaussage vieler kulturkritischer Ansätze besteht dabei darin, dass sie in Bezug auf das menschliche (Zusammen-)Leben einennatürlichgegebenen Zustand annehmen - einenNaturzustand, welcher der Wesensverfassung des Menschen entspricht. Dieser Urzustand wird dann mit fortschreitender kultureller Entwicklung durchKünstlichkeitenverstellt und verzerrt. Er wird überlagert von künstlichen sozialen Beziehungen und Herrschaftsformen (Jean-Jacques Rousseau) oder führt durch die Erfindung neuerProduktionsverhältnissezurEntfremdungdes Menschen von sich selbst, wieKarl Marxmeint.Friedrich Nietzschesieht in der vorsokratischenAntikenoch ein Zeitalter, in welchem derWille zur Machtungehemmt gelebt wurde, während mit dem „wissenschaftlich“ denkenden Sokrates und der Moral des Christentums ein Zerfall einsetzt, der im Zeitalter derDekadenzseinen Höhepunkt erreicht.Martin Heideggersieht ebenfalls bei denVorsokratikernnoch ein offenes und reflexives Verhältnis des Menschen zu philosophischen Anschauungen und Überlegungen, während in der Philosophie vonPlatonundAristoteleserstmals diese Erkenntnisse absolut gesetzt werden und so das Denken der Menschen auf Jahrhunderte in Kategorien zwängen, aus denen es sich selbst nicht ohne weiteres befreien kann. Der moralkritische AnsatzSigmund Freudsnimmt in Bezug auf die seelischen Verfassung des Menschen feststehende natürliche Bedürfnisse an, welche ihm durch künstliche moralische Vorschriften verwehrt werden und so den Menschen zu zwanghaften Ausgleichshandlungen drängen. Viele kulturkritische Werke spielten eine bedeutende Rolle dabei, zu verstehen, was Kultur überhaupt ausmacht. Erst durch das kritische Abstandnehmen und eventuelle Verurteilen der bestehenden Verhältnisse zeigt sich heute die Kultur nicht als unveränderlich Vorhandenes, sondern als einGeschehen, das auch hätte anders verlaufen können. Sie lassen Kultur erkennen als dieKontingenzdes Gewordenen. Sammelbände zu Kulturtheorien Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Begriffsvielfalt 2Begriffsgeschichte 2.1Wortherkunft 2.2Antike 2.3Neuzeit 2.3.1Kultur und Zivilisation 2.3.2Kulturnation und Staatsnation 2.4Moderne Entwicklungen 2.5Varianten und Grenzen des Kulturbegriffs 2.5.1Entgegensetzung von Kultur und Natur 2.5.2Kulturen: Die Pluralisierung des Kulturbegriffs 2.5.3Der Kulturbegriff außerhalb des westlichen Denkens 2.5.4Normative Verwendung des Begriffs 2.5.5Der Kulturbegriff in der Biologie 3Entstehung von Kultur 3.1Theorien der Kulturentstehung 3.2Biologische Voraussetzungen und Umweltbedingungen 3.3Kultur als Bewältigung 3.4Kultur als symbolische Sinnerzeugung 3.5Tradition und kulturelles Gedächtnis 4Aspekte kulturellen Lebens 4.1Tradition 4.2Sprache 4.3Handlung 4.4Geltung 4.5Identität 4.6Zeit 4.7Raum 5Schutz von Kultur 6Kulturkritik 7Siehe auch"
  },
  {
    "label": 0,
    "text": "Kunst – Wikipedia Kunst Inhaltsverzeichnis Etymologie und Wortgebrauch Geschichte des Kunstbegriffes Voraussetzungen und Funktionen Siehe auch Literatur Weblinks Einzelnachweise Vorgeschichte Altertum Mittelalter Frühe Neuzeit Aufklärung Moderne Postmoderne Biologie Psychologie und Neurowissenschaften Psychoanalyse Der Kunstbegriff in umfassender Bedeutung Rechtliche Stellung Kunst und bildende Kunst allgemein Romantik Kunst und Arbeitswelt Außereuropäische Kunst Kunst und Politik Philosophische Ästhetik und Kunstpraxis Das WortKunst(lateinischars, griechischtéchne[1]) bezeichnet im weitesten Sinne jede entwickelte Tätigkeit von Menschen, die aufWissen,Übung,Wahrnehmung,VorstellungundIntuitiongegründet ist (Heilkunst,[2]Kunst der freien Rede). Im engeren Sinne werden damit Ergebnisse gezielter menschlicher Tätigkeit benannt, die nicht eindeutig durch Funktionen festgelegt sind.[3]NachTasos Zembylasunterliegt der Formationsprozess des Kunstbegriffs einem ständigen Wandel, der sich entlang von dynamischen Diskursen, Praktiken und institutionellen Instanzen entfalte.[4] Kunst ist ein menschlichesKulturprodukt, das Ergebnis eineskreativenProzesses.[5]DasKunstwerksteht meist am Ende dieses Prozesses, kann aber auch der Prozess bzw. das Verfahren selbst sein. So wie die Kunst im gesamten ist das Kunstwerk selbst gekennzeichnet durch das Zusammenwirken von Inhalt und Form.[6]Ausübende der Kunst im engeren Sinne werdenKünstlergenannt. Die ursprüngliche Bedeutung des BegriffsKunstwurde auf alle Produkte menschlicherArbeitangewandt (vgl.Kunstfertigkeit) als Gegensatz zurNatur, was beispielsweise beiKunststoff,Künstliche Ernährung,Künstliches Aroma,Künstliche Intelligenzersichtlich wird. Jedoch versteht man seit derAufklärungunterKunstvor allem die Ausdrucksformen derschönen Künste:[7] Ausdrucksformen und Techniken der Kunst[8]haben sich seit Beginn derModernestark erweitert, so mit derFotografiein der bildenden Kunst oder mit der Etablierung desComicsals Verbindung bildender Kunst mit derNarrativitätder Literatur. Bei den darstellenden Künsten, Musik und Literatur lassen sich heute auch Ausdrucksformen der Neuen Medien wie Hörfunk, Fernsehen, Werbung und Internet hinzuzählen. Die klassische Einteilung verliert spätestens seit den letzten Jahrzehnten des 20. Jahrhunderts an Bedeutung. Kunstgattungen wie dieInstallationoder der Bereich derMedienkunstkennen die klassische Grundeinteilung nicht mehr. Kunstist ein deutsches Wort. Bereits imAlthochdeutschenlautete eskunst(Pluralkunsti), imMittelhochdeutschenkunst(Pl.künste). Ursprünglich istkunstein Substantivabstraktum zum Verbumkönnenmit der Bedeutung „das, was man beherrscht; Kenntnis, Wissen, Meisterschaft“. Die Redewendung „Kunst kommt von Können“ ist alsoetymologisch(dem Wortursprung nach) richtig. Zusätzlich wurde „Kunst“ inLehnbedeutungfür den lateinischen Begriffarsbenutzt, z. B. im Bildungskanon derSieben freien Künste, inLebenskunst,Liebeskunstusw. Kunst bezieht sich in diesem Sinne grundsätzlich auf alles, was Menschen können und was von Menschen gemacht ist. Der entsprechende Gegenbegriff istNatur, wie in dem alltäglichen Gegensatzpaarnatürlich – künstlich. Seit der Zeit derAufklärungwirdKunsthauptsächlich in einem engeren Sinne als Oberbegriff derÄsthetikverwendet, der dieKunstgattungen(bildende Kunst,darstellende Kunst,MusikundLiteratur) und ihre verschiedenen Stile und Strömungen zusammenfasst. Zugehörige Begriffe sind z. B.Kunstwerk,Künstler, künstlerisch.[9]Auf diesen Begriff geht der vorliegende Artikel näher ein. Der BegriffKunstwurde und wird mithin gebraucht: Die Kunst ist die dritte Stufe in der Evolution ästhetischer Praktiken.[13]In der ersten Stufe haben frühe Vertreter der Gattung Homo Gegenstände lediglich dekoriert, verziert oder auf andere Weise ästhetisch gestaltet. So lässt sich schon an 1,8 Millionen altenFaustkeilennachweisen, dass diese unter ästhetischen Gesichtspunkten bearbeitet wurden. Die zweite Stufe stellt die Herstellung vonSchmuckdar. Hier werden Gegenstände eigens zu dekorativen Zwecken hergestellt. Dazu gehören die durchlöcherten und mit Ocker eingefärbten, 80.000 Jahre alten Muschelschalen aus der südafrikanischenBlombos-Höhleund aus Marokko. Im Unterschied zu einer einfachen ästhetischen Dekoration hat Schmuck in der Regel eine symbolische Bedeutung und dient dazu, das Prestige seines Besitzers aufzuwerten. Die ältesten Zeugnisse der Kunst sind noch einmal jünger und tauchen erst vor rund 40.000 Jahren auf. Beschränkt sich die symbolische Bedeutung von Schmuck auf den sozialen Status seines Trägers, geht der symbolische Gehalt von Kunst darüber hinaus. Insbesondere die figürliche Kunst verweist auf etwas Äußeres, sie stellt etwas dar oderbezeichnetetwas. Sie setzt nicht nur symbolisches Denken voraus, sondern auchFantasie, die Fähigkeit sich etwas vorzustellen, was im jeweiligen Moment nicht real präsent ist.[14] Welche Funktion die frühe Kunst hatte, ob sie anfangs eine religiös-kultische oder eine profane Funktion hatte, ist unklar. Sowohl Malerei und Skulptur als auch Musik und Tanz treten bereits in der Altsteinzeit in Erscheinung. Zu denfrühesten Zeugnissenvon Kunst gehören die knapp 40.000 Jahre alten Elfenbeinfiguren aus demLonetalsowie die Flöten aus demGeißenklösterle. Fast gleich alt sind Fels- undHöhlenmalereieninAustralienund Indonesien. Diese sind sogar älter als die hierzulande bekannteren Höhlenmalereien in Frankreich und Spanien, etwa aus derGrotte Chauvet. Das altersgleichen Zeugnisse von Kunst in Mitteleuropa und in Australien sprechen dafür, dass der anatomisch moderneMensch(Homo sapiens) schon vorher, möglicherweise schon vor dem Verlassen Afrikas, zur Kunstherstellung fähig war, auch wenn archäologische Belege dafür bisher fehlen. Dafür spricht auch die handwerkliche Perfektion, die sich in den ältesten Kunstwerken von Beginn an zeigt. Historisch entwickelten sich die Künste aus ihrem Beitrag zur materiellen Organisation von Kulten undRitualen. In der Frühzeit menschlicher Entwicklung ist das Auftreten von Kunst einer von mehreren Indikatoren für die Bildung vonBewusstseinund menschlichemDenken.Kunstbezeichnet in diesem Zusammenhang Verrichtungen oder Darstellungen (z. B.Musik,Bemalung), die keinen unmittelbaren Nutzen zurLebenserhaltungerkennen lassen. Bei heutigenNaturvölkernlässt sich die frühe Kultfunktion von künstlerischen Ausdrucksformen ebenso studieren wie eineanthropologischeKonstante: das Bedürfnis (sich) zu schmücken, das sich imOrnamentzuerst herausgebildet hat. Diskutiert werden außerdem soziale Funktionen von künstlerisch bzw. ornamental gestalteten Artefakten wie Spangen,Fibeln, Waffen usw. in denClan­gesellschaften derUr- und Frühgeschichte. Damit fungiertKunstseit frühester Zeit auch alsDistinktionsmerkmal, wie es von der jüngeren Kunsttheorie und -soziologie diskutiert wird. Anthropologisch markiert Kunstproduktion vor ca. 40.000 Jahren (imAurignacien) den Übergang vomHomo sapienszum „Homo sapiens intellectus“. Da die Vorgeschichte per definitionem eine schriftlose Epoche ist, gibt es keinerlei Überlieferungen eines zeitgenössischen Kunstbegriffs. Von den frühen bis zu den spätenantikenKulturen, vom ägyptischenAlten Reichüber dasklassische Griechenlandbis zum spätenRom, sind eine Fülle von Kunstwerken erhalten: Architektur, Skulpturen, Fresken und Kleinkunst. Dass sie als solche bezeichnet werden, ist jedoch einAnachronismus, denn zur Zeit ihrer Entstehung galten Malerei und Bildhauerei nicht alsKunst, sondern alsHandwerk, ihre Erzeugnisse als Produkte von Handwerken, nicht aber Künstlern. DasTheaterwar bereits weit entwickelt und geachtet, aber wesentlich Bestandteil kultischer Handlungen. Alsfreie Künste(artes liberales)wurden in der Antike jene Kenntnisse und Fähigkeiten bezeichnet, die einem freien Mann – nicht aber einemSklaven– zur Verfügung stehen sollten.Martianus Capella(um 400 nach Chr.) hat insgesamt sieben Künste in zwei Gruppen eingeteilt: dasTriviumbeinhalteteGrammatik,DialektikundRhetorik; dasQuadriviumumfassteGeometrie,Arithmetik,AstronomieundMusik. Von denSchönen Künstenim modernen Sinn war also allein die Musik in der Antike eine anerkannte Kunst. Niederes Handwerk waren dagegen diemechanischen Künste(„artes mechanicae“), die mit der Hand ausgeführt werden mussten, worunter eben auch dieMalereioder dieBildhauereifielen. Malerei und Bildhauerei sowie die Heilkunst (in denAphorismendesHippokrates) wurden in der Antike aber auch als Kunst (téchnebzw.ars mechanica) und nicht als reine Technik (epistéme) angesehen.[15] DerGegensatz(Antagonismus) vonKunst, die vorwiegend aus demGeistentsteht, undKunst, die manuell gefertigt werden muss, wird sich in der bildenden Kunst über 2.000 Jahre immer wieder anders manifestieren, vomParagonein derRenaissance(dem Wettstreit der Kunstgattungen, welche die edelste von allen sei) über dendeutschen Idealismusdes 18. Jahrhunderts und seinen Anteil am modernen Kunstbegriff (der technisches Können nur noch als banales Werkzeug des Künstlers begreift seiner Idee Ausdruck zu verleihen) bis hin zurKonzeptkunstder 1960er Jahre, die die künstlerischeIdeegänzlich vom ausgeführten Gegenstand entkoppelt. Mit den Umbrüchen derVölkerwanderungszeitlöste sich das antike Kunstleben in Europa so gut wie auf. DermittelalterlicheKunstbegriff übernimmt jedoch das Schema derartes mechanicaewie derartes liberales, derfreien Künstedes(philosophischen) Grundstudiums, die in den drei großenFakultätenTheologie,JurisprudenzundMedizinvorausgesetzt wurden. Der bildende Künstler ist nach wie vor Handwerker und inZünftenwie alle anderen Berufe organisiert. Als Individuum tritt er selten in Erscheinung, dieSignatureines Werkes ist unüblich. Auftraggeber für fast alle künstlerischen Produktionen – Malerei, Bildhauerei, Musik, Theater – ist dieKirche. In geringerem Maße lässt sich auch derfeudaleAdelAuftragsarbeitenliefern. Es entstehenprofaneundsakraleAusdrucksformen, Bildtypen, Musikformen und anderes. Vertrat man in der Antike noch ein naturalistisches Menschenbild und versuchte, die Natur möglichst gut nachzuahmen, so definierte sichSchönheitim Mittelalter über den geistigen (religiösen) Gehalt einer Darstellung, wie er von denScholastikernals Schönheit Gottes erkannt wurde, die sich in der Kunst widerspiegeln sollte. Der Stellenwert der bildenden Kunst und derArbeitdes Künstlers ändern sich in derNeuzeitmit dem Übergang zu einerbürgerlichenGesellschaft: Wo vorher meist im Auftrag vonKircheundAdelWerke geschaffen werden, wächst mit dem gebildeten Kunstsammler ein neuerRezipiententypheran.[16] Dieser Prozess beginnt zuerst in Italien mit der Frührenaissance und setzt sich ab Mitte des 15. Jahrhunderts in ganz Europa fort. Die Städte erstarken und mit ihnen die Kaufleute, die ihre neue Stellung in der Feudalgesellschaft mit Kunst demonstrieren. Der Künstler emanzipiert sich, entdeckt sich alsSubjekt, und schafft Werke, deren Hauptzweck nicht mehr die Vorstellung einesGlaubens­inhalts oder der Macht einesFürstenist, sondern die fachkundige Debatte überEntwurf, Ausführung und Könnerschaft, undKünstlerwird Beruf. So entstehen hochkomplexeikonografischeBild- und Architekturprogramme, die zu enträtseln eine Aufgabe für das Kunstpublikum wird. Es entsteht eine neue literarische Gattung:Ekphrasis, Kunstliteratur, Schreiben über Künstler und Kunst, undBetrachtung(„Kunstgenuss“) als Bestandteil der künstlerischen Intention. Der nunmehrautonomeKünstler denkt über seine Rolle nach, was in der bildenden Kunst imParagoneöffentlich gemacht wird. Die „Wiedergeburt“, die im BegriffRenaissanceangesprochen wird, bezieht sich auf die erneute Anknüpfung an dieklassische Antike, auf deren Menschenbild und Naturbegriff die Kunstproduktion aufbaut. In der Musik und Literatur blühen profane Werke. DieReformationforciert die Schwächung der römisch-katholischen Kirche als wichtigstem Auftraggeber der Künstler, was auf demKonzil von Trientmit einem ausführlichen Gegenkonzept beantwortet wird. Die Notwendigkeit einer katholischenGegenreformationlegt den Grundstock für die Explosion der künstlerischen Produktion in Musik und bildender Kunst imBarock. Diente das Kunstwerk noch zu Beginn der Neuzeit dazu, sich „Merkwürdiges“ einzuprägen, so verlor es diese Funktion mit zunehmender Verbreitung desBuchdrucks. In der Folgezeit entsteht das Problem des ständigen „Neuheitsschwundes“ der Kunst: Sie muss seither immer wieder durch Innovationen überraschen. Damit wird sie zu einem autonomen gesellschaftlichen Subsystem.[17] In der zweiten Hälfte des 18. und am Anfang des 19. Jahrhunderts, im Zeitalter derAufklärung, begannen die gebildeten KreiseGemälde,SkulpturenundArchitektursowieLiteraturundMusikals Kunst im heutigen Wortsinn zu diskutieren. Themen verbindend wurde dieÄsthetikin Abgrenzung zum Hässlichen als Kategorie zur Qualifizierung von Kunstwerken begründet.Freiheitwurde zum Ideal für Politik, Wissenschaft sowie für die sich allmählich als eigenständige Bereiche herausbildenden Gattungen Literatur und Kunst. Der handwerkliche Aspekt künstlerischen Schaffens verlor an Bedeutung. Mit dem deutschen Idealismus stand die Idee über dem Artefakt. Eine der wichtigsten Voraussetzungen für diesen Prozess war die durch die beginnendeindustrielle RevolutionbeschleunigteSäkularisierung. Die Differenzierung zwischen Literatur und Kunst war das Ergebnis der kurz zuvor begonnenenLiteraturdiskussion, die sich nicht mehr mit allen geistigen Arbeiten befasste, sondern Romane, Dramen und Gedichte alsLiteraturin einem gewandelten Wortsinn zusammenfasste. Im Bestreben, ein größeres Publikum anzusprechen, wurde der TerminusKunstzunächst auf Gemälde und Skulpturen verengt, auf Gegenstände, die in denZeitungenundZeitschriften– den Journalen, die es seit dem frühen 18. Jahrhundert gab – vorgestellt und beurteilt wurden. Es entstand ein verbreitetesRezensions­wesen. Die BegriffeWerk,OriginalundGenieals Ausdrucksformen der Individualität desKünstlerswurden durchKantgeprägt. Man unterschied zwischeninnerenundäußerenBildern. Innere Bilder waren zum BeispielSprache,Vorstellungenund dieIdeen, äußere hingegen Einrichtungsgegenstände, Bauwerke oder handwerklich gefertigte Produkte. Dem Freiheitsgedanken gemäß ist der bildende Künstler nicht mehr einemAuftraggeberverpflichtet, sondern produziert unabhängig für einen neu entstehendenKunstmarkt. Damit wandeln sich zum einen die Themen, die statt religiöser undmythologischerMotive,PorträtundAllegorienun zum Beispiel auch Schilderungen aus der Arbeitswelt des aufkommenden Industriekapitalismus umfassen. Zum anderen entwickeln sich individuelle Stile, die nicht zuletzt alsMarkenzeichen, modern gesprochen alsMarketinginstrumentder konkurrierenden Künstler dienen. Auch Komponisten wieMozartverabschieden sich aus festen Anstellungen bei weltlichen oder kirchlichen Fürsten. Diese neue Freiheit ist mit entsprechenden Risiken verbunden, dasromantischeBild desverarmten Künstlers, verbunden mit demGeniebegriffsind die Folgen. Die Aufklärung bereitete den Kunstbegriff derModernevor. Emanzipierte sich am Ende des Mittelalters der Künstler zum autonomen Subjekt, so emanzipierte sich am Ende des barocken Feudalismus dasKunstwerkselbst und wurde autonom. Im Zeitalter von Maschinen, Arbeitsteilung und Automatisierung veränderte sich der Status von handwerklicher Tätigkeit in der Kunst. Kunst existiert nun nicht mehr in Funktionszusammenhängen, sondern allein aus sich heraus, wird zuL’art pour l’art. Die in Funktionszusammenhängen verbleibenden Kunstformen konstituieren sich unter dem neuen Oberbegriffangewandte Kunstfür das Kunstgewerbe. Während in derStilkundedie Stilepochenbezeichnungen nachträglich dem jeweiligen Kunstschaffen angehängt wurden, prägen nun die Künstler im Wechselspiel mit der neu aufgekommenenKunstkritikselber ihre Kategorien. Die zahlreichen, teils parallel entstehendenIsmensind jetzt eher kurzzeitigeStil-Begriffe als Epochenkonzepte. Die Bedeutung derFrauen in der Kunstnimmt zu. Mit dem Beginn der Moderne beginnt zugleich der Antagonismus derGegenmoderne. Waren bis zur Aufklärung die Adressaten für Kunst nur ein sehr kleiner Kreis (der Klerus, der Adel, das reiche Bürgertum), so erweitert sich das Publikum mit der Entstehung des frei zugänglichen Kunstmarktes, den zu seiner Förderung veranstalteten öffentlichen Ausstellungen (Salons) und den in der Presse eröffneten Debatten über Kunst, der massenhaft verlegten Literatur usw. beträchtlich. Zugleich konzentrierte sich die künstlerische Auseinandersetzung sowohl in bildender Kunst wie der Musik oder Literatur immer stärker auf die Untersuchung der eigenen Entstehungsbedingungen. In dem Maße, in dem sich die Kunst selbst thematisierte (Metakunst), verlor sie das Interesse der breiten Schichten, denen sie alsAvantgardeeigentlich vorangehen wollte. Blieben zuvor Konflikte um Kunst intern und waren beispielsweise patriotischer Natur (florentinischesDisegnocontra venezianischesColore) oder eine Frage des Geschmacks (Rubenisten contra Poussinisten,Streit derAnciens et Modernesusw.), so verweigern nun ganze Teile der Gesellschaft der Kunst ihrer Zeit die Akzeptanz. Es entwickelt sich eineGegenmoderne, die ihre Ausdrucksformen in diversen der modernen Kunst entgegengesetzten Stilen sucht – z. B. durchneoklassizistische, anderehistoristischeoder bewusstanachronistischausgerichtete Kunst. Dies kann als ein Protest gegen die Prinzipien moderner bzw. kontemporärer Kunst verstanden werden. Über diesen Protest weit hinaus ging die Diffamierung der modernen Kunst imNationalsozialismus, der mit dem SchlagwortEntartete KunstdieKlassische Moderneim Ganzen zu treffen versuchte und die sogenannteDeutsche Kunstmit brachialen Mitteln durchsetzte: durch Berufsverbote, höhnische Präsentationen wie in derAusstellung „Entartete Kunst“, bis hin zur Ermordung jüdischer Künstler imHolocaust. Ab November 1936 löste dasNS-Regimenach und nach alle Abteilungen der Kunst des frühen 20. Jahrhunderts in den deutschen Museen auf. In derSowjetunionentstanden in den 1920er Jahren die noch alsrevolutionärempfundenen AvantgardenKonstruktivismusundSuprematismus, mit Beginn desStalinismusgewann der anti-moderne Reflex die Oberhand und führte zumSozialistischen Realismusin Literatur, bildender Kunst und Musik. Entsprechend den politischen Widersprüchen im Anschluss an die Phase desTotalitarismusseit den 1930er Jahren, entwickelte sich innerhalb der Moderne der ausgehenden 1950er Jahre als zeitgenössische Widerstandsbewegung oder Post-Avantgarde in den 1960er Jahren unterschiedliche Strömungen, sowohl in Zirkeln West- als auch Osteuropas, die sich gegen die Normierung infolge desKalten Kriegesund Stalinismus in der Sowjetunion wandten und verwehrten. Sie knüpften dabei an die Tradition der Salons der Frühmoderne in den Metropolen an, hatten aber eine weiterführende und verbindend-vermittelnde Funktion. Durch die Brüche der Kriege in Europa und Asien während der 1930er und 1940er Jahre, gewannen diese infolge der staatlichen Reorganisation in den 1950er Jahren nur bedingt an Dynamik. Diese gewaltsame, durch den Staat hervorgerufene Unterbindung moderner Spielarten der Kunst ist allerdings nicht mit der Unzufriedenheit einiger Bevölkerungsteile über zeitgenössische künstlerische Ausdrucksformen (vor allem in der Architektur) gleichzusetzen. Ein Nebeneinander verschiedener Stile ist heute weitgehend akzeptiert und schafft eine große künstlerische Bandbreite in der heutigen oftmals als liberalistisch verstandenenGlobalkulturund dem Paradigma der Gleichzeitigkeit, verursacht durch die technische Digitalisierung des Alltags. DiepostmoderneAnschauungvon Kunst stellt zum Teil die Ideen von Freiheit, Originalität und Authentizität wieder in Frage, setzt bewusst Zitate anderer Künstler ein und verbindet historische und zeitgenössische Stile, Materialien und Methoden und unterschiedlicheKunstgattungenmiteinander.Kunstbetriebund Ausstellungsorte werden von einerMetaebeneaus hinterfragt(White Cube). Die Grenzen zwischenDesign,PopkulturundSubkultureinerseits undHochkulturandererseits verschwimmen. Zeitgenössische Kunst,[18]Kunst der Gegenwartund ähnliche Sammelbegriffe fassen gegenwartsbezogene Kunst nur sehr allgemein. Der BegriffKünstlerische Avantgardeist für die seit Beginn der Postmoderne entstehende Kunst überholt,[19]da es inoffenen Gesellschaftenund Kulturen höchstwahrscheinlich keine allgemeinverbindliche Richtung für eine Vorhut oder für Vorreiter geben kann. Daher wird der Begriff „zeitgenössische Kunst“ auch zur Umschreibung für künstlerische Arbeiten, Rauminstallationen oder prozesshafte Handlungen benutzt, die in der Gegenwart etwas so wahrnehmbar machen, dass sie kulturell bedeutend in die Zukunft wirken. Die in diesem Sinne freie und zeitgenössische Kunst ignoriert scheinbar alle Bedingungen, akademischen Regeln und Einteilungen, alle Kunststile, Kunstsparten und kulturellen Grenzen, während sie sich gleichzeitig die Freiheit nimmt, sie je nach künstlerischem Bedarf zu reflektieren, zu bearbeiten und zitathaft zu nutzen. Derartige Kunst repräsentiert einSystem Kunst,[20]das sich aus dem Zusammenwirken von mehreren Instanzen, Diskursen, institutionellen Akteuren und etablierten Praktiken ergibt.[21]Zeitgenössische Kunst als global und interkulturell funktionierendes System vereint die Ursprünge in verschiedenen Kulturen,Kunstgeschichtezum theoretischen Fundament von Kunst, wobei für die abendländische Kunsttradition dieantike Philosophieals historische Basis besonders bedeutend bleibt. Auch zeitgenössische Kunst lässt herkömmliche Einteilungen, wie Malerei, Bildhauerei, Tanz, Musik, Theater usw. durchscheinen, zeichnet sich jedoch gerade durch ihre Thematisierung, Infragestellung, Überwindung, Erweiterung, interdisziplinäre Integration und Ironisierung aus. Heute stehenFotografie, Videoprojektion undVideo-Installation,Environment, Rauminstallationen,HappeningundPerformanceneben Malerei und Theater, während dieMedienkünste, darunter dieKinetikundLichtkunstu. a. sich ohnehin so verorten, wie es jeweils mediengerecht und sachdienlich erscheint. Parallel zu dieser Entwicklung wurde Anfang der 1970er Jahre die Schnittstelle zwischen den zu diesem Zeitpunkt weitgehend getrennten Medien, im engen Verständnis des Kunstbegriffes, zwischen Malerei und Fotografie kunsthistorisch relevant durch Arbeiten der FotokünstlerPierre Cordier(Chimigramme),Paolo Monti(Chemigramm) undJosef H. Neumann(Chemogramme) geschlossen. Die Chemogramme von Josef H. Neumann schließen 1974 die Trennung von malerischem Grund und fotografischer Schicht, indem er sie, in einer bis zu diesem Zeitpunkt nie dagewesenen Symbiose, als nicht verwechselbares Unikat in gleichzeitiger malerischer und realer fotografischer Perspektive innerhalb einer fotografischen Schicht in Farben und Formen vereint.[22] Ähnlich wie in der Wissenschaft erschließt sich das umfassende Verständnis der möglichen Bedeutungen von Werken und Arbeiten oft erst durch eingehende Beschäftigung mit dem künstlerischen Gegenstand. Es wird in verschiedenen Kontexten interpretiert, die sich je nach Betrachter und Leser, je nach Publikum und den in das Geschehen Einbezogenen, sowie je nach Interessen der Kritiker und anderen professionellen Vermittlern wandeln und unterscheiden. In derKunsttheoriewird der zeitgenössische Kunstbegriff intensiv diskutiert. Sie stellt dabei künstlerische Praktiken, Prozesse, Institutionen und Akteuren (Künstler,Rezipienten, Manager, Investoren/Käufer, …) sowie die Kunstwerke selbst ins Zentrum der Untersuchung.[23] Mit der Frage, welche biologischen Grundlagen das Kunstbedürfnis des Menschen hat, bzw. welche psychologischen, soziologischen, ökonomischen und politischen Funktionen Kunst für denMenschenund dieGesellschafthat, beschäftigen sich dieBiologie, dieKunstsoziologie, diePsychologie, dieRechtswissenschaftund dieKulturwissenschaftenim Allgemeinen. Die rasante Entwicklung derBiowissenschaftenhat dazu geführt, dass auch höherekognitiveLeistungen des Menschen in den biologischen Disziplinen untersucht werden. Davon sind auch das künstlerische Gestaltungsbedürfnis und die ästhetischen Empfindungen nicht ausgenommen. Biologische Untersuchungen mit Bezug auf die Kunst finden insbesondere in derEvolutionstheorieund derNeurowissenschaftstatt. In der Evolutionsbiologie werden Verhaltensweisen in der Regel über einenSelektionsvorteilerklärt. Konkret bedeutet das, dass kunstbetreibende und kunstschätzende Menschen mehr Nachkommen zeugen müssten als die anderen. Ein derartiges Erklärungsmuster scheint in Bezug auf Kunst nicht unmittelbar einsichtig. Dennoch finden sich Kunstformen in allen historischen Epochen und Kulturbereichen, was darauf hinweist, dass ein Kunstbedürfnis biologisch gegeben und nichtalleinein Ergebnis sozialer Prägung ist. Für die biologische Verankerung des Kunstbedürfnisses können mehrere Erklärungen angeboten werden. Am wahrscheinlichsten ist die Kunst als Auswahlkriterium für die Partnerwahl. Diemenschliche Evolutionist durch eine Zunahme desGehirn­volumens und damit der kognitiven Fähigkeiten geprägt. Die Fähigkeit, Kunst zu produzieren, ist ein von außen erkennbarer Hinweis aufKreativität, welche auch in anderen Bereichen zu kreativen Lösungen führen kann. Menschen, die Zeit für Kunst hatten, hatten keine Probleme, die täglichen Bedürfnisse nach Nahrung und Sicherheit zu stillen, denn wer neben dem Alltag noch Reserven für primär sinnfreie Tätigkeiten wie Kunst hat, stellt damit seine Überlebensfähigkeit dar. Der Mensch als soziales Wesen hat viele Mechanismen entwickelt, um seine sozialen Gemeinschaften zu stärken. Auch die Kunst kann als Spender gruppenspezifischer Traditionen und Werte menschliche Gemeinschaften stützen. Eine ergänzende Hypothese geht davon aus, dass das Kunstbedürfnis ein Nebenprodukt (Epiphänomen) der Entwicklung anderer überlebensrelevanter, kognitiver Leistungen ist. Die Vorteile dieser kognitiven Fähigkeiten müssten demzufolge die Nachteile des Kunstbedürfnisses (Zeit, Material) übersteigen.[24] Eine Bestätigung soziobiologischer Theorien durch Experimente ist nicht durchführbar, da Kreuzungsexperimente mit Menschen ethisch nicht vertretbar sind. Die Theorien müssen deshalb spekulativ bleiben. Insbesondere die Abgrenzung zum Kunstbedürfnis als Produkt der kulturellen Evolution ist schwierig. In der Psychologie wird der gestalterische Aspekt von Kunst durch die Kreativitätsforschung untersucht, der Wahrnehmungs- und Bewertungsaspekt durch dieexperimentelle Ästhetik. Der Wert von Kunst wird zumindest in weiten Teilen im Ausdruck von Gefühlen gesehen.[25] Die Bewertung eines künstlerischen Werks unterliegt unterschiedlichen Faktoren. Zum Beispiel führen Charakteristika des bewertenden Individuums (wie seine Persönlichkeit und seinGeschmack) zu unterschiedlichen Präferenzen. Eine Studie von über 90.000 Personen zeigte, dassPersönlichkeitsmerkmale, wieOffenheit für Erfahrung, starke Korrelate der Präferenzen für bestimmte Gemälde und für das Genießen von Besuchen inKunstgaleriensind.[26] Die Bewertung von Kunst ist über verschiedene Epochen hinweg weder völlig übereinstimmend, noch völlig unabhängig voneinander: Bei der Bewertung des Lebenswerks vonRenaissance-Malern durch Kunsthistoriker aus über 450 Jahren beträgt derGrad der Übereinstimmung zwischen den BeurteilungenungefährW =0,5 (mögliche Werte: 0 bis 1).[27] Es zeigte sich, dass derZeitgeistebenso wie objektive Eigenschaften eines Werks, die nicht dem Zeitgeist unterliegen, eine Rolle für die Bewertung spielen. So wurde in einer Untersuchung von 15.618Themenaus der klassischen Musik die Bedeutung der objektiven Merkmale und des Zeitgeists untersucht. Sowohl die musikalische Originalität eines Themas relativ zu seinen zeitgenössischen Werken (dem Zeitgeist) als auch seine „absolute“ Originalität trugen in ähnlicher Größenordnung zur Popularität eines Themas bei.[28]Ähnliche Ergebnisse konnten auch für sprachliche Originalität gezeigt werden.[29]Auch der Kontext, in dem Kunst präsentiert wird, spielt eine wichtige Rolle bei deren Wahrnehmung.[30] Auf biologischen Grundlagen stützen sich die Neurowissenschaften bei der Erforschung des Kunstbedürfnisses. Ziel ist dabei die Zuordnung künstlerischen Schaffens zu neuronalen Prozessen oder zu unterschiedlichen neuronalenArealen. Hinzu kommt die enormeHeterogenitätkünstlerischer Aktivität. Sie führt dazu, dass verschiedene künstlerische Leistungen sich mit unterschiedlichen neuronalen Prozessenkorrelierenlassen. Sigmund Freudsah in der Kunst – wie in jeder kreativen Tätigkeit – eine Möglichkeit, den Trieb derLibidoauf nicht-sexuelle Weise zusublimieren. In der Psychoanalyse ist aber auch der Begriff der „Unkunst“ geläufig und wird häufig öffentlich kontrovers diskutiert, z. B. wenn es darum geht zu zeigen, wie der Mensch seine Macht über Tiere ausübt. Das Museum in Wolfsburg zeigte zum Beispiel 2022 eine Darbietung vonDamien Hirst, in der Hunderte Fliegen in einem Glaskasten ausschlüpften und durch eine elektronische Fliegenfalle getötet wurden. Es gibt die schönen Künste, aber auch die Ingenieurskunst, die Kunst der Rede oder der Diplomatie, den Ballkünstler, und auf sehr vielen Gebieten den Künstler in seinem Fach. Was ist, in dieser umfassenden Bedeutung, aller Kunst gemeinsam – und was unterscheidet dann die Künstler in den jeweiligen Fächern voneinander? Kunst in diesem sehr weiten Sinn ist eine kreative Tätigkeit (und deren Ergebnis), die mit höchster Effizienz ausgeübt wird; dass also, gemessen an den eingesetzten Mitteln, mit dem Ergebnis eine möglichst große Wirkung erzielt wird. Bei vergleichbarer Wirkung erfährt nicht der höhere, sondern der vergleichsweise maßvollere Aufwand die höhere Wertschätzung als Kunst. Das bedeutet jedoch nicht, dass das Instrumentarium nur einfach und bescheiden sein müsste oder dass es für den Künstler immer einfach ist, zur einfachsten Lösung eines Problems oder zu den wirkungsvollsten Ausdrucksmitteln zu gelangen. Die einzelnen Formen von Kunst unterscheiden sich aber in der Art der Wirkung, und diese hängt vom Sachgebiet ab. Das Ziel der Ingenieurskunst ist z. B. die tragfähige und solide Brücke, das Wesentliche am Essay ist die scharfsinnige Analyse, der Schwerpunkt der schönen Künste liegt vorwiegend im Wecken und Anregen von Gefühlserlebnissen. Man kann viele Tätigkeiten als Kunst im weitesten Sinn ausüben; die Kriterien dafür sindKreativitätund Effizienz. Kunst ist eine Erscheinung in jeder Kultur, Gegenstand sozialer Konventionen und – sofern eine Gesellschaft einRechtswesenentwickelt – ein Objekt derGesetzgebung. In demokratischen Ländern ist das Recht aufKunstfreiheitentweder in der Verfassung verankert oder im Rahmen derMeinungsfreiheitgarantiert. In Staaten mit anderer politischen Organisation wird die Kunstausübung häufig reglementiert und/oder zuPropaganda­zwecken instrumentalisiert. Diktaturen setzen Kunst häufig gezielt dazu ein, das jeweilige Regime zu stabilisieren. Freierkünstlerischer Ausdruckwird einer Zensur unterworfen und mit Repressionen bedroht oder ihnen tatsächlich ausgesetzt. Aufgrund derartiger Repressionen produzieren Künstler dann kritische Werke nicht (Schere im Kopf), veröffentlichen sie nicht oder gehen in eineinnere Emigration. Einige Künstler verinnerlichen die staatlichen, sozialen und/oder religiösen Anforderungen und produzieren – aus Überzeugung oder aus wirtschaftlichen Zwängen –affirmativeWerke. Plagiate,Imitateund stark von anderen Künstlern beeinflusste Werke gab und gibt es in jeder Phase der Kunstgeschichte. Wenn der Produzent seine Vorlagen verbirgt, ist dies alsKunstfälschungebenso strafbar wie eine Verletzung desUrheberrechts. Um eine solche Verletzung rechtlich fassbar zu machen, werden vom Gesetzgeber Kriterien eingeführt, die im Kunstbetrieb selbst keine Rolle spielen. So kann aus der Sicht des Urheberrechts ein Künstler ein Werk beispielsweise erst dann als sein Eigentum bezeichnen, wenn es eine ausreichendeSchöpfungshöheerreicht hat. Diese setzt eine persönliche, individuelle und geistige (menschliche) Schöpfung voraus, welche eine durch die menschlichen Sinne wahrnehmbare Form besitzt (sieheWerkbegriff des Urheberrechts bzgl. der Schöpfungshöhe). Die Kunstfreiheit ist in Deutschland ein durchArt. 5Abs. 3Grundgesetz[31]geschütztesGrundrecht. Kunstwerke selbst können einerseits alsKulturgüterrechtlichen Schutz durch nationale und internationale Bestimmungen und Organisationen (UNESCO,Blue Shieldetc.) genießen und andererseits auch rechtlichen Beschränkungen (Ausfuhrverbote etc.) unterworfen sein.[32] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie und Wortgebrauch 2Geschichte des Kunstbegriffes 2.1Vorgeschichte 2.2Altertum 2.3Mittelalter 2.4Frühe Neuzeit 2.5Aufklärung 2.6Moderne 2.7Postmoderne 3Voraussetzungen und Funktionen 3.1Biologie 3.2Psychologie und Neurowissenschaften 3.3Psychoanalyse 3.4Der Kunstbegriff in umfassender Bedeutung 3.5Rechtliche Stellung 4Siehe auch 5Literatur 5.1Kunst und bildende Kunst allgemein 5.2Romantik 5.3Kunst und Arbeitswelt 5.4Außereuropäische Kunst 5.5Kunst und Politik 5.6Philosophische Ästhetik und Kunstpraxis 6Weblinks 7Einzelnachweise Afrikaans Alemannisch Алтай тил አማርኛ Aragonés अंगिका"
  },
  {
    "label": 0,
    "text": "Literatur – Wikipedia Literatur Inhaltsverzeichnis Begriffsdifferenzierung Etymologie und Begriffsgeschichte Definitionen Geschichte des Diskussionsfeldes Arten von Literatur und Adressaten Buchmessen Literatur und Internet Literatursoftware Literaturdatenbanken Literaturen nach Sprachen und Nationen Bereiche schriftlicher und sprachlicher Überlieferung Die literarischen Gattungen Siehe auch (Sekundär-)Literatur Weblinks Einzelnachweise Ästhetik und kunstvolle Sprachbeherrschung Fiktionalität, gesellschaftliche Relevanz Literarischer Stil und Subjektivität Höhere strukturelle Komplexität und komplexeres Traditionsverhalten 18. Jahrhundert: Die Literaturkritik wendet sich „schöner Literatur“ zu Dramen, Romane und Poesie werden im 18. Jahrhundert zum Diskussionsfeld Seit dem 19. Jahrhundert: Literatur im kulturellen Leben der Nation Literaturen: Das international fragwürdige Konzept Tendenzen: Der „erweiterte Literaturbegriff“ – der „Tod der Literatur“? Sammlungen von Literatur im Internet Im Internet entstandene Literatur Nachschlagewerke Klassische Literaturdefinitionen Begriffs- und Diskursgeschichte Die „belles lettres“ werden zum Sonderfeld der Literaturdiskussion Das kritische Besprechungswesen entskandalisierte die Belletristik Auf dem Weg zur diskutablen Poesie wird die Oper ausgeschaltet Der Roman wird dagegen Teil der Poesie Die Diskussion „hoher Literatur“ und die Entskandalisierung der Öffentlichkeit Die Literaturgeschichte wird mit der Wende ins 19. Jahrhundert geschaffen Derliterarische Kanonverdrängt den religiösen Siegeszug der pluralistischen Diskussion Rückblick: Ein neuer Bildungsgegenstand wurde geschaffen AlsLiteraturbezeichnet man seit dem 19. Jahrhundert den Bereich aller mündlich (etwa durchVers­formen undRhythmus) oderschriftlichfixiertensprachlichenZeugnisse. Man spricht in diesem „weiten“ Begriffsverständnis im Hinblick auf die hier gegebene schriftliche Fixierung etwa von „Fachliteratur“ oder, im Bereich derMusik, von „Notenliteratur“ (etwaPartituren) bzw. ganz allgemein von „Literatur“ im Sinne der Gesamtheit oder von Teilen schriftlich notierter Musik. Die öffentliche Literaturdiskussion und -analyse ist demgegenüber seit dem 19. Jahrhundert auf Werke ausgerichtet, denen besondere Bedeutung alsKunstzugesprochen werden kann, und die man im selben Moment vonTrivialliteraturund ähnlichen Werken ohne vergleichbare „literarische“, sprich künstlerische Qualität, abgrenzt. Die Literatur zählt zu denGattungen der Kunst. Das Wort Literatur wurde bis in das 19. Jahrhundert hinein regulär für die Wissenschaften verwendet. Mit Literatur sind üblicherweise veröffentlichte Schriften gemeint. Die Gesamtheit der veröffentlichten Schriften eines Fachgebietes bzw. zu einer bestimmten Thematik oder Zielsetzung bildet ein „Schrifttum“. Nur eingeschränkt und nicht über den Buchhandel zugänglichePublikationenwerden alsgraue Literaturzusammengefasst. Die heutige begriffliche Differenzierung, die im weitesten Sinne alle sprachliche Überlieferung umfasst und dabei ein enges Feld „literarischer“ Kunstwerke konstituiert, richtete sich erst im Laufe des 19. Jahrhunderts ein. Das Wort stand zuvor fürGelehrsamkeit, die Wissenschaften, die Produktion derres publica literariaund der frühmodernenscientific community, seltener auch lediglich für Schriften dergriechischenundlateinischenAntike. Die Neudefinition des Wortes geschah im Wesentlichen unter Einfluss neuer Literaturzeitschriften und ihnen folgender Literaturgeschichten, die zwischen 1730 und 1830 sich schrittweise denbelles lettres, den schönen Wissenschaften öffneten, dem Bereich modischer und eleganter Bücher des internationalen Marktes und die dabei Werken derPoesieein zentrales Interesse schenkten. Es wurde im selbenProzessselbstverständlich, dass Literatur Besprochen wird in den nationalenPhilologien(wie derGermanistik, derRomanistik, derAnglistik), die die Ausgestaltung der nationalen Literaturen im 19. Jahrhundert im Wesentlichen vorantrieben, nahezu ausschließlich „hohe“ Literatur. Welche Werke unter welchen Gesichtspunkten besprochen werden, ist seitdem Gegenstand einer Debatte um die Bedeutung, die Werke in der jeweiligen Gesellschaft gewinnen. Der jeweilige „Kanon“ einer Nationalliteratur wird in der öffentlichen (und angreifbaren) Würdigung der „künstlerischen“ Qualität festgelegt, sowie in kontroversenTextinterpretationenderFiktionen, die Titeln tiefere Bedeutung zusprechen. In der neuen Ausgestaltung übernahm die Literatur im 19. Jahrhundert in den westlichensäkularenNationen Funktionen, die zuvor die Religionen und ihre Textgrundlagen als Debatten- und Bildungsgegenstände innehatten. In neuerer Zeit wurde das Thema der digitalen Schriftlichkeit ein Diskussionsgebiet der Literaturwissenschaft und Medienwissenschaft. Gerade bei dieser Art von Literatur ist es nicht mehr möglich, nach Kriterien zu beurteilen, die man für Literatur vergangener Jahrhunderte entwickelt hatte. Siehe dazu:Digitale Schriftlichkeit. Das Wort Literatur ist eine erst in der Frühmoderne in Mode kommende Ableitung deslateinischenlittera, der „Buchstabe“. Der Plurallitteraegewann bereits in der Antike eigene Bedeutungen als „Geschriebenes“, „Dokumente“, „Briefe“, „Gelehrsamkeit“, „Wissenschaft(en)“. ImFranzösischenundEnglischenblieb diese Bedeutung erhalten inlettresundlettersals Synonym für „Wissenschaften“. Das heutige Sprechen von Literatur entwickelte sich auf einem Umweg über das Deutsche und seine Äquivalente für die französische Wortfügungbelles lettres. Im Laufe des 17. Jahrhunderts setzte sich die französische Wortkombination für einen neuen Bereich eleganter Bücher auf dem europäischen Markt durch. Die zeitgenössische deutsche Übersetzung war hierfür „galante Wissenschaften“, was dem Publikumsanspruch Rechnung trug wie dem modischen Geschmack: Leser beiderlei Geschlechts lasen diese Ware und bestanden darauf, dass sie eine ganze eigene Wissenschaft benötigte, keine akademische pedantische. Als mit dem frühen 18. Jahrhundert das Wortgalantin Kritik geriet, setzte sich ein Sprechen von „schönen Wissenschaften“ durch, das im späten 18. Jahrhundert an Tragfähigkeit verlor, da es hier zunehmend um Poesie und Romane ging, eine unwissenschaftliche Materie. Das Sprechen von „schöner Literatur“ erlaubte es schließlich das engere im weiteren Begriffsfeld zu benennen. Man sprach ab Mitte des 18. Jahrhunderts von „Literatur“ mit der Option, jeweilige Schwerpunkte legen zu können. Mit dem Adjektiv „schöne“ wurde das Zentrum bezeichnet, das Literatur im engeren Sinn wurde. Je klarer das Zentrum definiert wurde, desto entbehrlicher wurde im 20. Jahrhundert die weitere Verwendung des Adjektivs. Aus dem Wortbelles lettresging im deutschen Buchhandel das Wort „Belletristik“ hervor, das heute eine Nachbarstellung einnimmt. Der Buchhandel führte die Verengung des Literaturbegriffs auf Dichtung der Nation, wie sie im 19. Jahrhundert geschah, am Ende nicht durch. Für Verlage ist der internationale Markt unterhaltender Titel ein unverzichtbares Geschäftsfeld. Man kann innerhalb der Belletristik ein kleineres Feld der Klassiker der Literatur abgrenzen[1]und dieses wiederum international sortieren. Das Wort Literatur hat seine zentrale Bedeutung in Literaturgeschichten, Literaturzeitschriften, in der Literaturkritik undLiteraturtheorie. In all diesen Bereichen geht es deutlich darum, Kontroversen über Literatur zu erzeugen. Mit der Belletristik wird im Deutschen eher ein unkontroverses, uneingeschränktes Feld ohne eigene Geschichte beibehalten. Es gibt bezeichnenderweise keine „Belletristikgeschichte“, keine „Belletristikkritik“ und keine nationalen „Belletristiken“, dafür jedoch „Literaturgeschichte“, und „Literaturkritik“ wie „Nationalliteraturen“.[2] Der heutige Literaturbegriff spiegelt denWortgebrauchder letzten zweihundert Jahre wider. Er zeichnet sich dabei gleichzeitig durch die Aufnahme einer Reihe historischer Kontroversen aus, die den modernen Streit darüber, welche Werke es verdienen sollten, als Literatur besprochen zu werden, fruchtbar in ihrer teilweisen Unvereinbarkeit bestimmen. Literaturstudenten wird seit dem 19. Jahrhundert die Beherrschung eines Handwerkszeugs der Textanalyse nach den verschiedenen Traditionen der Poetik, der Rhetorik, und der Textinterpretation abverlangt, die dem literarischen Text tiefere kulturelle Bedeutung beimessen soll. Moderne Schulen der Literaturtheorie nahmen hier einzelne Fragestellungen mit unterschiedlichen Schwerpunkten und divergierenden Wünschen an einen Kanon wichtigster Werke der jeweils zu schreibenden Literaturgeschichte auf. Die Vorstellung, dass Literatur ein Bereich besonders schöner Texte sein sollte, ist Erbmasse der antiken und frühneuzeitlichen Poesiediskussion. Der alternative Blick auf kunstvolle Sprachbeherrschung geht dagegen auf die Diskussion antikerRhetorikzurück. Während sich die Rhetorik als weitgehend unkontroverse, zweckorientierte Kunst handhaben ließ, bestand über die Frage des Schönen in derPoesieein langer Streit, der im 18. Jahrhundert im Wesentlichen als Kampf zwischenRegelpoetikern(Verfechtern einer nach Gesetzen schönen Poesie) und Verfechtern einesGeschmacksurteilsgeführt wurde. In der zweiten Hälfte des 18. Jahrhunderts setzte sich in Auflösung dieser Diskussion eine neue wissenschaftliche Debatte derÄsthetikdurch, die – so die Hoffnung – am Ende in allen Bereichen der Kunst gelten würde als eine Konstante menschlicher Wahrnehmung, wie sie Schönheit auch in der Natur entdeckte. Ende des 19. Jahrhunderts geriet der Blick auf die Ästhetik in grundsätzliche Kritik. Das hatte zum einen mit der kontroversen Begriffsaneignung durch dieÄsthetizistenzu tun, zum anderen mit Kunstwerken, die sich provokant von der Konzentration auf Schönheit verabschiedeten und einen eigenen Realismus im Umgang mit sozialer Realität einklagten. Die schonungslose Anerkennung von Missständen sollte ein anerkanntes Ziel werden. Optionen im Umgang mit dem Konflikt bestanden in der Erweiterung der ästhetischen Konzepte wie in der Diskreditierung der Forderung eigener ästhetischer Wahrheit. Dass Literatur sich im gegenwärtigen Begriff durch Fiktionalität und tiefere Bedeutung, eine Relevanz für die Gesellschaft, auszeichnet, ist im Wesentlichen Erbe der Romandiskussion, die Mitte des 18. Jahrhunderts von der Literaturbesprechung aufgenommen wurde. Weder dieAristotelische Poetiknoch die Nachfolgepoetikender frühen Moderne hatten Poesie über Fiktionalität erklärt.Romanehatten sie samt und sonders nicht als Poesie anerkannt. Der Vorschlag, Romane und womöglich Poesie generell über Fiktionalität zu definieren, findet sich erstmals klarer mitPierre Daniel HuetsTraktat über den Ursprung der Romane(1670) gemacht – als Möglichkeit, den theologischen Umgang mit Gleichnissen auf eine neue Lektüre von Romanen zu übertragen, bei dem es darum gehen soll, zu ermessen, welche kulturelle Bedeutung ein jeweiliger Titel hat. Beim Aufbau des modernen Besprechungsgegenstands Literatur war die Frage nach tieferer Bedeutung Anfang des 19. Jahrhunderts praktisch, da sie dem Literaturwissenschaftler neue Tätigkeiten abverlangt, vor allem die der Interpretation. Daneben schuf sie neue Möglichkeiten, Texte zu bewerten und sich speziell diskutierbar rätselhaften, fremdartigen Titeln zuzuwenden und über sie die eigene Nation und Geschichte neu zu erklären. Im 19. und 20. Jahrhundert entfaltete die Frage nach der Bedeutung des Textes in der Kultur zudem politische Dynamik, da sich an sie Forderungen nach aktivem Engagement anschließen ließen. Die Frage stilistischen Anspruchs ist im Wesentlichen Erbmasse der Diskussion neuesterbelles lettres. Poetiken waren davon ausgegangen, dass zwar einzelne Dichter die Kunst unterschiedlich handhabten, dass jedoch das Persönliche selbst nicht zu erstreben war. Schönheit galt es an sich anzustreben, der Künstler rang um die Schönheit. Mit der Romandiskussion wurde die Frage nach kulturellen Hintergründen akut, die Frage des individuellenAutorswar dabei wenig das Ziel. Anders war die Debatte in der Belletristik verlaufen. In ihr stand gerade die Frage nach den Titeln im Vordergrund, die den aktuellen Geschmack am besten befriedigten. Es ging im selben Moment um die Frage nach neuen Autoren, die mit eigenen Sichtweisen den Geschmack prägten. Diebelles lettressollten insgesamt, so ihre Verfechter sich durch Stil auszeichnen – gegenüber den minderwertigenVolksbüchernwie gegenüber der pedantischen Wissenschaftlichkeit. Romane und Memoiren wurden wesentliche Felder der Produktion modernen persönlichen Stils. Die Diskussion jeweiliger Leistungen der individuellen Perspektive ging im frühen 19. Jahrhundert in der heutigen Literaturdiskussion auf – die Frage nach subjektiver Wahrnehmung der Realität, wie sie sich in Literatur abzeichne, prädestinierte den neuen Bereich, der im 19. Jahrhundert aufgebaut wurde, dazu, ein Debattenfeld im Schulunterricht zu werden. Im modernen Literaturunterricht geht es seitdem zentral darum, Schüler zu subjektiven Stellungnahmen zu Literatur zu bewegen, ihre Subjektivität dabei öffentlich wahrzunehmen, Subjektivität behandelter Autoren zu erfassen. Im Lauf des 20. Jahrhunderts kam eine eigene, mutmaßlich neutrale, wissenschaftliche Analyse von Komplexität literarischer Werke auf. Auf sie richtete sich vor allem derStrukturalismusder 1960er und 1970er und ihm folgend derPoststrukturalismusder 1980er und 1990er aus. Betrachtet man die Untersuchungen mit historischer Perspektive, so nehmen sie aus allen Debattenfeldern Untersuchungsoptionen auf. Besondere Würdigung erhalten dabei Texte, die komplexer zu analysieren sind, die der Literaturbesprechung mehr Angriffsfläche der auszulotenden Kontexte geben. Der hochrangige Text ist unter dieser Prämisse der, der reich an – womöglich divergierenden – Bedeutungsebenen ist, sich intensiv mit Traditionen auseinandersetzt, sich komplex auf andere Texte bezieht, erst im Blick auf diese besser verstanden wird. Die Analysen sind insofern wissenschaftlich objektiv, als sie tatsächlich die wissenschaftliche Analysierbarkeit als Eigenschaft von Texten erfassen, die sich dank ihrer Qualitäten in der wissenschaftlichen Analyse halten, uns nachhaltig als Literatur damit beschäftigen. Hier lag, rückblickend betrachtet gleichzeitig die Option einer Mode von Texten, die sich auf die Literaturbetrachtung ausrichteten. DiePostmoderneging in Entdeckungen des Trivialen am Ende zunehmend konfrontativ bis ablehnend mit den hier definierten Ansprüchen an Kunst der Literatur um. Erst ab dem 19. Jahrhundert hat man zur Literatur nicht nur das Wissenschaftliche gezählt, sondern alles, was schriftlich niedergelegt war. Ab dem Jahrhundert unterschied man auch zwischen hoher Literatur, sprich Hochliteratur, und Literatur von wenig künstlerischer Qualität, sprich Trivialliteratur. Der Prozess, in dem im späten 18. und frühen 19. JahrhundertDramen,RomaneundGedichtezu „Literatur“ gemacht wurden (sie hingen vorher unter keinem Wort zusammen), muss unter unterschiedlichen Perspektiven gesehen werden. Ganz verschiedene Interessen waren daran beteiligt, die „Literatur“ zum breiten Debattenfeld zu machen. Auf eine einprägsame Formel gebracht, engten die Teilnehmer der Literaturdebatte ihre Diskussion ein und weiteten ihre Debatte damit aus: Seit Jahrhunderten hatten sie erfolgreich wissenschaftliche Schriften als „Literatur“ diskutiert – Poesie und Fiktionen interessierten sie dabei vor 1750 nur am Rande. In der zweiten Hälfte des 18. Jahrhunderts rückten sie ausgewählte Felder des populären Randgebiets in das Zentrum ihrer Rezensionen mit dem Effekt, dass ihre eigene Diskussion sich nun mit den freier besprechbaren Gegenständen ausweitete. Die Gründung der universitär verankerten Literaturwissenschaft festigte im 19. Jahrhundert den Prozess dieser Einengung des Debattenfeldes (auf Dramen, Romane und Gedichte) sowie die Ausdehnung der Diskussion selbst (vor allem auf die staatlichen Schulen und die öffentlichen Medien). Das Wort Literatur gilt heute zwar nicht mehr demselben Gegenstand wie vor 1750, es blieb jedoch kontinuierlich das Wort dessekundärenAustauschs über Literatur. Es findet sich aufTitelseitenvonLiteraturzeitschriften, in den Bezeichnungen von Lehrstühlen und universitären Seminaren derLiteraturwissenschaft, in den Titeln vonLiteraturgeschichten, in Wortfügungen wieLiteraturpapst,Literaturkritiker,Literaturhaus,Literaturpreis. Das Wort Literatur ist dabei (anders als Worte wie „Hammer“, die keine Debattengegenstände bezeichnen) vor allem ein Wort des Streits und der Frage: „Was soll eigentlich als Literatur Anerkennung finden?“ Es gibt eine Literaturdiskussion, und sie legt auf der Suche nach neuen Themen, neuer Literatur und neuen Literaturdefinitionen fortwährend neu fest, was gerade für Literatur erachtet wird. Sie tat dies in den letzten 300 Jahren mit solchem Wandel ihres Interesses, dass man für das Wort Literatur eben durchaus keine stabile inhaltliche Definition geben kann. Das große Thema des Austauschs über Literatur waren bis weit ins 18. Jahrhundert hinein die Wissenschaften. In der Praxis des Besprechungswesens reduzierte sich der Blick derLiteraturrezensentendabei auf neueste Publikationen, auf Schriften – ein Austausch, der zunehmend Leser außerhalb der Wissenschaften ansprach: Wissenschaftliche Journale erschienen in der zweiten Hälfte des 17. Jahrhunderts mit spannenden Themen in den Niederlanden auf Französisch. Englische kamen hinzu, deutsche boomten zwischen 1700 und 1730 im Geschäft, das die Universitäten Leipzigs, Halles und Jenas bestimmten. Der Reiz der wissenschaftlichen Journale war ihre Diskussionsfreudigkeit, ihre Offenheit für politische Themen, die Präsenz, die hier einzelne Literaturkritiker mit eigenen, sehr persönlich geführten Journalen (im deutschen etwa denGundlingianadesNikolaus Hieronymus Gundling) entwickelten. Zwischen 1730 und 1770 wandten sich deutscheliterarische Journalebahnbrechend der nationalenDichtungzu – im territorial und konfessionell zersplittertenSprachraumwar die Poesie der Nation ein Thema, das sich überregional und mit größten Freiheiten behandeln ließ. Die Gelehrsamkeit (dieres publica literaria) gewann mit Rezensionen derbelles lettres, derschönen Wissenschaften, derschönen Literatur(so die Dachbegriffe, die man wählte, um diese Werke ungeniert in wissenschaftlichen Zeitschriften ansprechen zu können), ein wachsendes Publikum. Aus dem modischen Ausnahmefall des Rezensionswesens wurde im Verlauf des 18. Jahrhunderts der Regelfall. Zu Beginn des 19. Jahrhunderts musste im Deutschen das Wort Literatur neu definiert werden. Literatur war (hielt man sich vor Augen, was da besprochen wurde) definitiv nicht der Wissenschaftsbetrieb, sondern eine textliche Produktion mit zentralen Feldern in der künstlerischen Produktion. Literatur wurde in der neuen Definition: Nach der neuen Definition war davon auszugehen, dass sich die Literatur in nationalen Traditionssträngen entwickelte: Wenn sie im Kern sprachliche Überlieferung war, dann mussten die Sprachen und die politisch definierten Sprachräume den einzelnen Überlieferungen Grenzen setzen – Grenzen, über die nur einKulturaustauschhinweghelfen kann. Ein Sprechen von „Literaturen“ im Plural entfaltete sich. Für dieNationalliteraturenwurden die nationalenPhilologienzuständig. Eine eigene Wissenschaft derKomparatistikuntersucht die Literaturen heute in Vergleichen. Die Definition von Literatur als „Gesamt der sprachlichen und schriftlichen Überlieferung“ erlaubt es den verschiedenen Wissenschaften, weiterhin in „Literaturverzeichnissen“ ihre eigenen Arbeiten als „Literatur“ zu listen (Fachliteratur). Die Definition im „engen Sinn“ ist dagegen gezieltarbiträrundzirkulärangelegt. Es blieb und bleibt darüber zu streiten, welche Werke als „künstlerische“ Leistungen anzuerkennen sind. Das, was Literatur werden sollte, hatte vor 1750 weder einen eigenen Oberbegriff noch größere Marktbedeutung. Poesie und Romane mussten erst unter eine einheitliche Diskussion gebracht werden, wobei gleichzeitig große Bereiche der Poesie- wie der Romanproduktion aus der Literaturdiskussion herausgehalten werden mussten, wenn diese ihr kritisches Gewicht bewahren wollte. Der Prozess, in dem ausgewählte Dramen, Romane und Gedichte „Literatur“ wurden, fand dabei in einem größeren statt: Seit dem 17. Jahrhundert gab es auf dem Buchmarkt diebelles lettres(englisch vor 1750 oft mitpolite literatureübersetzt, deutsch mit „galante Wissenschaften“ und ab 1750 „schöne Wissenschaften“). Dieses Feld besteht heute im Deutschen mit derBelletristikfort. Diebelles lettreswaren im 17. Jahrhundert unter denlettres, den Wissenschaften, für das Besprechungswesen ein unterhaltsamer Randbereich. Sie erwiesen sich im Lauf des 18. Jahrhunderts als popularisierbares Besprechungsfeld. Ihnen fehlten jedoch entscheidende Voraussetzungen, um staatlichen Schutz erlangen zu können: Diebelles lettreswaren und sind international und modisch (man kann von „nationalen Literaturen“ sprechen, nicht aber von „nationalen Belletristiken“), sie umfassten Memoires, Reiseberichte, politischen Klatsch, elegante Skandalpublikationen genauso wie Klassiker der antiken Dichter in neuen Übersetzungen (ihnen fehlt mit anderen Worten jede Ausrichtung auf eine Qualitätsdiskussion; man liest die mit Geschmack, es gibt „Literaturkritiker“, aber keine „Belletristikkritiker“). Die Belletristik war und ist vor allem aktuell und das selbst in ihren Klassikern (es gibt keine „Belletristikgeschichte“, wohl aber „Literaturgeschichte“) – das sind die wesentlichen Unterschiede zwischen Belletristik und Literatur, die aufzeigen, wie die Belletristik umgeformt werden musste, um die Literatur im heutigen Sinn zu schaffen. Staatliches Interesse – Achtung, mit der sie zum Unterrichtsgegenstand werden konnte – gewann die Belletristik durch die Einrichtung einer nationalen Debatte, in der es um hohe Kunst der nationalen Dichter ging. Romane, Dramen und Gedichte wurden in der Einrichtung dieser Diskussion zum zentralen Feld derbelles lettres, zu „schöner Literatur“, dem Kernbereich der literarischen Produktion. Der Bereich derbelles lettreswar vor 1750 klein, aber virulent. Unter 1500–3000 Titeln der jährlichen Gesamtproduktion, die um 1700 in den einzelnen großen Sprachen Französisch, Englisch und Deutsch auf den Markt kam, machten diebelles lettrespro Jahr 200–500 Titel aus; 20–50 Romane waren etwa dabei. Der Großteil der Buchproduktion entfiel auf die Bereichewissenschaftliche Literaturund religiöse Textproduktion vonGebetbüchernbis hoch zu theologischer Fachwissenschaft, sowie, wachsend: auf die politische Auseinandersetzung. Zu den Marktentwicklungen eingehender das StichwortBuchangebot (Geschichte). Die Literaturkritik, die Kritik der Wissenschaften, ließ sich zwischen 1730 und 1770 gezielt auf die skandalösesten Bereiche des kleinen belletristischen Marktes ein. Dort, wo es die skandalöse Oper und den ebenso skandalösen Roman gab, musste (so die Forderung der Kritiker) in nationalem Interesse Besseres entstehen. Mit größtem Einfluss agierte hier die deutsche Gelehrsamkeit. DieTragödiein Versen wurde das erste Projekt des neuen, sich der Poesie zuwendenden wissenschaftlichen Rezensionswesens. Frankreich und England hätten eine solche Tragödie zum Ruhm der eigenen Nation, führteJohann Christoph Gottschedin seiner Vorrede zumSterbenden Cato, 1731 aus, die den Ruf nach jener neuen deutschen Poesie begründete, aus der am Ende die neue hohe deutsche Nationalliteratur wurde. Die Attacke richtete sich (auch wenn Gottsched das nur in Nebensätzen klarstellte, und ansonsten das Theater der Wandertruppen angriff) gegen die Oper, die in der Poesie den Ton angab. Die Oper mochte Musik sein. Die neue, der Oper ferne Tragödie würde, so versprach es Gottsched, auf Aufmerksamkeit (und damit Werbung) des kritischen Rezensionswesens hoffen können, falls sie sich an die poetischen Regeln hielt, dieAristotelesformuliert hatte. Die Rückkehr zur aristotelischen Poetik blieb ein Desiderat der „Gottschedianer“. Mit dembürgerlichen Trauerspielgewann Mitte des 18. Jahrhunderts ein ganz anderes Drama – eines inProsa, das bürgerliche Helden tragödienfähig machte – die Aufmerksamkeit der Literaturkritik. Der Roman, der mitSamuel RichardsonsPamela, or Virtue Rewarded(1740) dem neuen Drama die wichtigsten Vorgaben gemacht hatte, fand im selben Moment das Interesse der Literaturrezension. War der Roman bis dahin eher Teil der dubiosen Historien als Poesie, so wurde nun die Poesiedefinition für den Roman geöffnet, so wie sie gegenüber derOper, demBallett, derKantateund demOratoriumverschlossen wurde. Der neue Poesiebegriff gab dem Fiktionalen und seiner diskutierbaren Bedeutung größeren Raum als Regeln und Konventionen. Die Diskutierbarkeit von Poesie nahm damit zu. Sie steigerte sich weiter damit, dass das Besprechungswesen zum nationalen Wettstreit der Dichter aufrief. Die poetischen Werke, die mit den 1730ern geschaffen wurden, um von der Literaturkritik besprochen zu werden, verdrängten nicht die bestehende belletristische Produktion. Der gesamte Markt der Belletristik wuchs in der zweiten Hälfte des 18. Jahrhunderts zum Massenmarkt. Die neue, auf die Besprechung zielende Produktion versetzte jedoch die öffentliche Literaturkritik in die Lage, nach Belieben bestimmen zu können, was öffentlicher Beachtung wert sein sollte und was nicht. Das Besprechungswesen sorgte mit seiner Entscheidungsgewalt über das Medienecho für eine Ausdifferenzierung des belletristischen Sektors und für eine Entskandalisierung der Öffentlichkeit: Für die öffentliche Auseinandersetzung bedeutete die neue Differenzierung eine Wohltat. Im frühen 18. Jahrhundert hatte man Romane, die hochrangigen Politikern Sexskandale andichteten, in wissenschaftlichen Journalen besprochen, falls die politische Bedeutung das erforderte. Man hatte die Informationen schlicht alscurieusgehandelt (siehe etwa die Rezension derAtalantisDelarivier Manleys in denDeutschen Acta Eruditorumvon 1713).[3]Kein Gespür für die Niedrigkeit der Debatte bestand da – man ging vielmehr davon aus, dass sich solche Informationen nicht anders verbreiten ließen, als in skandalösen Romanen. Mitte des 18. Jahrhunderts – die neue Mode derEmpfindsamkeitkam in diesem Geschehen auf – konnte man das „Niedere“ zwar nicht vom Buchmarkt verbannen, aber eben aus der Diskussion nehmen. Es mochte einen skandalösen Journalismus beschäftigen, der eines Tages eine eigeneBoulevardpresseentwickelte, nicht aber die gehobenen Debatten der Literatur. Die Literaturdebatte entwickelte auf dem Weg der von ihr angestrebten Marktreform eine besondere Suche nach Verantwortung für die Gesellschaft – und für die Kunst. Sie fragte nach denAutorendort, wo der Markt bislang weitgehend unbeachtet und anonym florierte. Sie löstePseudonymeauf und nannte die Autoren gezielt bei ihren bürgerlichen Namen (das war im 17. und 18. Jahrhundert durchaus unüblich, man sprach vor 1750 von „Menantes“ nicht von „Christian Friedrich Hunold“). Die neueLiteraturwissenschaftdiskutierte, welche Stellung die Autoren in derNationalliteraturgewannen und legte damit das höhere Ziel der Verantwortung fest. Sie schuf schließlich besondere Fachdiskussionen wie diepsychologischeInterpretation, um selbst das noch zu erfassen, was die Autoren nur unbewusst in ihre Texte gebracht hatten, doch eben nicht weniger in der literaturwissenschaftlichen Perspektive verantworteten. Rechtliche Regelungen des Autorstatus und desUrheberschutzesgaben demselben Prozess eine zweite Seite. Geschichten der deutschen Literatur offenbaren die Einschnitte des hier knapp skizzierten Geschehens, sobald man die besprochenen Werke auf der Zeitachse verteilt: Mit den 1730ern beginnt eine kontinuierliche und wachsende Produktion „deutscherDichtung“. Die Diskussionen, die seit 1730 geführt wurden, schlagen sich in Wellen von Werken nieder, die in diesen Diskussionen eine Rolle spielten. Vor 1730 liegt dagegen eine Lücke von 40 Jahren – die Lücke des belletristischen Marktes, dem die Gründungsväter der heutigen nationalen Literaturdiskussion als „Niedrigem“ und „Unwürdigen“ ihre Betrachtungen verweigerten. Mit dem „Mittelalter“, der „Renaissance“ und dem „Barock“ schuf dieLiteraturgeschichtsschreibungdes 18. und 19. Jahrhunderts für die Vergangenheit nationale Großepochen, die der Literatur, wie sie heute erscheint, eine (lückenhafte, nachträglich produzierte) Entwicklung geben. Der Streit in der Frage „Was ist Literatur?“, der mit dem 19. Jahrhundert aufkam, und der nach wie vor dieLiteraturwissenschaftbeschäftigt, ist kein Beweis dafür, dass die Literaturwissenschaft nicht einmal dies zuwege brachte: ihren Forschungsgegenstand klar zu definieren. Die Literaturwissenschaft wurde selbst die Anbieterin dieses Streits. Darüber, was Literatur sein soll und wie man sie adäquat betrachtet, muss tatsächlich gesellschaftsweit gestritten werden, wenn Literatur – Dramen, Romane und Gedichte – im Schulunterricht, in universitären Seminaren, im öffentlichen Kulturleben als geistige Leistung der Nation gewürdigt wird. Jede Interessengruppe, die hier nicht eigene Perspektiven und besondere Diskussionen einklagt, verabschiedet sich aus einer der wichtigsten Debatten der modernen Gesellschaft. Nach dem Vorbild der Literatur (als dem sprachlich fixierten nationalen Diskursgegenstand) wurden mit der Wende ins 19. Jahrhundert die internationaler verfassten Felder der bildendenKunstund der ernstenMusikdefiniert – Felder, die zu parallelen Marktdifferenzierungen führten: Auch hier entstanden „hohe“ gegenüber „niedrigen“ Gefilden: Die hohen sollten überall dort liegen, wo gesellschaftsweite Beachtung mit Recht eingefordert wird. DerKitschund dieUnterhaltungsmusik(„U-Musik“ im Gegensatz zur „E-Musik“) konnten im selben Moment als aller Beachtung unwürdige Produktionen abgetan werden. Die Literaturdebatte muss von allen Gruppen der Gesellschaft als Teil der größeren Debatte über dieKulturund die Kunst der Nation aufmerksam beobachtet werden: Sie nimmt mehr als andere Debatten Themen der Gesellschaft auf und sie gibt Themen an benachbarte Diskussionen weiter. Dass sie zum Streit Anlass gibt, ist das Erfolgsgeheimnis der Literaturdefinition des 19. Jahrhunderts: Literatur sollen die Sprachwerke sein, die die Menschheit besonders beschäftigen – das ist zirkulär und arbiträr definiert. Es liegt im selben Moment in der Hand aller, die über Literatur sprechen, festzulegen, was Literatur ist. Ordnung und Fixierung gewann die Literaturdebatte nicht mit der Begriffsdefinition „Literatur“, an der sich der Streit entzündet, sondern mit den Traditionen ihres eigenen Austauschs. Was als Literatur betrachtet werden will, muss sich für einen bestimmten Umgang mit literarischen Werken eignen. Die Literatur entwickelte sich im 19. Jahrhundert zur weltlichen Alternative gegenüber den Texten der Religion, die bislang die großen Debatten der Gesellschaft einforderten. Die Übernahme ethisch-moralischer Funktionen durch die schöne Literatur wurde imViktorianischenEngland vor allem vonMatthew Arnoldgefordert, der amPhilistertumder demMammonfrönendenPuritanerAnstoß nahm.[5]Die Literaturwissenschaft drang so mit ihrem Debattengegenstand – Dramen, Romane und Gedichte – in die Lücke, die die Theologie mit derSäkularisierungzu Beginn des 19. Jahrhunderts ließ. Dabei bewährten sich bestimmte Gattungen, die „literarischen“, besser als andere – Das Material, das im Lauf des 18. Jahrhunderts zu Literatur gemacht wurde, war zuvor nur im Ausnahmefall vonLiteraturzeitschriften(wissenschaftlichen Rezensionsorganen) besprochen worden. Der Austausch über Poesie und Fiktionen, überDramen,OpernundRomanegeschah vor 1750 vor allem in denTheaternund in den Romanen selbst. In den Theatern stritten die Fans über die besten Dramen und Opern. Man veranstaltete in London Wettkämpfe, bei denen man Themen ausschrieb und die beste Oper prämierte. Im Roman attackierten Autoren einander unterPseudonymenmit der beliebten Drohung, den Rivalen mit seinem wahren Namen auffliegen zu lassen. Hier griff der sekundäre Diskurs der Literaturkritik um 1750 mit neuen Debattenangeboten ein. Die Literaturdiskussion selbst war zuerst eine rein wissenschaftsinterne Angelegenheit gewesen: Als im 17. Jahrhundert Literaturzeitschriften aufkamen, besprachen in ihnen Wissenschaftler die Arbeiten anderer Wissenschaftler. Das Publikum dieses Streits weitete sich aus, dadurch, dass die Literaturzeitschriften Themen von öffentlichem Interesse intelligent ansprachen und da die Rezensenten sich auf das breitere Publikum mit neuen Besprechungen derbelles lettreseinließen. Wenn die Wissenschaften Dichter besprachen, gewann ihre Debatte eine ganz neue Freiheit: Fachintern, doch vor den Augen der wachsenden Öffentlichkeit besprach man hier Autoren, die außerhalb der eigenen Debatte standen. Man konnte mit ihnen weit kritischer umgehen als mit den Kollegen, die man bislang im Zentrum rezensierte. In dem Maße, in dem die Wissenschaften ihren ersten Besprechungsgegenstand (ihre eigene Arbeit) zugunsten des neuen (Poesie der Nation) erweiterten, öffneten sie die Literaturdebatte der Gesellschaft. Die Literaturdiskussion florierte fortan nicht mehr als vor allem internes Geschäft; sie agierte in ihrem Streit zugleich gegenüber zwei externen Teilnehmern: dem Publikum, das die Literaturdebatte verfolgt und vieldiskutierte Titel mit der Bereitschaft kauft, die Diskussionen fortzusetzen und gegenüber denAutoren, die nun als die Verfasser von „Primärliteratur“ dem „sekundären Diskurs“ beliebig distanziert gegenüberstehen können. Der Austausch gewann an Komplexität, als im 19. Jahrhundert dieNationein eigenes Interesse an der neuformulierten Literatur entwickelte. DieNationalliteraturließ sich an Universitäten und Schulen zum Unterrichtsgegenstand machen. Der Nationalstaat bot der Literaturwissenschaft eigene Institutionalisierung an:Lehrstühlean Universitäten. Die nationalenPhilologienwurden eingerichtet. Literaturwissenschaftler wurden berufen, um fürKultusministeriendieLehrplänezu erstellen, nach denen an den Schulen Literatur zu besprechen ist; sie bilden die Lehrer aus, die Literatur bis in die unteren Schulklassen hinab diskutieren. Die Verlagswelt stellte sich auf den neuen Austausch ein. Kommt ein neuer Roman auf den Markt, schickt sie komplett vorgefassteRezensionenmit Hinweisen auf die Debatten, die dieser Roman entfachen wird, an dieFeuilleton-Redaktionen der wichtigstenZeitungen,ZeitschriftenundFernsehsender. Die Autoren veränderten ihre Arbeit. Mit den 1750ern kamen ganz neue Dramen und Romane auf: schwergewichtige, schwerverständliche, die gesellschaftsweite Diskussionen entfachen müssen. Romane und Dramen wurden in ganz neuem Maße „anspruchsvoll“ – Anspruch auf öffentliche Würdigung ist das neue Thema. Um mehr Gewicht auf Debatten zu gewinnen, wurde es unter den Autoren Mode, Dramen, Romane und Gedichte inepochalenStrömungenzu verfassen,Schulenzu gründen, die einen bestimmtenStil, eine bestimmte Schreibweise (die „realistische“, die „naturalistische“ etc.), eine bestimmteKunsttheorie(die des „Surrealismus“, die des „Expressionismus“) verfochten. Autoren, die sich auf eine solche Weise verorten, werden, wenn die Aktion gelingt, als bahnbrechende besprochen, wenn sie zu spät auf den falschen Zug aufspringen, werden sie von der Kritik als „Epigonen“ gebrandmarkt. Dieses gesamte Spiel kennt kein Pendant vor 1750. Die meisten Stilrichtungen, die wir (wie das „Barock“ und „Rokoko“) vor 1750 ausmachen, sind erst später geschaffene Konstrukte, mit denen wir den Eindruck erwecken, dass Literatur schon immer Debatten fand, wie sie sie seit dem 19. Jahrhundert findet. Die Autoren organisierten sich in Assoziationen wie demP.E.N.-Club international. Sie formierten Gruppen wie die „Gruppe 47“ und Strömungen. MitManifestenbegannen sie, dem sekundären Diskurs Vorgaben zu machen. Im Einzelfall ließen sie sich auf Fehden mitLiteraturpäpstenein, um auf direktestem Weg die Literaturdiskussion auf sich zu ziehen. Autoren nehmenLiteraturpreisean oder schlagen sie, wieJean-Paul Sartreden ihm verliehenenNobelpreis für Literatur, im öffentlichen Affront aus. Sie haltenDichterlesungeninBuchhandlungen– undenkbar wäre das im frühen 18. Jahrhundert gewesen. Sie begeben sich in den „Widerstand“ gegen politische Systeme, sie schreibenExilliteraturaus der Emigration heraus. Mit all diesen Interaktionsformen gewann der Austausch über Literatur eine Bedeutung, die der Austausch über die Religion kaum hatte (geschweige denn der Austausch über Literatur im alten Wortsinn oder derjenige über Poesie und Romane, wie er vor 1750 bestand). Das brachte eigene Gefahren mit sich. Die Literaturwissenschaft und der von ihr ausgebildete freiere Bereich derLiteraturkritikin denMediensind erheblichen Einflussnahmen der Gesellschaft ausgesetzt. Die Gesellschaft klagt neue Debatten ein, fordert neue politische Orientierungen, erzwingt von der Literaturkritik Widerstand oder Anpassung. Es gibt in derpluralistischenGesellschaft in der Folge einefeministischeLiteraturwissenschaft wie einemarxistische, oder (scheinbar unpolitischer) einestrukturalistischeund so fort. EineGleichschaltungder Gesellschaft, wie sie dasDritte Reichdurchführte, greift konsequenterweise gezielt zuerst in denLiteraturbetriebein. Die institutionalisierte Literaturwissenschaft lässt sich sehr schnell gleichschalten, Lehrstühle werden neu besetzt, Lehrpläne bereinigt, Literaturpreise unter neuen Richtlinien vergeben. Die Gleichschaltung der Verlagswelt und der Autorenschaft ist die schwierigere Aufgabe derLiteraturpolitik, der totalitäre Staaten zur Kontrolle der in ihnen geführten Debatten große Aufmerksamkeit schenken müssen. Warum die Nation überhaupt ein solches Interesse am pluralistischen und jederzeit kritischen Gegenstand „Literatur“ und den Debatten nationaler „Kunst“ und „Kultur“ entwickelte: Europas Nationen antworteten mit der Einführung nationalstaatlicherBildungssystemeund der allgemeinenSchulpflicht– durchaus auch – auf dieFranzösische Revolution. Wer aufsteigen wollte, sollte, so das Versprechen, das jede weitere Revolution erübrigen musste, es in der Nation beliebig weit bringen können – vorausgesetzt, er nutzte die ihm angebotenen Bildungschancen. In der Praxis blieben Kinder unterer Schichten bei allerChancengleichheitfinanziell benachteiligt. Weit schwerer wog für sie jedoch, was sie an Erfahrungen frühzeitig in all den Schulfächern machten, in denen die neuen Themen angesagt waren: Wer in der Gesellschaft aufsteigen wollte, würde seinen Geschmackanpassenmüssen. Er würde sich ausschließlich für hohe Literatur, bildende Kunst und ernste Musik begeistern müssen und am Ende mit seinen nächsten Angehörigen keine Themen mehr teilen, ihre Zeitungen verachten wie ihre Nachrichten. Die Frage war nicht, ob man aufsteigen konnte. Die Frage war, ob man bei diesen Aussichten aufsteigen wollte? Erst das ausgehende 20. Jahrhundert brachte hier eine größereNivellierungder „Kulturen“ innerhalb der Gesellschaft – nicht wie in der linken politischen Theorie gedacht durch eine Erziehung, die Arbeiterkinder an die hohe Kultur heranführte, sondern durch neue Moden derPostmoderne, in denen „niedere“ Kultur, „Trash“, plötzlich „Kultstatus“ gewann. Der Verlierer im Kampf um gesellschaftliche Diskussionen und Aufmerksamkeit scheint bei alledem dieReligiongewesen zu sein. Die Literatur ist gerade an dieser Stelle eine interessant offene Konstruktion. Die Texte der Religion können dort, wo man Literatur diskutiert, jederzeit als die „zentralen Texte der gesamten sprachlichen Überlieferung“ eingestuft werden. Aus der Sicht der Literaturwissenschaft liegen die Texte der Religion nicht „außerhalb“, sondern mitten „im“ kulturellen Leben der Nation. Die Texte der Religion stehen zur Literatur als dem großen Bereich aller textlichen (nach Nationen geordneten) Überlieferung nahezu so ähnlich wie die Religionen selbst zu den Staaten, in denen sie agieren. Es ist dies der tiefere Grund, warum sich das Konzept der Literatur, wie es heute die Literaturwissenschaft beschäftigt, weitgehend ohne auf Widerstand zu stoßen, weltweit ausdehnen ließ. Die moderne Literaturdebatte folgt vor allem deutschen und französischen Konzepten des 18. und 19. Jahrhunderts. DeutscheJournalewieLessingsBriefe die Neueste Literatur betreffendwandten sich früh dem neuen Gegenstand zu. Sie taten dies gerade im Verweis auf ein nationales Defizit. Mit derFranzösischen Revolutionerreichte Frankreich das Interesse an einem säkularen textbasierten Bildungsgegenstand. Wer sich durch die englische Publizistik des 19. Jahrhunderts liest, wird dagegen feststellen, dass das Wort „Literatur“ hier noch bis Ende des 19. Jahrhunderts synonym für die Gelehrsamkeit stehen konnte. An Themen des nationalen Austauschs fehlte es in Großbritannien nicht – die Politik und die Religion lieferten sie zur freier Teilnahme an allen Diskussionen. Die Nation, die dieKirche im 16. Jahrhundert dem Staatsgefüge einverleibthatte, fand erst spät eine eigene der kontinentalenSäkularisationgleichkommende Debatte. Die wichtigste Geschichte der englischen Literatur, die im 19. Jahrhundert erschien,Hippolyte TainesHistory of English Literaturebrachte die neue Wortverwendung als Anstoß von außen ins Spiel und machte verhältnismäßig spät klar, welche Bedeutung England in der neu zu schreibenden Literaturgeschichte selbst gewinnen konnte. Das Konzept nationaler Literaturen wurde von Europa aus den Nationen der Welt vorgelegt. Es fand am Ende weltweit Akzeptanz. Der Buchmarkt gestaltete sich im selben Geschehen um: Aus einem im frühen 18. Jahrhundert marginalen Feld des Buchangebots wurde die zentrale Produktion. Es drohen mit dem Konzept nationaler Literaturen allerdings fragwürdige Wahrnehmungen: Aus einer nationalliterarischen Perspektive wurde dankbar auf das Konzept nationaler Literaturen zurückgegriffen, da es die jeweilige kulturelle Identität nicht antastete. DieKomparatistikentwickelte jedoch schon früh mit dem Konzept derWeltliteraturein transnationales Literaturmodell, das – jenseits einer nationalen oder ökonomischen Vorstellung von Literatur(markt) – ein kosmopolitisches Miteinander der Literaturen der Welt gegen die verengende nationale Perspektive setzte. Weitaus mehr Einsprüche rief der enge Literaturbegriff hervor. Sowohl die Schulen der textimmanenten Interpretation, die wie derStrukturalismusdie Bedeutung im einzeln vorliegenden Textstück suchen, als auch die Schulen der gesellschaftsbezogenen Literaturinterpretation vomMarxismusbis zu den Strömungen derLiteratursoziologie, die einen Blick auf die Gesellschaft einfordern, traten in der zweiten Hälfte des 20. Jahrhunderts für einen „weiten“ Literaturbegriff ein, der es Literaturkritikern erlauben würde, auch politische Texte, Werbung und Alltagstexteideologiekritischzu besprechen. So interpretieren die modernenKulturwissenschaftenliterarische Texte nicht nur im literaturtheoretischen und -historischen Kontext, sondern auch als historische Dokumente, als Beiträge zu philosophischen Diskussionen oder (in Form derCultural Studies) als Ausdruck der Dominanz herrschender oder der Unterdrückung marginalisierter (Sub-)Kulturen. Umgekehrt öffnen die Kulturwissenschaften den Blick für literarische Qualitäten der Geschichtsschreibung oder philosophische Aspekte von literarischen Texten. Die Vertreter desPoststrukturalismuserweiterten in den 1980er und 1990er Jahren ihrenText- wie ihrenSprachbegriffnoch entschiedener.Roland Bartheshatte in den 1950er Jahren bereits die Titelcover von Zeitschriften genauso wie das neue Design eines Autos in ihren Botschaften besprochen. Zur Selbstverständlichkeit wurde der erweiterte Sprachbegriff in derFilmwissenschaft. Hier spricht man ganz ohne weiteres von der „Bildsprache“ einesRegisseurs, und auch über eine solche Sprache können Literaturwissenschaftler sich äußern. Wenn die Literaturwissenschaft sich jedoch auf sprachliche Kunstwerke spezialisiert, hat dies durchaus Vorteile. Sie hält andere Wissenschaftler davon ab, in ihrem Forschungsfeld als Experten aufzutreten, kann jedoch letztlich sehr frei festlegen, was ihr Gegenstand ist. Sie kann sich so auf ein gut gehendes Kerngeschäft, Literatur im engen Sinn, ausrichten oder mit einem erweiterten Literaturbegriff auftreten. Der wiederkehrende Warnruf, der Tod der Literatur stehe bevor, ist auch ein Spiel mit der Aufmerksamkeit der Gesellschaft, die den Austausch über Literatur verfolgt und verteidigt. Neuerdings wird von einer „performativen Wende“ der Literatur unter den Bedingungen des Internets gesprochen, die auch die Grenzen zwischen Literatur und darstellenden Künsten bzw. zwischen Schriftlichkeit und Mündlichkeit relativiert: Das Erscheinen eines Textes im Internet könne alsperformativerAkt analog einer Theateraufführung verstanden werden. Das Internet sei nicht mehr nur ein Geflecht von Texten; die „Netzliteratur“, z. B. das Schreiben inChatrooms, sei vielmehr wesentlich durch performative Aspekte, d. h. durch Handlungen bestimmt. Die Kategorie der Performanz, die bisher nur auf Mündliches bezogen war, kann damit auch auf schriftliche Äußerungen übertragen werden: Zwischen ihrem Verfassen, ihrem Erscheinen und ihrer Lektüre muss (fast) keine Zeit mehr verstreichen.[6]Das ähnelt der Sprechsituation vonSpeaker’s Corner. Interessant ist auch die Digitale Bibliothek: E-Texte der Philosophie, Religion, Literatur etc. Im Internet wird aber nicht nur Literatur zur Verfügung gestellt, sondern auch Literatur geschrieben. Beispiele sindDigitale Poesie,Weblogsoder kollaboratives Schreiben im Netz. Digitale Literatur folgt anderen Kriterien als herkömmliche Literatur, sie ist von Aspekten der Technik, Ästhetik und Kommunikation geprägt. Das Internet eignet sich dafür um über zeitliche und räumliche Distanzen hinweg zu kommunizieren und multimediale Aspekte zu vereinen und zu integrieren. Außerdem unterliegen elektronische Medien einer beständigen Metamorphose. So haben beispielsweiseNeal Stephensonund sein Team mit dem Schreiben eines Romans(The Mongoliad)[7]im Internet begonnen, bei dem eine Community von Autoren interaktiv mitschreibt. Neben dem eigentlichen Text gibt es eine eigene E-Publishing-Plattform („Subutai“) mit Videos, Bildern, einem Wiki und einem Diskussionsforum zum Roman. Zur Verwaltung von Literatur gibt es mittlerweile zahlreiche Programme. Mit ihnen lassen sich z. B. eigene Literatursammlungen nach spezifischen Merkmalen kategorisieren. Die Abfragen brauchen teilweise nicht von Hand eingegeben zu werden, es reicht, z. B. den Autor bzw. den Titel einzugeben und daraufhin eine Suche in bestimmten Datenbanken zu tätigen. Die Ergebnisse können dann einfach übernommen werden. Eine Literaturdatenbank katalogisiert den Bestand aktueller und älterer Literatur. Hier finden vermehrt digitale Kataloge bzw. Online-Literaturdatenbanken ihren Gebrauch. Siehe auch die Artikel Belletristik/Schöne Literatur (Literatur über die Literatur) siehe auch:Literaturlexikon Die Autoren dieser Titel legen ein Corpus von in ihren Augen literarischen Werken fest und versuchen dann, in einer wissenschaftlichen und subjektiven Analyse dieser Werke auszumachen, was Literatur grundsätzlich auszeichnet. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Begriffsdifferenzierung 2Etymologie und Begriffsgeschichte 3Definitionen 3.1Ästhetik und kunstvolle Sprachbeherrschung 3.2Fiktionalität, gesellschaftliche Relevanz 3.3Literarischer Stil und Subjektivität 3.4Höhere strukturelle Komplexität und komplexeres Traditionsverhalten 4Geschichte des Diskussionsfeldes 4.118. Jahrhundert: Die Literaturkritik wendet sich „schöner Literatur“ zu 4.2Dramen, Romane und Poesie werden im 18. Jahrhundert zum Diskussionsfeld 4.2.1Die „belles lettres“ werden zum Sonderfeld der Literaturdiskussion 4.2.2Das kritische Besprechungswesen entskandalisierte die Belletristik 4.2.3Auf dem Weg zur diskutablen Poesie wird die Oper ausgeschaltet 4.2.4Der Roman wird dagegen Teil der Poesie 4.2.5Die Diskussion „hoher Literatur“ und die Entskandalisierung der Öffentlichkeit 4.2.6Die Literaturgeschichte wird mit der Wende ins 19. Jahrhundert geschaffen 4.3Seit dem 19. Jahrhundert: Literatur im kulturellen Leben der Nation 4.3.1Der literarische Kanon verdrängt den religiösen 4.3.2Siegeszug der pluralistischen Diskussion 4.3.3Rückblick: Ein neuer Bildungsgegenstand wurde geschaffen 4.4Literaturen: Das international fragwürdige Konzept 4.5Tendenzen: Der „erweiterte Literaturbegriff“ – der „Tod der Literatur“? 5Arten von Literatur und Adressaten 6Buchmessen 7Literatur und Internet 7.1Sammlungen von Literatur im Internet 7.2Im Internet entstandene Literatur 8Literatursoftware 9Literaturdatenbanken 10Literaturen nach Sprachen und Nationen 11Bereiche schriftlicher und sprachlicher Überlieferung"
  },
  {
    "label": 0,
    "text": "London – Wikipedia London Inhaltsverzeichnis Geographie Geschichte Bevölkerung Politik Kultur und Sehenswürdigkeiten Wirtschaft Medien Verkehrswesen Bildung Persönlichkeiten der Stadt London Siehe auch Literatur Weblinks Einzelnachweise Geographische Lage Geologie Stadtgliederung Klima Antike Mittelalter Frühe Neuzeit Moderne Religionen Bevölkerungsentwicklung Entwicklung der Wohnsituation Stadtregierung Städtepartnerschaften Städtefreundschaft Theater Musik Museen Bauwerke Parks Sport Regelmäßige Veranstaltungen Einkaufen Rundfunk und Fernsehen Printmedien Nachrichtenagenturen Schienenverkehr Straßenverkehr Luftverkehr Universitäten Kunstschulen Medizin und Forschung Straßen und Plätze Weltliche Bauwerke Sakrale Bauwerke London(deutsche Aussprache [ˈlɔndɔn] oder gelegentlich [ˈlɔndən],englische Aussprache[ˈlʌndən]) ist zugleich die Hauptstadt desVereinigten KönigreichsundEnglands. Die Stadt liegt an derThemseim Südosten Englands. Das heutige Verwaltungsgebiet mit 33 Stadtbezirken entstand 1965 durch die Gründung vonGreater London. Dort lebten 2022 rund 8,9 Millionen Menschen, davon rund 3,4 Millionen in den 13 StadtbezirkenInner Londons.[2][4]In der gesamtenMetropolregion Londonlebten 2019 knapp 14,4 Millionen Einwohner.[5] Im Jahr 50 n. Chr. von den Römern alsLondiniumgegründet, wurde die Stadt nach dernormannischen Eroberung1066 Hauptstadt und Königssitz desKönigreichs England. Bereits imMittelalterentwickelte sich die Stadt zu einem bedeutenden internationalen Handelsplatz. Unter der Herrschaft vonElisabeth I.stieg ihre Bedeutung als Hafenstadt derNordsee. Durch den Beginn derIndustrialisierungim 18. Jahrhundert wuchs auch die Bevölkerung Londons, sodass es um 1800 eine der ersten Städte war, die mehr als eine Million Einwohner zählte. Bis 1900 versechsfachte sich die Bevölkerung, und London war bis 1925 diegrößte Stadt der Welt. Es entwickelte sich zu einem bedeutenden Zentrum derTechnikundIndustrieund wird bis heute zu denWeltstädtengerechnet. London ist eines der bedeutendsten Kultur- und Handelszentren der Welt mit zahlreichen Universitäten, Hochschulen, Theatern und Museen. Mit einemBIPvon 801,66 Mrd. € im Jahr 2017 ist London die größte städtische WirtschaftEuropas.[6]Die Stadt zählt außerdem nebenNew York City,SingapurundHongkongzu den größtenFinanzplätzender Welt.[7]Historische Gebäude wie derPalace of WestminsteroderTower of Londonzählen zumUNESCO-Weltkulturerbe. Mit jährlich über 19 Millionen Touristen aus dem Ausland war London 2016 nachBangkokweltweit die zweitmeistbesuchte Stadt vorParis.[8][9] Die geografischen Koordinaten des Stadtzentrums in der Nähe desTrafalgar Squaresind 51° 30′ nördlicher Breite und 0° 8′ westlicher Länge. Die Lage nahe demNullmeridianist kein Zufall, denn dieser wurde durch das königlicheObservatorium, dasRoyal Greenwich Observatoryin Greenwich gelegt; er ist Ausgangspunkt derLängengradeund damit derZeitzonen. London erstreckt sich etwa 44,3 Kilometer entlang der schiffbarenThemseund liegt durchschnittlich 15 Meter über demMeeresspiegel. London entstand aus einer Siedlung am Nordufer, der heutigenCity of London. DieLondon Bridgewar bis 1739 die einzige Brücke über den Fluss. Aus diesem Grund befindet sich der größere Teil der Stadt nördlich des Flusses. Mit dem Bau weitererBrückenim 18. Jahrhundert und dem Bau der Eisenbahnen im 19. Jahrhundert begann sich die Stadt in allen Richtungen auszudehnen. Die Landschaft ist flach bis leicht wellig, wodurch das ungehinderte Wachstum begünstigt wurde. Die Themse war in früheren Zeiten wesentlich breiter und seichter als heute. Sie wird heute fast gänzlich durch Dämme begrenzt und die meisten der rund 15 Zuflüsse fließen unterirdisch.[10]DieGezeitenderNordseemachen sich in London noch deutlich bemerkbar, die Stadt ist deshalb durch Überschwemmungen undSturmflutengefährdet. BeiWoolwich– östlich vonGreenwichgelegen – wurde in den 1970er-Jahren dieThames Barriergebaut, um diese Gefahr einzudämmen. Südostengland mit der Hauptstadt London, der klimatisch meistbegünstigte TeilGroßbritanniens, unterscheidet sich in vielerlei Hinsicht von den anderen Teilen der Insel. Die geologische Struktur wird durch diemesozoischenSedimentebestimmt, die eine großzügig gegliederte Schichtstufenlandschaft entstehen ließen. Ihre Höhen ragen nirgends hoch auf, so dass der gesamte Raum die klimatischen Vorteile des Südostens genießt. Historisch profitierte London von seiner Lage inmitten einer Ackerbauregion. Der kontinentnahe Südosten galt von jeher als Schwergewicht des Inselreiches. Hier fassten die vom Festland kommenden Eroberer –Römer,Sachsen,Normannen– zuerst Fuß. Auch als mit der Entdeckung Amerikas und der Entwicklung der Überseeschifffahrt die Außenseiten der Insel infolge ihrer günstigeren Lage stärker belebt wurden, konnte sich das alte Kulturzentrum behaupten. London blieb das Tor zur Insel. London gliedert sich in 32 Stadtbezirke (London Boroughs) und dieCity of London. London befindet sich in der gemäßigtenKlimazone. Die Sommer sind warm, aber selten heiß; die Winter sind zwar kühl, doch sinkt die Temperatur selten unter den Gefrierpunkt. Der wärmste Monat ist Juli mit 18,6 Grad Celsius im Durchschnitt, der kälteste Januar mit 5,6 Grad Celsius im Mittel. Die höchste jemals in London gemessene Temperatur war jedoch 40,3 Grad Celsius, gemessen 2022. Die große überbaute Fläche hält die Wärme zurück und schafft dadurch einMikroklima. Manchmal ist es in der Stadt bis zu fünf Grad wärmer als in der umliegenden Landschaft. Die durchschnittliche Jahrestemperatur beträgt 11 Grad Celsius und die mittlere jährliche Niederschlagsmenge 557,5 Millimeter. In den Monaten Oktober, November und Dezember gibt es den meisten Niederschlag mit durchschnittlich 56 Millimeter und der wenigste im Februar bzw. Juli mit 36,3 bzw. 34,6 Millimeter im Durchschnitt. Schnee fällt eher selten, höchstens einige Zentimeter pro Jahr. Ereignisse wie die Schneekatastrophe von 1978 sind eine Seltenheit. Anfang Februar 2009 gab es das schlimmste Schneechaos seit 18 Jahren, als über 15 Zentimeter Neuschnee fiel. Keine Seltenheit sind dagegenInversionswetterlagen. Eine davon führte im Dezember 1952 zu einer großenSmog-Katastrophe. Modellrechnungen aus dem Jahr 2019 zu denFolgen des vom Menschen verursachten Klimawandelsergeben, dass London bereits bei Eintritt des als optimistisch eingeschätztenRCP4.5-Szenariosin eine andere Klimazone verlagert werden würde; demnach wäre das Klima in London bereits im Jahr 2050 dem bisherigen Klima im deutlich südlicher gelegenenBarcelonaähnlicher als dem bisherigen in London.[11] Die Existenz einer vorrömischen Siedlung derKeltenim Bereich derCity of Londonkonnte nicht nachgewiesen werden. Wahrscheinlich im Jahr 47 n. Chr. gründeten dieRömerdie StadtLondinium. Im Jahr 60 oder 61 n. Chr. zerstörten dieIcener, angeführt von KöniginBoudicca, die Siedlung. Londinium wurde wieder aufgebaut und löste zu Beginn des 2. JahrhundertsCamulodunum(Colchester) als HauptstadtBritanniensab. Ab 197 n. Chr. war Londinium Hauptstadt der ProvinzBritannia superior, ab etwa 300 n. Chr. der ProvinzMaxima Caesariensis. Rund um die Stadt wurden Wallanlagen errichtet. Im Jahr 410 n. Chr. zogen die Römer ihre Legionen zurück und die Bevölkerung war den Raubzügen germanischer Stämme zunehmend schutzlos ausgeliefert. Nach der Eroberung Englands durch dieAngelnundSachsenverfiel Londinium bis Ende des 5. Jahrhunderts zu einer unbewohnten Ansammlung von Ruinen. DieAngelsachsenmieden zunächst die unmittelbare Umgebung der zerstörten Stadt. Im späteren 7. Jahrhundert gründeten sie westlich davon die Siedlung Lundenwic, die zunächst zum KönigreichMercia, später zumKönigreich Essexgehörte. Unter der Führung vonAlfred dem Großen, dem König vonWessex, eroberten die Angelsachsen im Jahr 878 die Gegend an der Themsemündung von denDänenzurück. In den folgenden Jahren wurde das Gebiet innerhalb der römischen Stadtmauer wieder besiedelt. Die neu entstandene Stadt hieß Lundenburgh. Im Jahr 1066erobertendieNormannenEngland und London lösteWinchesterals Hauptstadt ab. Der neue HerrscherWilhelm I.bestätigte die besonderen Rechte Londons.Richard Löwenherzernannte 1189 den erstenLord Mayor(Bürgermeister), der dann ab 1215 von den immer mächtiger werdenden Kaufmannsgilden selbst gewählt wurde. 1209 wurde die erste aus Stein errichtete Brücke, dieLondon Bridge, fertiggestellt, die bis 1750 die einzige Brücke im heutigen Stadtzentrum war. Mehrere Male musste London Plünderungen durch aufständische Bauernheere erdulden, so zum Beispiel im Jahr 1381 während desBauernaufstands von 1381und 1450 während derJack-Cade-Rebellion. ImRosenkrieg, der 1485 mit der Krönung von Henry Tudor alsHeinrich VII.zu Ende ging, hielt die Stadt zur Partei derYorks. DieReformationbrach die Macht der Kirche, die bis dahin rund die Hälfte des Bodens besaß; die Neuverteilung kirchlicher Güter ab 1535 leitete eine Ära des wirtschaftlichen Wachstums ein und London stieg zu einer führenden Handelsstadt auf. London musste in seiner wechselvollen Geschichte einige Rückschläge hinnehmen: Nachdem im 16. Jahrhundert die Gründung der ersten großen Handelskompanien und derRoyal Exchangeden wirtschaftlichen Aufstieg vorangetrieben hatte, wurde die Stadt in den Jahren 1664 und 1665 von der „Großen Pest“ heimgesucht, die über 70.000 Menschenleben forderte. Im September 1666 verwüstete der „Große Brand von London“ weite Teile der Stadt. Etwa 13.000 Häuser und 89 Kirchen fielen, insbesondere weil damals noch viel Holz zum Bau verwendet wurde,[14]dem Großbrand zum Opfer.[15] Die Stadt wurde nach dem verheerenden Brand neu aufgebaut. Pläne für eine grundlegende Neugestaltung scheiterten jedoch an den zu hohen Kosten, weshalb die neuen Häuser im Wesentlichen entlang der alten verwinkelten Straßen errichtet wurden. Verantwortlich für den Wiederaufbau war der ArchitektChristopher Wren. In der Folge zogen fast alle adeligen Bewohner endgültig aus der alten Innenstadt weg und ließen sich im aufstrebendenWest Endneue repräsentative Wohnhäuser bauen. InsEast Endabgedrängt wurden die ärmsten Bevölkerungsschichten, die im expandierenden Hafen ihr Auskommen finden mussten. Ende des 17. Jahrhunderts stieg London zum bedeutendsten Finanzzentrum der Welt auf. Während des 18. Jahrhunderts wuchs London über die historischen Grenzen hinaus. Neue Brücken über die Themse ermöglichten die Ausbreitung der Stadt nach Süden. Im Juni 1780 war London Schauplatz derGordon Riots, als sich fanatische Protestanten gegen die Gleichberechtigung der Katholiken zur Wehr setzten. Im Laufe des 19. Jahrhunderts vervielfachte sich die Bevölkerungszahl, der Bau zahlreicher Vorort-Eisenbahnen und U-Bahnen ermöglichte eine rasche Ausbreitung des überbauten Gebiets. London errang während desviktorianischen Zeitaltersgroße Bedeutung als Hauptstadt desBritischen Weltreichs. 1851 war London laut Volkszählung mit 2.651.939 Einwohnern die größte Stadt Europas und das Zentrum der industrialisierten Welt. Hier fand im selben Jahr mit der „Great Exhibition“ die ersteWeltausstellungstatt. Der ausufernde Ballungsraum war in zahlreiche Kirchgemeinden und Gerichtsbezirke zersplittert. Als ersterZweckverbandwurde 1829 dieMetropolitan Policegegründet, die in der Folge in der ganzen Metropole die zuvor auf privater Basis betriebene Verbrechensbekämpfung übernahm. 1855 folgte mit demMetropolitan Board of Workseine Vereinheitlichung im Bereich des Bauwesens. Das unter der Leitung vonJoseph BazalgetteerrichteteLondoner Abwassersystemgilt als das größte Bauprojekt des gesamten 19. Jahrhunderts. Im Jahr 1889 wurde mit derCounty of Londonerstmals überhaupt eine einheitliche Verwaltungsregion für den gesamten Ballungsraum geschaffen. Die erste Hälfte des 20. Jahrhunderts war geprägt von der Ausdehnung des überbauten Gebiets in einem vorher nie gekannten Ausmaß. Die neuen Vororte lagen fast gänzlich außerhalb derCounty of London: in ganzMiddlesex, im Westen vonEssex, im Norden vonSurrey, im Nordwesten vonKentund im Süden vonHertfordshire. Während desZweiten Weltkriegs, vor allem 1940/41, erlitt London insbesondere in den östlichen Industriegebieten durch Angriffe der deutschenLuftwaffeschwere Zerstörungen. Diese Bombardements gingen mit dem Namen „The Blitz“ in die Geschichte der Stadt ein. Eine zweite Angriffswelle folgte 1944/45 im Rahmen desUnternehmens Steinbocksowie mit denV1- undV2-Raketen. Knapp 30.000 Einwohner starben, Hunderttausende wurden obdachlos. Nach Kriegsende sank die Einwohnerzahl beträchtlich, da viele Londoner sich in neuenSatellitenstädtenniederließen. 1965 wurde die VerwaltungsregionGreater Londongeschaffen, die auch die im 20. Jahrhundert entstandenen Vororte umfasst. Währenddessen büßte London seine Rolle als bedeutender Hafen ein, die Anlagen in denDocklandszerfielen. Im Jahr 1981 begann ein umfangreiches Stadtentwicklungsprogramm, Zehntausende von Arbeitsplätzen der Dienstleistungsbranchen wurden von der City of London auf dieIsle of Dogsverlagert oder neu geschaffen. In derCanary Wharfentstand ein ausgedehnter Hochhauskomplex. Die Einwohnerzahl stieg seit dem Tiefpunkt in den 1980er Jahren wieder an. In den folgenden Jahren festigte London seine Position als eine für die globale Finanzindustrie bedeutendsten Städte.[16] BeiislamistischenTerroranschlägen am7. Juli 2005wurden mehrere Dutzend Menschen getötet. Infolgedessen wurden die städtischen Sicherheitsvorkehrungen ausgeweitet.[17]2011 stieg die Bevölkerung auf über 8 Millionen an, sodass ein neuer Höchststand erreicht wurde.[18]Im Jahr 2012 fanden dieOlympischen Spiele in Londonstatt. Im Jahr 2020 war London die Stadt mit den weltweit drittmeisten Überwachungskameras pro Kopf.[19] DieVolkszählung2011 ergab folgende Religionsverteilung:[20] 20,7 Prozent der Londoner gehören keiner Religion an. Keine Angaben machten 8,6 Prozent der Bevölkerung. Die Mehrheit der Christen gehört deranglikanischenKirche von Englandan. Hauptkirche und Sitz des Bischofs derDiözese Londonist dieSt Paul’s Cathedral. Kirche des Königshauses istWestminster Abbey. DiekatholischeHauptkirche von Wales und England und Sitz desErzbischofs von Westminsterist dieWestminster Cathedral. Eine weitere katholische Metropolitankirche ist die Kathedrale desErzbistums Southwark,St George’s Cathedral, auf der Südseite der Themse. Seit das englische Königshaus den protestantischen Glauben (Anglikanische Kirche) angenommen hatte, gab es mehrere Jahrhunderte lang keine katholischen Gotteshäuser in London. Erst im 19. Jahrhundert etablierten sich wieder katholische Gemeinden. Weitere christliche Religionsgemeinschaften sind dieUnited Reformed Church, dieHeilsarmee, dieQuäkerund dieOrthodoxe Kirche. In London befindet sich das Hauptquartier derMissionsärztlichen Schwestern(engl.: Medical Mission Sisters (MMS)).[21] Die Stadt ist das Zentrum desIslamin Großbritannien. Etwa 38 Prozent der 2,7 Millionen britischen Muslime lebten laut Volkszählung 2011 in London. Siedlungszentren sind überwiegend die StadtbezirkeTower Hamlets,NewhamundRedbridge.[22]DieBait ul-Futuhist die größteMoscheeder Hauptstadt. DieEast London Mosquewurde 1985 erbaut. Die angebliche langjährige politische Tolerierungfundamentalistischer Strömungenundislamistischer Terrorplanungenhat der Stadt zeitweilig den Ruf eines „Londonistan“ eingetragen.[23] Von den 817.000 britischen Hindus lebte 2011 circa die Hälfte in London. Siedlungszentren sind vor allem die BezirkeBrentundHarrow.[24]DerNeasden Templewar bis zur Eröffnung des Shri Venkateswara (Balaji) Temple inTividale(West Midlands) im August 2006 der größte Hindu-Tempel außerhalb Indiens.[25] Eine größere Anzahl vonSikhslebt im StadtteilSouthall, gelegen im westlichen StadtbezirkEaling, sowie im StadtteilHounslow. Etwa 56 Prozent der 267.000 britischen Juden lebten 2001 in der Hauptstadt. Siedlungszentren sindStamford Hillim BezirkHackneyund Golders Green im BezirkBarnet.[22] Schon 140 n. Chr. lebten in London 30.000 Menschen, um 1300 waren es bereits 100.000 und 1801 überschritt die Einwohnerzahl der Stadt die Grenze von einer Million. London war von 1825 bis 1925 die bevölkerungsreichste Stadt der Welt, bis sie vonNew Yorküberholt wurde. Bei der Volkszählung im Jahre 2001 wurden 7.172.091 Einwohner gezählt,[27]2011 dann 8.173.900. Für die Stadt werden aufgrund des anhaltenden Wachstums bis 2020 9.134.000 und bis 2040 10.487.000 Einwohner prognostiziert.[28] London ist traditionellerweise ein Anziehungspunkt für verschiedene Nationalitäten, Kulturen und Religionen. Während zu Beginn des 20. Jahrhunderts hauptsächlichIren,Polen,Italienerund osteuropäischeJudennach London kamen, sind seit Mitte des 20. Jahrhunderts vor allem Menschen aus den ehemaligenbritischen Kolonien, wie beispielsweiseIndien,Pakistan,Bangladesch,Nigeriaoder derKaribik, eingewandert. Bei der Volkszählung 2011 stammten gebürtig 6,6 Prozent der Bevölkerung vom indischenSubkontinent, 4,9 Prozent aus anderen TeilenAsiens. 7 Prozent stammten ausAfrikaund 4,2 Prozent aus derKaribik.[20]Insgesamt wurden 37 Prozent außerhalb des Vereinigten Königreichs geboren. Die Anzahl der Leute in London, welche sich als „weiße Briten“ (englischwhite british) bezeichneten, sank von 58 % im Jahr 2001 auf 45 % im Jahr 2011.[29]Die Anzahl der allgemein weißen Bevölkerung 2011 in London lag bei 60 %. Ungefähr 20 % hatten asiatische Wurzeln und 13 % waren schwarz. Einen gemischten ethnischen Hintergrund hatten 5 %, die restlichen 2 % einen anderen.[26] DieAgglomerationvon London erstreckt sich über das eigentliche Stadtgebiet vonGreater Londonhinaus und zählt 8.278.251 Einwohner (2001)[30], 2010 wurde die Bevölkerung derGreater London Urban Areaauf 8.979.158 geschätzt. Dies sind mehr als inSchottlandundWaleszusammen. London ist damit eine der größten Agglomerationen Europas. Die folgende Übersicht zeigt die Einwohnerzahlen nach dem jeweiligenGebietsstand. Bis 1750 handelt es sich um Schätzungen, von 1801 bis 2001 um Volkszählungsergebnisse und 2006 sowie 2019 um eine Berechnung. Die Wohn- und Gewerbegebiete aus dem 19. Jahrhundert weisen eine relativ hohe Wohndichte und einen überproportionalen Anteil von Einwanderern sowie Menschen mit niedrigem Einkommen auf. Gering verdichtete Wohnformen, vor allem von Eigentümern bewohnte Einzel- und Doppelhäuser, sind hier das dominierende Siedlungsbild. Der frühere Gegensatz in den Wohn- und Lebensbedingungen der Bevölkerung britischer Nationalitäten mit hohem Einkommen im Westend und der Einwanderer mit niederem Einkommen im East End wird von entgegengesetzten Entwicklungstendenzen überlagert. Die Immobilienpreise in Großbritannien haben sich in den Jahren 2000 bis 2011 in etwa verdoppelt, London hat sich aber nach oben abgekoppelt. Der Durchschnittspreis der Londoner Häuser liegt beim Doppelten des britischen Durchschnitts. In zentral gelegenen, aber ruhigen Wohnstraßen, besonders im Westen, inKensingtonundChelsealagen die Preise im Jahr 2011 im Durchschnitt bei fast 6 Millionen Euro und damit beim Dreißigfachen des britischen Durchschnitts. Im Premiumbereich werden 55 Prozent der Häuser von Ausländern erworben. Bis 2016 wurde eine weitere Steigerung um 20 Prozent prognostiziert.[31] Eine neue Entwicklung verbirgt sich hinter dem Begriff „poor doors“ (etwa: „Türen für Arme“): Da bei Luxus-Neubauten immer auch zugleich Sozialwohnungen entstehen müssen, planen Architekten für die sozial schwächeren Mieter einen eigenen Eingang und ein getrenntes Treppenhaus.[32] Zum Jahresende 2015 überstieg erstmals in der modernen Geschichte der Stadt die Zahl der vermieteten Wohnungen die des selbstgenutzten Eigentums. Die Entwicklung beschleunigt sich weiter, für 2025 werden über 60 % Mieter erwartet. Gründe dafür werden in dem stetig wachsenden Immobilienmarkt sowie in der enorm steigenden Einwohnerzahl der letzten Jahre und Jahrzehnte gesehen.[33] Im Jahre 1965 wurde die VerwaltungsregionGreater London, ein Zusammenschluss der altenCounty of LondonmitMiddlesexsowie Teilen der GrafschaftenEssex,Hertfordshire,KentundSurrey, gegründet. Greater London ist unterteilt in 32London Boroughsund dieCity of London. Die Boroughs sind für die lokale Selbstverwaltung und den Betrieb der meisten öffentlichen Einrichtungen auf ihrem Gebiet zuständig. Die City of London wird historisch bedingt von derCity of London Corporationverwaltet. DieGreater London Authority(GLA) koordiniert die Zusammenarbeit zwischen den einzelnen Boroughs, ist für die strategische Planung zuständig und betreibt öffentliche Einrichtungen, die in der ganzen Stadt tätig sind; dazu gehören dieLondon Fire Brigade, die Polizei und der öffentliche Verkehr. Die GLA besteht aus demMayor of London(Oberbürgermeister) und derLondon Assembly(Stadtparlament mit 25 Sitzen), die beide ihren Sitz im Crystal Bulding haben. Der aktuelle Mayor of London istSadiq Khan(Labour Party). Sein Vorgänger warBoris Johnson, dessen VorgängerKen Livingstone. Letzter trat im Jahre 2000 gegen den offiziellenLabour-Kandidaten an, wurde nach einem Nominierungsdebakel aus der Partei ausgeschlossen, 2004 unter Kritik wieder aufgenommen und haushoch für eine zweite Amtszeit bestätigt, ehe er letztlich bei der Wahl 2008 Johnson unterlag. DerLord Mayor of London, der Bürgermeister der City of London, übt lediglich zeremonielle Funktionen aus. Am 5. Mai 2016 wurde Sadiq Khan (Labour) zum neuen Mayor of London gewählt. Damit ist erstmals einMuslimoberster Repräsentant der britischen Hauptstadt. Frühere Verwaltungsbehörden waren dieMetropolitan Board of Works(MBW) von 1855 bis 1889, derLondon County Council(LCC) von 1889 bis 1965 und derGreater London Council(GLC) von 1965 bis 1986. Der GLC wurde von PremierministerinMargaret Thatchernach politischen Auseinandersetzungen zwischen der Regierung und dem GLC-VorsitzendenKen Livingstoneaufgelöst. 14 Jahre lang besaß London keine übergeordnete Verwaltung; die meisten Aufgaben wurden an die Boroughs übertragen, einzelne direkt an die Zentralregierung. Diese Maßnahme führte zu großen Koordinationsproblemen. Auch nach der Einsetzung der GLA im Jahr 2000 besitzen die Boroughs eine größereAutonomieals noch zu Zeiten der GLC. Die Polizeibehörde der 32 London Boroughs ist derMetropolitan Police Service, besser bekannt unter dem Namen Metropolitan Police oder kurz als „the Met“. Die City of London besitzt eine eigene Polizeibehörde, dieCity of London Police. London unterhält mit folgenden Städten Partnerschaften: London hat im Dezember 2009 eine Freundschaft mit folgender Stadt geschlossen: London bietet eine breite Palette an kulturellen Veranstaltungen. Im LondonerWestendsind mehr als ein Dutzend Theater zu Hause. Gespielt wird alles von der Klassik bis zur Moderne. Dort wurden unter anderemAndrew Lloyd WebbersMusicalsCatsundDas Phantom der Operuraufgeführt. DasRoyal National Theatreder National Theatre Company in South Bank und dasBarbican CentrederRoyal Shakespeare Companygehören zu den vielen Zentren des professionellen Theaterschauspiels. DasRoyal Court Theatre, eine der traditionsreichsten Bühnen in London, wurde im Februar 2000 nach vier Jahren Umbauzeit wiedereröffnet. DasRoyal Opera HouseinCovent Gardenist das bedeutendste britische Opernhaus. Es ist die Heimat der Royal Opera und desRoyal Ballet. Das erste Theatergebäude an dieser Stelle, das damaligeTheatre Royal(siehePatent Theatre) wurde von Edward Shepherd entworfen. Es wurde am 7. Dezember 1732 mit einer Aufführung vonWilliam CongrevesThe way of the worlderöffnet. Obwohl schon ab 1735 auch Opern, zum Beispiel vonHändel, aufgeführt wurden, blieb das Haus doch hauptsächlich ein Schauspielhaus. DasTheatre Royal Drury Laneist ein Theater im Londoner West End. Seit Mitte der 1980er-Jahre war es die Heimat großer Musicalproduktionen wie42nd Street,Miss SaigonundMy Fair Lady. DasLondon Palladiumist das wohl berühmteste Londoner Theater. In den 1950er-Jahren wurde die in Großbritannien populäre VarietéaufführungSunday Night at the London Palladiumlive im Fernsehen ausgestrahlt. DasTheatre Royal Haymarket(Haymarket Theatre)ist ein Theater am Londoner Haymarket. Es wurde 1720 von John Potter alsLittle Theatregegründet – in Anspielung auf das größereKing’s Theatre(heuteHer Majesty’s Theatre), das sich ebenfalls am Haymarket befand. Das Her Majesty’s Theatre wird hauptsächlich für Musicalaufführungen genutzt. Seit dem 9. Oktober 1986 wird täglichDas Phantom der Operaufgeführt. Das St. Martin’s Theatre im West End ist seit 1974 Spielort für das StückDie MausefallevonAgatha Christie. Zuvor wurde das Stück über einundzwanzig Jahre im Ambassador Theatre gespielt, bevor es nahtlos an seine jetzige Spielstätte umzog. Durch die ununterbrochene Laufzeit seit 1952 istDie Mausefalledas am längsten ununterbrochen aufgeführte Theaterstück der Welt. DasGlobe Theatream Südufer der Themse ist eine Rekonstruktion des Freiluftschauspielhauses, das 1599 entworfen wurde. Für dieses Theater schriebWilliam Shakespeareviele seiner größten Stücke. Die Spielzeit läuft von Mai bis September mit Produktionen von Shakespeare, seinen Zeitgenossen und von modernen Autoren. Ein weiteres bekanntes Theater ist dasLondon Coliseum, in dem dieEnglish National OperaCompany untergebracht ist. DasLondon Dungeonist kein Theater im herkömmlichen Sinne. Das Gruselkabinett befindet sich seit März 2013 in der Westminster Bridge Road und präsentiert seinen Besuchern bekannte Ereignisse der Stadtgeschichte aus den vergangenen 2000 Jahren. Schauspieler führen durch die unterirdischen Gewölbe und lassen unter anderem dieGroße Pest von London, denGroßen Brand von London,Jack the RipperundSweeney Toddwieder zum Leben erwachen. London beheimatet fünf professionelle Symphonieorchester. Diese sind dasLondon Symphony Orchestra, dasLondon Philharmonic Orchestra, dasRoyal Philharmonic Orchestra, dasPhilharmonia Orchestraund dasBBC Symphony Orchestra. Der Höhepunkt eines jeden Jahres ist die von derBBCweltweit übertragene „Last Night of the Proms“ aus derRoyal Albert Hall. Konzerthäuser sind dieBarbican Hall, dieRoyal Festival Hallund dieSaint John’s Churchin Westminster. Einer der beliebtesten Konzertsäle ist dieWigmore Hallhinter der Oxford Street. Im Juni 2002 sind nach umfangreichen Renovierungsarbeiten Teile des 1988 im heutigen Finanzviertel entdeckten römischen Amphitheaters der Öffentlichkeit zugänglich gemacht worden. AmTrafalgar Squaresteht die KircheSt Martin-in-the-Fields. Sie wurde in den Jahren 1721 bis 1726 nach den Plänen des ArchitektenJames Gibbsgebaut. In der Kirche finden häufig Konzerte statt; zu den dort auftretenden Orchestern zählen unter anderem dieAcademy of St Martin in the Fieldsund das EnsembleNew Trinity Baroqueaus den USA. In derKryptawurde ein Café eingerichtet, in dem manchmalJazz-Gruppen auftreten. Die Pfarrei beherbergt auch einen der berühmtesten Kirchenchöre der Welt. In der City of Westminster befinden sich dieAbbey Road Studios. Das Gebäude in der gleichnamigen Straße wurde 1929 vonEMIgekauft, die Studios am 12. November 1931 eröffnet. In der Eröffnungszeremonie dirigierte SirEdward ElgardasLondon Symphony Orchestrain Studio 1 und die historische Aufzeichnung vonLand of Hope and Gloryentstand. DieBeatleswidmeten dem Musikaufnahmestudio das Album „Abbey Road“ (1969). Pink Floyd, die in den 1970er-Jahren ihre Alben in den Studios einspielten, galt bald als die „Hausband“ des Studios. Unter anderem entstand hier „The Dark Side of the Moon“. Seit den 1980er-Jahren wird das Studio 1 auch als Aufnahmestudio für orchestrale Filmmusiken benutzt. Der erste Film, der hier seine musikalische Untermalung erhielt, warJäger des verlorenen Schatzesmit der Musik vonJohn Williams. Auch die Musik fürDer Herr der Ringeund dieHarry-Potter-Filmewurden hier eingespielt. The O₂ist ein Unterhaltungskomplex, welcher früher unter dem Namen Millennium Dome bekannt war. Zahlreiche bekannte internationale Künstler hatten in der O2 Arena, der eigentlichen Konzerthalle, Auftritte, so etwaBritney Spears,Justin Timberlakeund dieSpice Girls. Zu den größten und bekanntesten Museen weltweit zählt dasBritische Museumin Bloomsbury. In ihm befinden sich über sechs Millionen Ausstellungsstücke. Berühmt ist auch der Reading Room, ein kreisrunder Lesesaal, in dem schonKarl MarxundMahatma Gandhistudierten. Rechtzeitig zum Millennium wurde derQueen Elizabeth II Great Court(Architekt:Norman Foster) fertiggestellt. Es ist der größte überdachte Innenhof Europas. DasVictoria and Albert Museumim Stadtteil South Kensington verfügt über eine Sammlung von Kunstschätzen aus aller Welt, darunter Skulpturen, Kleidung und Kostüme, kostbare Porzellan- und Glasgefäße, Möbelstücke und Musikinstrumente. Unweit davon befinden sich dasScience Museum(Wissenschaftsmuseum)und dasNatural History Museum(Naturhistorisches Museum). ImScience Museumwerden in den auf fünf Ebenen angeordneten Galerien Ausstellungen aus den Bereichen Astronomie, Meteorologie, Biochemie, Elektronik, Navigation, Luftfahrt und Fotografie gezeigt. Zu den Klassikern unter den Ausstellungsstücken zählen Teleskope vonGalileo Galileiund ein Mikroskop vonGeorge Adams, die erste DampflokomotivePuffing Billy, das erste Telefon vonAlexander Graham Bell, einRolls-Royceaus dem Jahre 1909, ein Flugapparat vonOtto Lilienthalsowie die Kommandokapsel desRaumschiffsApollo 10. DasNatural History Museumbeinhaltet etwa 40 Millionen verschiedene Objekte aus derFloraundFauna, darunter zahlreicheDinosaurierskelette,Fossilien(unter ihnen einArchaeopteryx), ein 30 Meter langes Skelett einesBlauwalsoder das Modell des um 1690 ausgestorbenenDodo-Vogels. DieNational Galleryam Trafalgar Square besitzt eine reichhaltige Gemäldesammlung, die von den frühen Anfängen in Italien bis hin zu Werken von Cézanne und Seurat reicht. Nebenan befindet sich dieNational Portrait Gallery, in der über 9000 Porträts ausgestellt sind. Im Jahr 1897 wurde dieTate Galleryauf der Uferstraße zwischen Chelsea und Westminster eröffnet. Sie umfasst die größte Sammlung britischer Gemälde vom 16. Jahrhundert bis in die Gegenwart. Gegenüber der St Paul’s Cathedral wurde im Juni 2000 dieTate Modern, ein Ableger der Tate Gallery, eröffnet, die von den beidenBaslerArchitektenHerzog & de Meuronerrichtet wurde. Moderne Kunst zeigt die Saatchi Gallery nahe dem Sloane Square. Sie wurde 1985 von Charles Saatchi eröffnet. DasImperial War Museum(Reichskriegsmuseum)ist eines der bedeutendsten Kriegsmuseen weltweit. Es zeigt in erster Linie Exponate aus den beiden Weltkriegen, wie Kanonen und Fahrzeuge. Eine von vier Etagen widmet sich ausführlich demDritten Reich. Kleinere Abteilungen gelten einigen anderen Kriegen des 20. Jahrhunderts wie beispielsweise demVietnamkriegund demFalklandkrieg. Zusätzlich gibt es Wechselausstellungen. Madame TussaudsWachsfigurenkabinettist eine der größten Attraktionen der Hauptstadt. Ausgestellt werden lebensnah nachempfundene Wachsfiguren von historischen Gestalten und Personen der aktuellenZeitgeschichte, wie Sportler, Filmstars, Modeschöpfer und Models. Einen Platz in der Ausstellung von Madame Tussauds zu erhalten, zählt heute zu den größten Ehren, die einem Menschen zuteilwerden kann. Die Gründerin des 1835 eröffneten Museums war Marie Tussaud (1761–1850). Direkt in der City liegt dasMuseum of London, dessen Ausstellungen die Entwicklung Londons von seinen Anfängen bis zum heutigen Tag zeigt. Weitere bekannte Museen und Ausstellungen sind dieCabinet War Rooms, dasLondon Transport Museum,Somerset Houseund dasSherlock Holmes Museumin der221B Baker Street. Seit 2001 ist der Eintritt in sämtliche staatlichen Museen und Galerien kostenlos. Davon ausgenommen ist Madame Tussauds, da es sich um eine private Ausstellung handelt. Auch in den Cabinet War Rooms wird Eintritt verlangt. Den CWR ist das Churchill-Museum angeschlossen, für dessen Besuch kein separater Eintritt verlangt wird. DerTrafalgar Squareist ein großer Platz im Zentrum der britischen Hauptstadt, als deren eigentliches Zentrum er vielen gilt. Er ist der größte Platz Londons und seit dem Mittelalter ein zentraler Treffpunkt. 2003 wurde er nach einem größeren Umbau wiedereröffnet. In der Mitte des Platzes steht ein Denkmal, das die Londoner als Dank für AdmiralNelsonsSieg der Briten über die Franzosen in derSchlacht von Trafalgarsetzten. Die 1842 erbaute Nelson Column (deutsch: Nelsonsäule) mit dem Admiral auf der Spitze ist mit 55 Metern so hoch wie Nelsons FlaggschiffVictoryvom Kiel bis zur Mastspitze. Etwa zwei Drittel der Strecke von Trafalgar Square bisParliament SquareheißtWhitehall, das restliche Drittel heißtParliament Street. DasKenotaph, das wichtigste Kriegsdenkmal in Großbritannien, befindet sich in der Mitte der Straße und ist der Ort der jährlichen Gedenkfeiern amRemembrance Day. Der zentrale Teil der Straße wird von militärischen Gebäuden beherrscht, darunter das britische Verteidigungsministerium (englisch:Ministry of Defence) und die früheren Hauptquartiere derBritish Army(heuteHorse Guards) und derRoyal Navy(Admiralty). DieDowning Streetist die berühmte Straße im Stadtzentrum, auf der sich seit mehr als zweihundert Jahren die offiziellen Amts- und Wohnsitze von zwei der wichtigsten britischen Regierungsmitglieder befinden – des Premierministers des Vereinigten Königreichs und des Schatzkanzlers. Die berühmteste Hausnummer in der Downing Street ist die Nr. 10. Hier befindet sich der offizielle Amts- und Wohnsitz des ersten Lords des Schatzamtes und somit des Premierministers, da beide Ämter von ein und derselben Person bekleidet werden. Die Downing Street ist eine Seitenstraße der Whitehall im Zentrum von London, nur wenige Schritte vom Parlamentsgebäude entfernt und läuft in Richtung des Buckingham Palace. Die StraßePiccadillybefindet sich in der Innenstadt und gehört zu den bekanntesten Straßen der Stadt. Sie erstreckt sich vomPiccadilly Circusim Nordosten bis zumHyde Park Cornerim Südwesten. Sehenswert ist das vor allem auf Lebensmittel spezialisierte GeschäftFortnum & Masonaus dem Jahre 1707, dasHotel Ritzmit seinerneoklassizistischenArchitektur von 1906 und dieRoyal Academy of Artsaus dem Jahre 1868 imBurlington House. Der Piccadilly Circus ist vor allem durch seinen Eros-Brunnen und die riesige Leuchtreklamewand an einem gewundenen Eckhaus bekannt. Der Platz wurde 1819 erbaut, um die Regent Street mit der Einkaufsstraße Piccadilly zu verbinden. Aufgrund seiner zentralen Lage im Herzen des West Ends, seiner Nähe zu großen Einkaufs- und Vergnügungsmöglichkeiten sowie zu den großen Verkehrsadern, die sich hier kreuzen, ist er ein sehr stark besuchter Treffpunkt. Am nördlichen Ufer derThemsebefindet sich derTower von London, ein im Mittelalter errichteter Komplex aus mehreren befestigten Gebäuden entlang des Flusses, der als Festung, Waffenkammer(stronghouse), königlicher Palast und Gefängnis, insbesondere für Gefangene der Oberklasse, diente. Außerdem waren dort die Münze, das Staatsarchiv, ein Waffenarsenal und ein Observatorium untergebracht. Bis zuJakob I.wohnten alle englischen Könige und Königinnen zeitweise dort. Es war üblich, dass der Monarch vor dem Tag seiner Krönung im Tower übernachtete und dann in feierlichem Zug durch die Stadt nach Westminster ritt. Heute werden im Tower diebritischen Kronjuwelenaufbewahrt, ferner eine reichhaltige Waffensammlung. 1078 ordneteWilhelm der Erobereran, denWhite Towerhier zu bauen. Er sollte dieNormannenvor den Menschen derCity of London, aber auch London überhaupt schützen. In den folgenden Jahrhunderten wurde die Festung ständig erweitert. Sie wird von einem breiten Wassergraben umgeben. Ein Außenwall schützt die inneren Gebäude. In der Mitte des Geländes steht der mächtige „Weiße Turm“. Von weitem wirkt er quadratisch, aber drei der Ecken bilden keine rechten Winkel und alle vier Seiten sind verschieden lang. DieUNESCOhat das Bauwerk 1988 zumWeltkulturerbe der Menschheiterklärt. DieTower Bridgeist eine Straßenbrücke über die Themse. Sie verbindet die City of London auf der Nordseite mit dem Stadtteil Southwark im gleichnamigen Stadtbezirk (London Borough of Southwark) auf der Südseite. Es handelt sich hierbei um eine imneugotischenStil errichteteKlappbrückeund um die am östlichsten gelegene Themsebrücke; darüber führt die Hauptstraße A100. Am Nordufer befinden sich der Tower of London (nach dem die Brücke benannt ist) und dieSt Katharine Docks, am Südufer dieCity Hall. Die Brücke ist im Besitz von Bridge House Estates, einer Wohlfahrtsorganisation derCity of London Corporation, die auch für den Unterhalt zuständig ist. Gelegentlich wird die Tower Bridge fälschlicherweiseLondon Bridgegenannt, diese jedoch ist die nächste Brücke stromaufwärts. Die Tower Bridge ist 244 Meter lang, die Höhe der beiden Brückentürme beträgt 65 Meter. Die Fahrbahn zwischen den 61 Meter voneinander entfernten Türmen liegt neun Meter über dem Fluss, die Fußgängerbrücke 43 Meter. Die beiden Baskülen können bis zu einem Winkel von 83 Grad hochgeklappt werden, um größeren Schiffen die Durchfahrt zu ermöglichen. Fertiggestellt wurde die Tower Bridge im Jahre 1894. Palace of Westminster Bekanntester Turm in London ist der 98 Meter hohe Elizabeth Tower, in dem sichBig Benbefindet, die mit 13 Tonnen schwerste der fünf Glocken, welche den bekanntenWestminsterschlagspielen. Der Uhrturm ist Teil desPalace of Westminster, einem monumentalen, imneugotischenStil errichteten Gebäude, in dem das aus demHouse of Commonsund demHouse of Lordsbestehendebritische Parlamenttagt. Der Palast befindet sich in derCity of WestminsteramParliament Square, in unmittelbarer Nähe zuWhitehall. Er wurde von der UNESCO 1987 zum Weltkulturerbe erklärt. Der älteste erhaltene Teil des Palastes ist dieWestminster Hallaus dem Jahr 1097. Ursprünglich diente er als Residenz der englischen Könige, doch seit 1529 hat kein Monarch mehr hier gelebt. Vom ursprünglichen Gebäude ist nur wenig erhalten geblieben, da es im Jahr 1834 bei einem verheerenden Großbrand fast vollständig zerstört wurde. Der für den Wiederaufbau verantwortliche Architekt warCharles Barry. Die wichtigsten Räume des Palastes sind die Ratssäle des House of Commons und des House of Lords. Daneben gibt es rund 1.100 weitere Räume, darunter Sitzungszimmer, Bibliotheken, Lobbys, Speisesäle, Bars und Sporthallen. Der BegriffWestminsterist im britischen Sprachgebrauch oft gleichbedeutend für den Parlamentsbetrieb, ist also einMetonymfür Parlament. DerBuckingham Palaceim Stadtbezirk City of Westminster ist die offizielle Residenz des britischen Monarchen in London. Neben seiner Funktion als Wohnung von KönigCharles III.dient er auch als Austragungsort für offizielle Anlässe des Staates. So werden in ihm ausländische Staatsoberhäupter bei ihrem Besuch in Großbritannien empfangen. Daneben ist er ein wichtiger Anziehungspunkt für Touristen. Die ursprüngliche georgianische Inneneinrichtung beinhaltete auf Vorschlag von Sir Charles Long die großzügige Verwendung von Marmormalerei („Scagliola“) in leuchtenden Farben sowie blaue und rosafarbeneLapislazuli. Unter KönigEduard VII.fand eine großangelegte Neuausstattung im Stil derBelle Époquestatt. Dabei wurde ein Farbschema aus einer Kombination von Cremetönen und Gold verwendet. Viele der kleineren Empfangsräume sind im chinesischen Regency-Architekturstil gehalten. Sie wurden mit Möbelstücken und Dekorationen ausgestattet, die nach dem Tod KönigGeorgs IV.aus demRoyal PavilioninBrightonsowie ausCarlton Househerbeigeschafft wurden. DerSt James’s Palacebefindet sich in der City of Westminster. Das Gebäude war bis 1837 die offizielle Londoner Residenz des jeweiligen britischen Monarchen. Er ist heute noch der offizielle Verwaltungssitz des königlichen Hofes. Hier werden die Botschafter Großbritanniensakkreditiert. Auch die Proklamation eines neuen Monarchen findet hier statt. Das Gebäude wurde in der Zeit von 1532 bis 1540 durchHeinrich VIII.errichtet. Heute wird der Palast vomPrince of Walesund anderen Verwandten des Königs bewohnt. Der frühere Sitz der britischenKöniginmutter,Clarence House, liegt innerhalb der Palastmauern. Das Anwesen wird nur durch denSt. James’s Parkvom Buckingham Palace getrennt. Ein interessantes Schauspiel ist dieWachablösungam Palast. In den Sommermonaten April bis Juli findet diese täglich, ansonsten alle zwei Tage statt. Hampton Court Palaceist ein Schloss im Stadtbezirk Richmond upon Thames, unmittelbar neben demBushy Park. Gebäude und Parkanlagen wurden unter den verschiedenen Bewohnern verändert und erweitert, sodass heute Architekturelemente desTudorstilsund des englischenBarockerhalten sind. Das Schloss war Wohnsitz zahlreicher britischer Könige und Königinnen. Seit der RegierungszeitGeorg III.bewohnen britische Monarchen andere Londoner Schlösser undKönigin Viktoriaöffnete 1838 den Palast für die Öffentlichkeit. Teilbereiche des Palastes wurden an verdiente Veteranen vermietet. 1986 brach in solch einer Wohnung ein Feuer aus, das den Palast teilweise zerstörte. Die Wiederaufbaumaßnahmen dauerten bis 1995 an. DerKensington Palaceliegt im StadtbezirkKensington and Chelsea. Das von SirChristopher Wrenumgestaltete Schloss war früher ein privater Landbesitz und wurde im Jahre 1689 vonMary II.undWilhelm III.ausgebaut, um im Winter nicht die Feuchtigkeit derWhitehallertragen zu müssen. In den nächsten 70 Jahren wurde der Palast immer wichtiger für das gesellschaftliche und politische Leben des Landes. Zu Lebenszeiten vonGeorge I.undGeorge II.wurde das Anwesen verschwenderisch mit Prunkgemächern ausgestattet und erhielt eine herausragende Möbel- und Gemäldesammlung. Besonders bekannt sind vor allem die aufwendigen Deckenverzierungen vonWilliam Kent. Nachdem George II. im Jahre 1760 plötzlich starb, verlor das Gebäude immer mehr an Bedeutung. Bis heute lebte nie wieder ein regierender Monarch hier. Allerdings werden Teile des Palastes von Mitgliedern der Königsfamilie bewohnt. Seit der Jahrtausendwende erlebt London im Bereich der Wolkenkratzer einen Bauboom, der sich unter anderem in dem 310 Meter hohenThe Shard, dem 278 Meter hohen22 Bishopsgateund rund dreißig weiteren Wolkenkratzern mit einer Höhe von mehr als 150 Metern zeigt.The Shardwar von Juli bis Oktober 2012 das höchste Gebäude Europas. Östlich des Stadtzentrums befinden sich beidseits der Themse dieDocklands, zu denen auchCanary Wharfmit demOne Canada Squaregehört. Mit einer Höhe von 236 Metern und 50 Stockwerken ist es nachThe Sharddas zweithöchste bewohnbare Gebäude in Großbritannien. (DerFernsehturm Emley Moor, das höchste freistehende Bauwerk Großbritanniens beiHuddersfield, ist 330 Meter hoch.) Das Gebäude wird flankiert von zwei weiteren Wolkenkratzern, die zehn Jahre später entstanden sind und beide 200 Meter hoch sind:HSBC Tower(8 Canada Square) undCitigroup Centre(25 Canada Square). Weitere Wolkenkratzer befinden sich im Zentrum Londons, darunter derTower 42und30 St Mary Axe. Am Südufer der Themse, nahe derWestminster Bridge, steht das RiesenradLondon Eye. Die Anlage, die mit einer Höhe von 135,36 Metern bis Anfang 2006 das höchste Riesenrad der Welt war, sollte bereits zum Jahreswechsel 2000 fertiggestellt werden. Aufgrund von Sicherheitsmängeln ist die Konstruktion aber erst einige Wochen später in Betrieb genommen worden. Das London Eye besitzt 32 fast vollständig aus Glas geformte Kapseln, in denen jeweils bis zu 25 Personen Platz finden. Das Rad dreht sich mit einer Geschwindigkeit von 0,26 m/s und braucht für eine Umdrehung 30 Minuten. Sind die Sichtverhältnisse optimal, kann man vom Riesenrad aus bis zu 40 Kilometer weit sehen, unter anderem bis zum etwas außerhalb der Stadt gelegenenSchloss Windsor. Drehachse und Stützen des Riesenrads wurden von der tschechischen MaschinenbaufirmaŠkodageliefert, die Nabe (Pendelrollenlager) vonFAG KugelfischerausSchweinfurt. Battersea Power Station DieBattersea Power Stationist ein ehemaligesKohlekraftwerkim StadtteilWandsworth, das von 1933 bis 1983 in Betrieb war. Das markante Gebäude mit vier Schornsteinen befindet sich am Südufer der Themse in der Nähe derGrosvenor Bridge. Die Battersea Power Station ist auf Musikalben zahlreicher britischer Pop- und Rockbands abgebildet. Am bekanntesten ist die Abbildung auf dem Cover des 1977 erschienenen AlbumsAnimalsvonPink Floyd, das dasElektrizitätswerkmit einem großen Plastikschwein zwischen den Kaminen schwebend zeigt. Weitere Beispiele sind das AlbumQuadropheniavonThe Who(1973),Adventures Beyond The UltraworldvonThe Orb,Live Frogs: Set 2vonLes Claypool’s Frog Brigade (eine Coverversion vonAnimals) undPower BalladsvonLondon Electricity. DieThames Barrierauf der Themse im StadtteilWoolwichist das größte bewegliche Flutschutzwehr der Welt. Die Planungen für das Bauwerk begannen nach einer schweren Sturmflut im Jahre 1953, bei der 307 Menschen ums Leben kamen. 1974 wurde mit dem Bau begonnen. Die Einweihung erfolgte am 8. Mai 1984 durch KöniginElisabeth II. Das Sperrwerk besteht aus zehn schwenkbaren Toren. Um den Schiffsverkehr nicht zu behindern, sind sie im offenen Zustand auf den Boden der Themse abgesenkt. Schiffe mit bis zu 16 MeternTiefgangkönnen dann problemlos das Sperrwerk passieren. Die vier mittleren Tore, durch die der Schiffsverkehr läuft, sind je 60 Meter breit, 10,5 Meter hoch und wiegen je 1500 Tonnen. Das gesamte Bauwerk hat eine Länge von 523 Metern. Droht eine Sturmflut, können die Tore innerhalb von 15 Minuten geschlossen werden. DieMillennium Bridgebietet seit dem 10. Juni 2000 eine direkte Verbindung zwischen derSt Paul’s Cathedralund derTate Gallery of Modern Art. In derCity of London, etwa 300 Meter nördlich der Themse, steht die vonChristopher WrenentworfeneSt Paul’s Cathedral, die Hauptkirche derAnglikanischen Kirchein London. Die Schnitzarbeiten des Chorgestühls stammen vonGrinling Gibbons, die schmiedeeisernen Chorschranken vonJean Tijou. Erst 1890 wurden die Glasmosaiken an der Decke über demChorvon William Richmond fertiggestellt. DerHochaltar, nach Plänen Wrens gebaut, ist das Werk von Dykes Bower und Godfrey Allan, die ihn 1958 vollendeten. Die Kathedrale hat eine kreuzförmige Grundfläche, die in Ost-West-Richtung ausgerichtet ist. In der Mitte dieses Kreuzes befindet sich eine Kuppel, auf der sich eine 750 Tonnen schwereLaternebefindet, die in 111 Meter Höhe endet. Um diese gewaltige Last abzuleiten, befindet sich zwischen der äußeren und der inneren Kuppel ein konischer Steinaufbau, der auf den massigen Vierungspfeilern ruht. An der Kuppelbasis in etwa 30 Meter Höhe befindet sich in der Kirche ein ringförmiger Umgang mit einem Durchmesser von 34 Meter, die sogenannteWhispering Gallery, dieFlüstergalerie. Der Schall wird hier durch die gebogenen Wände immer wieder zurück in das Innere des Rings reflektiert, sodass ein geflüstertes Wort auf die andere Seite der Kuppel getragen werden kann. Sie ist 365 Fuß hoch, einen Fuß für jeden Tag im Jahr. Steigt man bis zur Spitze hinauf, so gelangt man auf die Golden Gallery, mit der Möglichkeit einer Aussicht über London. Unter der Kirche befindet sich eine weitläufigeKrypta, in der zahlreiche bedeutende Persönlichkeiten der britischen Geschichte beigesetzt sind. DieSt Margaret’s Churchist eine anglikanische Kirche. Sie befindet sich im Stadtteil City of Westminster amParliament Square, unmittelbar neben derWestminster Abbeyund gegenüber demPalace of Westminster. Es ist die Pfarreikirche desbritischen Parlaments. Sehenswert ist das östliche Fenster mitflämischerGlasmalerei aus dem Jahr 1509, angefertigt in Erinnerung an die Verlobung vonArthur Tudor, dem älteren Bruder vonHeinrich VIII., mitKatharina von Aragon. Andere Glasfenster erinnern anWilliam Caxton, den ersten englischen Buchdrucker, SirWalter Raleigh, der hier 1618 begraben wurde, und an den PoetenJohn Milton, ein Mitglied der Kirchgemeinde. Zu den Personen, die in der Kirche ihre letzte Ruhestätte fanden, gehört der böhmische KupferstecherWenzel Hollar. Zahlreiche Berühmtheiten wurden in St Margaret’s getraut, darunterSamuel Pepysund seine Frau sowieWinston Churchillund Clementine Hozier. Die Kirche wurde 1987 von der UNESCO zum Weltkulturerbe erklärt. Westminster Abbeyist eine Kirche im Stadtteil City of Westminster. Traditionell werden hier dieKönige von Englandgekrönt und beigesetzt. DieStiftskirchedesKollegiatstiftsSt. Peter, Westminstergehört zurKirche von England, ist aber aufgrund ihrer Funktion keinerDiözesezugehörig, sondernEigenkirche(royal peculia) der britischen Monarchie. Der Haupteingang befindet sich an der Westseite. Das Portal wird von Darstellungen der vier christlichen Tugenden Wahrheit, Gerechtigkeit, Barmherzigkeit, Friede sowie vonMärtyrerndes 20. Jahrhunderts gerahmt. ImMittelschiffliegt das Grab desUnbekannten Soldaten. In Erde von den belgischen Schlachtfeldern ruht dort ein unbekannter Soldat des Ersten Weltkriegs „inmitten der Könige, weil er seinem Gott und Vaterland gut diente“, wie eine Inschrift auf schwarzem Marmor verkündet. Der Gefallenen beider Weltkriege wird auch in der St.-Georgs-Kapelle gedacht. Im linken (nördlichen) Querschiff sind zahlreiche berühmte britische Staatsmänner bestattet, unter anderemWilliam Pitt,Palmerston,Benjamin DisraeliundWilliam Gladstone. Vom nördlichen Teil des Querschiffs betritt man die hinter dem Hochaltar gelegene „KapelleEduards des Bekenners“. In der Mitte befindet sich der Sarg des 1066 gestorbenen Königs. Dahinter steht der Krönungsstuhl, in dem sich bis 1996 derStein von Sconebefand.Auf diesem Stein wurden jahrhundertelang die schottischen Könige gekrönt, bis ihnEduard I.im Jahr 1297 den Schotten abnahm. Zu Weihnachten 1950 wurde der Stein gestohlen und erst nach langem Suchen wiedergefunden. 1996 wurde er offiziell an Schottland zurückgegeben und befindet sich seitdem imSchloss von Edinburgh. Der Stein gilt als ein Symbol für die Einheit der Königreiche England und Schottland.In dieser Kapelle befinden sich außerdem die Särge vonHeinrich III., Eduard I.,Eduard III.,Richard II.undHeinrich V.Die UNESCO erklärte die Kirche 1987 zum Weltkulturerbe. Westminster Cathedral DieWestminster Cathedralist diekatholischeHauptkirche von Wales und England. Sie befindet sich in derCity of Westminster. ErzbischofNicholas Wiseman(1802–1865) begann mit den Spendensammlungen für die neue Kathedrale. Er war der erste römisch-katholischeKardinalund Erzbischof in England nach derReformation. Doch erst im Jahr 1895 konnte mit dem Bau begonnen werden. Eröffnet wurde die Kathedrale im Jahr 1903. Man entschied sich beim Bau für denbyzantinischen Stil. Von außen besticht das Gebäude durch die aufwendig gestaltete Backsteinfassade, die hohe Kuppel und nicht zuletzt durch den für diese Breiten völlig untypischen freistehendenGlockenturm. Im Inneren überrascht sie durch die Raumwirkung und vor allem durch die Mosaiken an Decken und Wänden, die ständig vervollständigt werden. In der Holy Souls Chapel im Seitenschiff wurden mehr als 100 verschiedene Marmorsorten verarbeitet. DerNeasden Temple(ShriSwaminarayanMandir) im StadtbezirkBrentist nach dem Tempel inTividale(West Midlands) der größteHindu-Tempel außerhalb Indiens. Errichtet wurde er in den 1990er-Jahren von einer hinduistischen Sekte, der Swaminarayan-Mission ausAhmedabad(Indien). Die Kuppeln und Türmchen bestehen ausCarrara-Marmorund bulgarischemKalkstein; innen sind die Altäre mit Blumenschmuck hinduistischer Götter (Murtis) ausgestattet. Jeder der 26.300 bearbeiteten Steine besitzt ein anderes Motiv. Innerhalb von drei Jahren wurde das Bauwerk zusammengefügt und am 20. August 1995 eröffnet. In der Konstruktion ist auf Eisenträger verzichtet worden, da Stahl nach hinduistischem Verständnis Magnetwellen abstrahle, die dieMeditationsruhestören. Der Tempel beherbergt die ständige Ausstellung „Understanding Hinduism“ (Begreifen des Hinduismus) und ein Kulturzentrum. DieAziziye-Moscheeim StadtteilStoke Newingtonwird hauptsächlich von der türkischen Gemeinde in Anspruch genommen. London besitzt eine große Anzahl von luxuriösen Grünanlagen. DieRoyal Parkswaren einst den englischen beziehungsweise britischen Monarchen vorbehalten und wurden zu Beginn des 19. Jahrhunderts in öffentlich zugängliche Parkanlagen umgewandelt. Über 200 Parkanlagen breiten sich auf rund 220 Quadratkilometern aus.[34] Greenwich Parkist einer dieser königlichen Parks in London. Er liegt im StadtbezirkGreenwichim Südosten von London. Im Jahr 1997 wurden der Greenwich Park und die dazugehörigen Gebäude von derUNESCOzumWeltkulturerbe der Menschheiterklärt. Am Nordrand des 73 Hektar großen Geländes befinden sich dasNational Maritime Museumund dasQueen’s House, ein ehemaliger königlicher Palast. Auf einem Hügel in der Mitte des Parks befindet sich dasRoyal Greenwich Observatory. Der kleine Platz vor dem Observatorium wird durch eine Statue von GeneralJames Wolfegeschmückt. DerHyde Parkmit derMarble Archund demSpeakers’ Corner, der an dieKensington Gardensangrenzt, ist lange Zeit als die „Lunge Londons“ bezeichnet worden. Von eleganten Wohngebäuden umgeben, die für den Prinzregenten entworfen wurden, ist derRegent’s Parkim Norden des West Ends. Dieser Park beinhaltet auch den zoologischen Garten (London Zoo). Mitten im Stadtzentrum befinden sich derGreen Parkund derSt. James’s Park. DieRoyal Botanic Gardens(Kew Gardens)sind eine ausgedehnte Parkanlage mit bedeutendenGewächshäusern. Sie sind zwischen Richmond upon Thames undKewim Südwesten Londons gelegen und zählen zu den ältesten botanischen Gärten der Welt. Es sind dort Pflanzen und Gewächse zu sehen, die nirgendwo sonst in Europa oder gar auf der nördlichen Halbkugel anzutreffen sind. Neben den weltbekannten viktorianischen Gewächshäusern finden sich in Kew Gardens auch großflächige Parkanlagen mit sehr altenRhododendrongewächsen. Am 3. Juli 2003 wurden die Royal Botanic Gardens von der UNESCO in die Liste des Weltkulturerbes aufgenommen. Richmond Parkist mit einer Fläche von zehn Quadratkilometern der größte der königlichen Parks. Er liegt in den Stadtbezirken Richmond upon Thames undKingston upon Thamesim Südwesten von London. Ursprünglich war Richmond Park das Hirschjagdgebiet von KönigEdward I., heute ist er der größte ummauerte Park Europas in einem städtischen Gebiet. Hauptattraktionen sind eine Herde mit 650WapitisundDamhirschen, die freien Auslauf haben, sowie dieIsabella Plantation, ein Gebiet mit zahlreichen seltenen Pflanzenarten. Im Januar 2001 ist der Thames Barrier Park fertiggestellt worden; die Anlage entstand bei den Stauwerken der Themse (Thames Barrier) auf alten Dockanlagen. In den äußeren Stadtbezirken von London befinden sich noch einige weitere ausgedehnte Grünflächen, wieBushy ParkundHampstead Heath. Londoner Fußballvereine gewannen 21-mal die Landesmeisterschaft und insgesamt 41-mal denFA Cup, den landesweiten Pokalwettbewerb; beide Male ist derFC Arsenalmit 13 Landesmeistertiteln bzw. 14 Pokalsiegen erfolgreichster Londoner Klub, gefolgt vomFC Chelseamit 6 Meistertiteln und 8 Pokalsiegen. Beide Vereine gewannen zudem mehrfach imEuropapokal. In London gibt es derzeit (Stand 2024/25) mindestens 17 professionelle Fußballclubs; die meisten sind nach dem Stadtteil benannt, in dem sie ihre Heimspiele austragen. In derPremier Leaguesind in derSaison 2024/25neben Arsenal und Chelsea derFC Brentford,Crystal Palace,Fulham,Tottenham HotspurundWest Ham Unitedvertreten. In derFootball League Championship, der zweithöchsten Spielklasse, spielenMillwallund dieQueens Park Rangers. Dazu gibt es noch die Football-League-KlubsCharlton Athletic,AFC Wimbledon,Leyton OrientundFC Bromleysowie in der fünftklassigenNational LeaguedenFC Barnet,Sutton United,Dagenham & Redbridgeund denFC Wealdstone. In London gibt es sechs professionelleRugby-Union-Vereine, davon spielen fünf in der höchsten Liga, derEnglish Premiership:Wasps,Saracens,Harlequins,London Irishund die 2014 aufgestiegenenLondon Welsh. In London befindet sich mit demTwickenham Stadiumdas größte reine Rugbystadion weltweit. Hier bestreitet dieenglische Rugby-Union-Nationalmannschafttraditionell ihre Heimspiele, so beim jährlichenSix Nations. Das Stadion war auch Austragungsort des Finales bei derWeltmeisterschaftzwischenAustralienundNeuseeland, das die „All Blacks“ mit 34:17 gewannen. Nahe Twickenham befindet sichThe Stoop, Austragungsort des Finales derFrauen-Weltmeisterschaft 2010zwischenEnglandundNeuseeland, das die „Black Ferns“ mit 13:10 gewannen. DieLondon Scottishspielen in der 2. Liga, derChamp Rugby. DerRugby-League-VereinLondon Broncosspielt in derSuper League. InWembley, einem Teil des StadtbezirksBrent, befand sich das legendäreWembley-Stadion. Dort fanden die Endspiele derFußball-Weltmeisterschaft 1966, derFußball-Europameisterschaft 1996und derFußball-Europameisterschaft 2021statt. Es wurde mit der offiziellen Eröffnung im Jahr 2007 durch einen Neubau ersetzt. Das Stadion ist jährlich Austragungsort des Finales imFA Cup, dem größten rundenbasierten Pokalwettbewerb im englischen Fußball. DieRugby Leagueveranstaltet seit 1929 ihrChallenge CupFinale im Stadion. Außer für besondere Ereignisse war Wembley auch für regelmäßige Veranstaltungen Austragungsort, wie zum BeispielWindhundrennenoderMotorradrennen. Auch die Wrestling-Liga WWF (heuteWWE) veranstaltete 1992 den Summerslam-Event im Wembley-Stadion. Eine Attraktion ist dasBoat Racezwischen den beiden renommiertesten englischen Universitäten,OxfordundCambridge. Das berühmte Ruderrennen ihrer beidenAchterfindet jährlich im März oder April auf derThemsestatt. Sehr beliebt in London istCricket. DerMiddlesex County Cricket Clubspielt inLord’s, dem berühmtesten Cricketstadion der Welt, welches demMarylebone Cricket Clubgehört. DerSurrey County Cricket Clubim StadionThe Oval. Lord’s war bereits Austragungsort von fünf Endspielen bei Cricket World Cups (1975,1979,1983,1999und2019), mehr als jedes andere Cricketstadion. Am 14. Juli 2019 gewannEnglanderstmals den Cricket World Cup nach Anzahl derBoundaries, nachdem das eigentliche Spiel und das nötigeSuper OvergegenNeuseelandunentschieden endeten. InWimbledonfindet jeweils im Juni das wichtigste derGrand-Slam-Tennisturniere statt. Im April wird jeweils derLondon-Marathondurchgeführt, einer der beliebtestenMarathonläufeder Welt überhaupt. Der Start derTour de France 2007fand im Juli in London statt. Mit der Vergabe derOlympischen Spiele 2012an die britische Hauptstadt war London die erste Stadt, welche zum dritten Mal – nach1908und1948– die Spiele ausrichtete. Ebenso war London zweimal Gastgeber derCommonwealth Games: 1911 (inoffiziell alsInter-Empire Championships) und1934(British Empire Games). Am 1. Januar findet die Neujahrsparade vom Parliament Square bis zum Berkeley Square statt. Daschinesische NeujahrsfestinChinatownim StadtteilSohofindet am zweitenNeumondnach derWintersonnenwende, also zwischen dem 21. Januar und 21. Februar statt. Da der chinesische Kalender im Gegensatz zum gregorianischen Kalender astronomisch definiert ist, fällt das chinesische Neujahr jedes Jahr auf einen anderen Tag. Mit einer Kranzniederlegung vor dem Banqueting House und einer Prozession abSt James’s Palacewird Ende Januar an die Hinrichtung KönigKarls I.am 30. Januar 1649 erinnert (Commemoration of King Charles I.). Die Wachablösung (Changing of the Guard) der Queen’s Guard amBuckingham Palacegehört zu den ältesten und bekanntesten Zeremonien und findet an fast allen Tagen des Jahres statt. Die Ablösung wird durch Militärkapellen begleitet, die traditionelle Märsche, Stücke beliebter Theatershows des West Ends und bekannte Popsongs spielen. Bei der angeblich 700 Jahre altenSchlüsselzeremonie(Ceremony of the Keys) werden die Haupttore desTower of Londonjeden Abend vom Hauptwärter des Towers (Chief Yeoman Warder), eskortiert von Gardisten, verschlossen. Salutschüsse werden am 6. Februar (Tag der Thronbesteigung), am 21. April (Geburtstag der Königin), am 2. Juni (Tag der Krönung) und am 10. Juni (Geburtstag des Duke of Edinburgh) abgefeuert. Fallen die Termine auf einen Sonntag, werden die Salutschüsse am folgenden Tag abgefeuert. Um 12 Uhr werden 41 Schüsse von der King’s Troop der Royal Horse Artillery imHyde Parkabgegeben und um 13 Uhr feuert die Ehrenartilleriekompanie (Honourable Artillery Company) 62 Schüsse beim Tower of London ab. Salutschüsse werden auch bei der Fahnenparade und der Parlamentseröffnung abgegeben. DieShakespeare’s Birthday Celebrationsfinden anlässlich von Shakespeares Geburtstag am 23. April jedes Jahr an dem Sonnabend, der diesem Tag am nächsten liegt, im Shakespeare’s Globe Theatre statt. Ein klassisches Musikfestival ist dasHampton Court Palace Music FestivalAnfang bis Mitte Juni imHampton Court Palace. DasCity of London Festivalwird mit Musik, Theater und Tanz von Ende Juni bis Mitte Juli an verschiedenen Orten veranstaltet. Promenadenkonzerte(The BBC Proms)gibt es von Juli bis September in derRoyal Albert Hall. DerNotting Hill Carnival, Europas größter Straßenkarneval mit karibischem Flair, findet Ende August inNotting Hillstatt. Im September gibt es beimThames FestivalKunst, Sport und zahlreiche Veranstaltungen auf dem Fluss zwischen der Waterloo- und der Blackfriars-Brücke. Jährlich am ersten Sonntag im Oktober findet in der KircheSt Martin-in-the-Fieldsder Erntedankgottesdienst(Pearly Harvest Festival Service)der Londoner Markthändler (Cockney Pearly Kings and Queens) statt. DieTrafalgar Day Paradeanlässlich AdmiralHoratio NelsonsSieg in derSeeschlacht von Trafalgarkann man sich am 21. Oktober auf demTrafalgar Squareansehen. DieBonfire Nightist ein Feuerwerk zum Gedenken an die Aufdeckung der Schießpulververschwörung (Gunpowder Plot) gegen das englische Parlament und die Verhaftung ihres AnführersGuy Fawkesam 5. November 1605. Sie findet Sonnabendnacht um den 5. November in fast allen Teilen Londons statt. In der britischen Hauptstadt gibt es mehr als 30.000 Geschäfte. Eine Besonderheit der Stadt ist, dass sich in einigen Vierteln bestimmte Branchen konzentrieren. So befinden sich in der King′s Road zahlreiche Modegeschäfte, in der Old und New Bond Street viele Designerläden und Galerien. Saville Row undJermyn Streetsind für seine Maßschneidereien bekannt, Oxford und Regent Street für seine Bekleidungsgeschäfte und großen Kaufhäuser wie beispielsweiseHamleysoderSelfridges.HMVist der einzig verbliebene CD-/Vinyl-Megastore über drei Etagen an der Oxford Street. In der Tottenham Court Road konzentrieren sich überwiegend Elektronik- und Computergeschäfte. Die Charing Cross Road ist bekannt für ihre Buchläden.Waterstones, einer der größten Buchläden der Welt, befindet sich amPiccadilly Circus. Zahlreiche Kleidungs- und Schuhgeschäfte sind in der Neal Street zu finden. Covent Garden beheimatet viele Spezialgeschäfte, Cafés und Stände, an denen Kunsthandwerk verkauft wird. Große Einkaufspassagen sind derLeadenhall Market, die Burlington Arcade und die Piccadilly Arcade.Harrodsist eines der bekanntesten Kaufhäuser der Stadt. Das Gebäude befindet sich an der Brompton Road im Stadtbezirk Knightsbridge im Südwesten der Innenstadt. Berühmt ist die im Erdgeschoss liegende Lebensmittelabteilung, mit ihren sogenannten „Food Halls“ und deren unterschiedlichen Ausstattungen im Jugendstil. Sehenswert ist die Beleuchtung der Fassade, die aus rund 100.000 Glühlampen besteht. Am 30. Oktober 2008 eröffnete Europas größtes innerstädtisches EinkaufszentrumWestfieldim StadtteilShepherd′s Bush. Seit 2011 ist Westfield Stratford direkt am Olympiapark Europas größtes Einkaufszentrum. Beim Besuch der Märkte Londons kann man die örtliche Kultur kennenlernen. Erwähnenswert sind der Wochenendmarkt in der Chalk Farm Road bei Camden Lock und der Antiquitäten- und Flohmarkt in derPortobello Road. Die Sonntagsmärkte in der Petticoat Lane undBrick LaneimEast Endbieten fast alles vom Obst und Gemüse bis zu Antiquitäten und Schmuck. Ein Blumenmarkt ist in der Columbia Road zu finden, Märkte für Antiquitäten und Kunsthandwerk befinden sich in Spitalfields und in der Camden Passage in Islington. Der Brixton Market an der Electric Avenue bietet eine große Auswahl an karibischem Essen. Laut einer Studie aus dem Jahr 2014 erwirtschafte der Großraum London einBruttoinlandsproduktvon 836 Milliarden US-Dollar (KKB). In der Rangliste der leistungsstärksten Metropolregionen weltweit belegte er damit den 5. Platz hinterTokio, New York City,Los AngelesundSeoul.[35] In London haben die produzierenden Industriezweige seit vielen Jahren an Bedeutung verloren. Gegenwärtig sind lediglich noch zehn Prozent der Arbeitnehmer in diesem Sektor beschäftigt. Die Druck- und Verlagsindustrie schreibt noch die besten Umsatzzahlen. Sie stellt ein Viertel der genannten Arbeitsplätze und hat einen Anteil von einem Drittel an der gesamten Produktion in London. DieHigh-Tech-Industrie, die auf elektronische und pharmazeutische Erzeugnisse spezialisiert ist, arbeitet erfolgreich mit hohen Umsätzen. Viele der Industriebetriebe, die sich überwiegend in den äußeren Stadtbezirken befinden, tendieren dazu, sich völlig aus London zurückzuziehen. Im Sektor derLeichtindustriesind Bekleidungswerke und Brauereien vertreten. Über den Hafen von London werden heute nur noch zehn Prozent des Binnen- und Außenhandels Großbritanniens abgewickelt. Seit 1971 ist die wirtschaftliche Wachstumsrate der Stadt mit 1,4 Prozent geringer als die des gesamten Landes in einer Höhe von 1,9 Prozent. Trotzdem weist London eine positive Handelsbilanz auf, was überwiegend auf den Dienstleistungssektor – insbesondere die Bereiche Finanzdienstleistungen und Tourismus – zurückzuführen ist. Jährlich besuchen etwa 16 Millionen Touristen die Stadt. Die Hauptstadt besaß 2004 einen Anteil von 19 Prozent am nationalenBruttoinlandsprodukt(BIP).[36]Der Anteil der Metropolregion am britischen BIP lag 1999 bei 30 Prozent.[37]Mehr als die Hälfte der 100 größten Konzerne des Landes und über 100 der 500 größten Unternehmen in Europa haben ihren Hauptsitz in London. Die Stadt ist zudem weiterhin der größte der drei globalenFinanzplätze. Die Internationale Börse des Vereinigten Königreichs und derRepublik Irlandbefindet sich in derCity of London. Die Aufhebung der Regulierungen, bekannt unter dem BegriffBig Bang, ermöglichte 1986 den Einstieg in die moderne Welt des elektronischen Finanzwesens. Die WarenbörseLondon Metal Exchangeist die bedeutendste der Welt, die WertpapierbörseLondon Stock Exchangebelegt weltweit den dritten Platz hinter derNew York Stock Exchangeund derTokioter Börse. DieICE Futures(früher „International Petroleum Exchange“, IPE) ist Handelsplattform für die in Europa führende ÖlsorteBrent. Sie ist die größteTerminbörsefürOptionenundFuturesaufErdöl,ErdgasundElektrizitätin Europa. DerLondon Bullion Marketist der wichtigsteaußerbörsliche HandelsplatzfürGoldundSilber. Hier wird seit 1919 derWeltmarktpreis für Goldund seit 1897 derWeltmarktpreis für Silberfestgestellt. Die Preisbildung für die EdelmetallePlatinundPalladiumfindet amLondon Platinum and Palladium Market(LPPM) statt. Der LPPM stellt wie der London Bullion Market die Ausnahme unter den Rohstoffmärkten dar: er ist keine Börse, sondern einOTC-Markt. Einige der wichtigsten Banken des Landes, wie beispielsweise dieBank of England,Barclays, dieBarings BankundHSBC, haben ihren Sitz in der Hauptstadt. Mehrere Hundert internationale Banken besitzen Filialen in London. Ein anderer Dienstleistungsbereich sind die Versicherungen, denen die Stadt seit über 300 Jahren ihren Wohlstand verdankt.Lloyd’s of Londonist die bekannteste Institution, nicht als Versicherungsgesellschaft im eigentlichen Sinn, sondern als eine Börse für Versicherungsverträge. Es ist eine Gemeinschaft von Versicherern, die fast jede Art von Versicherungen auf dem internationalen Markt übernimmt. Eine Besonderheit der Londoner Stromversorgung stellte der Einsatz derHGÜ Kingsnorth, der bis heute einzigen innerstädtischen Anlage zurHochspannungs-Gleichstrom-Übertragungdar. Diese 1975 in Betrieb genommene Anlage muss sich offenbar nicht sehr bewährt haben und wurde inzwischen stillgelegt. Für die Wasserversorgung der Stadt ist das privatisierte UnternehmenThames Waterzuständig. London verfügt über ein rund 150 Jahre altes Wasserleitungsnetz, in welches von jeher nur spärlich investiert wurde. Rund 30 Prozent des Leitungswassers versickern täglich im Londoner Untergrund. London ist Hauptsitz bedeutender Rundfunk- und Fernsehanstalten wie (BBC,ITV,Channel 4,FiveundSky). ImBush HousezwischenAldwychundStrandwaren bis zum Jahr 2012 derBBC World Serviceund die AbteilungNeue Mediendes BBCi beheimatet. Die BBC wurde am 18. Oktober 1922 in London als unabhängiger Radiosender gegründet. Die erste Ausstrahlung eines Programms fand am 14. November 1922 aus einem Londoner Studio statt. Die BBC betreibt mehrere Rundfunk- und Fernsehsender. Alle wichtigen Tages- und Wochenzeitungen des Landes haben ihren Sitz in London. DieFleet Streetwar seit dem 18. Jahrhundert traditionell die Heimat der britischen Presse. DieBoulevardzeitungenThe Sun,Daily Express,Daily Mail(konservativ) undDaily Mirror(Labour nahestehend) und ihre sonntäglich erscheinenden Schwesterzeitungen sind überwiegend die größten Zeitungen der Stadt und erreichen teilweise Auflagen in Millionenhöhe. DerDaily Telegraphist eine im Jahr 1855 gegründete konservative Tageszeitung. Die Auflage beträgt 905.000 Exemplare (Stand 2005). Die Zeitung fällt häufig durch eine äußerst EU-kritische Berichterstattung auf. Schwesterzeitung ist das WochenblattThe Sunday Telegraph. Ein weiterer Titel der Gruppe ist das MagazinThe Spectator. The Timesist eine konservative Tageszeitung mit einer Auflage von 693.000 Exemplaren. Außerhalb Großbritanniens wird sie manchmal alsThe London TimesoderThe Times of Londonbezeichnet, um sie von vielen anderen Zeitungen mit dem NamenTimeszu unterscheiden.The Timeswurde im Jahr 1785 alsThe Daily Universal Registergegründet. The Guardianist eine 1821 gegründete Tageszeitung mit einer Auflage von 380.000 Exemplaren. Sie wird zusammen mit demDaily TelegraphundThe Timeszu den seriösen und angesehenen Zeitungen in Großbritannien gezählt – den „Quality Papers“ –, in Abgrenzung zu den Boulevardblättern, den „Tabloids“. Ihre politische Gesamtausrichtung lässt sich als linksliberal beschreiben. Mit demObservererscheint im gleichen Verlagshaus auch eine bedeutendeSonntagszeitung, die die Ausrichtung ihrer Schwesterzeitung teilt. The Independentist eine der vier großen seriösen Tageszeitungen Großbritanniens. Die Sonntagsausgabe erscheint unter dem NamenThe Independent on Sunday. Er ist ebenso wieThe Guardianeher dem linken Meinungsspektrum zuzuordnen.The Independentwurde 1986 gegründet und hat eine Auflage von 260.000 Exemplaren. DieFinancial Timesist eine der wichtigsten Wirtschaftszeitungen der Welt, von der bis zum 7. Dezember 2012 auch einedeutsche Ausgabeerschien. MitReutershat eine der weltweit größten Nachrichtenagenturen ihren Sitz in der Hauptstadt. Das Unternehmen wurde vonPaul Julius Reuterzunächst 1850 inAachengegründet, dort übermittelte er perBrieftaubenAktiendaten zwischen Aachen undBrüssel. Als zwischen den beiden Städten eineTelegrafenverbindungeingerichtet wurde, stellte Reuters den „Flug-Dienst“ ein. Nach seiner Emigration nach London gründete er erneut ein Unternehmen um über dasSeekabelzwischenDoverundCalaisBörsenkurse nachPariszu übermitteln. Heute erwirtschaftet das Unternehmen 90 Prozent seines Umsatzes mit Börsen- und Wirtschaftsinformationen. London ist Dreh- undAngelpunktdes Straßen-, Schienen- und Luftverkehrs im Vereinigten Königreich. Das Verkehrswesen fällt in die direkte Zuständigkeit desMayor of London, des Oberbürgermeisters, der die betrieblichen Belange an die VerkehrsgesellschaftTransport for London(TfL) delegiert. TfL ist für den größten Teil desöffentlichen Personennahverkehrszuständig. Dazu gehören U-Bahn, Busse, Straßenbahnen und Stadtbahnen, nicht jedoch der Eisenbahnvorortsverkehr und der Flugverkehr. Darüber hinaus reglementiert TfL das Taxiwesen und ist für den Unterhalt der wichtigsten Hauptstraßen verantwortlich. Siehe auch:Eisenbahnknoten London Herzstück des öffentlichen Personennahverkehrs ist dieLondon Underground, die ältesteU-Bahnder Welt, deren erstes Teilstück im Jahr 1863 eröffnet wurde. Die U-Bahn wird jährlich von mehr als einer Milliarde Fahrgästen genutzt. Sie erschließt die Innenstadt und die meisten Vororte nördlich derThemse. Der Süden der Stadt wird hingegen hauptsächlich von einem engmaschigen Netz von Vorortbahnen erschlossen. Die fahrerlose StadtbahnDocklands Light Railwayerschließt das ehemalige Hafengelände derDocklandsim Osten der Stadt und hat maßgeblich zur Regenerierung dieses Stadtteils beigetragen. Nachdem die ersteLondoner Straßenbahnim Jahr 1952 eingestellt wurde, verkehrt seit 2000 im südlichen Stadtteil Croydon die neue StraßenbahnTramlink. Von wenigen Ausnahmen abgesehen durchqueren Vororts- und Intercity-Schnellzüge nicht das Stadtgebiet, sondern verkehren zu und ab 14 Hauptbahnhöfen, die rund um die Innenstadt verteilt sind. DieEurostar-Züge verbinden London durch denEurotunnelmitParisundBrüssel. Siehe auch:Liste der Buslinien in London Während im Stadtzentrum die meisten Fahrten mit öffentlichen Verkehrsmitteln erfolgen, wird der Verkehr in den äußeren Stadtteilen vom Auto dominiert. Die innere Ringstraße um das Stadtzentrum, dieA406(North Circular Road) und dieA205(South Circular) in den Vororten sowie die AutobahnM25um das gesamte Ballungsgebiet herum verbinden zahlreiche radiale und viel befahrene Ausfallstraßen miteinander. Autobahnen führen nur in Ausnahmefällen bis in die inneren Stadtteile. Im Jahr 2003 wurde eineInnenstadtmautmit dem NamenLondon Congestion Chargeeingeführt, um das Verkehrsaufkommen in der von engen und oft verstopften Straßen geprägten Innenstadt zu reduzieren. Die zu zahlende tägliche Gebühr betrug bis zum 4. Juli 2005 5 Pfund, bis zum 3. Januar 2011 8 Pfund und wurde dann auf 10 Pfund pro Tag angehoben. SogenannteLow-Traffic Neighbourhoodsdienen seit 2020 in innenstädtischen Wohn- und Ausgehvierteln zur Reduzierung desDurchgangsverkehrsund der Erweiterung von Aufenthaltsflächen.[38] Das Netz der LondonerBusseerschließt sämtliche Stadtteile mit einemdichten Liniennetz. Täglich werden auf über 700Linienrund sechs Millionen Fahrgäste befördert, etwa doppelt so viel wie mit der London Underground. Die rotenDoppeldeckerbussesind ebenso ein international bekanntes Symbol der Stadt wie dieschwarzen Taxis. London stellt mitHeathrowden meist frequentierten Flughafen Europas. Zusammen mit den weiteren fünf internationalen FlughäfenGatwick,Luton,Stansted,City AirportundLondon Southend Airportbildet die Metropole ein wichtiges Zentrum des internationalen Luftverkehrs. Im Jahr 2006 wurden insgesamt 137 Millionen Passagiere auf Londoner Flughäfen abgefertigt. Heathrow und City Airport befinden sich innerhalb vonGreater London, die übrigen außerhalb. Gatwick, Heathrow und Stansted werden durch Airport-Express-Züge sowie Reisebusse verschiedener Anbieter mit der Innenstadt verbunden. Außerdem besitzt Heathrow als einziger der Londoner Flughäfen einen U-Bahn-Anschluss. Der City Airport ist auch über dieDocklands Light Railwayangebunden. Daneben existieren in und um London mehrereFlugplätzefür privaten und kommerziellen Luftverkehr. Dies sindNortholt Aerodrome,Biggin Hill AirportundFarnborough Airfield. DieUniversitätenundHochschulenin London können auf eine lange Geschichte zurückblicken. London ist auch die Stadt mit den meisten Studenten. Die Universitäten Londons können in zwei Gruppen eingeteilt werden. Die föderal organisierteUniversity of Londonist mit über 100.000 Studenten eine der größten Universitäten Europas. Sie besteht aus über 50 Colleges und Instituten, die über einen hohen Grad an Autonomie verfügen. Die größten und prestigeträchtigsten Colleges sindUniversity College London,King’s College,Queen Mary, dieLondon School of Economics and Political Scienceund dieLondon Business School. Kleinere Schulen und Institute sind auf bestimmte Wissensgebiete spezialisiert, wie dieSchool of Oriental and African Studies, dasInstitute of Educationund dasBirkbeck College. Daneben existieren weitere Universitäten, die nicht der University of London angeschlossen sind, wie dasImperial Collegeund dieCity Universityim historischen Stadtzentrum. Einige Universitäten waren früherTechnische Hochschulen, bis sie im Jahr 1992 durch eine Gesetzesänderung den Status einer Universität erhielten (so etwa dieUniversity of East London), während andere lange vor der Gründung der University of London entstanden waren. Zu diesen zählen dieMiddlesex Universityim Norden Londons, dieBrunel Universityim Westen und dieLondon South Bank University. London ist das britische Zentrum der künstlerischen Ausbildung. Die vierKonservatoriensind dasRoyal College of Music, dieRoyal Academy of Music, dasTrinity College of Musicund dieGuildhall School of Music and Drama. Auf die Schauspielerei spezialisiert sind dieRoyal Academy of Dramatic Art(RADA) und dieCentral School of Speech and Drama. Mit Kunst befassen sich dasCentral Saint Martins College of Art and Design, dasChelsea College of Art and Designund dieCamberwell School of Art(alle Teil derUniversity of the Arts London), daneben auch dasGoldsmiths Collegeund dieSlade School of Fine Art(beide Bestandteil der University of London) sowie dasRoyal College of Artund dieWimbledon School of Art. Die ehemalige Hornsey School of Art ist heute ein Teil derMiddlesex University. Es gibt zahlreichemedizinische Fakultätenin London. Einige bestehen schon seit Jahrhunderten, darunterQueen Mary’s School of Medicine and Dentistry,Guy’s HospitalundSt Thomas’ Hospital. DasImperial Collegeist ein führendes Zentrum der wissenschaftlichen Forschung und ist hinsichtlich seiner Reputation mit demMassachusetts Institute of Technologyzu vergleichen. Ebenfalls von Bedeutung ist dieRoyal Institution. District sui generis:City of London London Boroughs:Barking and Dagenham|Barnet|Bexley|Brent|Bromley|Camden|City of Westminster|Croydon|Ealing|Enfield|Greenwich|Hackney|Hammersmith and Fulham|Haringey|Harrow|Havering|Hillingdon|Hounslow|Islington|Kensington and Chelsea|Kingston|Lambeth|Lewisham|Merton|Newham|Redbridge|Richmond|Southwark|Sutton|Tower Hamlets|Waltham Forest|Wandsworth Sonstige Einteilung:Greater LondonundCity of London|Inner LondonundOuter London Ehemalige Stadtbezirke imCounty of LondonBattersea|Bermondsey|Bethnal Green|Camberwell|Chelsea|Deptford|Finsbury|Fulham|Greenwich|Hackney|Hammersmith|Hampstead|Holborn|Islington|Kensington|Lambeth|Lewisham|Paddington|Poplar|Shoreditch|Southwark|St Marylebone|St Pancras|Stepney|Stoke Newington|Wandsworth|Westminster|Woolwich Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geographie 1.1Geographische Lage 1.2Geologie 1.3Stadtgliederung 1.4Klima 2Geschichte 2.1Antike 2.2Mittelalter 2.3Frühe Neuzeit 2.4Moderne 3Bevölkerung 3.1Religionen 3.2Bevölkerungsentwicklung 3.3Entwicklung der Wohnsituation 4Politik 4.1Stadtregierung 4.2Städtepartnerschaften 4.3Städtefreundschaft 5Kultur und Sehenswürdigkeiten 5.1Theater 5.2Musik 5.3Museen 5.4Bauwerke 5.4.1Straßen und Plätze 5.4.2Weltliche Bauwerke 5.4.3Sakrale Bauwerke 5.5Parks 5.6Sport 5.7Regelmäßige Veranstaltungen 5.8Einkaufen 6Wirtschaft"
  },
  {
    "label": 0,
    "text": "Los Angeles – Wikipedia Los Angeles Inhaltsverzeichnis Name der Stadt Geographie Geschichte Bevölkerung Politik Kultur und Sehenswürdigkeiten Wirtschaft und Infrastruktur Söhne und Töchter der Stadt Siehe auch Literatur Weblinks Einzelnachweise Lage Geologie Stadtgliederung Siedlungsform Klima Umweltprobleme Stadtgründung Industrialisierung Wirtschaftszentrum Ethnizitäten Religionen Bevölkerungsentwicklung Kriminalität Stadtregierung Flagge und Siegel Städtepartnerschaften Städtefreundschaften Musik und Theater Museen Bauwerke Parks Sport Freizeit und Erholung Regelmäßige Veranstaltungen Gastronomie Handel Wirtschaft Verkehr Medien Bildung Football Eishockey Baseball Basketball Fußball Motorsport Inklusion Frühling Sommer Herbst und Winter Fernverkehr Nahverkehr Printmedien Rundfunk und Fernsehen Filmindustrie Autobahnen Eisenbahnverkehr Flugverkehr Schifffahrt Schienenverkehr Straßenverkehr Los Angeles[[lɔs ˈændʒələs]ⓘ][2](ausspanischLos Ángeles[losˈaŋxeles], deutsch‚Die Engel‘), offiziellCity of Los Angeles,[3]häufigL.A.abgekürzt, ist die größte Stadt imUS-BundesstaatKalifornien. Sie liegt amPazifischen Ozeanund amLos Angeles River. Los Angeles ist mit 3.898.747 Einwohnern (2020)[4]im Stadtgebiet nachNew York Cityund vorChicagodie zweitgrößte Stadt derVereinigten Staaten.[5]Mit 11,8 Millionen Einwohnern in derAgglomeration[6]sowie über 12,8 Millionen Einwohnern in derMetropolregion(Metropolitan Statistical Area)und knapp 17,8 Millionen in der erweiterten Metropolregion steht dieGreater Los Angeles Areaunter den größtenMetropolregionen der Weltan 21. Stelle.[7]Die Einwohner von Los Angeles werdenAngelenos[ændʒəˈlinoʊz] genannt. Die Stadt ist Hauptstadt und Verwaltungssitz desLos Angeles Countyund das Wirtschafts-, Geschäfts- und Kulturzentrum Kaliforniens mit zahlreichen Universitäten wie derUSCundUCLA, Hochschulen, Forschungsinstituten, Theatern und Museen. Los Angeles ist weltgrößter Standort für dieFlugzeug- undRaumfahrtindustrieund bekannt für die dort ansässige Film- und Fernsehindustrie (Hollywood) und Musikszene. Die Metropolregion Los Angeles erbrachte 2017 eine Wirtschaftsleistung von rund 1,04 Billionen US-Dollar. Unter den Städten der Welt belegt es damit den dritten Rang hinterTokiound New York City.[8]DieSchere zwischen Arm und Reichist in der Stadt allerdings auch sehr groß, bspw. hat Los Angeles Stand 2023 etwa 75.000[9]Obdachloseund damit fast so viele wie das ungefähr doppelt so große New York City.[10] Die Stadt wurde am 4. September 1781 offiziell vomspanischen GouverneurFelipe de NevealsEl Pueblo de la Reina de Los Ángeles(spanisch„Das Dorf der Königin der Engel“) gegründet.[11]Der Name bezog sich damit also aufMariaalsKönigin der Engel, ein Marientitel, der aus derLauretanischen Litaneistammt. Der heutige NameLos Angelesist eine Verkürzung des Gründungsnamens auf „die Engel“. Weit verbreitet war ursprünglich auch der inoffizielle NameEl Pueblo de Nuestra Señora la Reina de los Ángeles del Río de Porciúncula[el ˈpweβlo de ˈnwestɾa seˈɲoɾa la ˈrei̯na de los ˈaŋxeles del ˈrio de poɾsiˈuŋkula] (Das DorfUnserer Lieben Frau, Königin der Engel des FlussesPortiuncula).[11]Er nimmt konkreten Bezug auf dieBasilikaSanta Maria degli Angeliin Assisi, erbaut über derPortiuncula-Kapelle, in derFranz von Assisistarb. Er war der Gründer des Ordens, dem dieMissionareangehörten, die mit den Entdeckern nach Amerika kamen. Im Gegensatz zur landläufigen Meinung, dass der ursprüngliche Name so lautete, haben Wissenschaftler aus offiziellen Dokumenten von Gouverneur Felipe de Neve, GeneralbefehlshaberTheodor de Croixund Vizekönig Antonio María de Bucareli y Ursúa festgestellt, dass die Siedlung einfach den NamenEl Pueblo de la Reina de Los Ángelestrug. Die häufig (auch umgangssprachlich) verwendete Abkürzung lautet „L.A.“. Wegen der spanischen Bedeutung des Stadtnamens trägt die Stadt den BeinamenCity of Angels (Stadt derEngel). Der Stadtkern und die Vororte von Los Angeles liegen in einer hügeligen Küstenregion durchschnittlich 100 Meter über demMeeresspiegel.[12]Im Westen und Süden grenzt die Stadt an dieBucht von Santa MonicadesPazifischen Ozeans. Im Osten und Norden ist sie von Gebirgsketten umgeben. Ebenfalls im Norden der Stadt liegt dasSan Fernando Valley, in dem ein Drittel der Bewohner in Einfamilienhäusern lebt. Das Tal ist durch denGriffith Parkund dieSanta Monica MountainsvonHollywoodund der Innenstadt abgeschnitten. Das administrative Stadtgebiet hat eine Fläche von 1290,6 km². Davon sind 1214,9 km² Land- und 75,7 km² Wasserfläche. Das Stadtgebiet erstreckt sich 71 km in Nord-Süd-Richtung und 47 km in Ost-West-Richtung. Es ist durch ein System von Schnellstraßen aus Stahl- und Betonkonstruktionen miteinander verbunden. Die Stadt wird durch die größte Kraftfahrzeugdichte der Welt belastet, die Auto- und Industrieabgase sind zu einem drängendenUmweltproblemgeworden. Daher gehört Los Angeles zu den Städten mit der größten Belastung durchSmogin den Vereinigten Staaten. Die Stadt wird manchmal alshorizontal city(horizontale Stadt) bezeichnet, da sie relativ wenigeWolkenkratzerbesitzt und das gesamte Stadtgebiet sehr weitläufig ist. Hohe Grundstückspreise führen im Zentrum von Los Angeles trotzdem dazu, dass Hochhäuser sich dort inzwischen auch durchsetzen. Die Agglomeration, also das durchgängig bebaute Gebiet, nimmt eine Fläche von 4320 km² ein. Zur Metropolregion von Los Angeles gibt es verschiedene Definitionen: Am Ostrand der Metropolregion verläuft dieSan-Andreas-Verwerfung; außerdem befindet sich direkt unter der Stadt diePuente-Hills-Verwerfung. Die daraus resultierendeErdbebengefahrerklärt die strengen Bauvorschriften, die im gesamten südkalifornischen Erdbebengebiet angewandt werden. Seit 1800 wurde Los Angeles von neun großen Erdbeben mit einerMagnitudevon sechs und höher und Tausenden von kleineren Beben erschüttert. Zahlreiche Gebäude sinderdbebensichergebaut, was die Anzahl an Todesopfern bei Beben wesentlich verringert. Bis zum Jahre 1958 bestand zudem eine gesetzliche Vorschrift, wonach die Obergrenze für Gebäude 45 Meter beziehungsweise 14 Stockwerke nicht übersteigen durfte. Eine Ausnahme war nur das Rathaus von 1928 mit 138 Metern. Erdbebensichere Konstruktionen machten das Gesetz später überflüssig. Die Gefährdung durch Erdbeben, die Abkehr von dichter Bebauung und die Festlegung eines Leitbildes, wonach Los Angeles eher eine Art „City in the Garden“ (Stadt im Garten) werden sollte, standen als Idee hinter dieser Vorschrift. Auch dies ist eine Erklärung für die Ausdehnung der Stadt. Das schwerste Erdbeben in der jüngeren Geschichte war dasFort-Tejon-Erdbebenam 9. Januar 1857, nahe den heutigen GemeindenWrightwoodundPalmdale. Das Beben der Magnitude 7,9 verursachte nur geringe Schäden, da die Region zu dieser Zeit nur gering besiedelt war. Würde ein derartiges Beben heute auftreten, entstände ein Schaden von mehreren Milliarden US-Dollar, die Verluste an Menschenleben wären erheblich. Am 10. März 1933 forderte dasLong-Beach-Erdbebender Magnitude 6,4 120 Todesopfer. InLong Beachund weiteren Orten entstand ein Sachschaden von 50 Millionen US-Dollar. Zahlreiche der beschädigten Gebäude waren nicht erdbebensicher gebaut. Ein weiteres schweres Erdbeben war das sogenannteSan-Fernando-Erdbeben von 1971(auchSylmar-Erdbeben) mit einer Stärke von 6,6. Es ereignete sich am 9. Februar 1971 und verursachte im San Fernando Valley einen Schaden von 500 Millionen US-Dollar und forderte 65 Todesopfer. Das Beben am 28. Juni 1991 hatte eine Stärke von 5,8, verursachte aber wegen seiner Tiefe keinen Oberflächenbruch. Es entstanden dennoch Schäden in Höhe von 40 Millionen US-Dollar, vor allem imSan Gabriel Valley. Am 17. Januar 1994 erschütterte ein Erdbeben der Stärke 6,7 die Stadt Los Angeles. DasEpizentrumdesNorthridge-Erdbebenslag im StadtteilResedaim San Fernando Valley. 57 Menschen kamen dabei ums Leben, 12.000 Personen wurden verletzt. Wichtige Verkehrsverbindungen und schätzungsweise 100.000 Gebäude wurden beschädigt oder zerstört. Nach Angaben derFederal Emergency Management Agency(FEMA) entstand ein Sachschaden in Höhe von 40 Milliarden US-Dollar.[13] Im Jahr 2008 publizierten derGeologische Dienst der USA(USGS) und das Südkalifornische Erdbebenzentrum eine neue Studie. Die Forscher prognostizierten darin für Kalifornien bis 2038 mit einer 99,7-prozentigen Wahrscheinlichkeit ein schweres Erdbeben der Stärke 6,7. Ein Beben der Stärke 7,5 oder mehr wurde mit einer 46-prozentigen Wahrscheinlichkeit vorausgesagt. Der Großraum Los Angeles ist dabei etwas stärker gefährdet alsSan Franciscoim Norden des Bundesstaates. So liegt die Möglichkeit eines Bebens der Stärke 6,7 in Los Angeles bei 67 Prozent und in San Francisco bei 63 Prozent.[14] Am 29. Juli 2008 wurde um 11:42 Uhr Ortszeit ein Erdbeben der Stärke 5,4 registriert. Das Epizentrum lag ca. 46 km ostsüdöstlich vom Stadtzentrum entfernt, in der Nähe vonChino Hills.[15] Los Angeles gliedert sich in 15 Bezirke (Districts). Die Einwohnerzahlen beziehen sich auf dieVolkszählungvom 1. April 2000.[16] Die Region Los Angeles ist durch eine extremeZersiedelunggekennzeichnet. Die gesamte Stadt ist mosaikartig aufgelöst in eine Vielzahl von einzelnen Stadtbezirken. Die Bezirke haben unterschiedliche Funktionen wie Geschäftsviertel oder Wohnviertel und sind deutlich voneinander unterschieden. In den einzelnen Wohnbezirken leben Bevölkerungsgruppen, die sich hinsichtlich Herkunft, Bildung, Einkommen oder Altersstruktur ebenfalls deutlich unterscheiden. Teilweise sind die Bezirke voneinander isoliert, und nicht selten sogar durch Mauern oder Zäune abgeschottet.[17]In der gesamten Stadtregion gibt es kein dominantes Zentrum. Die Innenstadt, dieDowntown Los Angeles, verfügt zwar über die einzige Ansammlung an Wolkenkratzern in dem riesigen Häusermeer und hat auch eine symbolische Bedeutung, ist jedoch lediglich einer von vielen Stadtbezirken. Innerhalb der Agglomeration von Los Angeles existieren als Teil derGreater Los Angeles Areaviele selbstständige Bezirke mit eigenem Zentrum, eigenem Charakter und mit spezialisierten Funktionen, so etwaLong Beach,Santa Ana,AnaheimoderPasadena. Diese Diversifizierung und Fragmentierung ist einerseits ein Problem, kann andererseits jedoch auch einer der Gründe für den Aufstieg der Stadt sein. Los Angeles ist eine derheterogensten[18]Städte der Welt, geprägt vomIndividualismusseiner Bewohner. UnterschiedlicheMilieusundLebensstileexistieren nebeneinander,[19]wodurch der städtische Raum nicht mehr als zusammenhängende Einheit zu erkennen ist. Dies spiegelt sich insbesondere auch in dem Fehlen eines klar erkennbaren Zentrums wider.[20]Für diese postmodernistischen Siedlungsformen sind seit Ende des 20. Jahrhunderts eine Reihe neuer Begriffe entwickelt worden, so etwaExopolis,Postmetropolisoder auch der Begriff derZwischenstadt.[21][22][23]Häufig werden solche Städte auch alsEdgeless CitiesoderEdge Citiesbezeichnet, alssuburbane,posturbaneoder auchdesurbaneRäume.[24][25][26]Mit dem Terminus desStadtlandhybridenwird versucht, den Entwicklungen postmoderner Siedlungsformen – mit Fokus auf Los Angeles – Rechnung zu tragen. Solche Agglomerationen stellen eine neue Art der räumlichen Entwicklung dar, „deren Voraussetzung eine umfangreiche Suburbanisierung darstellt.“[19]Der Erforschung dieser Entwicklungen widmete sich insbesondere in den späten 1980er und 1990er Jahren die Forschergruppe derLos Angeles School of Urbanism.[27] Die Stadt liegt in dersubtropischenKlimazone. Die durchschnittliche Jahrestemperatur beträgt 18 Grad Celsius und die jährliche Niederschlagsmenge 379,2 Millimeter im Mittel. Die wärmsten Monate sind Juli und August mit durchschnittlich 22,8 Grad Celsius und der kälteste der Januar mit 13,2 Grad Celsius im Mittel. Fast der gesamte Jahresniederschlag fällt in den Monaten November bis März, dementsprechend ist es zwischen Mai und Oktober überwiegend trocken. Im Sommer herrschen in Los Angeles tagsüber meist Temperaturen um die 28 bis 30 Grad Celsius. Ohne die Lage an der Pazifikküste wäre es noch wärmer, da der Wind vom Meer die Temperaturen abschwächt. Im Winter dagegen ist es etwas kälter, wobei die Tagestemperaturen fast nie unter 15 Grad Celsius sinken. Nachts sinkt die Temperatur durchschnittlich um circa zehn Grad Celsius ab. Die Luftfeuchtigkeit liegt bei 50 bis 75 Prozent. Typisch für das Klima in der Region von Los Angeles sind auch mehrere, relativ deutlich voneinander abgrenzbare kleinere Klimazonen. Der Grund hierfür liegt vor allem in den verschiedenen Gebirgszügen, die das Becken von Los Angeles zum Beispiel vomSan Fernando Valleyund anderen, weiter landeinwärts gelegenen Gebieten trennen. Im San Fernando Valley beispielsweise ist es im Sommer oft mehrere Grade wärmer als an der Küste, während es im Winter dort spürbar kühler ist. Alle zwei bis drei Jahre kommt es zu Hitzewellen, bei denen die Temperaturen über 40 Grad Celsius steigen können. Ursache dafür sind die sogenanntenSanta-Ana-Winde, die über dieSanta-Ana-Bergeaus den östlich der Stadt gelegenen Wüsten kommen. Dabei entstehen immer wieder ausgedehnte Wald- und Buschbrände. Am 3. Oktober 1933 starben imGriffith Parkbei einem der schlimmsten Buschbrände in den Vereinigten Staaten 29 Feuerwehrleute. Zwischen Ende Oktober und Anfang November 1993 vernichteten Brände im Stadtbereich von Los Angeles tausende Hektar bebauten Landes.[28]Die höchste Temperatur wurde am 22. Juli 2006 in Woodland Hills mit 48,3 Grad Celsius gemessen, die tiefste Temperatur am 6. Februar 1989 inCanoga Parkmit −7,8 Grad Celsius.[29][30] Sorgen bereitet die großeLuftverschmutzung– hohe Werte anOzon-,Stickoxid- undKohlenwasserstoff– in Los Angeles, verursacht durch die dort ansässige Industrie und den Autoverkehr. Der Großraum Los Angeles/Long Beach/Riverside zählte 2007 nach einem Bericht der American Lung Association zum städtischen Gebiet mit der höchsten Luftverschmutzung in den Vereinigten Staaten.[32]Neben Industrien und Alltagsverkehr der privaten Autos und Lastwagen kann die Luftverschmutzung zu einem wesentlichen Teil dem Hafen-Binnenverkehr zugeschrieben werden, d. h. den Lastwagen, die zwischen Kai und Ablage Güter tragen und bei denen sich oft die Motoren im Leerlauf befinden. Abhilfe sollen elektrische Schwerlastwagen schaffen, die seit 2009 eingeführt werden.[33] Die Schadstoffkonzentrationen sind besonders im morgendlichen und abendlichen Berufsverkehr am höchsten. Derphotochemische Smog, dessen Hauptbestandteil das Ozon ist, erreicht dagegen am Mittag seine höchste Konzentration. Da die Stadt von Gebirgsketten umgeben ist, wird der Luftaustausch mit dem weiten Umland gehemmt. Der Seewind erreicht lediglich das Becken von Los Angeles, ohne dieInversionswetterlageüber der Stadt aufzulösen. Ozon und andere Chemikalien führen bei der Bevölkerung zu Husten, Augenreizungen, Kopfschmerzen und Lungenfunktionsstörungen. Seit Anfang der 1980er Jahre werden Anstrengungen zur Lösung des Problems unternommen. So wurden noch 1979 an 120 Tagen im Jahr erhöhte Schadstoffwerte gemessen, 1996 nach Einführung noch stärkerer Abgasbestimmungen für Neuwagen (Katalysatoren waren seit 1973 Vorschrift) nur noch an sieben Tagen. Im neuen Jahrtausend ist die Anzahl an Smogtagen fast auf null gesunken. Trotzdem entlässt der Großraum Los Angeles immer noch eine der landesweit größten Mengen an toxischen Gasen in die Atmosphäre. Hauptgrund ist der nur schwach entwickelte öffentliche Verkehr. Die Luftverschmutzung in Los Angeles gefährdet auch die weiter entfernt liegenden Gebirgsseen und schneebedeckten Regionen derSierra Nevadaund ist verantwortlich für dasWaldsterben. Starke Winde tragen die Schadstoffe bis nachPalm SpringsimRiverside Countyund führen dort zu erhöhten Schadstoffkonzentrationen. Der erste Europäer in der Region war der EntdeckerJuan Rodríguez Cabrillo, der 1542 das Land fürSpanienbeanspruchte, ohne jedoch eine Siedlung zu gründen. 1771 gründeten spanische Mönche desFranziskanerordensdie Mission „San Gabriel“ nahe Whittier Narrows, die später den Ausgangspunkt für die Besiedlung der Region bildete. Diespanischen Missionen in Kaliforniensollten den Einfluss Spaniens und die Macht der Kirche erweitern. Um sie vor Angriffen der dort lebendenUreinwohnerzu schützen, wurde die Mission durch ein Fort befestigt. Nachdem sich die Mission etabliert hatte, schickte der spanischeGouverneurKaliforniens, Felipe de Neve, elf Familien, um das Land zu bebauen. Am 4. September 1781 wurde die Gemeinde Los Angeles mit 44 Siedlern auf dem Gebiet derTongva-Ureinwohner gegründet. Damals wurde dort noch hauptsächlich Viehzucht betrieben. In den folgenden Jahrzehnten etablierten sich die ersten US-amerikanischen Siedler in der Region. 1821 fiel Los Angeles anMexiko, das nach einem langen Krieg –Mexikanischer Unabhängigkeitskrieg– seine Unabhängigkeit von Spanien errungen hatte. 1835 wurde die Siedlung vom mexikanischen Kongress zur Stadt und gleichzeitig zur Hauptstadt von Mexikos nördlichster RegionAlta Californiaerhoben. Der zweite Teil des Erlasses wurde allerdings niemals durchgesetzt und bald wieder revidiert. So bliebMontereybis 1849 die Hauptstadt von Kalifornien. Die Bevölkerung von Los Angeles wuchs bis 1836 auf 2.228 Einwohner an, um danach wieder vorübergehend zurückzugehen. Bis Mitte des 19. Jahrhunderts war Los Angeles eine mexikanische Gemeinde, die sich aber vorwiegend aus US-amerikanischen Siedlern, armen chinesischen Arbeitern und einigen wenigen wohlhabenden mexikanischen Großgrundbesitzern zusammensetzte. 1846 proklamierte der Schulmeister William Ide die Unabhängigkeit Kaliforniens. Die Republik bestand nur kurze Zeit. Während desMexikanisch-Amerikanischen Kriegeszwischen 1846 und 1848 wurden Alta California und damit auch Los Angeles von US-amerikanischen Soldaten besetzt und den USA angegliedert. 1848 wurde nördlich vonSacramentoGold gefunden, was den bekanntenGoldrauschauslöste. Zahlreiche Goldsucher kamen in die Region, wovon auch Los Angeles profitierte, indem es Fleisch, Obst und Gemüse an die Goldsucher verkaufte. Kurz danach kam der erste von zwei europäischen Apothekern (beide deutscher Abstammung) in Los Angeles an.[34]Der erste war der Apotheker Theodore Wollweber. Im Jahr 1861 siedelte sich Adolph Junge als zweiter Apotheker an. Er betrieb seinen „Drug-Store“ im „Temple Block“ in der Main Street rund 20 Jahre lang.[35]Der später sehr bekannte deutsche Apotheker F. J. Gieze arbeitete 1874 als Angestellter bei Junge. Junges Rezeptbuch und sein Nachlass befinden sich heute imNatural History Museum of Los Angeles County. Der Ort erhielt am 4. April 1850 das US-Stadtrecht im Rahmen der Gründung des Staates Kalifornien durch die USA; er hatte damals 1.610 Einwohner. VomSezessionskrieg(1861–1865) zwischen den aus den Vereinigten Staaten ausgetretenenSüdstaaten– derKonföderation– und den in der Union verbliebenenNordstaatenblieb Los Angeles nahezu unberührt. Überschwemmungen 1861/62 und eine anschließende verheerende Dürre führten zum Ruin vieler auf Viehzucht spezialisierter Farmen. Gleichzeitig begann, vor allem durch die Grundstücksspekulation, ein neuer Aufschwung. Die Folge war ein Anstieg der Grundstückspreise und der Zuzug von zahlreichen chinesischen, japanischen und europäischen Einwanderern. Die Ablehnung derchinesischen Migrantendurch weite Teile der US-Bevölkerung steigerte sich in der wirtschaftlichen Krisenzeit der 1870er Jahre zu einer antichinesischen Hysterie. Am 24. Oktober 1871 kam es in Los Angeles zu einem rassistischen Aufstand, nachdem bei einem Krieg zwischen rivalisierenden chinesischen Banden zufälligerweise ein Weißer getötet wurde. Im weiteren Verlauf des Aufstandes fiel einMobvon mehr als 500 europäischstämmigen Personen inChinatownein und tötete 19 der Bewohner. Unter den Getöteten war lediglich einer an dem ursprünglichen Bandenkrieg beteiligt. Außerdem war ein Weißer getötet worden, der versucht hatte, das Morden zu verhindern. Die größte Zuwanderergruppe stammte aus dem Mittleren Westen, aus Staaten wieIowaundIndiana, und löste als neue politische Klasse die alte mexikanische Elite ab. Schon bald wurden die alten Großfarmen parzelliert und die Bevölkerung wuchs. Von 1870 bis 1900 erhöhte sich die Einwohnerzahl der Stadt von knapp 6.000 auf ca. 102.000 Einwohner. Von 1900 bis 1910 stieg die Einwohnerzahl sogar um mehr als das Dreifache (1910: 319.000 Einwohner). Zu diesem enormen Wachstum trugen auch umfangreiche Eingemeindungen bei, darunter der StädteWilmingtonundSan Pedro(beide 1909) im Süden sowieHollywood(1910) im Westen. Von 1899 bis 1914 wurde in San Pedro der heutigeHafen von Los Angelesgebaut. 1915 kam es zur gebietsmäßig größten Erweiterung, als der Großteil desSan Fernando ValleyLos Angeles angegliedert wurde.[36] Wichtiger Entwicklungsmotor war die Eisenbahn. 1869 wurde die erste Eisenbahnlinie in Los Angeles eröffnet. Sie verlief über 34 Kilometer vom heutigen Stadtzentrum zum damals noch unabhängigen San Pedro, wo sich heute der Hafen von Los Angeles befindet. Nachdem dieUnion Pacific Railroadim Jahr 1876 (San Francisco) und die Bahnstrecke nachSanta Feim Jahre 1885 die Verbindungen zum Rest des Landes hergestellt hatten, beschleunigte sich das Wachstum der Stadt. Zu dieser Zeit wurden neue Bewässerungstechniken erprobt und für die Region geeignete Anbauprodukte wie z. B. Orangen entdeckt. Die Stadt wurde in den Metropolen im Osten der USA bald zu einem Synonym für gute Gesundheit, eine saubere Umwelt, reichlich Sonne und riesige Zitrusplantagen. Die beiden konkurrierenden Eisenbahngesellschaften unterboten sich mit Niedrigpreisen für Tickets von der Ostküste und trugen damit dazu bei, Ansiedler nach Los Angeles zu locken. Ab 1890 wurdeKohlegefördert. Als weitaus wichtiger für die weitere Entwicklung erwiesen sich die umfangreichenErdölvorräteunter der Stadt, die am 20. April 1893 erstmals in der Nähe des heutigenDodger Stadiumangebohrt wurden[37]. Von da an breiteten sich dieBohrtürmein kurzer Zeit in weiten Teilen der Region aus. In der ersten Hälfte des 20. Jahrhunderts wurde Los Angeles zu einem der wichtigsten Zentren der Erdölförderung. 1923 wurde ein Viertel der Weltfördermenge in der Region Los Angeles gefördert. Auch heute noch spielt die Ölförderung eine gewisse Rolle. Um das enorme Wachstum auch für die weitere Zukunft sicherzustellen, machten sich die Verantwortlichen der Stadt in den 1890er Jahren Gedanken um die Wasserversorgung von Los Angeles. Für eine in einerhalbtrockenen Klimazonegelegenen Stadt war und ist dies ein sehr wichtiges Thema. Bis dahin wurde Los Angeles durch den nahegelegenenLos Angeles Rivermit Trink- und Brauchwasser versorgt. Als dessen Wasservorräte nicht mehr ausreichten, wurde 1913 im Norden das ersteLos-Angeles-Aquädukterrichtet, das Wasser aus dem 300 Kilometer entferntenOwens Valleynach Los Angeles beförderte. Mithilfe der billigen Trinkwasserversorgung war Los Angeles in den Jahren vor und nach 1910 in der Lage, zahlreiche Umlandgemeinden einzugemeinden, darunterWilmington,San Pedro(beide 1909) undHollywood(1910). 1910 entdeckten Filmproduzenten die Region Los Angeles als ideales Produktionsgelände und zogen vonNew YorkundChicagonach Hollywood. In den folgenden Jahren hattenGilbert M. Andersonmit seinenWesternundMack Sennettmit seinenSlapstick-Komödien weltweit große Erfolge. Als 1927 der FilmDer Jazzsängerdem Tonfilm zum Durchbruch verhalf, setzte erneut ein Aufschwung ein. Die Filmindustrie mit Sitz in Hollywood wurde zu einem der wichtigsten Wirtschaftszweige in den Vereinigten Staaten und lockte viele Neuankömmlinge nach Los Angeles. Der erste Monumentalfilm in Farbe –David O. SelznicksFilmVom Winde verweht– kam 1939 in die Kinos und gewann zehnOscars. In den 1920er Jahren kam es zur Eingemeindung zahlreicher weiterer Nachbargemeinden wieSawtelle(1922),Hyde Park(1923),Eagle Rock(1923),Venice(1925),Watts(1926),Barnes City(1927) undTujunga(1932).Santa MonicaundBeverly Hills, die wirtschaftlich und kulturell als Teile von Los Angeles betrachtet werden, konnten verwaltungsmäßig ihre Selbständigkeit aber bis heute bewahren. Zu den wichtigsten Ereignissen der 1930er Jahre zählen dieOlympischen Spiele des Jahres 1932. Erstmals in der Geschichte der Spiele wurde zu diesem Anlass für die männlichen Teilnehmer einOlympisches Dorfgebaut. Heute ist das Dorf ein Teil des StadtteilsBaldwin Hills. Im gleichen Jahr 1932 stieg die Einwohnerzahl von Los Angeles auf über eine Million. Bereits Mitte des 20. Jahrhunderts war das kleine Vorstadthaus mit Swimming Pool und Doppelgarage zum dauerhaften Symbol der Stadt geworden. Ein großer Aufschwung setzte während desZweiten Weltkriegsmit der Luft- und Raumfahrtindustrie ein, die sich kriegsbedingt in großer Anzahl im Großraum Los Angeles ansiedelten. Zahlreiche deutschsprachige Kulturschaffende und Intellektuelle wählten Los Angeles als Zufluchtsort vor demNS-Regime. Dazu zählten unter anderemBertolt Brecht,Marlene Dietrich,Lion Feuchtwanger,Otto Klemperer,Fritz Lang,Ernst Lubitsch,Heinrich Mann,Thomas Mann,Luise RainerundBilly Wilder. Auch viele Künstler aus anderen Ländern fanden hier eine neue Heimat, darunterLuis Buñuel,Jean Renoir,Igor StrawinskyundArturo Toscanini. Zahlreiche große Werke der Dichter, Dirigenten, Regisseure und Maler entstanden im Exil; die Emigranten machten das Los Angeles der1940erJahre zu einem lebendigen Zentrum der europäischen Kultur. Auch mit rassistischer Gewalt hatte die Stadt weiterhin zu kämpfen. 1943 kam es zu einer Serie von Unruhen, die alsZoot Suit Riotsin die Geschichte eingingen. Sie entflammten zwischen den in der Stadt stationierten Soldaten und mexikanisch-amerikanischen Jugendbanden. Bereits in den1920erJahren wurde trotz eines leistungsfähigen Straßenbahnnetzes das Automobil zum bevorzugten Verkehrsmittel derAngelenos. Damals gab es in Los Angeles mehr Autos pro Einwohner als in jeder anderenUS-Großstadt. Am 1. Januar 1940 wurde derArroyo Seco Parkway, die erste Stadtautobahn, zwischenPasadenaund dem nördlichen Stadtzentrum eröffnet. Damit war der Kurs für die weitere Entwicklung geprägt. Nach dem Krieg kauften Unternehmen wieGeneral Motors,Greyhound Linesund dieFirestone Tire & Rubber Companydas Straßenbahnnetz derLos Angeles Railwayauf und demontierten es anschließend – dieser Vorgang ging als dergroße amerikanische Straßenbahnskandalin die Geschichte ein. An dessen Stelle entstand ein weitverzweigtes Autobahnnetz. Nach der Prämisse der 1950er Jahre sollte kein Bürger mehr als sechs Kilometer von einer Autobahnauffahrt leben. 1960 wurde das erste Bürohochhaus in die ehemals niedrige Silhouette der Stadt gebaut, deren höchstes Bauwerk bis dahin das Rathaus (City Hall) von 1928 war. Im August 1965 kam es zu schwerenRassenunruhenin Los Angeles’ südlichem StadtteilWatts. Während derWatts-Unruhenstarben innerhalb von sechs Tagen 32 Menschen und 874 wurden verletzt. Der Sachschaden betrug 45 Millionen US-Dollar. Der stark zunehmende Autoverkehr verursachte ab Ende der 1970er Jahre zunehmend Probleme.Smog, Dezentralisierung und eine stark steigende Kriminalität prägten das Bild der Stadt. Seit Anfang der 1990er Jahre arbeitet LA wieder am Bau einesStadtbahnnetzes, nachdem derÖffentliche Nahverkehrder Millionenmetropole fast drei Jahrzehnte lang mit Bussen auskommen musste. Der finanzielle und bürokratische Aufwand zum Bau derMetroist jedoch ungleich größer als beim Bau des ursprünglichen Netzes, da nicht nur die Stadt heute sehr viel dichter bebaut ist, sondern beim Bau der unterirdischen Strecken auch Rücksicht aufErdbebengenommen werden muss. DieUnruhen in Los Angelesvom 29. April bis 2. Mai 1992 waren eine der größten Rassenunruhen in derGeschichte der USA. Auslöser war der Freispruch von vier Polizisten weißer Hautfarbe, die der Misshandlung des AfroamerikanersRodney Kingangeklagt waren. Bei den Unruhen starben 53 Menschen, 2.383 wurden verletzt, der Sachschaden betrug 800 Millionen US-Dollar.[38] Mit dem Ende desKalten Krieges1990 war ein wichtiger Wirtschaftszweig der Stadt, die Luft- und Raumfahrtindustrie, stark betroffen. Seit denTerroranschlägen am 11. September 2001, der Neuausrichtung der Verteidigungsstrategie zur Bekämpfung desTerrorismusund der Radikalisierung desPutin-Regimeshat dieser Industriezweig wieder eine große wirtschaftliche und strategische Bedeutung für die USA. Anfang des 21. Jahrhunderts ist der Status der Stadt als kulturelles und wirtschaftliches Zentrum der Region unumstritten. Probleme, mit denen die Stadt zu kämpfen hat, sind die Arbeitslosigkeit, der enorme Verkehr, Umweltbelastungen und Bandenkriminalität. In Los Angeles leben Menschen aus insgesamt 140 Ländern, die 224 verschiedene Sprachen sprechen. DieVolkszählung von 2020ergab rund 3,9 Millionen Einwohner. Die größte Bevölkerungsgruppe stellten die 1,8 MillionenLatinosmit einem Anteil von 48,5 % an der Gesamtbevölkerung der Stadt, 28,6 % der Einwohner gehörten der(nicht hispanischen)weißen Bevölkerungsgruppe an. Eine ebenfalls stark zunehmende Bevölkerungsgruppe bilden die Asiaten, überwiegend Chinesen, Japaner, Koreaner, Vietnamesen, Thais und Filipinos. Zusammen mit den aus Iran stammenden, häufig jüdischen, Einwohnern (mehr als eine halbe Million[39][40]) hatten sie einen Anteil von 11,6 % an der Gesamtbevölkerung der Stadt. Die Afroamerikaner hatten einen Anteil von 8,9 %, dieUreinwohnerbildeten mit 0,7 % bzw. 28.215 Personen die kleinste Bevölkerungsgruppe.[41]In 60 % der Haushalte wird eine andere Sprache als Englisch gesprochen, hauptsächlich Spanisch. Aufgrund eines englischsprachigen Lebensumfelds, wie in der Schule oder in den Medien, spricht und versteht ein Großteil der in den USA geborenen Latinos jedoch gutes Englisch.[4][42][43] Obgleich sich in der Stadt und imLos Angeles Countygenerell die Kulturen stark mischen, gibt es wie in alleninternationalen StädtenundMetropolregionenmit multikultureller Bevölkerung eine gewisse Aufteilung der Wohngegenden nachethnischeroder kultureller Herkunft. Fast immer gehen solche „Aufteilungen“ fließend ineinander über und verschmelzen miteinander. Stadtgebiete mit einem sehr hohen Anteil anLatinossind klassischerweiseEast Los AngelesundMontebello, in welchen sich seit den 1970er Jahren, und teilweise auch schon zu Anfang des Jahrhunderts, immer mehr Menschen aus Lateinamerika, speziell Mexiko, aber auch El Salvador oder Honduras niedergelassen haben. Zudem istSouth Los Angeles, auchSouth Centralgenannt, ein Bezirk mit sehr großer hispanischer Bevölkerung, deren Anteil in manchen östlich gelegenen Vierteln nicht selten bei über 90 % liegt. Viertel und Städte wieHuntington Park,South Gate,Lynnwood,Bell (Kalifornien),Bell GardenssowieLennox,HawthorneundWestlakesind ebenfalls sehr stark hispanisch geprägt. Im westlichen South Los Angeles leben vieleAfroamerikaner. Ebenso sind Stadtgebiete beziehungsweise Städte wieInglewood,Crenshaw,Leimert Park,WestmontundComptonmehrheitlich von Schwarzen bewohnt.Asiaten, hauptsächlich Ost- und Südostasiaten wie Chinesen, Taiwaner und Vietnamesen chinesischer Abstammung leben zum Großteil inMonterey Parkund in der Gegend umAlhambraim westlichen San Gabriel Valley. Darüber hinaus gibt es signifikante Bevölkerungsanteile von Asiaten in den Städten des L.A. County wieTorrance,Carson,GardenasowieKoreatownundChinatown. Die Mehrheit der weißen Bevölkerung lebt entlang der Pazifischen Küste vonRancho Palos VerdesüberRedondo Beach,Manhattan Beach,El Segundo,Marina del Reybis einschließlichSanta Monicasowie zwischenLakewoodund dem östlichenLong Beach. Des Weiteren lebt eine größere weiße Bevölkerung im Bereich zwischen Santa Monica undHollywood. Die Gebiete dazwischen umfassen West Los Angeles,Culver City,Beverly Hills,West HollywoodundHancock Park. Die auf und am Rand derSanta Monica Mountainsgelegenen Viertel wiePacific Palisades,Brentwood,Westwood,Bel Air,Hollywood Hills,Loz FelizundSilver Lakehaben ebenfalls zum Großteil weiße Einwohner.[44] Los Angeles ist die Heimat von Anhängern vieler Religionen. Unter den mehr als 100christlichenGlaubensrichtungen ist dierömisch-katholische Kircheaufgrund der hohen Zahl anHispanicsdominierend. Weitere wichtige christlicheKonfessionensind dieAdventisten,Baptisten,Lutheraner,Methodisten,OrthodoxenundZeugen Jehovas. Kleinere religiöse Gruppen sind Anhänger derBahai-Religion, desBuddhismus,Hinduismus,Islam, derjüdischen Religion, desMormonentums,Sikhismus,Sufismusund desZoroastrismus. Die römisch-katholische Kirche der Region ist imErzbistum Los Angelesorganisiert. Es wurde am 1. Juni 1922 als Bistum Los Angeles–San Diego errichtet und am 11. Juli 1936 zumErzbistumerhoben. Mit 4,2 Millionen Gläubigen (2004) gilt es als das größte Bistum in den Vereinigten Staaten.[45]Es ist einer der typischenKardinalssitzeder Katholischen Kirche, dessenErzbischofzur Erledigung seiner Hirtenpflichten sechsWeihbischöfezur Verfügung stehen. Die Kirchenprovinz umfasst heute die BistümerFresno,Monterey,Orange,San BernardinoundSan Diego. Erzbischof von Los Angeles ist seit 2011José Horacio Gómez. Die 2002 nach Plänen des spanischen ArchitektenRafael MoneofertiggestellteKathedrale Unserer Lieben Frau von den Engeln(Cathedral of Our Lady of the Angels)ist die Bischofskirche des Erzbistums. In der Stadt steht mit dem 1956 errichteten „Los Angeles California Temple“ das (nach dem Salt Lake Temple) zweitgrößte Heiligtum derKirche Jesu Christi der Heiligen der Letzten Tage(Mormonen). Der Tempel ist 112 Meter lang, 82 Meter breit und besitzt eine Höhe von 78 Metern. Auf der Spitze des Tempelturmes ist eine Statue des Engels und ProphetenMoroniangebracht.[46] Los Angeles verfügt mit 490.000 Personen (2001) über die (nachNew YorkundMiami) drittgrößte jüdische Gemeinde in den Vereinigten Staaten.[47]VieleSynagogenbefinden sich in der Stadt, die meisten im San Fernando Valley und in West Los Angeles. Aufgrund der vielen Einwanderer aus Asien verzeichnete die buddhistische Gemeinde in den letzten Jahren einen starken Zuwachs. Los Angeles hat gegenwärtig die größte buddhistische Bevölkerung in den Vereinigten Staaten und beherbergt eine große Vielzahl anSchulen und Systemen des Buddhismus. Es gibt mehr als 300 buddhistische Tempel über die Stadt verteilt. In Los Angeles praktizierten seit Anfang des 20. Jahrhunderts zahlreiche Hindu-SwamisundGurus. DieSelf-Realization Fellowship(Gemeinschaft der Selbst-Verwirklichung)hat ihren Sitz inMount Washingtonund besitzt einen Tempel inHollywoodund einen privaten Park inPacific Palisades. Die gemeinnützige religiöse Gesellschaft wurde 1920 vonParamahansa Yoganandagegründet. Hier errichtete auch der Gründer derHare-Krishna-Bewegung(ISKCON),Bhaktivedanta Swami, deren weltweites Hauptzentrum ab 1968. ISKCONsKrishna-Tempelin der Watseka-Avenue ist täglich geöffnet und wird von vielen Hindus indischer Abstammung unterstützt. Am WagenfestRatha Yatraam Venice Beach nehmen in jedem Juli mehrere zehntausend Menschen teil. Die Bevölkerungsexplosion im 20. Jahrhundert – 1900 hatte die Stadt erst 102.000 Einwohner – traf Los Angeles gänzlich unvorbereitet. Neue Gebäude wie Einfamilienhäuser, Handelsgebäude oder Einkaufszentren wurden meist als Flachbauten errichtet, wodurch sich die Stadt enorm in der Fläche ausdehnte. Die Einwohnerzahl nimmt auch heute noch kontinuierlich zu. Eine bedeutende Rolle spielt dabei auch die Zuwanderung ausLateinamerika, insbesondere ausMexiko, sowie auch ausAsien. Mehr als ein Drittel der Bevölkerung von Los Angeles wurde außerhalb der USA geboren. Die folgende Übersicht zeigt die Einwohnerzahlen der Stadt nach dem jeweiligen Gebietsstand. Für die Jahre 1800 bis 2010 sind die Volkszählungsergebnisse angegeben. Für den 1. Juli 2013 liegt die Schätzung des United States Census Bureau bei 3.884.307 Einwohnern. Dies entspricht einem sehr hohen Wachstum verglichen mit dem Zeitraum 1990 bis 2010.[4] Probleme bereitet die hohe Kriminalität, darunter besonders die Bandenkriminalität. Los Angeles zählt zu den Städten mit der höchsten Zahl an Straßenbanden. Nach einer Schätzung desLos Angeles Police Departments(LAPD) sind in der Stadt mehr als 400 Gangs mit mindestens 39.000 Mitgliedern aktiv. Ihnen stehen 350 Sonderbeamte der Polizei gegenüber, die auf Bandenkriminalität spezialisiert sind. Einige Gegenden gelten wegen der Straßenbanden besonders in der Nacht als sehr gefährlich. Dazu zählen vor allem viele Stadtteile im BezirkSouth Los Angelessowie die VororteComptonundLynwood. Die Mordrate ist in Compton etwa sechsmal höher als im US-weiten Landesdurchschnitt.[48] Insgesamt wurden 2006 in Los Angeles 132.034 Straftaten registriert (2005 = 142.506 und 2004 = 163.626). Die Gesamt-Kriminalitätsrate sank zum Vergleichszeitraum 2005 um 7,3 Prozent, gegenüber 2004 um 19,3 Prozent. In den einzelnen Sparten verlief die Entwicklung 2006 folgendermaßen (in Klammern Rückgang gegenüber 2005): 481 Morde (−1,8 Prozent), 903 Vergewaltigungen (−7,2 Prozent), 14.235 Raubüberfälle (+5,5 Prozent), 14.118 schwere Körperverletzungen (−9,1 Prozent), 20.020 Einbruchdiebstähle in Privatwohnungen und -gebäuden (−8,2 Prozent), 29.911 Diebstähle aus Kraftfahrzeugen (−8,8 Prozent), 27.779 Diebstähle persönlicher Sachen/andere Diebstähle (−9,3 Prozent) und 24.587 Autodiebstähle (−8,1 Prozent).[49] An der Spitze derStadtverwaltung von Los Angelessteht der Bürgermeister (Mayor), der von der Bevölkerung für eine Amtszeit von vier Jahren gewählt wird. Er ernennt Leiter der verschiedenen Ressorts der Stadtverwaltung und andere Beamte, in einigen Fällen auch mit Zustimmung des Stadtrats. Der Bürgermeister besitzt das Recht, gegen Verordnungen seinVetoeinzulegen. Dem Stadtrat(City Council)von Los Angeles gehören 15 District-Council-Mitglieder an. Er gliedert sich regional nach den Bezirken(Districts)der Stadt. Die Stadtverordnetenversammlung verabschiedet Gesetze, legt die Grundsteuer fest und teilt den einzelnen Ressorts der Stadtverwaltung ihre finanziellen Mittel zu. Stadtratswahlen finden ebenfalls alle vier Jahre statt. Tom Bradleywar zwischen 1973 und 1993 der erste und bislang einzige schwarze Bürgermeister der Stadt und mit fünf Wahlperioden zudem der längstamtierende. Während seiner Amtszeit hatte sich Los Angeles stark verändert und war zu einer Weltstadt geworden. Ihm sind ein Stern auf demHollywood Walk of Fameund derTom Bradley International TerminalamLos Angeles International Airportgewidmet. Die Bürgermeisterwahlen 2005 gewannAntonio Villaraigosamit großem Vorsprung. Er übernahm am 1. Juli 2005 das Amt von seinem VorgängerJames Hahnund ist damit der erstehispanischeBürgermeister von Los Angeles seit den frühen 1870er Jahren. Als bis dahin letzter Latino-Bürgermeister der Stadt war Cristóbal Aguilar im Jahre 1872 aus dem Amt geschieden.[50] Villaraigosa nahm schon kurz nach seiner Amtseinführung den Kampf gegen die verbreiteteKorruptionauf, indem er beispielsweise alleLobbyistenaus den städtischen Kommissionen entfernte. Wichtige Themen während seiner Amtszeit waren unter anderem der Bau von Sozialwohnungen und die Anlage öffentlicher Grünflächen. Erfolge erzielte er in der öffentlichen Sicherheit. Die Kriminalitätsrate ging zwischen 2005 und 2015 um 21,8 Prozent zurück, mit zuletzt wieder steigender Tendenz. Beispielsweise hatte es im Jahr 1992 noch 1094 Morde in Los Angeles gegeben. Seit sechs Jahren (Stand 2015) sind es weniger als 300 Morde pro Jahr, zuletzt mit einem Anstieg von 260 (2014) auf 283 (2015).[51] Am 21. Mai 2013 avancierteEric Garcettizum ersten jüdischen Bürgermeister der Stadt und war mit 42 Jahren zudem der jüngste Bürgermeister seit über einem Jahrhundert. Seit dem 12. Dezember 2022 bekleidetKaren Bassals erste Frau das Amt der Bürgermeisterin von Los Angeles. DieFlagge von Los Angeleswurde 1931 zum 150-jährigen Jubiläum der Stadt entworfen. Sie besteht aus drei eingekerbten vertikalen Streifen in den Farben Grün, Gold und Rot, die die wichtigsten Früchte repräsentieren, die in Kalifornien angebaut werden: Oliven, Orangen und Trauben. In der Mitte ist dasStadtsiegelabgebildet. Das Siegel enthält das Wappen der Stadt Los Angeles, bestehend aus vier Feldern. Die vier Felder beinhalten denWappenschild der Vereinigten Staaten, dieFlagge Kaliforniens, dasWappen Mexikosin der Version von 1867 bis 1968 und die Wappen vonKastilienundLeón. Die beiden letzten stehen für die Geschichte der Stadt unter Mexiko, resp. als spanische Kolonie. Umgeben ist das Wappen von den bereits oben genannten Früchten sowie vom gesetzmäßigen Namen der Stadt(City of Los Angeles)und dem Gründungsdatum(1781). Los Angeles unterhält Partnerschaften mit folgenden Städten:[52] Los Angeles unterhält Freundschaften mit folgenden Städten:[53] Am 23. Oktober 2003 eröffnete die vonFrank GehryentworfeneWalt Disney Concert Hall. In dem Gebäude sind dasLos Angeles Philharmonic Orchestraunter der Leitung von Gustavo Dudamel und dieLA Master Chorälebeheimatet. Der Bau der Konzerthalle ist auf eineöffentlich-private Partnerschafts-Initiative zwischenLillian Disney(1899–1997), weiteren Mitgliedern der Disney-Familie sowie Sponsoren und dem Regierungsbezirk von Los Angeles zurückzuführen. DieLos Angeles Operahat ihre Spielstätte imDorothy Chandler Pavilion, einem Teil desLos Angeles Music Center. In einer Hügellandschaft inHollywoodliegt die 1922 eröffneteHollywood Bowl, ein natürlichesAmphitheater. Das Hollywood Bowl Orchestra hat hier seinen Sitz, während der Sommersaison auch das Los Angeles Philharmonic Orchestra. DieBeatles,Frank SinatraundMikhail Baryshnikovsind nur einige der legendären Künstler, die hier aufgetreten sind. Diverse Konzerte finden dort in den Sommermonaten statt und der Zutritt auf das Gelände ist täglich über das ganze Jahr kostenfrei möglich. ImTheater Districtam Broadway befinden sich zahlreiche große Filmtheater. Das 1893 erbauteBradbury Buildingmit seinem sonnendurchfluteten Atrium und stilvollen, schmiedeeisernen Balkonen war in dem FilmBlade Runnerzu sehen, genau wie das benachbarte KinoMillion Dollar Theateraus dem Jahre 1918, das inzwischen einen allgemein zugänglichen Kirchengemeindesaal beherbergt. Das erste Kino der Vereinigten Staaten, dasElectric Theatre, eröffnete 1902 in Los Angeles. Das noch extravagantereLos Angeles Theaterwurde 1931 in nur 90 Tagen für die Uraufführung vonCharlie ChaplinsFilmLichter der Großstadtfertiggestellt und lädt jedes Jahr im Juni anlässlich des FilmfestivalsLast Remaining SeatsCineasten in seinen Vorführsaal. Zu den regelmäßig genutzten Traditionskinos gehört dasOrpheum, ein prachtvolles Beispiel derNeorenaissancemit ausladenden Treppen und prunkvollen Kronleuchtern. Auf demHollywood Boulevardsteht dasGrauman’s Chinese Theatre. Es wurde 1927 im Stil einer chinesischen Pagode eröffnet. Weltberühmt wurde das Kino durch die Hand- und Schuhabdrücke zahlreicher Filmstars, die in Zementblöcken im Eingangsbereich des Kinos verewigt wurden. Nahe befinden sich das 1922 in einem pseudo-ägyptischen Stil eröffneteGrauman’s Egyptian Theatreund dasDolby Theatre(bis Februar 2012 Kodak Theatre), in welchem seit dem Jahr 2002 dieOscars(Academy Awards) derAcademy of Motion Picture Arts and Sciencesvergeben werden. Los Angeles und der Großraum Los Angeles sind Standort für Hunderte von Museen und Sammlungen mit unterschiedlichen Schwerpunkten.[54] Das größte und ältesteKunstmuseumvon Los Angeles ist dasLos Angeles County Museum of Artam Wilshire Boulevard, gegründet 1910. Es beherbergt eine Sammlung von über 100.000 Werken von derAntikebis zurGegenwart. Damit ist es das größte Museum der Vereinigten Staaten westlich vonChicago. Besonders umfangreich und bedeutungsvoll sind die Sammlungen zur amerikanischen,islamischenund koreanischen Kunst. Ein weiteres wichtiges Kunstmuseum ist dasJ. Paul Getty Museum. Es wurde 1953 von dem Öl-MilliardärJ. Paul Gettybegründet und befand sich ursprünglich in einem Nachbau derVilla dei PapirivonHerculaneumin Malibu (Getty Villa). Seit 1997 befindet es sich imGetty Centerin Brentwood, einem Distrikt von West Los Angeles in denSanta Monica Mountains, zusammen mit weiteren wissenschaftlichen Einrichtungen, wie demGetty Research Institute, demGetty Conservation Institute, das auf dem Gebiet derRestaurierungtätig ist und demGetty Leadership Institute, das sich um die Aus- und Fortbildung von Führungspersonal für Museen kümmert. Entworfen wurde der Gebäudekomplex von dem ArchitektenRichard Meier, der bereits Mitte der 1980er Jahre mit dem Entwurf beauftragt wurde. Die Bauarbeiten begannen im Jahre 1991 und dauerten bis 1997. Das kostenlos zugängliche Museum sammelt Antiken, Zeichnungen, Buchmalerei, Gemälde, Photographie sowie Skulptur und Kunsthandwerk. AmCalifornia Plaza, an der Grand Avenue, steht das 1986 eröffneteMuseum of Contemporary Art. Der Komplex ist ein verspieltes Arrangement aus rötlichen geometrischen Blöcken, entworfen vom ArchitektenIsozaki Arata, der ihn als „kleines Dorf im Tal der Wolkenkratzer“ bezeichnete. Es beherbergt eine eindrucksvolle Sammlung von Gemälden und Skulpturen vonFranz Kline,Mark Rothko,Robert Rauschenberg,Claes OldenburgundAntoni Tàpies. Das imExposition Parkstehende und 1913 gegründeteNatural History Museum of Los Angeles Countyist das drittgrößte naturhistorische Museum in den Vereinigten Staaten. Es verfügt über bedeutende Sammlungen zu Fauna und Flora, inklusiveDinosauriern, aber auch zur Stadtgeschichte.[55]In der Nähe ist auch der Standort desCalifornia Science Centers.[56]Zu den naturkundlichen Museen gehört auch dasGeorge C. Page Museuman denLa Brea Tar Pitsam Wilshire Boulevard. Zu den zahlreichen Museen im Großraum Los Angeles gehören dasHammer Museum(mit einer Sammlung von vornehmlich alten Meistern und französischen Impressionisten), dasSouthwest Museum(es beherbergt hauptsächlich indianische Ausstellungsstücke), dasLos Angeles Museum of the Holocaust, dasMuseum of Jurassic Technologyund dieHuntington Library(mit einer Sammlung von Manuskripten, Büchern und insbesondere Gemälden). InSanta Monicabefindet sich dasSanta Monica Museum of Artin der Michigan Avenue mit einer Sammlung zeitgenössischer Kunst. DasGeneral Phineas Banning Residence Museumim StadtteilWilmington, untergebracht in einem Gebäude imGreek-Revival-Stil, beinhaltet eine Sammlung von Postkutschen aus dem 19. Jh. und eine Ausstellung über die Geschichte des Hafens von Wilmington. Das Civic Center beherbergt verschiedene Regierungsgebäude (einschließlich der City Hall, des Rathauses von Los Angeles) sowie dasLos Angeles Music Centermit der KonzerthalleDorothy Chandler Pavilion, demAhmanson Theatre, demMark Taper Forumund derWalt Disney Concert Hall. Im September 2002 wurde im Zentrum der Stadt die katholische KathedraleOur Lady of the Angelseingeweiht; der als postmoderne Interpretation einer spanischen Missionsstation gestaltete Kirchenbau mit einem 40 Meter hohen Glockenturm ist Teil eines Kulturareals, das die Innenstadt neu beleben soll. Interessant sind auch derFarmers Market, Chinatown,Little Tokyosowie das nahe gelegeneCulver City. In dem VorortInglewoodbefindet sich das Great Western Forum. Für Filminteressierte einen Besuch wert sind dieUniversal Studiosin Universal City, dieWarner Bros.Studios in Burbank undCBSTelevision City in Hollywood. DasHollywood Sign(Hollywood-Schild) liegt im Norden der Stadt und ist vomGriffith Observatorygut zu sehen. Auf demHollywood Boulevardim Stadtzentrum befinden sich viele Theater und Kinos (Grauman’s Chinese Theatre,Grauman’s Egyptian Theatre,Kodak Theatreund andere) sowie der berühmteHollywood Walk of Fame, auf dem zahlreiche Sterne eingelassen wurden, um Prominente zu ehren. Weitere Sehenswürdigkeiten sind der 335 Meter hoheWilshire Grand Tower(daszehnthöchste Gebäude der USAund das höchste westlich desMississippis), derU.S. Bank Tower, dasAon Center, dieBank of America Plaza, derCity National Tower(früher Bank of America Tower) und dasStaples Center(alle in Downtown). In Wilshire befindet sich auch dieFirst Congregational Churchmit der zweitgrößten Orgel der Welt. In der Alameda Street in Downtown befindet sich die 1939 fertiggestellteLos Angeles Union Station, der Hauptbahnhof von Los Angeles. Der Baustil kombiniert spanischen Kolonialstil, Mission Revival und spätenArt décomit maurischen Architekturelementen. DieLos Angeles Public Libraryin der Fifth Street ist ein eher verstecktes architektonisches Juwel. Die Bibliothek bietet unter anderem alte bemalte Holzdecken und eine kirchenartige Turmhalle. Im Broadway Theater District steht das 13-stöckigeEastern Columbia Building. DasArt-déco-Haus wurde 1930 nach Plänen des ArchitektenClaud Beelmaneröffnet. Die markantenWatts Towersbefinden sich im StadtteilWattsund dasCapitol Records Buildingin Hollywood.The Magic Castleist das private Klubhaus derAcademy of Magical Arts, eines Zauberkünstlerklubs in derFranklin Avenuein Hollywood. Vom Getty Center,Dodger Stadiumin Elysian Park oder Mount Wilson bieten sich bei gutem Wetter weitreichende Aussichten über das Stadtgebiet. DasEncounter RestaurantamLos Angeles International Airportgilt unter Angelenos als das heimliche Wahrzeichen der Stadt. Die bekanntesten Hotels in Los Angeles sind dasHotel Beverly Wilshire(erbaut 1928), dasChateau Marmont HotelinWest Hollywood(erbaut 1929 nach dem Vorbild desChateau Amboiseim französischenLoiretal), dasBeverly Hilton Hotel(1955), dasMondrian Hotel(1959) in West Hollywood und dasWestin Bonaventure Hotel(1976) in Downtown. In Los Angeles und Umgebung befinden sich zahlreiche Meisterwerke der Villenarchitektur. Dazu gehören unter anderem dasGamble Houseder Architekten Charles und Henry Green,Richard NeutrasLovell Houseund die BautenIrving Gills. Viele der Gebäude des in Europa nahezu unbekannten Architekten Irving Gill sind zerstört oder weitestgehend verändert worden, was zu fortschreitenden Verlusten seines Schaffens führt. Weitere Beispiele der Wohnarchitektur sind dasSchindler Housedes ArchitektenRudolph Michael Schindlerin West Hollywood sowieFrank Lloyd WrightsHollyhock HouseinLittle ArmeniaundEnnis Housein Los Feliz. Besonders dasHollyhock Housezählt zu den bedeutendsten Werken Wrights. Das Haus erinnert mit seinen glatten Putzflächen und Ornamenten aus Betonfertigteilen an einen Maya-Tempel aus Mittelamerika. Im Hof befinden sich Wasserspiele in der Form eines Amphitheaters. Weitere bemerkenswerte Gebäude stammen vonJohn Lautner, einem Schüler Frank Lloyd Wrights. Berühmtestes Beispiel ist die 1960 entstandeneMalin Residence, auchChemospheregenannt. Das auf einer steilen Böschung über dem San Fernando Valley erbaute Gebäude erinnert in der Form an ein achtkantigesUFO, wobei ein einziger Betonpfeiler die Konstruktion trägt, was tatsächlich den Effekt des Schwebens verstärkt. Weit verstreut über das Stadtgebiet finden sich ferner die sogenanntenConcept Houses, darunterPierre KoenigsBailey House, auch bekannt alsCase Study HouseNo. 21und dasStahl House(Case Study House No. 22). Beide Häuser wurden auf zuvor als unbebaubar gehaltenen Grundstücken errichtet. Diese können ebenso besichtigt werden wie dasEames Housedes DesignersCharles Eamesund seiner Ehefrau, der KünstlerinRay Eames, inSanta Monica. Die Parks in Los Angeles und Umgebung enthalten zahlreiche Freizeit- und Kultureinrichtungen. ImEl Pueblo de Los Angeles State Historic Parkliegt die 1822 fertiggestellte Plaza Church. ImGriffith Park, dem bei weitem größten Park von Los Angeles, befinden sich dasGene Autry Western Heritage Museum, einObservatorium, einPlanetariumund derZoo von Los Angeles. DerExposition Parkbeheimatet dasLos Angeles Memorial Coliseum, ein Museum für Wissenschaft und Industrie sowie ein Naturhistorisches Museum. Im zentral gelegenenElysian Parksteht dasDodger Stadium(Chavez Ravine) und außerdem noch die L.A. Police Academy. Ebenfalls direkt im Zentrum von Los Angeles liegt derEcho Park. Dort diente der Echo Park Lake als Kulisse im erfolgreichen FilmChinatown. Nicht nur für Kinder interessant sind die FreizeitparksDisneylandundDisney’s California Adventuremit dem 30 Meter hohenGrizzly Peakim nahegelegenenAnaheim. In der Nähe der Stadt gibt es denAngeles National Forestund die endlosen Pazifikstrände – die Küste von Los Angeles erstreckt sich über etwa 115 Kilometer – inSanta Monicaoder auchVenice Beach. ImHancock Parkvon Los Angeles befinden sich dieLa Brea Tar Pits, eine Ansammlung von mit natürlichemAsphaltgefüllter Gruben unterschiedlicher Größe. Die asphaltreichen Sedimente von Rancho La Brea sind bekannt als eine der anFossilienreichsten Fundstellen aus demPleistozänoder Eiszeitalter. DasGeorge C. Page Museumbeherbergt eine Ausstellung von Tierskeletten, darunter eine umfangreiche Sammlung von Schädeln desAenocyon dirus, eines ausgestorbenen wolfsähnlichen Hundes. Ebenfalls im Hancock Park steht dasLos Angeles County Museum of Artmit seiner Kunstsammlung. DieUniversal Studios Hollywoodsind ein Themenpark, der sich in zwei Bereiche unterteilt – den „Upper Lot“ und den „Lower Lot“. Beide sind verbunden durch die weltweit größte und höchsteRolltreppenkonstruktion, die sich über mehrere Ebenen erstreckt. Der Upper Lot erstreckt sich vom Eingang des Parks bis zumThe Simpsons Ride, bei welchem auch die Rolltreppen in den unteren Parkteil führen. Neben aufwendigen Live-Shows bietet der Upper Lot auch die eigentlicheStudio Tram Tour, die eine Hauptattraktion darstellt. Der Lower Lot liegt mitten in den eigentlichen Studios und bietet drei Attraktionen, denJurassic Park – The Ride,Transformers the Ride 3DundRevenge of the Mummy. Zu den beliebtesten Sportarten in Los Angeles gehören vor allem die MannschaftssportartenAmerican Football,Baseball,BasketballundEishockey. Sehr verbreitet sind auchFußball,LacrosseundVolleyball. Individualsportarten wieBowling,Golf,Leichtathletik,SchwimmenundTennissind gleichfalls sehr populär. Los Angeles war Austragungsort derOlympischen Sommerspiele 1932und derSommerspiele 1984und wird2028die Spiele zum dritten Male ausrichten. Zu den vielen Sportveranstaltungen der Weltstadt zählen unter anderem der jährlich ausgetrageneLos-Angeles-Marathonund das TennisturnierCountrywide ClassicderATP Tour. Zahlreiche bekannte Sportvereine sind in Los Angeles und Umgebung beheimatet. Im College Football sind mit denUCLA Bruinsund denUSC Trojanszwei erfolgreiche Mannschaften in Los Angeles angesiedelt. Beide Teams spielen in derPacific-12 ConferencederNational Collegiate Athletic Association. Im Profi-Football wurde bereits 1926 mit denLos Angeles Buccaneerseine Mannschaft für dieNational Football Leaguein der Stadt gegründet. Diese Mannschaft konnte jedoch keine Heimspiele bestreiten und spielte nur eine Saison. Ab derNFL-Saison 1946spielten die ursprünglich in Cleveland ansässigenRamsin Los Angeles. Zur gleichen Zeit nahmen auch die Los Angeles Don der konkurrierenden All-America Football Conference den Spielbetrieb auf. Nach dem Niedergang der AAFC am Ende der Saison 1949, blieben die Rams das einzige professionelle Football-Team in der Stadt. Gespielt wurde imLos Angeles Memorial Coliseum. 1960 wurde in der Stadt die Mannschaft derChargersder neu gegründetenAmerican Football Leagueangesiedelt. Dieses Team zog jedoch in der folgenden Saison nach San Diego um. 1984 verlegte dieOakland Raidersihren Spielbetrieb nach Los Angeles, nachdem sich der Teameigner mit der Stadt Oakland nicht über einen Stadionneubau einigen konnte. Die Raiders spielten ebenfalls im Memorial Coliseum. Da man sich mit der Stadt Los Angeles Anfang der 90er Jahre ebenfalls nicht über den Bau eines neuen Stadions einigen konnte, zogen 1995 die Rams nachSt. Louisund die Raiders zurück nachOakland. Stan Kroenke, neuer Eigentümer der Rams, setzte den Bau eines neuen Stadions in Los Angeles durch, weshalb die Mannschaft 2016 nach Los Angeles zurückkehrte. Mit Beginn derSaison 2017zogen auch dieChargerszurück in die Stadt. Beide Teams spielen seit derSaison 2020im neu errichtetenSoFi StadiuminInglewood. DieAnaheim Duckssind ein Eishockeyklub, der in derNational Hockey League(NHL) spielt. In derSaison 2006/07gewannen sie erstmals denStanley Cupund den Divisiontitel. Weitere Erfolge sind der Gewinn des Conferencetitels in derSaison 2002/03und 2006/07. Der 1992 gegründete Verein trägt seine Heimspiele im 18.000 Zuschauer fassendenHonda Centeraus. DieLos Angeles Kingssind eine Eishockeymannschaft, die in der National Hockey League (NHL) spielt. Die größten Erfolge waren der zweifache Gewinn desStanley Cups, kleinere Erfolge feierte die Mannschaft Anfang der Neunziger. Das Team wurde 1967 gegründet und trägt seine Heimspiele imStaples Center(Kapazität: 18.000 Zuschauer) aus. Von 1988 bis 1996 spielteWayne Gretzky, einer der erfolgreichsten Eishockeyspieler überhaupt, bei den Kings. DieLos Angeles Angels of Anaheimsind ein Baseballteam, das in derMajor League Baseball(MLB) spielt. Größter Erfolg war der Gewinn des Titels derWorld Series 2002. Der 1961 entstandene Verein trägt seine Heimspiele imAngel Stadium of Anaheim(Kapazität: 45.000 Zuschauer) aus. DieLos Angeles Dodgerssind eine Baseballmannschaft, die in der Major League Baseball (MLB) spielt. DieL.A. Dodgersentstanden aus denBrooklyn Dodgersdurch Umzug im Jahre 1959. Bisher konnten sieben Titel derWorld Seriesgewonnen werden. Das Team trägt seine Heimspiele im 56.000 Zuschauer fassendenDodger Stadiumaus. DieLos Angeles Lakerssind ein Team derNational Basketball Association(NBA). Die Mannschaft gewann insgesamt 17 Meistertitel (nur dieBoston Celticsmit 18 Titeln haben mehr) – sowie 29 Conferencetitel und 27 Divisiontitel. Seit den Gründungstagen der Liga gehörten die Lakers zur NBA, und ebenso zu den erfolgreichsten Teams. Heimspielstätte ist das Staples Center. Mit denLos Angeles Clippersspielt ein zweites Team in der National Basketball Association (NBA). DieLos Angeles Sparksspielen in derWomen’s National Basketball Association(WNBA). Die MetropolregionGreater Los Angeles Areabeheimatet mit denLA Galaxyund demLos Angeles FC(LAFC) gleich zwei Vereine derMajor League Soccer(MLS), der höchsten Fußballliga in den USA. DieLos Angeles Galaxytragen ihre Heimspiele in dem an das Stadtgebiet angrenzendeCarson, im 27.000 Personen fassendemDignity Health Sports Parkaus. Das 1995 gegründeteFranchisegehört zu den erfolgreichsten der MLS und ist mit 5 gewonnenenMLS Cupsaktuell Rekordmeister der Liga. Auch auf internationaler Ebene konnte der Klub Erfolge verzeichnen, ist er unter anderem einer der wenigen US-Klubs die ein Finalspiel derCONCACAF Champions Leaguefür sich entscheiden konnten. Seit derSaison 2018ist mit demLos Angeles Football Clubein weiterer Fußballverein in Los Angeles ansässig. Heimstätte des LAFC ist das nahe dem Stadtzentrum gelegeneBanc of California Stadium, welches eine Kapazität von 22.000 Personen aufweist. Auch der LAFC konnte in seiner relativ kurzen Spielzeit bereits einige Erfolge erzielen, so zum Beispiel das Gewinnen desMLS Supporters’ Shieldoder das Erreichen einesCONCACAF-Champions-League-Finales. Mit einer Bewertung von 860 Millionen US-Dollar ist der Los Angeles Football Club aktuell das wertvollste Franchise der MLS.[57] Das Aufeinandertreffen der beiden Klubs ist neben dem Hudson River Derby zwischenRB New YorkundNYCFCdas einzigeDerbyder MLS und wird aufgrund der schlechten Verkehrssituation im Metropolgebiet inoffiziell „El Tráfico“ genannt. Zwischen den beiden Vereinen besteht eine hohe Rivalität, was immer wieder zu kleineren Ausschreitungen im Rahmen des zuvor genanntem „Los Angeles Derby“ führt. Im August 1982 fand im Los Angeles Coliseum vor über 30.000 Zuschauern dasSpeedway-Einzel-WM Finale 1982 statt, das der US-AmerikanerBruce Penhallgewann, womit er seinen WM-Titel verteidigen konnte, ehe er nach diesem Rennen seinen Rücktritt verkündete und als Schauspieler nach Hollywood ging. DieSpecial Olympics World Summer Games 1972fanden in Los Angeles statt. Austragungsorte waren dieUniversity of California(UCLA) und dasSanta Monica College. Die Zahl der Teilnehmer wird in den Quellen mit 2.500,[58]2.600 bzw. 3.000[59]angegeben, die Zahl der teilnehmenden Nationen mit 3.[58]Die ehemaligen ZehnkampfgewinnerBill ToomeyundRafer Johnsonwaren Mitglieder desSpecial Olympics National Advisory Council,der die Spiele organisierte.[60] Die 14. Special Olympics World Summer Games fanden vom 25. Juli bis zum 2. August 2015 ebenfalls in Los Angeles statt. Es nahmen fast 6500 Athleten aus 165 Ländern teil.[61]Bei den Special Olympics 2015 halfen 30.000 Freiwillige und es kamen ungefähr 350.000 Zuschauer zu den Wettkämpfen.[62]Aus Anlass der Spiele wurden auch dasHost Town Programmund dasHealthy Athlete Programmdurchgeführt. Für die Eröffnungs- und Schlussfeier benutzten die Veranstalter der Special Olympics dasLos Angeles Memorial Coliseum, die eigentlichen Spiele fanden an 8 verschiedenen Veranstaltungsorten in und um Los Angeles statt:[63] Einer der beliebtesten Strände an der Pazifikküste von Los Angeles istVenice Beach. Am Venice Boardwalk, der breiten Promenade, auch alsOcean Front Walkbekannt, treffen sich an Wochenenden (im Sommer täglich) zahlreiche Musiker, Maler und Artisten. Der Aufenthalt am Strand ist nach Sonnenuntergang verboten. Das an palmengesäumten Stränden und Hängen über dem Pazifik gelegeneSanta Monica, unmittelbar nördlich von Venice, ist der älteste, größte und bekannteste Badeort der Metropole. In dem betont gesundheitsbewussten, liberalen Bezirk leben viele Schriftsteller und Rockstars. Der Ort ist aber auch bekannt für strenge Mietpreiskontrollen und Bauvorschriften. Bekannt ist auch die Strandkolonie vonMalibu, die von zahlreichen Prominenten bewohnt wird. Surfrider Beach, südlich des Piers, ist seit den 1950er Jahren ein beliebter Surfertreffpunkt. Ein großer Teil desMalibu Creek State Parkan der Las Virgenes Road weiter nördlich, unweit desMulholland Drive, gehörte den Studios der20th-Century Fox. In den von immergrünem Gebüsch überwachsenen Hängen wurden viele Tarzan-Filme gedreht. Im tausend Hektar großen Park befinden sich ein See, mehrere Wasserfälle und Wanderwege. Entlang der Küste südlich von Los Angeles führt der Pacific Coast Highway vorbei an den Start- und Landebahnen des LAX-Flughafens zu einem Küstenstreifen mit mehreren Stränden der South Bay: Manhattan Beach, Hermosa Beach und Redondo Beach. InLong Beachist den letzten Jahren viel investiert worden, wodurch unter anderem eine Reihe neuer Bürogebäude und Hotels entstand. Die größte Sehenswürdigkeit von Long Beach ist der DampferQueen Mary, von den 1930er bis zu den 1960er Jahren das Flaggschiff derCunard Lineund heute ein Hotel. Jedes Jahr am Ostersonntag findet derEaster Sunrise Service at the Bowlstatt, ein ökumenischer Ostergottesdienst am frühen Morgen in derHollywood BowlinHollywood. DerJimmy Stewart Relay Marathon, ein Staffellauf für wohltätige Zwecke, wird Mitte April imGriffith Parkausgetragen. Weitere Veranstaltungen im Monat April sind dasRosenblütenfestim Exposition Park Rose Garden, dasL.A. Times Bücherfestauf dem Campus derUniversity of California, Los Angeles, dasPacific Islander Festivalim Ken Malloy Harbor Regional Park, dieFiesta Broadwayim Homestead Museum und dasUSC Springfestauf dem Campus derUniversity of Southern California. Anfang Mai treffen sich Personen, die sich als Native Americans identifizieren, zumUCLA Pow Wowauf dem Intramural Field, UCLA Campus. Zu sehen sind traditionelle Musik, Tanz und Kunsthandwerk. Feste im Mai sind dasCinco de Mayo, ein Straßenfest in Downtown und dasFamily Fun Fest, ein japanisches Kulturfest anlässlich des Tags der Kinder inLittle Tokyo. Ebenfalls im Mai werden dasValley Greek Festivalin der St. Sophia Cathedral, dasNoHo Theatre & Arts Festivalauf dem Lankershim Boulevard und Magnolia Avenue in North Hollywood und dasUCLA Jazz Reggae Festivalauf dem Intramural Field, UCLA Campus, veranstaltet. Am letzten Montag im Mai findet dieMemorial DayParadeim Canoga Park, San Fernando Valley, statt. Im Juni werden zahlreiche Festivals veranstaltet. Dazu gehören unter anderem dasPlayboy Jazz Festivalund dasMariachi USA Festivalin der Hollywood Bowl. Weitere Festivals sind dasLos Angeles Independent Film Festival(Directors Guild of America,Sunset Boulevard) und dasHollywood Ha-Ha Comedy Festival(Noho Arts District, North Hollywood). DasHollywood Bowl Summer Festivalfindet von Ende Juni bis Mitte September in der Hollywood Bowl statt. Die Feierlichkeiten zumIndependence Day, demamerikanischen Unabhängigkeitstag, werden am 4. Juli in der ganzen Stadt begangen. Ebenfalls im Juli gibt es an verschiedenen Orten der Stadt dasOutfest– LA Gay and Lesbian Film Festival(Schwulen- undLesben-Filmfestival), am Echo Park Lake dasLotus Festivalund imLos Angeles Tennis CenterinWestwooddas Herren-TennisturnierCountrywide ClassicderATP Tour. Im August werden auch zahlreiche Festivals veranstaltet. Dazu gehören dasNisei Week Japanese Festival, ein japanisches Kulturfestival in Little Tokyo, dasLos Angeles Festivalan zahlreichen Orten der Stadt und dasJVC Jazzfestivalin der Hollywood Bowl. DasSweet & Hot Jazz Festivalfindet von August bis September im Los Angeles AirportMarriott Hotelstatt.El Diecisis de Septiembre, die Feiern zum mexikanischen Unabhängigkeitstag, sind am 16. September in Broadway und South Los Angeles zu sehen. Veranstaltungen Anfang und Mitte September sind dasPort of Los Angeles Lobster Festival, ein Hummerfest im Hafen an der Harbor Waterfront und dasHerbst-Mond-Festin Chinatown. Im Oktober finden dasHollywood Film Festivalan verschiedenen Orten in Hollywood und dasLos Angeles International Short Film Festival(Kurzfilmfestival) an der Los Angeles Film School in Hollywood statt. Der November bietet dasAFI Los Angeles International Film Festivalan verschiedenen Orten der Stadt, das Mariachi Festival (Mexikanisches Musikfest) auf dem Mariachi Plaza in Boyle Heights und dieHollywood Christmas Paradeauf dem Sunset Boulevard in Hollywood. Von Ende November bis Ende Dezember ist dasGriffith Park Light Festival(Lichterfest) im Griffith Park, Downtown, zu sehen. DieMartin Luther King Kingdom Day Parade, eine Parade anlässlich des Geburtstags vonMartin Luther Kingam 15. Januar, zieht amMartin Luther King Dayvom Martin Luther King Boulevard bis zum Western Boulevard. Der Februar bietet dasPan African Film & Art Festival, veranstaltet im Magic Johnson Theaters am Baldwin Hills Crenshaw Plaza und dasChinesische Neujahrsfestmit Golden Dragon Parade in North Broadway, Chinatown. Ebenfalls im Februar findet dieOscar-Verleihung imKodak Theatrein Hollywood statt. Anfang März wird derLos-Angeles-Marathonausgetragen. Er führt von der Grand Avenue durch das gesamte Stadtzentrum von Los Angeles. Los Angeles bietet eine große Auswahl an internationalen Restaurants und Lokalitäten. Zu den beliebten Speisen gehören die traditionelle amerikanische Küche mit viel Fleisch und Kartoffeln, koreanische Tempura-Gerichte, Sushi, japanische Gerichte mit Udon- und Sobanudeln, die mexikanischen Enchilades und Tamales, pikante Burritos und Pastrami-Sandwiches. Daneben gibt eschinesische Restaurants, wo Abalonen, Krebse, Garnelen und Ente serviert werden. Gern gegessen werden auch „gedippte“ Sandwiches mit reichlich Truthahn, Schweinefleisch, Rindfleisch oder Lamm sowie italienische Ossobuco und Risotto. Die auch in Los Angeles angeboteneCajun-Küchebietet Spezialitäten desCajun Country. Eine der bekanntesten Einkaufsstraßen ist derRodeo DriveinBeverly Hills, einem Vorort von Los Angeles. Zahlreiche bekannte Modemarken sind hier zu finden. Die Straße beherbergt auch die teuersten Einkaufszentren der Welt. Zu ihnen zählenThe Rodeo Collectionmit fünf Verkaufsebenen ober- und unterhalb der Erde sowie dasTwo Rodeo, das den Eindruck zweier kleiner europäischer Kopfsteinpflasterstraßen mit Brunnen, Piazza und Balkonen erweckt. Am Ende der Straße stehen weitere große Kaufhäuser, darunterBarneys New YorkundSaks Fifth Avenue. Filialen der KaufhäuserMacy’sundBloomingdale’sbefinden sich im nahe gelegenenBeverly Centerund demWestfield Century City(früherCentury City Shopping Center & Marketplace). Das Einkaufszentrum in Century City, ein Beispiel des Modernismus der 1960er Jahre, wurde ohne Überdachung im Stil eines Marktplatzes errichtet. In der Melrose Avenue, gelegen zwischen dem Santa Monica Boulevard und der Hoover Street in Silver Lake, gibt es zahlreiche Einzelhandelsgeschäfte. Am bekanntesten ist die BoutiqueRed Balls, die im Vorspann der FernsehserieMelrose Placezu sehen ist. Eine weitere Einkaufspromenade ist der Universal CityWalk neben denUniversal Studios Hollywood. Hier befinden sich zahlreiche Geschäfte und Restaurants. Auch in der Innenstadt von Los Angeles gibt es viele Kaufhäuser und Galerien, darunter derGrand Central Marketam South Broadway. Im Angebot sind frisch zubereitete Spezialitäten, Früchte und Gemüse. Die Stadt ist ein führendes Produktions-, Handels-, Transport- und Finanzzentrum in den USA. Dort sind unter anderem die chemische Industrie, Elektronik-, Bekleidungs- und Nahrungsmittelindustrie, Metallverarbeitung, Bauwirtschaft und Verlage ansässig. Los Angeles ist weltgrößter Standort für dieLuftfahrtindustrie- undRaumfahrtindustriesowie ein wichtiges Zentrum für Kino-, Radio-, Fernseh- und Musikproduktionen. Der Fremdenverkehr spielt ebenfalls eine wichtige Rolle für die Wirtschaft von Los Angeles. Mit 5,6 Millionen ausländischen Besuchern stand Los Angeles 2016 auf Platz 21 der meistbesuchten Städte weltweit. Touristen in der Stadt machten im selben Jahr Ausgaben von 8,1 Milliarden US-Dollar. Die meisten ausländischen Besucher stammten aus Asien, Europa und Lateinamerika.[64] DieArbeitslosenquoteder Stadt lag 2010 mit durchschnittlich 13,9 % über der des BundesstaatesKalifornien(12,4 %) und war auch höher als der nationale Durchschnitt in den USA (9,6 %). Die Entwicklung verlief in den letzten Jahren folgendermaßen: 2006 (5,3 %), 2007 (5,6 %), 2008 (8,3 %), 2009 (12,7 %) und 2010 (13,9 %).[65] Bis Ende der 1990er Jahre war Los Angeles Sitz von vielen großen Finanzinstituten im Westen der Vereinigten Staaten. Zusammenschlüsse mit anderen Instituten führten zur Verlegung des Hauptsitzes in andere Städte. So kam es beispielsweise 1992 zur Fusion der Security Pacific Bank mit derBank of America, 1996 zum Zusammenschluss der First Interstate Bancorp mitWells Fargound 1998 zur Fusion der Great Western Bank mit derWashington Mutual. Los Angeles war auch Sitz der Pacific Exchange, bis sie im Jahre 2001 geschlossen wurde. In einer Rangliste der wichtigsten Finanzzentren weltweit belegte Los Angeles den 17. Platz (Stand: 2018).[66] Zahlreiche große Unternehmen und Gesellschaften sowie Forschungseinrichtungen haben in der Stadt ihren Hauptsitz. Dazu gehören unter anderem Konzerne der Unterhaltungsindustrie wie20th Century Fox,DreamWorks SKG,Paramount Pictures,The Walt Disney CompanyundWarner Bros., dieAcademy of Motion Picture Arts and Sciences(bekannt durch dieOscar-Verleihung), der SpielwarenherstellerMattel, die Hersteller von ComputerspielenActivisionundTHQ, der Luft- und RaumfahrtkonzernNorthrop Grumman, das Raumfahrt-KonsortiumSea Launch, der Öl- und GaslieferantUnocal, dieRAND Corporation(Denkfabrikfür die US-Streitkräfte), dieICANN(Verwalter von Namen und Adressen im Internet), die HotelketteHilton, der BaukonzernKB Homeund der VerlagTokyopop. Auch dasFortune-500-UnternehmenAECOMunterhält seinen Hauptsitz in Los Angeles. Viele Laboratorien und Forschungszentren, die der WeltraumorganisationNASAangehören, wie beispielsweise dasDryden Flight Research Center, dasJet Propulsion Laboratoryund derGoldstone Deep Space Communications Complexhaben sich in der Metropolregion Los Angeles angesiedelt. In einer Rangliste der Städte nach ihrer Lebensqualität belegte Los Angeles im Jahre 2018 den 64. Platz unter 231 untersuchten Städten weltweit.[67] Über ein gut ausgebautes Netz von inner- und zwischenstaatlichen Fernstraßen (Highways) ist Los Angeles mit allen größeren Städten des Landes verbunden. Auf derInterstate 5kann man von Los Angeles aus die im Norden gelegene StadtSeattleim BundesstaatWashingtonund in südlicher RichtungSan Diego, die zweitgrößte Stadt Kaliforniens, erreichen. Der Pacific Coast Highway (State Highway 1) verläuft in Richtung Norden entlang der kalifornischen Küste nachSanta BarbaraundSan Francisco. DieInterstate 10verbindet Los Angeles mitPhoenix(Arizona), dieInterstate 15mitLas Vegas(Nevada) undSalt Lake City(Utah) und dieInterstate 40mitOklahoma City(Oklahoma) undMemphis(Tennessee). Insbesondere für den Verkehr innerhalb des Bundesstaates Kalifornien, teilweise aber auch für Langstrecken hat der Überlandbusverkehr, wie dieGreyhound Lines, große Bedeutung. Die Stadt ist ein bedeutender Eisenbahnknotenpunkt (transkontinental und regional). Die EisenbahngesellschaftAmtrakbetreibt Züge von Los Angeles überVan NuysnachBakersfield. In Los Angeles besteht eine Umsteigemöglichkeit in denPacific Surfliner, der von San Diego über Los Angeles nachSan Luis Obispofährt. Der BahnhofUnion Stationin Los Angeles ist Endstation zahlreicher Hauptstrecken, die durch den südlichen Teil derRocky Mountainsführen. Er ist auch der am weitesten im Süden gelegene Bahnhof der Strecke entlang der Westküste nach Seattle. Das regionale BahnnetzMetrolinkim Süden Kaliforniens verbindet Los Angeles mit allen wichtigen Städten der Region. Von der Union Station gelangt man über die U-Bahn (Rote Linie) nach Downtown, Hollywood und weiteren Orten. Der Großraum Los Angeles besitzt zahlreiche Flughäfen. Der größte ist derLos Angeles International Airport, auch allgemein bekannt unter seinemIATA-Kürzel LAX, der südwestlich des Stadtzentrums liegt. Er verfügt über vier Pisten und zehn Abfertigungshallen. Dort werden jährlich über 80 Millionen Passagiere abgefertigt. Mit diesem Passagieraufkommen ist der Flughafen seit vielen Jahren unter denTop 10 der größten Flughäfen weltweit. bedienen vor allem nationale Ziele und sind merklich kleiner; bedienen den Privatflugverkehr. DerHafen von Los Angelesin der Bucht von San Pedro ist Nordamerikas größterContainerhafenund der zehntgrößte weltweit. 2006 betrug das Containervolumen 8,5 MillionenTEU(20-Fuß-Container), 2005 waren es noch 7,5 Millionen TEU gewesen.[68]Der durch die Schaffung einer Hafenkommission (Board of Harbor Commissioners) am 9. Dezember 1907 gegründete Hafen nimmt eine Fläche von 30 Quadratkilometern ein, dies auf einem Küstenstreifen von 69 Kilometern. Angrenzend liegt der separate Hafen vonLong Beach. Der Hafen von Los Angeles ist das größte Zentrum fürPassagierschiffsreisenan der Westküste der USA; von hier aus werden jährlich über eine Million Passagiere transportiert. Das renovierte „World Cruise Center“ wird als sicherster Passagierschifffahrtskomplex der USA bezeichnet. Der Hafen wird durch die „Pacific Harbor Line“ (PHL) bedient. Von dort aus gelangen die intermodalen Eisenbahnwaggons durch den Alameda Corridor nach Los Angeles. In Los Angeles existiert kein klassischer Verkehrsverbund, wie er aus Deutschland und anderen Ländern bekannt ist. Generell fungieren die Metro, Busse und Regionalzüge (Metrolink) als getrennte Verkehrsunternehmen, beim Umstieg müssen neue Fahrscheine gekauft werden. Hinzu muss beim Bus passend bezahlt werden, die Busfahrer halten kein Wechselgeld bereit. Das hat sich mit Einführung des Transit Access Pass (TAP) in 2007, der ähnlich derOyster-Cardin London funktioniert, geändert. Mit Ausnahme des Metrolink-Zuges, für den weiterhin ein extra Fahrschein gekauft werden muss, können mit der TAP-Karte alle Metrozüge, Schnellbusse und einfachen Busse genutzt und bezahlt werden, aber auch viele Busse im Los Angeles County sowie die Shuttlebusse (FlyAway) vom LAX-Flughafen, und sogar der städtische Fahrradverleih (Metro Bike Share) akzeptiert die Karte. Ab dem 29. November 2019 kann bei der Metro nur noch die TAP-Karte als Zahlungsmittel verwendet werden, die an den Metro-Ticketautomaten erhältlich ist. Am 3. Juli 1873 fuhren die erstenPferdestraßenbahnenin der Stadt. 1885 wurde dasStraßenbahnsystemelektrifiziert. Es wurde unter der Bezeichnung „Red Cars“ berühmt, obwohl nur die Fernverbindungen in Richtung San Fernando Valley und Orange County rot lackiert waren, die Waggons in den Grenzen der Stadt Los Angeles waren gelb/weiß. 1925 erlebte das Red-Car-Netz mit rund 1.900 Netzkilometern die größte Ausdehnung und während des Zweiten Weltkriegs die höchsten Fahrgastzahlen. Doch bereits ab 1936 begannen Automobilkonzerne wie General Motors die in privatem Besitz befindlichen Straßenbahngesellschaften über vorgeschobene Unternehmen zu kaufen und nach und nach stillzulegen und durch Buslinien mit Bussen der jeweiligen Fahrzeughersteller zu ersetzen (sog.Großer Amerikanischer Straßenbahnskandal).[69]Am 31. März 1963 wurde der Betrieb des Netzes zu Gunsten des Individualverkehrs eingestellt, die letzte Bahn von Downtown nach Hollywood fuhr mit der Aufschrift „into oblivion“, was so viel wie „ins Vergessen“ bedeutet. Viele Freeways wurden auf den Streckenverläufen der Red Cars gebaut. Trotzdem sind heute noch viele Gleisanlagen erhalten und mit derWidmung(Right-of-Way)ausgestattet, was eine Reaktivierung vereinfachen würde. Nach 27 Jahren Unterbrechung verkehrte am 14. Juli 1990 wieder eine Straßenbahn in Los Angeles. Heute ist das 117 Kilometer langeStadtbahnnetzTeil derMetro Los Angeles.[70] Das Netz der Metro besitzt sechs Linien, davon sind jedoch nur die „Red Line“ und die „Purple Line“ Voll-U-Bahnen, die anderen vier sindStadtbahnstrecken(Light Rail). Die „Red Line“ eröffnete am 30. Januar 1993 ihren Betrieb auf einem 28 Kilometer langen Streckenabschnitt. Betreiber des auch als MetroRail bezeichneten Systems ist die „Los Angeles County Metropolitan Transportation Authority“ (LACMTA). Seit den 1980er Jahren investierte die Stadt mehrere Milliarden Dollar in die Erneuerung des Schienennetzes – mit nur mäßigem Erfolg. Denn gegenwärtig ist die Zersiedelung des Stadtgebietes so weit fortgeschritten, dassHaltestellenzu Fuß kaum noch erreichbar sind. Die Arbeiten für ein U-Bahn-Netz wurden vorübergehend eingestellt, da die Kosten an derErdbeben-gefährdeten Westküste zu hoch waren. Dagegen wird das überwiegend an der Oberfläche verlaufende Stadtbahnnetz schneller ausgebaut. Nach der „Blue Line“ (1990), der „Green Line“ (1995) und der „Gold Line“ (2003) wurde mit der „Expo Line“ 2012 die vierte Straßenbahnlinie in Los Angeles eröffnet. Sie verbindet die Innenstadt mit der Pazifikküste beiSanta Monica. Zwischen 2006 und April 2012 wurde der erste neun Meilen (14,5 Kilometer) lange Abschnitt zwischen den Haltestellen 23. Straße und Culver City errichtet[71], im Mai 2016 wurde die Verlängerung zur Pazifikküste eröffnet. Zwei Buslinien, dieOrange Lineund dieSilver Line, werden als Metrobus bezeichnet und auch auf der Metro-Streckenkarte vermerkt. Es sind Schnellbusse, die überwiegend auf eigener Fahrbahn verkehren und selten halten. Die Fahrradmitnahme ist in den Metrozügen wie den Metrobussen kostenlos möglich. Alle Busse besitzen einen angebrachten Radträger für zwei bis drei Räder an der Front. Die Metrozüge besitzen extra Abstellflächen für Räder, die mit einem gelben Radsymbol gekennzeichnet sind. Im Gegensatz zu den Metrolink-Zügen, dürfen bei der Metro auchE-Fahrrädermitgenommen werden. Unter der BezeichnungMetrolinkwird der Eisenbahn-Vorortverkehr im Großraum der kalifornischen Stadt Los Angeles mit sieben Linien betrieben. Alle Linien starten an derUnion Stationvon Los Angeles. Die Metrolink-Züge sind hauptsächlich auf Pendler aus den Vororten morgens nach und abends von Los Angeles abgestimmt, am Wochenende reduzieren sich die Verbindungen deshalb auf rund ein Drittel gegenüber denen an Werktagen. Die Höchstgeschwindigkeit der Züge ist mit bis zu 140 km/h für amerikanische Verhältnisse relativ hoch. Eingesetzt werden fast ausschließlich Doppelstock-Waggons. Die Fahrradmitnahme ist in allen Metrolink-Zügen kostenlos möglich. Jeder Waggon kann bis zu drei Räder aufnehmen. Manche Züge besitzen auch einen gelben, sogenanntenBike-CarWaggon, dort können bis zu neun Fahrräder transportiert werden. Metrolink-Fahrscheine gelten auch für die Weiterfahrt in der Metro und den Bussen von Los Angeles. Trolleybusseverkehrten zwischen dem 11. September 1910 und 1915, in den Jahren 1922 und 1937 sowie vom 3. August 1947 bis 31. März 1963 in der Stadt.[72]Omnibussefahren zwischen fünf Uhr morgens und zwei Uhr nachts etwa alle 15 Minuten entlang der wichtigsten Hauptstraßen, zusätzlich gibt es Express- und Nachtlinien. Fahrpläne befinden sich an zahlreichenBusterminals. Das Busnetz der Stadt ist das Rückgrat des öffentlichen Verkehrs, wird aber überwiegend von weniger Verdienenden benutzt. Zahlreiche Busgesellschaften bedienen die Metropolregion, die größte unter ihnen ist die öffentliche „Los Angeles County Metropolitan Transportation Authority“ (LACMTA), die mit 2600 Bussen ein etwa 8000 Kilometer langes Streckennetz betreibt.[73]Im nördlichen und nordwestlichen Teil verkehren die Busse der „Santa Monica Big Blue Bus Line“ und im südlichen Teil „Orange County Transit“. Im Flughafenbereich bedient die „Culver City Bus Company“ ein kleines Streckennetz. DieBusse der LACMTA, die auch die Metro betreibt, bestehen aus drei Kategorien. Die orangefarbenen Metro-Local-Busse befahren die meisten Hauptstraßen und halten in kurzem Abstand (200 Meter). Sie haben Liniennummern von 1-399 und 600er Nummern. Die Metro-Rapid-Busse sind feuerrot und befahren meist die gleichen Linien wie die Metro-Local-Busse, allerdings halten diese nur an den großen Kreuzungen. Die Liniennummern sind 700er und 900er Zahlen. Zusätzlich existieren noch acht Express-Buslinien, die Metro-Express-Busse, die überwiegend über Autobahnen geführt werden und sehr selten halten. Farbgebung der Metro-Express-Busse ist überwiegend in Orange, ältere Busse tragen auch noch einen dunkelblauen Anstrich. Die Liniennummern besitzen Zahlen um die 500. Insgesamt nutzen nur etwa zehn Prozent der Bewohner der Stadt die öffentlichen Verkehrsmittel. Neun von zehn Beschäftigten fahren mit dem eigenen Auto zur Arbeit. So hat Los Angeles auch die höchsteKraftfahrzeugdichteder Welt. Mehrspurige Autobahnen in den USA sind zu einem Symbol des Verkehrs im 20. Jahrhundert geworden. Trotz des gut ausgebauten Autobahnnetzes kommt man mit dem Auto in Los Angeles in der Regel nicht mehr sehr schnell voran. Die individuelle Fortbewegung per Automobil und die dadurch erzeugte Luftverschmutzung (Smog) sind heute Probleme ersten Ranges. In Los Angeles sind Millionen von Berufspendlerndurchschnittlich gut eine Stunde pro Weg auf den Autobahnen unterwegs. Immense Verkehrsstauungen sind die Folge, deren Ursache in der weitläufigen Siedlungsstruktur liegen. Die kalifornischeMetropolebesaß noch in den 1920er Jahren das weltweit größte Schienennetz für den Nahverkehr. Bereits damals siedelte man dort nicht so kompakt wie in Europa. In den nachfolgenden Jahrzehnten wurde die Straßenbahn nach und nach durch das Auto ersetzt. Um die Staus zu verringern, baute man die Autobahnen seit den 1940er Jahren auf bis zu 15Streifenaus. Außerdem versucht man durch die Einrichtung der sogenanntenDiamond Lanes, welcheFahrgemeinschaftenvorbehalten sind, die Bildung von Fahrgemeinschaften zu fördern und somit dasVerkehrsaufkommenzu senken. Der verstärkte Ausbau derRadwegewurde zusammen mit dem des Metroausbaus gestartet. So darf auch ein (E-)Fahrrad kostenlos in derMetround demMetrolinkmitgenommen werden. Aber insbesondere bei denRadschnellwegen(in den USAClass Igenannt) hat Los Angeles und das Umland in den letzten Jahren ein Netz aus mehreren hundert Kilometern geschaffen. Meist wurden und werden entlang von neugebauten Metro- und Metrobuslinien auch Radwege (Bike Lanes) angelegt, wie bei der Orange-Line in North Hollywood (20 km) und der Expo-Line zum Pazifik (24 km). Außerdem gilt auf vielen normalen Straßen innerorts eine Höchstgeschwindigkeit von nur 40 km/h, die aufgrund der hohen Strafen von Autofahrern auch eingehalten wird, was das Fahren im gemischten Verkehr mit Autos entspannter als in Europa macht. Auch das VorfahrtsrechtAll way(wer zuerst an eine Kreuzung heranfährt, darf auch zuerst weiterfahren), das meist erlaubte Rechtsabbiegen bei einer roten Ampel und das erlaubte Befahren von Gehwegen sowie der Umstand, dass Radfahrern selbstverständlich eine ganze Spur zugestanden wird, hat die Zahl der Fahrradfahrer in Los Angeles in den letzten Jahren stark anwachsen lassen. Wer unter 18 Jahre alt ist, muss einen Helm tragen. Die größte Tageszeitung in der Region ist dieLos Angeles Times,La Opiniónist die größte spanischsprachige Zeitung. „Investor’s Business Daily“ hat seine Büros in Los Angeles, mit Hauptsitz inPlaya Del Rey. Es gibt auch eine Reihe von kleineren regionalen Zeitungen, alternativen Wochenzeitungen und Zeitschriften, darunter die Zeitung „Daily News“ (mit Fokus auf das San Fernando Valley), „LA Weekly“, „Los Angeles CityBeat“, „LA Record“ (berichtet über die Musikszene im Großraum von Los Angeles), „Los Angeles Magazine“, „Los Angeles Business Journal“, „Los Angeles Daily Journal“ (renommiertesjuristischesMagazin),The Hollywood ReporterundVariety(Unterhaltungsmagazine) sowie „Los Angeles Downtown News“. Zusätzlich zu den englisch- und spanischsprachigen Zeitungen, werden zahlreiche lokale Zeitschriften für Immigranten in ihrer Muttersprache veröffentlicht, darunter in Armenisch, Koreanisch, Niederländisch, Persisch, Russisch, Chinesisch und Japanisch. Viele Städte um Los Angeles besitzen ihre eigenen Tageszeitungen, die teilweise auch in Los Angeles gelesen werden. Beispiele sind „The Daily Breeze“ (für die South Bay), und „The Long Beach Press-Telegram“. Der Großraum Los Angeles besitzt eine Vielzahl von lokalen Radio- und Fernsehstationen und ist (nachNew York) der zweitgrößte Medienmarkt in Nordamerika. Die erste Radiostation in Los Angeles war KNX. Sie begann 1920 experimentelle Programme auszustrahlen und bekam im Dezember 1921 eine kommerzielle Sendelizenz. Die erste Fernsehstation in Los Angeles (und die erste inKalifornien) war KTLA, die am 22. Januar 1947 mit der Ausstrahlung von Programmen begann. Die großen zu einem Netzwerk verbundenen Fernsehstationen in Los Angeles sind KABC-TV 7 (ABC), KCBS 2 (CBS), KNBC 4 (NBC), KTTV 11 (FOX), KTLA 5 (The CW), KCOP-TV 13 (MyNetworkTV) und KPXN 30 (ION Television). Es gibt auch dreiPBS-Stationen in der Region, einschließlich KCET 28, KOCE-TV 50, 58 und KLCS. World TV betreibt auf zwei TV-Kanälen die Stationen KNET-LP 25 und LP KSFV-6. Es gibt auch mehrere spanischsprachige Fernsehnetzwerke, darunter KMEX-TV 34 (Univision), KFTR 46 (TeleFutura), KVEA 52 (Telemundo) und KAZA 54 (Azteca América). KTBN 40 (Trinity Broadcasting Network) ist eine religiöse Station. Mehrere unabhängige TV-Stationen befinden sich in der Region, einschließlich der KCAL-TV-9 (im Besitz vonCBS Corporation), KSCI 18 (konzentriert sich hauptsächlich auf Programme in asiatischen Sprachen), KWHY-TV 22 (sendet in spanischer Sprache), KNLA-LP 27 (spanische Sprache), KSMV-LP 33 (Variety), KPAL-LP 38, KXLA 44, KDOC-TV 56 (mit Fokus auf klassische Programme und örtliche Sportvereine), KJLA 57 (Variety), und KRCA 62 (spanische Sprache). Hollywoodist nicht nur ein Stadtteil von Los Angeles, sondern ein Synonym für die US-amerikanische Filmindustrie. Der Ort, der am 1. Februar 1887 von der Familie Wilcox gegründet wurde, war einst der Standort einer der größtenpresbyterianischen Kirchendes Landes und lag noch um 1900 etwa 13 Kilometer von Los Angeles entfernt. Er begann seinen Aufstieg 1911, als David Horsley’s Nestor Company hier das erste Filmstudio eröffnete. Es ist heute ein Museum und beherbergt eine Sammlung interessanter Erinnerungsstücke aus derStummfilmzeit. 1911 übersiedelten 15 weitere, „Independents“ genannte, Unternehmen vonNew York, dem damaligen Zentrum der Filmindustrie. Es gab mehrere Gründe für diesen Umzug. Die wichtigsten waren das geeignetere Klima und die längeren Tage (zu dieser Zeit hatte man noch kein adäquates Kunstlicht, man drehte also entweder im Freien oder in einem Studio mit Glasdach oder Ähnlichem). Mitentscheidend war zweifelsohne auch die große Entfernung zu New York, von wo aus die mächtigeMotion Picture Patents Company(MPPC) alle ihr nicht angeschlossenen Unternehmen mit hohen Strafen und Lizenzgebühren bedrohte. Der neue Industriezweig expandierte rasch und brachte schnellen Erfolg und Reichtum. Zahlreiche Regisseure und Produzenten, wieCecil B. DeMille,Samuel Goldwyn,Jesse L. LaskyundAdolph Zukorhatten in Hollywood Büros eröffnet. Doch erst mitD. W. GriffithsStummfilmeposDie Geburt einer Nation(1915) entwickelte sich die Filmproduktion zu einer eigenständigen Industrie mit entsprechender Technik und Spezialisierung. Zahlreiche kleinere Unternehmen gingen zu dieser Zeit inKonkursoder wurden von den großen Studios übernommen, die in den 1930er Jahren beinahe ausnahmslos an neue Standorte weiter außerhalb wie Culver City, Burbank oder West Los Angeles umzogen. Von den großen Namen blieb nurParamount Picturesin Hollywood. 1927 zählte Los Angeles 247 Filmgesellschaften, 58 großeFilmateliersund einen Ausstoß von rund 800 Langfilmen.[74] Für den Niedergang der Studios in den 1950er Jahren waren neben der Kartellgesetzgebung der US-Regierung auch die sich erholende Filmindustrie im Ausland nach demZweiten Weltkriegund natürlich das Fernsehen verantwortlich. Ein neuer Aufschwung begann erst in den 1970er und 1980er Jahren durch Regisseure wieSteven SpielbergundGeorge Lucas. Sie entwickelten das Konzept desBlockbusters– eines teuer produzierten, mit zahlreichen Spezialeffekten ausgestatteten Monumentalfilms, der ein so breites Publikum wie möglich ansprechen sollte. Dank eines damals begonnenen Trends und der finanziellen Konsolidierung der US-amerikanischen Medienkonzerne ist Hollywood heute noch ein Symbol für den dauerhaften Erfolg der US-amerikanischen Filmindustrie und professionell produzierte, unterhaltsame Filme mit weltbekannten Schauspielern und Happy End. Durch die Nähe zur Filmindustrie wurde die Stadt selbst zum Schauplatz vieler Filme. Die Stadt beherbergt zahlreiche bedeutende Universitäten, Hochschulen, Forschungs- und Bildungseinrichtungen. DieCalifornia State University, Los Angeles(CSULA) ist eine von drei staatlichen Universitäten in Los Angeles. An ihr sind 21.000 Studenten eingeschrieben. Die CSULA wurde 1947 alsLos Angeles State Collegegegründet und 1964 inCalifornia State College at Los Angelesumbenannt. Ihren heutigen Namen erhielt sie 1972, als sie Teil des Systems derCalifornia State Universitywurde. Eine weitere staatliche Universität ist dieCalifornia State University, Northridge(CSUN). Sie ist eine der größten Hochschulen des Systems der California State University. Die Hochschule wurde 1958 alsSan Fernando Valley State Collegeeröffnet und erhielt 1972 ihren heutigen Namen. An ihr sind etwa 33.000 Studenten eingeschrieben. Die staatlicheUniversity of California, Los Angeles(UCLA) wurde 1919 gegründet und ist der zweitältesteCampusderUniversity of California. 2003 waren etwa 37.000 Studierende eingeschrieben. An den zwölfFakultätenarbeiten mehr als 22.000 Angestellte. Die Anderson School of Management – eine der bekanntesten Business Schools in den USA – gehört zur UCLA. DieUniversity of Southern California(USC) ist die älteste Privatuniversität inSüdkalifornien. Sie wurde im Jahre 1880 eröffnet. Das Lehrangebot der Universität wurde stetig erweitert, in der Anfangszeit kam fast jedes Jahr eine neueFakultäthinzu. Sie beschäftigt heute rund 3.000 Vollzeitlehrkräfte, etwa 32.000 Studenten sind an ihr eingeschrieben. DieLoyola Marymount University(LMU) ist eine von 28 Mitgliedshochschulen derAssociation of Jesuit Colleges and Universities. Derzeit sind rund 9.000 Studenten eingeschrieben. Die Hochschule entstand 1973 aus dem Zusammenschluss des Marymount College mit der Loyola University. Der Ursprung der Bildungseinrichtung geht auf dasSt. Vincent’s Collegezurück, welches 1865 eröffnet wurde. Die öffentlichen Schulen Los Angeles’ werden vomVereinigten Schulbezirk von Los Angeles(Los Angeles Unified School District)verwaltet. Aus den Medien bekannt ist dieCrenshaw High Schoolim StadtteilSouth Los Angeles, an der zurzeit etwa 2600 Schüler lernen. DieHigh Schoolwar unter anderem Drehort der FilmeBoyz n the Hood – Jungs im Viertelvon 1991 undLove & Basketballvon 2000. Auch die FernsehserieMoesha(1996–2001) spielt an der Schule. Im Jahre 1986 gab es in der Los Angeles Public Library, der öffentlichen Zentralbibliothek von Los Angeles (2,1 Millionen Bände), zwei Großbrände. Die 1926 fertiggestellte Einrichtung wurde 1993 nach einem Umbau wiedereröffnet. Seit 2001 trägt die Bibliothek den Namen des früheren Bürgermeisters von Los Angeles,Richard Riordan. Die Stadt ist Heimat zahlreicher prominenter Persönlichkeiten. Dazu gehören unter anderem die SchauspielerinnenBridget Fonda,Jodie Foster,Angelina Jolie,Elizabeth Mitchell,Marilyn Monroe,Mary-Kate und Ashley Olsen,Gwyneth PaltrowundAnna May Wong, der SchauspielerLeonardo DiCaprio, der Schauspieler und SängerDavid Faustino, der Produzent und RapperDr. Dre, der Rapper und SchauspielerIce Cube, der RapperThe Game, der Sänger und ProduzentMichael Landau, der Sänger und PianistRandy Newman, der BasketballspielerMichael Cooper, die TennisspielerinDarlene Hard, der KomponistJohn Cage, der MusikerJames Hetfield, der Science-Fiction-SchriftstellerLarry Nivenund der BildhauerIsamu Noguchi. Agoura Hills•Alhambra•Arcadia•Artesia•Avalon•Azusa•Baldwin Park•Bell•Bell Gardens•Bellflower•Beverly Hills•Bradbury•Burbank•Calabasas•Carson•Cerritos•City of Industry•Claremont•Commerce•Compton•Covina•Cudahy•Culver City•Diamond Bar•Downey•Duarte•El Monte•El Segundo•Gardena•Glendale•Glendora•Hawaiian Gardens•Hawthorne•Hermosa Beach•Hidden Hills•Huntington Park•Inglewood•Irwindale•La Cañada Flintridge•La Habra Heights•La Mirada•La Puente•La Verne•Lakewood•Lancaster•Lawndale•Lomita•Long Beach•Los Angeles•Lynwood•Malibu•Manhattan Beach•Maywood•Monrovia•Montebello•Monterey Park•Norwalk•Palmdale•Palos Verdes Estates•Paramount•Pasadena•Pico Rivera•Pomona•Rancho Palos Verdes•Redondo Beach•Rolling Hills•Rolling Hills Estates•Rosemead•San Dimas•San Fernando•San Gabriel•San Marino•Santa Clarita•Santa Fe Springs•Santa Monica•Sierra Madre•Signal Hill•South El Monte•South Gate•South Pasadena•Temple City•Torrance•Vernon•Walnut•West Covina•West Hollywood•Westlake Village•Whittier Acton•Agua Dulce•Alondra Park•Altadena•Avocado Heights•Castaic•Charter Oak•Citrus•Del Aire•Desert View Highlands•East Los Angeles•East Pasadena•East Rancho Dominguez•East San Gabriel•East Whittier•Elizabeth Lake•Florence-Graham•Green Valley•Hacienda Heights•Hasley Canyon•La Crescenta-Montrose•Ladera Heights•Lake Hughes•Lake Los Angeles•Lennox•Leona Valley•Littlerock•Marina del Rey•Mayflower Village•North El Monte•Pepperdine University•Quartz Hill•Rose Hills•Rowland Heights•San Pasqual•South Monrovia Island•South San Gabriel•South San Jose Hills•South Whittier•Stevenson Ranch•Sun Village•Topanga•Val Verde•Valinda•View Park-Windsor Hills•Vincent•Walnut Park•West Athens•West Carson•West Puente Valley•West Rancho Dominguez•West Whittier-Los Nietos•Westmont•Willowbrook Alyeupkigna•Awigna•Azucsagna•Bairdstown•Bartolo•Cahuenga•Chokishgna•Chowigna•Cow Springs•Eldoradoville•Falling Springs•Fort Tejon•Guirardo•Hahamongna•Harasgna•Holland Summit•Hollands•Holton•Honmoyausha•Houtgna•Hyperion•Isanthcogna•Juyubit•King’s Station•Kuruvungna•Kowanga•Las Tunas•Los Angeles Township•Lyons Station•Machado•Malibu Mar Vista•Maugna•Mentryville•Motordrome•Mud Spring•Nacaugna•Okowvinjha•Palisades del Rey•Pasinogna•Petroleopolis•Pimocagna•Port Ballona•Pubugna•Puvunga•Quapa•San Jose Township•Savannah•Saway-yanga•Seobit•Sibagna•Sisitcanogna•Soledad Township•Sonagna•Suangna•Takuyumam•Tejon Township•Toviseanga•Toybipet•Tuyunga•Virgenes•Wahoo•Widow Smith’s Station•Wilmar•Wilsona•Yaanga Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Name der Stadt 2Geographie 2.1Lage 2.2Geologie 2.3Stadtgliederung 2.4Siedlungsform 2.5Klima 2.6Umweltprobleme 3Geschichte 3.1Stadtgründung 3.2Industrialisierung 3.3Wirtschaftszentrum 4Bevölkerung 4.1Ethnizitäten 4.2Religionen 4.3Bevölkerungsentwicklung 4.4Kriminalität 5Politik 5.1Stadtregierung 5.2Flagge und Siegel 5.3Städtepartnerschaften 5.4Städtefreundschaften 6Kultur und Sehenswürdigkeiten 6.1Musik und Theater 6.2Museen 6.3Bauwerke 6.4Parks 6.5Sport 6.5.1Football 6.5.2Eishockey 6.5.3Baseball"
  },
  {
    "label": 0,
    "text": "Lyrik – Wikipedia Lyrik Inhaltsverzeichnis Begriffsgeschichte Funktionale Bestimmung Kriterien der sprachlichen Form Geschichte der Lyrik Gegenwart Beispiele Anthologien Interpretationen und Materialien Einführungen und weiterführende Literatur Weblinks Einzelnachweise Lyrik Gedicht Formelemente und Gedichtformen Genres und Subgenres Altertum und Antike Europäisches Mittelalter Entwicklungen ab dem Spätmittelalter Status der Lyrik im Islam Deutschsprachige Lyrik Internationale Lyrik Deutschsprachige Lyrik Englischsprachige Lyrik Französischsprachige Lyrik Griechische Lyrik Italienische Lyrik Polnische Lyrik Portugiesische Lyrik Russische Lyrik Slowenische Lyrik Spanischsprachige Lyrik Tschechische Lyrik Japanische Lyrik Persische Lyrik Lyrik(altgriechischλυρική (ποίησις)lyrikḗ (poíēsis), deutsch‚die zum Spiel derLyragehörendeDichtung‘) ist eine der dreiliterarischen HauptgattungennebenEpikundDramatik. Lyrische Texte werdenGedichtegenannt. Die Unterscheidung der literarischen Gattungen Lyrik,EpikundDramatikgeht auf diegriechische Antikezurück, insbesondere auf diePoetikdesAristoteles. Der OrdnungsbegriffLyrik(in der Formlyrische Poesie) wird seit der Neuordnung des Gattungsschemas im 18. Jahrhundert als Gattungsbezeichnung verwendet. Imallgemeinen Sprachgebrauchwird er seit dem 19. Jahrhundert oftsynonymmit den umfassenderen BegriffenPoesieundDichtunggebraucht. Der BegriffLyrikverweist auf seinen historischen Ursprung im antiken Griechenland. Dort wurde der Vortrag von Dichtung in der Regel von einerLyraoderKitharabegleitet. Davon geblieben ist eine formale Verbindung lyrischer Texte zumLied, die sich auch darin zeigt, dass Gedichte bis heute gern vertont werden. Als „Gedicht“ wurden ursprünglich alleschriftlichabgefasstenliterarischenTexte bezeichnet; in dem Wort „Dichtung“ hat sich noch etwas von dieser Bedeutung erhalten. Seit etwa dem 17. Jahrhundert wird der Begriff nur noch für Texte verwendet, die zur Gattung Lyrik gehören; bis heute bezieht sich die Rede von einem Gedicht auf einen (einzelnen, nicht notwendig schriftlich fixierten) lyrischen Text. Erstmals in diesem Sinn verwendetMartin Opitzden Begriff „geticht(e)“ 1624 in seinemBuch von der Deutschen Poeterey; für Opitz ist das formästhetische Kriterium ausschlaggebend, dass Gedichte inVersenabgefasst sind.[1]Gedichte sind allerdings von Werken derVersepikzu unterscheiden, die ebenfalls durchgängig in Versen abgefasst sind, aber zur GattungEpikzählen. Ein umfangreiches (häufig mehrteiliges) lyrisches Werk mit unter UmständenepischenElementen wird alsLanggedichtbezeichnet, ein zyklisch angelegtes alsGedichtzyklusund eines, das prosaförmige Textsequenzen und Gedichte oder lyrische Textsequenzen kombiniert, alsProsimetrum. Eine Sonderform des Langgedichts ist dasPoem. Eine Form von Gedichten, in denen Vers undartifizieller Zeilenbruchzugunsten anderer Merkmale lyrischer Texte suspendiert werden, ist seit dem frühen 19. Jahrhundert dasProsagedicht. Lyrische Texte unterscheiden sich von epischen und dramatischen vor allem durch ihr meist geringes Textvolumen, ihre komprimierte und konturierte Textgestalt, ihre sprachliche Form, ihre semantische Dichte (Ausdruckskraft) und sprachliche Ökonomie (Prägnanz) und eine subjektive Auffassung ihrer Gegenstände. Sie vermitteln Emotionen, Wahrnehmungen und/oder Gedanken eines individuellen Subjekts, dessen Perspektive der des Verfassers entsprechen kann, aber nicht muss (siehe auchlyrisches Ich). Beziehungen zwischen diesem Subjekt, der es umgebenden Welt und dem (sprachlichen) Medium, in dem es sich artikuliert, werden dabei oft in hohem Maße reflektiert und abstrahiert. Lyrische Texte sind textgrafisch und/oder phonisch meist stark konturiert. In der Regel unterscheiden sie sich vonProsatextenbereits in ihrer äußeren Form (sieheVers,Strophe). Im Lauf der Gattungsgeschichte verschieben sich die Kriterien vor allem bezüglich des Verses und schwächen sich bereits im 19. Jahrhundert zunehmend ab. So finden sich zuerst in der Lyrik vonFriedrich Gottlieb Klopstockund später auch beiJohann Wolfgang von GoetheundFriedrich Hölderlinreimlose Gedichte infreien Rhythmen, eine Form Versgestaltung, die sich in der Lyrik vonNovalisendgültig von ihren antiken Vorbildern ablöst. Im 19. Jahrhundert werden diese Formen in Frankreich zumvers libreweiterentwickelt. Mit dem weitgehenden Verzicht auf seine metrische Organisation und der Orientierung an der lebendigen Rede nähert sich derfreie Versrhythmisch undprosodischProsadiktionen an, ohne jedoch ganz mit ihnen zusammenzufallen. Zentrales Distinktionsmerkmal und Formelement lyrischer Texte bleibt letztlich der Vers selbst, der sich aus dem absichtsvollen, sinnstiftenden Umbruch der Zeile (sieheEnjambement) ergibt – im Unterschied zu den technisch bedingten Zeilenumbrüchen in Prosatexten, die keiner textimmanenten Logik folgen und für die Konstitution der Textbedeutung irrelevant sind. Eine besondere Rolle bei der phonischen Gestaltung von Gedichten spielen die lautlichen Qualitäten des verwendeten Sprachmaterials, von einfachenAssonanzenbis hin zu Formen derOnomatopoesie. Im 20. Jahrhundert entwickelten sich zahlreiche Formen derLautpoesie, die diesen Aspekt in den Mittelpunkt stellen. Bei einzelnen Autoren der antiken und mittelalterlichen Lyrik, vor allem jedoch in der Lyrik des Barock und später in literarischen Avantgarden des 20. Jahrhunderts, etwa derkonkreten Poesie, wird umgekehrt die grafische Gestalt des Textes zu einem eigenständigen und teilweise dominanten Formelement erhoben (siehe auchVisuelle Poesie). Darüber hinaus sind Gedichte sprachlich häufig komplex strukturiert. Zahlreiche formale und rhetorische Darstellungsmittel (siehe beispielsweiseReim,rhetorische Figur,Metapher) führen nicht selten zu einer vom Gewohnten abweichenden Anordnung von Wörtern, Wortgruppen und Sätzen und sind auch für die vielschichtigen Bedeutungen verantwortlich, die man mit Lyrik verbindet. Aus der Sicht eher linguistisch orientierter Lyriktheorien wird ein lyrischer Text als überstrukturierter Text aufgefasst. Diese Überstrukturierung bezieht sich auf die in der Sprachwissenschaft angesetzten Ebenen jeder sprachlichen Äußerung wiePhonologie,SemantikoderSyntax. So werden Reime als phonologische Überstrukturierung aufgefasst, Metaphern als semantische usw.[2] Zahlreiche lyrikspezifische Elemente sprachlicher Gestaltung sind historisch tradiert, kommen aber bis heute in vielen Gedichten zum Einsatz. Auf unterschiedenen Ebenen der Textorganisation unterscheidet manVersfuß(Anapäst,Daktylus,Jambus,Trochäusu. a.),Versmaß(Alexandriner,Blankvers,Hexameter,Pentameteru. a.),Strophenform(Odenstrophen wieAlkäische Strophe,Asklepiadeische StropheundSapphische Strophe,Chevy-Chase-Strophe,Distichon,Sestine,Stanze(mit Sonderformen wieSiziliane,Nonarime,Huitain,Spenserstrophe),Terzineu. a.) und – formal verschieden streng definierte –Gedichtformen(Elegie,Epigramm,Ghasel,HaikuundSenryū,Hymne,Lied,Ode,Ritornell,Sonett,Villanelleu. a.). Hinzu kommen Gedichtformen, die auf einer bestimmten Organisation des Textes auf Zeichenebene beruhen, etwaAnagramm,Lipogramm,PalindromundAkrostichon. Historische, heute nur noch selten verwendete Gedichtformen sind u. a.Dithyrambos,Kanzone,MadrigalundRondeau. Gedichte, die sich diesen und ähnlichen Bestimmungen entziehen, haben nicht selten eine explizitfreie Form. Gedichtformen können auch oder auch noch auf anderen Ebenen als der formalen Textorganisation definiert sein, etwa dasBildreihengedichtin Bezug auf die Art und Verbindung der semantischen Einheiten,Rollenlyrikin Bezug auf die Perspektivierung undProsagedichtein Bezug auf die Abwesenheit von Vers und Zäsur. Der Fokus auf materiale Eigenschaften der Sprache führt in der Lyrik zuBildgedichtenundLautgedichten, die Aktivierung struktureller Aspekte schriftsprachlicher Repräsentation zuFlächengedichtenundListengedichten, die Einbeziehung von Informationstechnologien in die Textgenese zuFlarf. Transdialektale und -linguale Sprachmischung finden sich bereits in der frühneuzeitlichenMakkaronischen Dichtung. Über das einzelne Gedicht hinaus geht etwa die 14 Sonette und ein „Meistersonett“ umfassende Form desSonettenkranzes, über den einzelnen Autor z. B. das japanischeRenga(Kettengedicht), über den einzelnen Sprecher beziehungsweise Vortragenden dasSimultangedicht. Funktional bestimmteGenresvon Lyrik sind u. a.Kinderlyrik, religiöse Dichtung,Gelegenheitsdichtungund die sog.Unsinnspoesie, deren zahlreiche Spielarten ihrerseits (streng) formal definiert sind (z. B.Clerihew,Klapphornvers,Leberreim,Limerick,Wirtinnenvers). Thematisch bestimmte Genres sindLiebeslyrik,Naturlyrikundpolitische Lyrik; im Sprachbezug der sprachreflexiven Lyrik fallen thematische und formale Bestimmung zusammen. Historische Strömungen der Lyrik sind in der Regel funktional, thematisch und formal bestimmt und lassen sich daher auch als lyrische Genres auffassen – u. a.Trobadordichtung,Minnesang,Sangspruchdichtung,Bukolikbzw.SchäferdichtungundMeistersang. Historische Strömungen des 20. Jahrhunderts sind z. B. die deutschenaturmagische Lyrikoder die US-amerikanischeConfessional Poetry. Genreübergreifende Formen gebundener Rede sind beispielsweiseBallade,RomanzeundHaibun. Performativ bestimmt ist etwaSpoken Word. AuchLiedtextealler Genres sowieHip-HopundRaphaben Gemeinsamkeiten mit lyrischen Texten. Die Lyrik ist eine der frühen literarischen Formen. Wenn auch die ältesten überlieferten lyrischen Texte nicht als Gedichte im heutigen Sinne verstanden wurden, reicht das Vorkommen von Versstrukturen mit Endreimen und/oderAlliterationenhin, um etwa dieMerseburger Zaubersprücheoder frühe religiöse Texte als lyrische Texte einzustufen. Der heutige Begriff von Lyrik geht auf den antiken griechischen Kulturkreis zurück; dort war die Lyrik zunächst das zurLyragesungene Lied, das in den Chorgesängen derantiken Dramenund im religiösen Kultus seinen „Sitz im Leben“ hatte. Bis heute steht Lyrik in einer gewissen Beziehung zurMusikund zum Lied. Im volkssprachlichenMittelaltertreten Individualpersönlichkeiten vor allem imMinnesangund in derSpruchdichtunghervor: die provencalischenTrobadoursab dem Ende des 11. Jahrhunderts,Heinrich von Veldeke, der als erster deutschsprachiger Dichter[4]gilt, undWalther von der Vogelweideim 12. Jahrhundert,Heinrich von MorungenundFrauenlobim 13. Jahrhundert,Oswald von Wolkensteinim – spätmittelalterlichen – 15. Jahrhundert. Hauptsächlich wurde die mittelalterliche Lyrik gesungen und mündlich tradiert; die Quellen, zunächst Handschriften und später auch Drucke, auf denen das heutige Wissen über die Lyrik des Mittelalters beruht, sind häufig erst lange nach der Entstehung der Texte entstanden. Deren Urfassungen sowie die Transformationen, denen sie vor ihrer Niederschrift unterlegen haben, lassen sich nur selten durch Quellenvergleiche rekonstruieren. Geistliche Lyrik (z. B. dieSequenzen) und die lateinischeVagantendichtungsind oft anonym in größeren Sammlungen überliefert, etwa derCarmina Burana(11./12. Jahrhundert). DieMeistersingerdes ständisch geprägten Spätmittelalters (u. a.Hans Sachs, 16. Jahrhundert) inszenierten ihre Dichtung als ein lern- und abprüfbares Silben- undTöne-Handwerk. Im altenglischen EposBeowulfsingt ein Skop von der Weltschöpfung. Das GedichtThe Battle of Maldon(Schlacht um Maldon)lässt sich bereits auf das 11. Jahrhundert datieren. Nach der Christianisierung Englands entstanden zahlreiche religiöse Gedichte, wobei sich an manchen Elegien wie imThe Wanderernoch die Umbrüche der Zeit bemerkbar machen. Naturgedichte wieThe Seafarerbeinhalten heidnische und christliche Motive. Einer der ersten namhaften Lyriker istCynewulf. Nach der Eroberung Englands durch normannische Truppen im Jahre 1066 verschwand das Altenglische als allgemeine Literatursprache. Das in mittelenglischer Sprache verfasste WerkBrutdes DichtersLayamongehört zu den wichtigsten Dichtungen des 13. Jahrhunderts. Es ist nicht nur mit angelsächsischem Vokabular durchsetzt, sondern steht am Anfang der literarischenArtus-Rezeption in England, zu der auch die bekannte VersdichtungSir Gawain and the Green Knightzählt. Im 14. Jahrhundert entstehenAllegorienund Gedichte wiePiers Plowman,PatienceundPearl. Als Formerneuerer ersetzteGeoffrey Chaucerim 15. Jahrhundert den germanischenStabreimdurch denEndreimund passte den ursprünglich französischen Balladenvers der englischen Sprache an. DieserRhyme royalbesteht aus sieben Versen,jambischen Fünfhebernund demReimschema[ababbcc]. Die starke Wirkung Chaucers zeigte sich in der hohen Zahl seiner Nachahmer, zu denen u. a.John Gower,John LydgateundJohn Hocclevezählen. Selbst der schottische KönigJames I.verfasste Gedichte im Stil Chaucers. Im 16. Jahrhundert schriebSir Thomas Wyattdie ersten Sonette in englischer Sprache.Sir Phillip SidneysSonettzyklusAstrophel and Stellesetzte das schon in Wyatts Dichtung angelegte englische Sonett schließlich durch. Daneben verfasste der JesuitRobert Southwellreligiöse Gedichte undThomas CampionLieder. Die englische Sonettdichtung fand ihren Höhepunkt mitWilliam Shakespeare. Weitere Sonettdichter sindWalter Raleigh,Michael DraytonundSamuel Daniel.Edmund Spenserschrieb die VersepenThe Shepheardes CalenderundThe Faerie Queene. John Donnesmetaphysische Dichtung grenzte sich im 17. Jahrhundert von der starren Sonettdichtung der englischenRenaissanceab. DieCavalier poetsBen Jonson,Richard LovelaceundEdmund Wallernahmen sich weltlicher Themen an. Im späten 18. Jahrhundert überwandenThomas GrayundRobert Burnsdie Auswirkungen der Restauration, deren Dichtung sich hauptsächlich auf die Übersetzung lateinischer Klassiker beschränkte, und besonders der spätere Nationaldichter Schottlands Burns ebnete den Weg zur englischen Romantik. Die Romantik repräsentieren die DichterWilliam Blake,William Wordsworth,Samuel Taylor Coleridge,Percy Bysshe Shelley,Lord ByronundJohn Keats. Zur viktorianischen Epoche werdenAlfred TennysonundRobert Browninggezählt. Hauptvertreter desSymbolismuswar derIreWilliam Butler Yeats, aber auch spätere Dichter der Moderne wie derWaliserDylan Thomaskönnen teilweise zu dieser Richtung gerechnet werden. BedeutendeUS-amerikanischeLyriker sind u. a.Edgar Allan Poe,Walt WhitmanundEmily Dickinsonim 19. Jahrhundert,Wallace Stevens,E. E. Cummings,William Carlos Williams,Ezra Pound,Elizabeth Bishop,Sylvia Plath,Anne Sexton,Allen GinsbergundJohn Ashberyim 20. Jahrhundert. Eine wichtige Rolle spielte die Lyrik auch in der Popkultur seit den 1960er Jahren, etwa beiJohn Lennon,Cat Stevens,Bob Dylan,Leonard Cohenund anderenSongwritern. Die französischsprachige Lyrik beginnt mit den Trouvèren im 12. Jahrhundert, deren Werke auf altfranzösisch verfasst wurden. DieTrobadordichtungim Süden Frankreichs wurde inprovenzalischer Sprachegeschrieben.Marie de Franceverwendete Versformen für ihreLais,Chrétien de Troyesfür seine VerserzählungenErec et EnideundLe Conte du Graal ou le Roman de Perceval.Guillaume de LorrisundJean de Meung, die Autoren desLe Roman de la Rose, nutzten paarweise reimende Verse für ihren Roman. Im 14. Jahrhundert verfassteEustache Deschampsüber tausend Balladen, manchen gilt er gar als Begründer dieser Form. Zu seiner Dichtung werden traditionelle Liebesdichtungen, Satiren und Sentenzen gezählt, sowie die PoetikL'art de dictier et de fere chançons, ballades, virelais et rondeau. Als bedeutendster Dichter der Zeit giltFrançois Villon. Sein HauptwerkLe Testament, das sechzehnBalladenund dreiRondeausenthält, wurde auch außerhalb des französischen Sprachraums über Jahrhunderte hinweg rezipiert. Sein Dichtung beeinflusste Mitte des 19. Jahrhunderts diePräraffaelitenund im 20. Jahrhundert die deutschsprachigen Naturalisten, später die Expressionisten und prägte die Gelegenheitsdichtung.Charles de Valois, duc d’Orléans, der Villon in seinem Schloss aufnahm und ihn später wegen eines Spottgedichtes in den Kerker werfen ließ, dichtete selbst in englischer und französischer Sprache. Bedeutende Lyriker der Renaissance warenPierre de RonsardundJoachim Du Bellay, für die französische Klassik istFrançois de Malherbe, für die AufklärungJacques Delillezu nennen. In derRomantiksindAlphonse de Lamartine,Alfred de MussetundVictor Hugovon Bedeutung, etwa zeitgleich schrieben die „Parnassiens“Théophile GautierundThéodore de Banville. Die großen französischen Dichter der frühen Moderne (Symbolismus) sindCharles Baudelaire,Arthur Rimbaud,Paul VerlaineundStéphane Mallarmé. Bedeutende Lyriker zu Beginn des 20. Jahrhunderts sindGuillaume ApollinaireundPaul Valéry, späterAndré Breton,Paul Éluard,Ivan Goll,Tristan Tzara,Yves Bonnefoyu. a. m. Die wichtigsten (neu-)griechischen Lyriker der Moderne warenKonstantinos Kavafis,Kostis Palamas,Odysseas Elytis,Giorgios SeferisundGiannis Ritsos. InItalienwaren die Lyriker derRenaissanceDante Alighieri(13. Jahrhundert) undPetrarca(14. Jahrhundert) bahnbrechend, weitere wirkmächtige Lyriker warenMichelangelo(15. Jahrhundert) undTorquato Tasso(16. Jahrhundert).Giacomo Leopardi(Anfang 19. Jahrhundert) undGabriele D’Annunzio(19./20. Jahrhundert) waren jeder auf seine Weise Erneuerer der italienischen Dichtung; im 20. Jahrhundert warenGiuseppe Ungaretti,Eugenio MontaleundAndrea Zanzotto– auch international – wegweisend. Der Nationaldichter Polens istAdam Mickiewicz(19. Jahrhundert). Die wichtigsten polnischen Lyriker des 20. Jahrhunderts warenCzesław Miłosz,Zbigniew Herbert,Tadeusz RóżewiczundWisława Szymborska; ein bedeutender Gegenwartsdichter istAdam Zagajewski. Der Nationaldichter Portugals istLuís de Camões(16. Jahrhundert), ihm ist auch der Nationalfeiertag gewidmet;[5]neben ihm stehtAntónio Ferreira. Einflussreiche Lyriker des 19. Jahrhunderts sind der RomantikerSoares de Passosund die symbolistischen DichterAntero de QuentalundCesário Verde. In der portugiesischen Lyrik des 20. Jahrhunderts istFernando Pessoadie wichtigste Stimme; ein weiterer Dichter von Weltrang istEugénio de Andrade. Nach den russischen Nationaldichtern des 19. JahrhundertsAlexander PuschkinundMichail Lermontowwaren in der ersten Hälfte des 20. Jahrhunderts vor allemSergei Jessenin,Osip Mandelstam,Anna Achmatova,Marina Zwetajewa,Boris PasternakundWladimir Majakowskiherausragende russische Dichter. Für die Entwicklung des russischen Futurismus und nachfolgende Avantgarden sindVelimir ChlebnikovundAlexei Krutschonychentscheidend. In der zweiten Hälfte des 20. Jahrhunderts lebten viele bedeutende russische Lyriker außerhalb des Landes, etwaJoseph Brodskyin den USA undAlexeij Parschtschikovin Deutschland. Als slowenischer Nationaldichter giltFrance Prešeren, weitere bedeutende Lyriker des 19. Jahrhunderts warenDragotin KetteundJosip Murn. Im 20. Jahrhundert sindSrečko KosovelundMatej Borzu nennen, die bedeutendsten Dichter warenDane ZajcundTomaž Šalamun. Luis de GóngoraundFrancisco de Quevedo(16./17. Jahrhundert) sind die wichtigsten Lyriker desspanischen Barock. Bedeutende Lyriker des 20. Jahrhunderts sind u. a.Juan Ramón Jiménez,Antonio Machadosowie die Lyriker derGeneración del 27Ramón Gómez de la Serna,Rafael Alberti,Vicente Aleixandre,Jorge Guillén,Pedro Salinas,Miguel HernándezundFederico García Lorca. BedeutendespanischsprachigeLyriker Chiles sindPablo NerudaundNicanor Parra. Der bedeutendste spanischsprachige Lyriker Mexikos istOctavio Paz, der PerusCésar Vallejo. Bedeutende tschechische Lyriker des 20. Jahrhunderts sind u. a.Jiří Wolker,Vítězslav Nezval,Konstantin Biebl,Jiří Orten,František Halas,Vladimír Holan,Jaroslav Seifert,Jan SkácelundJiří Kolář, in jüngerer Zeit u. a.Jáchym TopolundPetr Borkovec. Der allgemeine Begriff fürGedichtim Japanischen istuta(歌, in Zusammensetzungen auch-kaoder唄), was auch „Lied“ bedeutet. Traditionell unterscheidet man japanische Gedichte(Waka)und chinesische Gedichte(Kanshi). Die Hauptformen desWakasind das Kurzgedicht,Tanka, mit 5-7-5-7-7-Morenund das Langgedicht,Chōka, mit 5-7-5-7- … -5-7-7-Moren. Aus der Verkettung von Tanka entstand das Kettengedicht,Renga, dessen Eröffnungsvers mit 5-7-7-Moren später zur eigenständigen GedichtformHaikuwurde. Ähnlich kurz ist auch dasSenryū, das außerhalb Japans nach dem Haiku die bekannteste Form japanischer Poesie darstellt. Gedichte sind bereits in den beiden ältesten überlieferten japanischen Werken, den ReichschronikenKojikiundNihonshokivon 712 bzw. 720 n. Chr. enthalten. 759 n. Chr. erschien mit demMan’yōshūdie erste Gedichtanthologie, die knapp 4500 Gedichte umfasst, wobei ein Teil der Gedichte bis in das frühe 6. Jahrhundert n. Chr. zurückreicht. Obwohl die Werke imMan’yōshūzum Großteil der Hofdichtung zuzuordnen sind, finden sich darin auch Gedichte aus dem einfachen Volk, etwa Soldatengedichte. Die japanischen Kaiser ließen von 905 mit demKokin-wakashūbis 1439 mit demShinshokukokin-wakashūregelmäßig Waka-Anthologien wie dieSammlungen aus einundzwanzig Epochenzusammenstellen. Die bedeutendsten Dichter bis ins 12. Jahrhundert wurden als „Die Sechsunddreißig Unsterblichen der Dichtkunst“ bezeichnet. Als die bedeutendsten Dichter derEdo-Zeit(17.–19. Jahrhundert) geltenMatsuo Bashō,Yosa BusonundKobayashi Issa, während für die ModerneHagiwara Sakutarō,Ishikawa Takuboku,Masaoka Shiki,Miyazawa Kenji,Ogiwara Seisensui,Takamura KōtarōundYosano Akikozu nennen sind. Zu den bedeutendsten persischen Dichtern gehörtAbū ʾl-Qāsim Firdausī(940–1020). Das von ihm verfasste EposSchāhnāme(persischشاهنامه,DMG'Šāhnāma, auchŠāhnāmeh, „Königsbuch“ oder „Buch der Könige“) gilt alsNationaleposderpersischsprachigen Welt. Mit nahezu 60.000 Versen ist es mehr als doppelt so umfangreich wie Homers Epen und mehr als sechsmal so lang wie dasNibelungenlied. Ein weiterer herausragender Dichter istHafis(14. Jahrhundert), dessen Werk unter anderemGoethezu seinemWest-östlichen Divaninspirierte. Auf diesem Weg nahm Hafis Dichtung nachhaltig Einfluss auch auf die europäische Lyrik. Weitere bekannte klassische Dichter sindSaadi,Nezami,Rūmīsowie der MathematikerOmar Chayyām. Im 20. Jahrhundert giltForugh Farrochzadals eine der bekanntesten iranischen Dichterinnen. ImKoranist den zumeist schicksalsgläubigen altarabischen Dichtern ein eigener, kritischer Abschnitt gewidmet. Die letzten vier Verse der „Die Dichter“(asch-Schuʿara)genanntenSure 26setzen sie mitWahrsagernund ziellos Umherirrenden gleich, die vonDschinnoder gar demSatanselbst besessen seien und ihren Einfluss auf das Stammesleben falsch nützten. Der ProphetMohammedgrenzt sich zwar von ihnen ab, bescheinigt aber (in den letzten beiden später offenbarten bzw. hinzugefügten Versen) zumindest einigen unter ihnen Rechtgläubigkeit. Der Gesamtinhalt der Sure ist eine Zusammenfassung der wichtigstenProphetengeschichtendesIslam, die Mohammed trösten und die Ungläubigen warnen sollen. Die muslimisch-arabischen Dichter erfreuten sich nach Mohammed unter denUmayyadenhöchster Protektion, sofern sie dieQuraischglorifizierten und halfen, dieNichtaraberzu arabisieren. Hauptthema der Dichtung vor bzw. bis Mohammed war die Suche des liebenden (und deshalb umherirrenden) Dichters nach der verlorenen Geliebten. Weltweit sind Gedichte (auch) im 21. Jahrhundert in vielen Kulturkreisen und Literatursprachen von großer Bedeutung. Imdeutschen Sprachraumhatten (säkulare) lyrische Schreibweisen nur selten einen solchen Stellenwert, im 20. Jahrhundert ging ihre Rezeption – nicht aber ihre Produktion – eher noch zurück. Unabhängig davon entwickeln sich beständig neue Formen von Lyrik und poetischer Sprachverwendung, z. B. in derDigitalen Poesie, imPoesiefilm, imSpoken Word, im Musikbereich (Hip-Hop,Sprechgesang) und inJugendkulturen(Insta-Poetry). Auch die Lyrik im engeren Sinn hat sich in den vergangenen Jahrzehnten tiefgreifend verändert und bezüglich ihrer Formate, Darstellungsmittel und Gegenstände in alle erdenklichen Richtungen erweitert. Dabei kommt es immer wieder auch zu Adaptionen und Variationen historischer Formate und Mittel, etwa vonmetrischenundgereimtenGedichten im US-amerikanischenNew Formalism. Eine Konstante der Lyrikgeschichte ist dieÜbersetzung von Gedichtenaus allen und in alle erreichbaren Sprachen, die sich im beginnenden 21. Jahrhundert aufgrund neuer technischer Möglichkeiten der Kommunikation und Vernetzung nicht nur in den deutschsprachigen Literaturen ausgeweitet, diversifiziert und ansatzweise auch institutionalisiert hat. →Liste der Wikipedia-Artikel zu Gedichten Im Folgenden werden nur deutschsprachige Längsschnitt-Anthologien deutscher und Überblicks-Anthologien internationaler Lyrik aufgeführt, die das Feld lyrischer Schreibweisen nicht nach Themen oder Genres ausmessen. Zu deutschsprachigen Lyrikanthologien siehe den HauptartikelListe deutschsprachiger Lyrikanthologien, zu weiteren maßgeblichen Anthologien deutschsprachiger Lyrik den AbschnittAnthologienim ArtikelDeutschsprachige Lyrik. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Begriffsgeschichte 1.1Lyrik 1.2Gedicht 2Funktionale Bestimmung 3Kriterien der sprachlichen Form 3.1Formelemente und Gedichtformen 3.2Genres und Subgenres 4Geschichte der Lyrik 4.1Altertum und Antike 4.2Europäisches Mittelalter 4.3Entwicklungen ab dem Spätmittelalter 4.3.1Deutschsprachige Lyrik 4.3.2Englischsprachige Lyrik 4.3.3Französischsprachige Lyrik 4.3.4Griechische Lyrik 4.3.5Italienische Lyrik 4.3.6Polnische Lyrik 4.3.7Portugiesische Lyrik 4.3.8Russische Lyrik 4.3.9Slowenische Lyrik 4.3.10Spanischsprachige Lyrik 4.3.11Tschechische Lyrik 4.3.12Japanische Lyrik 4.3.13Persische Lyrik 4.4Status der Lyrik im Islam 5Gegenwart 6Beispiele 7Anthologien 7.1Deutschsprachige Lyrik 7.2Internationale Lyrik 8Interpretationen und Materialien"
  },
  {
    "label": 0,
    "text": "Malerei – Wikipedia Malerei Inhaltsverzeichnis Maltechniken Geschichte Basiselemente von Gemälden Schutzpatron Siehe auch Literatur Weblinks Einzelnachweise Vorgeschichte Europa Amerika Asien Afrika Australien und Neuseeland Internationale Tendenzen seit der zweiten Hälfte des 20. Jahrhunderts Allgemeines Altertum Altertum Mittelalter Renaissance Barock Rokoko Klassizismus und Romantik Moderne Präkolumbische Kunst Vereinigte Staaten und Kanada Lateinamerika Australien Neuseeland Malereiist die Kunst, Bilder zu malen. Die Bilder werdenGemäldegenannt. DieMaltechnikbesteht darin, feuchteFarbenmittelsPinsel,Spachteloder anderer Werkzeuge auf einen Malgrund aufzubringen. Nicht zu den Maltechniken gerechnet werden in der RegelDrucktechniken, auch wenn dabei mit feuchter Farbe gearbeitet wird. Ausführende der Malerei werden alsMaleroderKunstmalerbezeichnet. Die Malerei zählt neben derArchitektur, derBildhauerei, derGrafikund derZeichnungzu den klassischen Gattungen derbildenden Kunst. Neben derTafel-undWandmalereiunterscheidet man ferner dieGlas-undBuchmalereisowie die Malerei auf Ton,Keramikoder sonstigen Untergründen (Papier, Kunststoff). Die Malerei kann auf eine lange Geschichte zurückblicken. Älteste Zeugnisse der Malerei sind unter anderemHöhlen-undFelsmalereieninEuropa,Amerika,AsienundAustralien. Die bevorzugten Maltechniken im Altertum waren dieEnkaustikund dieTemperamalerei. Es wurde auf Holz, Ton (Vasenmalerei), Stein oder auf den frischen Putz gemalt (sieheFreskenmalerei). Aufgrund der geringen Menge von archäologischen Zeugnissen lässt sich das Farbenspektrum der Antike nur sehr unzureichend rekonstruieren. Es wurde jedoch beobachtet, dass sich die Farben Rot und Blau am besten erhalten haben. Weitere Beispiele für Maltechniken, die im Laufe der Geschichte zum Einsatz kamen, sind dieFreskomalerei,Temperamalerei,Ölmalerei,AquarellmalereiundGouachemalerei. In der modernen Malerei erweitern neuentwickelte Bindemittel ständig die Ausdrucksmöglichkeiten. Eine Alternative oder Ergänzung zur Ölfarbe ist die um 1960 für die künstlerische Verwendung in Europa eingeführteAcrylfarbe, die schnell trocknet und dabei ihre Leuchtkraft behält. Normalerweise wird mit der Hand gemalt. Es gibt aber auch Künstler, die mit dem Mund oder dem Fuß malen (sieheVereinigung der mund- und fussmalenden Künstler). Das älteste Zeugnis der Malerei sind dieHöhlenmalereienmit Tierdarstellungen aus derletzten Eiszeitund demJungpaläolithikum. Diese Malereien und Zeichnungen, daneben aber auchReliefsundPlastiken, sind als künstlerische Ausdrucksformen aus der Zeit von etwa 45.000 bis 10.000 v. Chr. bekannt. Die ältesten bekannten Höhlenmalereien wurden in denHöhlen im Maros-Pangkep Karstauf Sulawesi in Indonesien entdeckt, sie sind etwa 40.000 bis 45.500 Jahre alt. Europäische Höhlenmalereien wurden vor allem inSpanienundFrankreichentdeckt (frankokantabrische Höhlenkunst). Dazu zählen die im Jahr 1940 entdeckten Wandbilder in derHöhle von Lascauxin Südfrankreich, die meist Tiermotive zeigen:Rinder,HirscheundPferdewurden in beeindruckender Weise dargestellt und gehören damit zu den ältesten bekannten malerischen Motiven der Menschheit.[1] Auch ausAfrika,AsienundAustraliensind früheFels-und Höhlenmalereien bekannt. ImNahen Ostengibt es schon für dasNeolithikumzahlreiche Belege für frühe Malereien. Hier ist vor allem das Aufkommen von bemalter Keramik und die Wandmalerei zu nennen. Zahlreiche Reste prähistorischer Wandmalereien fanden sich imkleinasiatischenÇatalhöyükund datieren um 6000 v. Chr. In den Kulturen desAlten Orientswar die Malerei, und wohl besonders dieWandmalerei, ab 10.000 v. Chr. weit verbreitet, doch ist relativ wenig davon erhalten geblieben. Einige Beispiele stammen aus dem Palast vonMari, während aus den Palästen derHethiterlediglich Fragmente existieren, die aber kaum ein Bild der einst vorhandenen Bemalung erlauben. Von denAssyrernsind schließlich weitere Beispiele, vor allem von Wandmalerei, bekannt. Späteren Datums sind die Wandmalereien desalten Ägyptens(ab 3000 v. Chr.) und dieminoischeFreskomalereiaufKreta(ab 2000 v. Chr.). Die Malerei der alten Ägypter ist vor allem von Wandmalereien aus Grabkapellen, von Särgen und von der Bemalung vonTotenbüchernbekannt. Im dritten Jahrhundert n. Chr. bezeichnete der griechische SchriftstellerPhilostratosdie Malerei als eine Erfindung der Götter (Eikones 1). Durch diese und andere Aussagen antiker Autoren ist bezeugt, dass die Malerei besonders hoch angesehen war, höher sogar als dieBildhauerei. In dem antiken Griechenland wurde alsBildträgerhauptsächlich Holz verwendet, außerdem Stein, Ton und Stuck. Die Verwendung von Elfenbein, Glas und Leinwand tritt kaum auf, jedoch sind nur wenige Reste der antiken griechischen Malerei erhalten. Nach dem Untergang derminoisch-mykenischen Kulturmit ihrer qualitätvollen Freskomalerei (zum Beispiel inKnossos) setzte die griechische Wandmalerei erst wieder im achten Jahrhundert v. Chr. ein. Die Anzahl und die Art der archäologischen Zeugnisse der römischen Malerei unterscheiden sich wesentlich von der griechischen Malerei. Erhalten sind zahlreiche Zeugnisserömischer Wandmalerei, wobei es offensichtlich ein römisches Phänomen ist und nicht etwa nur Zufall der Überlieferungssituation.Plinius der Älterebeklagt in seinerNaturalis historia(35,118) den weitgehenden Wechsel von der Tafel- zur Wandmalerei. Viele dieser Fresken stammen aus den im Jahre 79 n. Chr. verschütteten StädtenPompejiundHerculaneum. Die Malerei der Antike überlebte vor allem in den Wandmalereien der Kirchen undVignettenvon Büchern. Eine erste Blütezeit erreichte die Malerei mit derbyzantinischen Kunst, unter anderem mit derIkonenmalerei, die auch in der russischen Kunst übernommen wurde.[2]ImMittelalterwar ferner dieBuchmalereivon überragender Bedeutung.[3]DieTafelmalereials Malerei auf eigenständigem Maluntergrund – also auf der Holztafel, dann aufLeinwand– entwickelte sich erst allmählich neu. In der Gotik ab dem 12. Jahrhundert entwickelte sich im Vergleich zur byzantinischen und romanischen Bildsprache ein zunehmender Naturalismus. Bedeutende Künstler aus derMalerei in der Gotiksind unter anderemGiotto, dessenFreskenwegweisend auch für die spätereRenaissancesind,Simone Martini,Robert Campin,Jan van Eyck,Hieronymus BoschundMatthias Grünewald.[4] Bis in die Neuzeit hinein war die europäische Malerei, durch die kirchlichen Auftraggeber, fast ausschließlich religiös geprägt. Es kam zur Ausmalung von Kirchen und Klöstern und zur Ausschmückung von Kirchenchören und Andachtskapellen mit Altarbildern. AndereSujetswie diePorträtmalerei,Genremalerei,Landschaftsmalereioder dasStilllebenkamen erst im späten Mittelalter und in derMalerei der Renaissancehinzu. Eine bedeutende Entwicklung nahm dieMalerei in der Renaissancemit der nachhaltigen Weiterentwicklung der Techniken derÖlmalerei, derZentralperspektive, der Darstellung allegorischer Motive der Antike und bei der Betonung individueller Charaktere in der Menschendarstellung unter Zuhilfenahme der Kenntnisse derAnatomie. Jan van Eyck(um 1390–1441) schuf erstmals selbständige Bildnisse undPorträts;Albrecht Dürer(1471–1528) malte 1493 das erste bekannte autonomeSelbstbildnisin Europa;Albrecht Altdorfer(um 1480–1538) machte als erster Maler dieLandschaftzum Hauptthema. Vor allem die italienische Malerei der Hochrenaissance in den StädtenFlorenz,VenedigundRombrachte bedeutende Werke der MalerLeonardo da Vinci(1452–1519),Michelangelo(1475–1564),Raffael(1483–1520) undTizian(ca. 1488–1576) hervor, die in Perspektive, Form, Farbe, Ausdruckskraft und malerischer Brillanz beispielgebend sind und die Bildende Kunst Europas bis heute nachhaltig beeinflusst haben. Die Malerei des Barock nahm ihren Anfang im Italien des ausgehenden 16. Jahrhunderts. Die Malerei wurde im Laufe des 17. Jahrhunderts verstärkt funktionalisiert: Sie wurde entweder von der Kirche in den Dienst derGegenreformationgestellt oder von denabsolutistischenHöfen zur Verherrlichung des Regenten eingesetzt. DasTafelbildentwickelte sich zu einem begehrten Sammelobjekt für Adlige, Könige und das aufsteigende Bürgertum.[5]Bedeutende Maler sind in ItalienMichelangelo da Caravaggio, in SpanienDiego VelázquezundBartolomé Esteban Murillound in den NiederlandenPeter Paul RubensundRembrandt, der als Meister desChiaroscuro(Helldunkel) gilt. In denNiederlandensetzte sich zu dieser Zeit mit der Unabhängigkeit und mit derReformationeine neue Gesellschaftsordnung durch, die durch geistige Strenge gekennzeichnet war. In der Malerei lag die Betonung auf Alltagsszenen.[6]So ist diese Periode eine große Zeit derStillleben, etwa vonPieter Claesz. Der MalerJan Vermeervermittelt in seinen Bildern ruhige, andächtige Stille.[7]Die Zeit war jedoch auch die große Zeit vonRembrandt, der in seinen Bildern wie etwa dieDie Blendung Simsonsgroße Dramatik erzeugte.[6] In Frankreich dominierten Ideale aus der klassischen Antike, die besonders in der Landschaftsmalerei eine Rolle spielten. Die französischen MalerNicolas PoussinundClaude Lorrain, die beide jahrelang in Italien lebten, ließen sich in ihren Landschaftsbildern davon inspirieren.[8] DerRokokostilentwickelte sich als eine stark verspielte Variante des Barock, für dieMuschelmusterund andereornamentaleElemente typisch waren. Ein früher französischer Vertreter dieses Stils ist der MalerAntoine Watteau, dessen Hauptmotive unter anderem Szenen mit singenden und tanzenden Liebenden sowieSchauspieltruppenwaren.François Boucherist ein weiterer typischer Vertreter des Rokoko, auch seine Bilder zeichnen sich durch Verspieltheit aus, die ernste Themen möglichst meidet.[9]Der größte Vertreter des italienischen Rokoko istGiambattista Tiepolo, der vor allem für seineFreskenbekannt ist, wie etwa dieHochzeitsallegorieimPalazzo RezzonicoinVenedig. In Venedig hielt sich der Spätrokoko mitFrancesco Guardiauch noch, als dieser Stil im Rest Europas bereits außer Mode war.[10] Um die Mitte des 18. Jahrhunderts entstand derKlassizismusals ablehnende Reaktion auf den Spätbarock und den Rokoko. Der Klassizismus strebte danach, Werte wieGerechtigkeit,EhreundPatriotismuskünstlerisch zu vermitteln. Vorbild war für die Maler dabei die klassische Antike. Der Klassizismus endete im frühen 19. Jahrhundert bereits wieder. Beinahe zeitgleich entstand die Bewegung derRomantik, die ihre Ursprünge Ende des 18. Jahrhunderts hat, aber erst in der ersten Hälfte des 19. Jahrhunderts zur dominanten Kunstrichtung wurde.[11] Klassizistische Maler in Frankreich griffen unter anderem Motive aus der Antike auf oder, wieJacques-Louis Davidmit seinemDer ermordete Marat, malten Ereignisse derfranzösischen Revolution. Einen Höhepunkt erreichte der Klassizismus mit den Werken vonJean-Auguste-Dominique Ingres. Eine weitere Malerin der Zeit, deren Malstil aber auch Elemente des Barock noch aufgriff, warMarie Elisabeth Louise Vigée-Lebrun.[12] Maler der britischen Schule im 18. Jahrhundert waren weder dem Klassizismus noch der Romantik eindeutig zuzuordnen. Ein bedeutender Vertreter dieser Zeit warThomas Gainsborough, dessen Bilder sich durch eine gelungene Kombination vonPorträtmit Landschaftselementen auszeichneten. Ein weiterer wichtiger Porträtist dieser Zeit warJoshua Reynolds. Eine Alleinstellung nimmt der spanische MalerFrancisco Goyaein. Zählt er anfangs zu den Klassizisten, entwickelt er später doch einen ganz eigenen Stil.[13] Die Romantik bildete einen Gegenpol zum Klassizismus, indem sie ihren Schwerpunkt nicht auf die Antike, sondern auf die moderne Welt legte. Außerdem räumte die Romantik dem Wilden, dem Unkontrollierbaren in Mensch und Natur einen Platz ein, im Gegensatz zum Klassizismus, der strenge Ansichten zuÄsthetikund Themenwahl pflegte. Als wichtiger Vertreter der Romantik in Frankreich ist zunächstThéodore Géricaultzu nennen, dessenFloß der Medusaals Symbol für die Romantiker sehr wichtig war.[14]Weitere Vertreter der Romantik sind der DeutscheCaspar David Friedrichund der EngländerWilliam Turner, beide bekannt für ihre Landschaftsmalerei. Mit denPräraffaelitenund demMystizismuseinesWilliam Blakeentsteht eine Art der Malerei, die das Gefühl in den Mittelpunkt stellt. DiesymbolistischeMalerei wendet sich von realistischen Darstellung ab und versucht eine Wirklichkeit zu schaffen, die nur in der Malerei existieren kann. In dieser Tradition stehen dann auch spätere Entwicklungen in der Geschichte der Malerei, wieJugendstil,ExpressionismusundSurrealismus.[15] Mit dem Aufkommen derFotografiemusste sich die Malerei neuen Herausforderungen und Aufgaben stellen, die nicht ohne Einfluss auf ihre Entwicklung im19. Jahrhundertblieben. Bis Mitte des 19. Jahrhunderts entstanden Gemälde überwiegend in Künstlerateliers. DasBraith-Mali-MuseuminBiberach an der Rißbeherbergt solche Originalateliers. Mit den französischen Malern desImpressionismusbeginnt dieFreilichtmalerei. Zu dieser Richtung gehörenCamille Pissarro(1830–1903),Édouard Manet(1832–1883),Edgar Degas(1834–1917),Paul Cézanne(1839–1906),Alfred Sisley(1839–1899),Claude Monet(1840–1926),Berthe Morisot(1841–1895) undPierre-Auguste Renoir(1841–1919).[16]Paul Cézanne kann schon, wie in gewissem Maße auchVincent van Gogh,Paul GauguinundEdvard Munch, als Wegbereiter der Moderne bezeichnet werden. Gerade Cézannes Werke markieren den Übergang, löst er sich doch zunehmend von der Wiedergabe der Realität und wendet sich den Mitteln der Malerei, der reinen Form und Farbe, zu. Wichtige Strömungen der Moderne sind in der ersten Hälfte des 20. Jahrhunderts derFauvismus, derKubismus, derDadaismus, derSurrealismusund dieRussische Avantgarde. Nationale Besonderheiten sind derExpressionismusund dieNeue Sachlichkeit(Deutschland), derFuturismus(Italien), derKubo-Futurismus,Konstruktivismusund derSuprematismus(Russland) sowie derVortizismus(England). Eine weitere Entwicklung der Kunst der Moderne ist dieAbstrakte Malerei, die sich wiederum in Form unterschiedlicher Stile darstellt. Weitere wichtige Stilrichtungen des 20. Jahrhunderts sind derTachismusund weitgehend nach 1950 dasInformel. Legendäre Maler wiePiet Mondrian(1872–1944),Kasimir Malewitsch(1879–1935) undPablo Picasso(1881–1973) haben die Malerei des20. Jahrhundertskünstlerisch entscheidend geprägt.[17] Die ältesten Kunstzeugnisse in Südamerika sindHöhlenmalereienin derCaverna da Pedra PintadainBrasilienaus dem 12. Jahrtausend v. Chr. In Nordamerika ist einBisonschädelmit aufgebrachter Zickzacklinie in roter Farbe aus dem 11. Jahrtausend v. Chr. von einem Jagdplatz derFolsom-KulturinOklahomadie älteste bekannte Malerei Nordamerikas. Aus präkolumbischen Kulturen sind vornehmlichArchitektur,KeramikundPlastiken,Goldschmiedekunstund weiteres Kunsthandwerk überliefert. Zu bedeutenden erhaltenen Malereien zählen die Wandgemälde aus der archäologischen FundstätteCacaxtlaim BundesstaatTlaxcalainMexiko. Die Ursprünge derSandbilderderNavajoin den heutigen Vereinigten Staaten gehen weit in die Vergangenheit zurück. Die ersten Zuwanderer aus Europa ignorierten die Kunst der Ureinwohner weitgehend. Die Ureinwohner wurden jedoch Motive der Malerei, so etwa in denIndianer- undInuit-Porträts vonJohn White(etwa 1540 bis etwa 1593). InKanadaentstand Anfang des 20. Jahrhunderts durch dieGroup of Seveneine eigene kanadische Malerei, die sich von ihren europäischen Vorbildern löste. Eine wichtige Vertreterin der kanadischen Malerei, die mit der Group of Seven verbunden war, istEmily Carr. Die erste bekannte Schule der Malerei auf US-amerikanischen Boden entstand 1820, dieHudson River School, eine Gruppe amerikanischeLandschaftsmaler. Erst ab dem 20. Jahrhundert löste sich die Kunst in den Vereinigten Staaten von ihren europäischen Vorbildern. Zu den ersten eigenständigen Kunstrichtungen, die in den USA ihren Ursprung hatten, gehörte derAmerikanische Realismus, zu dessen Vertretern unter anderemRobert Henri,George Bellows,Edward HopperundDiego Riveragehören.[18]Als führende Kunstrichtung abgelöst wurde der Amerikanische Realismus in den Vereinigten Staaten in den 1940er und 1950er Jahren durch denAbstrakten Expressionismus, zu deren führenden PersönlichkeitenJackson Pollock,Willem de KooningundMark Rothkozählen.[19]Nennenswert ist ferner diePop Art, die ihre Anfänge in den 1950er Jahren in Großbritannien hatte, aber ab den 1960er Jahren schufen auch junge amerikanische Künstler Pop-Art-Werke, darunterAndy WarholundRoy Lichtenstein.[20] Die heutige amerikanische Malerei lässt sich nicht mehr auf einen Stil oder eine Schule festlegen, denn Kunst des 21. Jahrhunderts ist häufig ein Produkt einer globalisierten Welt.[21] In den 1920er- und 1930er-Jahren entwickelten einige intellektuelle Gruppierungen ein Interesse an den volkstümlichen Wurzeln der lateinamerikanischen Kultur. Neben den europäischen Einflüssen wurden nun auch die afrikanischen und amerikanisch-indianischen Anteile an der Kultur anerkannt. Dies mündete in vielen lateinamerikanischen Ländern inAvantgardebewegungen, die neue und eigene künstlerische Ausdrucksformen entwickelten.[22] InMexikoz. B. entwickelte sich derMuralismo, der dieWandmalereiins Zentrum stellte. Ein wichtiger Vertreter dieser Avantgarde der MalerDiego Rivera. In Brasilien suchte Tarsila do Amaral mit lebendigen Farben sowie der Verwendung von tropischer Fauna und Flora als Motive einen Beitrag zur Entwicklung einer eigenständigen brasilianischen Malerei. In Uruguay führte der Maler Joaquín Torres-García den Modernismus in die Malerei ein, indem er abstrakte Grundformen mit Motiven und Bildern aus präkolumbischer Zeit und aus dem Leben der industrialisierten Gegenwart kombinierte. Ab den späten 1930er Jahren setzten sich Künstler in Lateinamerika vorrangig mit gesellschaftlichen Problemen, auch als Reaktion auf den Spanischen Bürgerkrieg, den Zweiten Weltkrieg und den Kalten Krieg.[22] Internationale Berühmtheit erlangte die mexikanische MalerinFrida Kahlo, deren Werke Inspirationen aus der alten mexikanischen Kultur mit Einsichten in ihre eigene Gefühlswelt verband. InKubakombinierteWifredo LamSurrealismus, seine Bewunderung fürPablo Picassound afrokubanischeReligiosität.[23] Heutige lateinamerikanische Künstler stehen unter anderem vor der Frage, ob sie Kunst für ein heimisches Publikum schaffen oder eher den internationalen Kunstmarkt bedienen.[23] Die Malerei in Asien hat ihre Ursprünge vor über 40.000 Jahren inHöhlen im Maros-Pangkep Karstin Sulawesi, Indonesien. Ein weiteres, bedeutendes Zeugnis der Malereien aus der Frühzeit der Menschheit sind die steinzeitlichen Felsmalereien vonBhimbetka, ein archäologischer Fundplatz imindischenBundesstaatMadhya PradeshundUNESCO-Weltkulturerbe. Zu weiteren bedeutenden alten Zeugnissen asiatischer Malerei gehören dieFreskenin denAjanta-Höhlenauf dem indischen Subkontinent von ca. 450–500. Dieindische,chinesische,koreanischeundjapanische Malereikönnen auf eine längere, bedeutende Geschichte zurückblicken. In Japan sind vor allem dieTuschmalereienvon Weltrang. Gleiches gilt für die Tuschmalerei in China und diechinesische Kalligrafie. Einen Höhepunkt erreichte die chinesische Malerei in derSong-Dynastie. Die indische Malerei erreichte ihren Höhepunkt in derMogulzeit. Weitere bedeutende Malstile vom asiatischen Kontinent sind dieMandalasaus der Tradition desBuddhismus, dieSeidenmalereiausVietnamund die islamischeBuchmalerei. Vorlage:Panorama/Wartung/Dir Afrikanische Kunst umfasst die künstlerische Produktion vieler sehr verschiedenerEthnienauf dem afrikanischen Kontinent. Häufig wird eine Unterscheidung zwischen der KunstSubsahara-Afrikasund der Kunst derberberischundarabischbeeinflussten Staaten im Norden Afrikas gemacht. In der traditionellen afrikanischen Kunst der Subsahara ist die Malerei unterrepräsentiert, da viele Ethnien traditionell eherSkulpturen,Metallguss,Schnitzarbeiten,Webarbeiten,SchmuckundTonwarenherstellten. Aufgrund der klimatischen Gegebenheiten und aufgrund der Tatsache, dass viele Kunstgegenstände aus Holz und anderen Naturmaterialien hergestellt wurden, sind relativ wenige historische Objekte der traditionellen afrikanischen Kunst überliefert. Anders stellt sich die Situation im nördlichen Afrika dar. So ist die Malerei Teil deraltägyptischen Kunst, die eine jahrtausendalte Tradition hat.Byzantinischbeeinflusst ist dieMalerei im christlichen Nubien, die ab dem 18. Jahrhundert belegt ist. Verschiedene Faktoren trugen zur Entstehung einer modernen Malerei in Afrika bei: Zum einen wurden noch zurKolonialzeitKünstlerwerkstätten durch Europäer inKolonialverwaltung,BildungoderMissioneingerichtet, zum anderen entstandenKunstschulenundAkademien, die jedoch dem Modell westlicherKunsterziehungverpflichtet waren und die zunächst keine eigenständige afrikanische Kunst hervorbrachten. Ein weiterer, wesentlicher Faktor für die Entstehung moderner afrikanischer Kunst war eine Generationautodidaktischarbeitender Künstler ab dem Anfang des 20. Jahrhunderts, die moderne Arbeitsweisen verwandten.[24] Der Durchbruch für moderne afrikanische Kunst erfolgte in den späten 1940er und 1950er Jahren, als afrikanische Künstler wie Ben Enwonwu (Nigeria), Ibrahim e-Salahi (Sudan), Farid Belkahia (Marokko) oder Papa Ibra Tall (Senegal) sich etablierten. Sie lösten sich aus den engen akademischen Vorgaben und erarbeiteten die Grundlagen für moderne Kunstbewegung in Afrika. Werke moderner afrikanischer Künstler reichen von der Umsetzung tradierter Motiven und Techniken über das Aufgreifen westlicher Stile und Trends bis zu einer Kombination westlicher und traditioneller Arbeitsweisen.[25] Zeitgenössische Künstler aus afrikanischen Ländern erhalten heutzutage mehr Aufmerksamkeit: So sind Kunstmessen in Afrika wie die Biennalen in Kairo, Johannesburg oder Dakar zu international beachteten Ereignissen der Kunstwelt geworden. Die Globalisierung ermöglicht ferner, dass afrikanische Künstler sich leichter mit neuesten Trends in der Kunstszene auseinandersetzen können oder an diesen mitarbeiten.[25]Auch das Interesse der Forschung und der Museen an zeitgenössischer afrikanischer Kunst ist in Europa stark gestiegen.[26] Die Kunst derAborigineskann auf eine mehrere tausend Jahre alte Tradition zurückblicken, vonFelsmalereienbis zu heutiger Kunst. Bedeutende traditionelle Maltechniken und -stile sind dieKörpermalerei, mit der Aborigines ihre Körper mit traditionellen Mustern z. B. fürZeremonienund Tänze bedecken, Baumrinden-Malerei und Sandmalerei. Bedeutende Preise auf internationalen Kunstmärkten erzielen die Baumrinden-Malereien der Aborigines. Diese Malereien werden auf dem geglätteten Inneren von Baumrinde aufgetragen, die von Eukalyptus-Bäumen stammen. Aus den Wüstenregionen Australiens stammen die Ursprünge der Sandmalerei. Heutige Maler greifen auch auf Leinwand und Farbe zurück, so etwa im Dot-Painting, und ersetzen die vergänglichen Sandmalereien damit durch eine dauerhafte Form. Die ersten Gemälde europäischer Siedler orientierten sich an europäischen Vorbildern, verwendeten meist Tiere oder Aborigines als Motive. Mit der Aneignung des französischenImpressionismusdurch die Heidelberger Schule inMelbournegegen Ende des 19. Jahrhunderts gelang australischer Kunst europäischer Siedler erstmals internationale Anerkennung. In der voreuropäischen Zeit bestand das Malhandwerk vor allem aus dem kunstvollen Verzieren von Holzgegenständen wiePaddelnoderKanus, der Bemalung vonMonumentenund den Dachbalken der Versammlungshäusern derMaorisowie das kunstvolle Bemalen von Steinen und Steinfelsen inHöhlenoderGrotten. Mit dem Eintreffen der Europäer wird die Malerei als Kunst und Kunsthandwerk zunehmend europäisch beeinflusst. Es gibt nur wenige Beispiele und Zeugnisse der traditionellen Malerei dermaorischenUrbevölkerung. Man vermutet, dass die Malerei nicht den gleichen Stellenwert in derMāori-Kultur besaß wie zum Beispiel die Schnitzereikunst, das Herstellen von Skulpturen oder Tā moko, die Kunst desTätowierens. Die Malerei der europäischen Siedler in Neuseeland wurde im 19. und 20. Jahrhundert durch einen konservativen englischen Stil dominiert. Viele Maler konzentrierten sich aufLandschaftsdarstellungen, aber einige Maler gewannen ihren Lebensunterhalt auch die Porträtierung vonMāori. Zwischen 1900 und 1930 reisten viele der ambitioniertesten Maler Neuseelands nach Europa, um dort die modernen Kunstrichtungen zu studieren. Die Künstlerin Francis Hodges ist eine der erfolgreichsten neuseeländischen Malerinnen dieser Zeit. Ab den 1930er und 1940er Jahren kann man von einer eigenständigen neuseeländischen Malerei sprechen, die vor allem lokale Themen und Landschaften verarbeitete.[27] Durch eine größere Mobilität und leichteren Zugang zu Kunstmagazinen, Büchern undWanderausstellungenmoderner Kunst hatten die Neuseeländer nach dem Zweiten Weltkrieg zunehmend besseren Zugang zu Kunst der Avantgarde. Dadurch erreichten neuere Kunstrichtungen wieExpressionismus,KubismusoderAbstraktionverspätet auch Neuseeland und wurden dort auch spät akzeptiert. Heute ist neuseeländische Kunst durch die Globalisierung Teil der internationalen Kunst, die sich z. T. wenig von der in New York oder London produzierten Kunst unterscheidet.[27] In den 1960er und 1970er Jahren erlebte dieMāori-Community ein kulturelles und nationalistisches Revival. Als Konsequenz produzierten jüngereMāori-Künstler Werke, die eine Synthese traditioneller Kunst und europäischer Techniken darstellt. So flossen in die Malerei einerseits symbolische Musterungen und geometrische Designs aus derMāori-Tradition ein, andererseits Materialien und Techniken aus der westlichen Kunst. Mit der AusstellungTe Maoriim Metropolitan Museum of Art in New York 1984 erreichteMāori-Kunst und Kultur auch in Neuseeland eine größere Anerkennung.[27] In der bipolaren Welt desKalten Kriegesschlug auch die Kunst und mit ihr die Malerei zwei Wege ein. In der sowjetischen Einflusssphäre wurde sie vornehmlich zu staatstragender Propaganda verpflichtet (u. a.Sozialistischer Realismus). Bekannte Maler im Sozialismus warenWilli Sitte,Bernhard Heisig,Werner Tübke. Die westliche Welt spielte dagegen bis in die achtziger Jahre hinein die Gedanken der Moderne weiter. Es fand eine stetige Erweiterung des Kunstbegriffes statt (Konzeptkunst,Fluxus,Happening). Die klassischen Kategorien, von Malerei, Bildhauerei, Graphik verloren immer mehr an Gewicht. Die fünfziger Jahre waren geprägt von größtenteils abstrakt expressiven oder konstruktiven Tendenzen, die unter den BegriffenInformel,Tachismus, amerikanischer „abstrakter Expressionismus“ undKonkrete Kunstzusammengefasst werden können, in denen die Malerei noch eine wesentliche Rolle spielt. Mit der wirtschaftlichen Konsolidierung der westlichen Hemisphäre und ihrem kulturellen Zentrum New York spaltete sich die Kunstwelt ab den sechziger Jahren in einen formalistischen Zweig,Minimal Artund letztendlichKonzeptkunstauf der einen Seite, in diePop Artauf der anderen Seite auf. Letztere bezieht sich auf die bunte Werbe- und Mediensprache und baut somit auch auf das gemalte und repräsentative Bild. (u. a.Andy Warhol,Robert Rauschenberg,Roy Lichtenstein, in Deutschland als „kapitalistischer Realismus“ mitGerhard Richter,Sigmar Polke). In frühen Versuchen, schon in den dreißiger Jahren und aktiv seit 1955 begannEdmund Kestingmit chemischer Malerei, wie er diese Technik nannte, zu experimentieren. Mit Masken und Schablonen belichtete Kesting, ohne Verwendung einer Kamera, ausschließlich durch Auftragen von chemischen Fotoprodukten, wie Entwickler oder Fixierer, schwarzweißes Fotopapier. Parallel zu dieser Entwicklung wird später, in der Mitte des letzten Jahrhunderts, die Schnittstelle zwischen den zu diesem Zeitpunkt noch weitestgehend getrennten Medien Malerei und Fotografie kunsthistorisch relevant durch Arbeiten der FotokünstlerPierre Cordier(1956)Chemigramm,Paolo Monti(1970) (Chimigramm) undJosef H. Neumann(1974)Chemogrammgeschlossen. Die Chemogramme von Josef H. Neumann weisen in der Besonderheit, im Gegensatz zur kameralosen chemischen Malerei, erstmals die Trennung von malerischem Grund und fotografischer Schicht auf. Neumann vereint 1974, durch parallele Verwendung einer Kamera innerhalb seiner Chemogramme, gleichzeitig die Medien Malerei und reale fotografische Perspektive.[28] Der schnelle Wechsel der verschiedensten Stile (Action Painting,Op-Art,Fotorealismus,Hard Edgeund andere; siehe auchStilrichtungen in der Malerei) fand in den achtziger Jahren mit dem Einläuten derPostmoderneein Ende, zugleich sorgten die „Neuen Wilden“ (unter anderemJörg Immendorff,Walter Dahn,Kurt Schulzke,Albert Oehlen,Markus Oehlen,Salomé;Georg Baselitz,A.R. PenckundMarkus Lüpertzentstammen einer älteren Generationen, gewannen aber in jener Zeit sehr an Gewicht) und die „Transavantgarde“ (darunterSandro Chia,Enzo Cucchi,Francesco Clemente) für eine Renaissance des expressiven gemalten Bildes. Die Auseinandersetzung mit den neuen digitalen Medien prägten die Tendenzen der neunziger Jahre, während der Fall derBerliner Mauer, 1989, auch international den Boden für junge Maler und Malerinnen bereitete, die ihre Ausbildung an Hochschulen von Leipzig, Berlin und Dresden erfahren hatten und später ab der Jahrtausendwende zum Teil unter dem Begriff der „Neuen Leipziger Schule“ zusammengefasst wurden (insbesondereNeo Rauch). Das erste Jahrzehnt des 21. Jahrhunderts beschäftigte sich malerisch vornehmlich mit einer teils mythologisierenden Aufarbeitung des vergangenen Jahrhunderts (z. B.Jonathan MeeseoderAndy Hope 1930). Das gemalte Bild wurde immer mehr zu einem Stilmittel unter vielen, um die Aussage des Künstlers zu transportieren. Hier zählte auch keine Diskussion mehr, sei sie nunfigurativ, expressiv, oder konstruktiv. Weitere bedeutende Maler nach 1945 warenFrank Stella,Barnett Newman,Jasper Johns,Asger Jorn,Martin Kippenberger,Jackson Pollock,Günther Förg,Wolf Vostell,Imi KnoebelundAnselm Kiefer. Aussoziologischer Sichtgehört die breite Masse an Malern besonders oft zu denWorking Poor, das heißt zu den Personen, die trotz Arbeit in Armut leben.[29] Die Basiselemente von Gemälden sind die Intensität, die Farbe und der Farbton und der Rhythmus. Bei Gemälden der zeitgenössischen Kunst werden zusätzlich nicht-traditionelle Basiselemente beschrieben.[30]Die Intensität eines Gemäldes wird durch Schattierung, Kontrastierung und den Einsatz von benachbarten Elementen mit unterschiedlichen Farbintensitäten definiert. Das Nebeneinanderstellen von Bildelementen mit der gleichen Farbintensität kann nur symbolische Differenzierung hervorrufen.[31]Die Wahrnehmung der Farbe und des Farbtons ist subjektiv, kann aber kulturell unterschiedliche psychologische Effekte hervorrufen. So wird die Farbe Schwarz im Westen eher mit Trauer in Verbindung gebracht, während die Farbe der Trauer im Osten weiß ist. Es gibt zahlreiche Theorien derFarbenlehrevon bekannten Künstlern, Wissenschaftlern und Autoren wieGoethe,KandinskyundNewton. Der Rhythmus ist im Bild abstrakt definiert als eine Pause innerhalb einer Sequenz und beschreibt die Verteilung von Formen, Farben und Schattierungen.[32]Die nicht-traditionellen Basiselemente wurden durch moderne Künstler eingeführt und umfassen Techniken, die durch die traditionellen Basiselemente nicht erfasst werden. Dies sind z. B. Collagen oder der Einsatz von Materialien wie Sand, Stroh oder Holz für die Verleihung von Texturen. Der EvangelistLukasist derSchutzpatronder Kunstmaler. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Maltechniken 2Geschichte 2.1Vorgeschichte 2.2Europa 2.2.1Altertum 2.2.2Mittelalter 2.2.3Renaissance 2.2.4Barock 2.2.5Rokoko 2.2.6Klassizismus und Romantik 2.2.7Moderne 2.3Amerika 2.3.1Präkolumbische Kunst 2.3.2Vereinigte Staaten und Kanada 2.3.3Lateinamerika 2.4Asien 2.5Afrika 2.6Australien und Neuseeland 2.6.1Australien 2.6.2Neuseeland 2.7Internationale Tendenzen seit der zweiten Hälfte des 20. Jahrhunderts 3Basiselemente von Gemälden 4Schutzpatron 5Siehe auch 6Literatur 6.1Allgemeines 6.2Altertum 7Weblinks 8Einzelnachweise Afrikaans Alemannisch"
  },
  {
    "label": 0,
    "text": "Memphis (Tennessee) – Wikipedia Memphis (Tennessee) Inhaltsverzeichnis Geographie Geschichte Politik Bevölkerung Wirtschaft und Verkehr Bildung Medien Kultur und Freizeit Persönlichkeiten Anmerkungen Literatur Weblinks Indianische Besiedlung und erste Europäer Frühes 19. Jahrhundert Sezessionskrieg und danach: Wohlstand und Emanzipation Gelbfieber Jahrhundertwende und die Ära Crump Civil Rights Movement und danach Einwohnerentwicklung Religion Radio Musik Museen Sehenswürdigkeiten Regelmäßige Veranstaltungen Essen und Trinken Sport Söhne und Töchter der Stadt Persönlichkeiten, die vor Ort gewirkt haben Musikbands Die Ermordung Martin Luther Kings Die Folgen und die langsame Erholung der Stadt Memphisist die zweitgrößte Stadt imUS-BundesstaatTennesseeund County-Sitz desShelby County. Die Stadt liegt im äußersten Südwesten Tennessees am Ostufer desMississippi River. DasU.S. Census Bureauhat bei derVolkszählung 2020eine Einwohnerzahl von 633.104[2]ermittelt. Die Stadt ist eine derMetropolender klassischenSüdstaaten. Nachdem Memphis bis in denSezessionskriegund die 1870er hinein floriert hatte, suchten mehrere Katastrophen die Stadt heim. In jüngster Zeit verdankt sie ihren wirtschaftlichen Aufschwung vor allem der TransportfirmaFedEx, die mit Abstand größter Arbeitgeber der Stadt ist. Die Stadt ist ein wichtiger Ort sowohl für die Entwicklung desBluesund desSoulsals auch für die desRock ’n’ Rolls.Elvis Presleylebte in Memphis, viele Größen der Rockmusik begannen ihre Karriere dort. DieBeale Streetist eines der Zentren des Blues. Memphis liegt im US-amerikanischen Südosten im Dreistaaten-Eck zwischenTennessee,Mississippi(südlich von Memphis) undArkansasan der Mündung desWolf Riverin denMississippi River. Sowohl die Geographie als auch die geschichtliche Entwicklung und Kultur werden maßgeblich von der Lage am Mississippi und der geographischen Nähe derLower Mississippi Delta Regionbestimmt. Die Stadt wurde 1912 und 1937 von schweren Überflutungen des Flusses heimgesucht. Jenseits des Mississippi, durch drei Brücken verbunden, findet sich die KleinstadtWest Memphisin Arkansas. Memphis liegt in der immerfeuchtensubtropischenKlimazone. Die durchschnittliche Jahrestemperatur beträgt 18,5 Grad Celsius, kältester Monat ist der Januar mit einer Temperatur von fünf Grad Celsius, wärmster der Juli mit einem Schnitt von 28 Grad, bei oft hoherLuftfeuchtigkeit. Diese liegt das ganze Jahr über zwischen 80 % (vormittags) und 50 % (nachmittags). Die Stadt liegt im Einzugsgebiet vonTornadossowie in unmittelbarer Nähe desNew Madrid Faultund ist deswegenerdbebengefährdet.[3] Das Gebiet, in dem Memphis liegt, wurde ursprünglich von dem Indianervolk derChickasawbesiedelt. Der spanische EntdeckerHernando de Sotobereiste wahrscheinlich um 1541 die Gegend. Erste dauerhafte europäische Besiedlungsversuche gingen vonFrankreichaus. Fort Prudhomme entstand 1682, Fort Assumption 1739. Nach demenglisch-französischenKrieg übernahm England die Kontrolle des Territoriums, de jure stand es zu dieser Zeit noch den Chickasaw zu. Chickasaw, Franzosen, Engländer und Spanier lebten weitgehend friedlich miteinander, bis Tennessee 1790 ein US-Territorium und schließlich 1796US-Bundesstaatwurde. Obwohl offiziell immer noch Indianerland, zogen weiße Siedler in die Gegend, die Chickasaw gaben schließlich 1818 ihr nördliches Territorium auf, in dem das heutige Memphis liegt. Die eigentliche Stadtgründung erfolgte 1819, als Gründungsväter gelten der General und US-PräsidentAndrew Jackson, der GeneralJames Winchesterund der Richter John Overton. Sie errichteten die Stadt auf 5000Acreund wollten sie zu einem Stützpunkt an der damaligen Westgrenze der Vereinigten Staaten ausbauen, von der aus sich der Handel im Tal des Mississippi kontrollieren ließ.[4]Zu diesem Zeitpunkt hatte Memphis ungefähr 50 Einwohner. Die Stadt wurde nach der Hauptstadt des antiken ÄgyptensMemphisbenannt. Der Sohn von Winchester wurde erster Bürgermeister. Afroamerikaner, die später prägend für die Stadtgeschichte wurden, spielten zu dieser Zeit nur eine geringe Rolle. Die afroamerikanische Bevölkerung selbst war auf denPlantagendes Mississippi-Deltasversklavt, den Bedarf an ungelernten Arbeitskräften deckten vor allemIren.[5]Die katholische St. Mary’s Church mit dem ältesten Altar der USA stammt ebenfalls aus dieser Zeit. Memphis war Zentrum der aufstrebendenBaumwollindustrie. Verkehrstechnisch günstig gelegen und umgeben von fruchtbarem Boden entwickelte sich hier ein Handelszentrum. In Memphis standen zeitweise die größten Baumwollspeicher der Welt, der größte geschlossene Baumwollmarkt der Welt fand hier statt, weltgrößter Produzent von Produkten aus Baumwollsamen, wichtigsterHartholz-Markt der USA, sowie der zweitgrößte Markt für Medikamente und der drittgrößte für Lebensmittel.[4]Viele Pioniere und Händler, die sich weiter in den amerikanischen Westen orientierten, nutzten die Stadt am Mississippi als Basislager und Aufenthaltsort. Gleichzeitig mit dem Aufbau des Baumwollhandels entwickelte sich Memphis ebenfalls zu einem Zentrum des amerikanischen Sklavenhandels, die hier auf die großen Plantagen im Mississippi Delta verkauft wurden.[4] Endgültig an das Verkehrsnetz der USA angebunden wurde Memphis 1845, als fast gleichzeitig ein Hafen derUS-Marinein der Stadt eröffnete und dieMemphis and Charleston Railroad, die die Stadt mit der Atlantikküste verband, eröffnete. Memphis war zu dieser Zeit die sechstgrößte Stadt der USA. Im Gegensatz zu vielen anderen Städten der Südstaaten war derSezessionskriegfür Memphis eine Zeit ungebrochener Prosperität. Die direkten Kampfhandlungen in der Stadt beschränkten sich auf ein eintägiges Gefecht auf dem Mississippi, ansonsten profitierte die Stadt vor allem von ihrer verkehrsgünstigen Lage. Die Wirtschaft florierte in dieser Zeit mit Händlern, die gleichzeitig Baumwolle an dieNordstaatenverkauften und Munition und Stahlwaren an dieSüdstaaten.[4] Im Krieg selbst wurde Memphis aufgrund seiner Lage wichtiger Nachschubpunkt, zuerst als Nachschubdepot für die Südstaaten. Bei demGefecht um Memphis, bei dem dieMarine der Nordstaaten1862 dieMarine der Konföderiertenbesiegte, hatten die an den Ufern des Mississippis sitzenden Einwohner der Stadt eine gute Aussicht, ohne ernsthafte Gefahren befürchten zu müssen. Nach dem Sieg der Nordstaaten diente die Stadt als militärischesHauptquartierdesNordstaaten-GeneralsUlysses S. Grant. Die frühe Eroberung durch die Nordstaaten verhinderte weitere Kampfhandlungen und Zerstörungen während des Krieges. Memphis unterlag auch keiner formalenReconstruction.[5]Als sich 1866 vieleschwarze Soldatender Nordstaaten in der Stadt aufhielten, kam es am 1. Mai erst zu Ausschreitungen und in den zwei folgenden Tagen zu schweren Verfolgungen. Bis zum 3. Mai wurden 48 Menschen, davon 46 Schwarze und zwei Weiße (denen Sympathie zu den Schwarzen vorgeworfen wurde), ermordet.[6]Viele Wohnhäuser und Kirchen von schwarzen Einwohnern der Stadt wurden abgebrannt. Nachdem Tennessee im Juli 1866 den14. Zusatzartikel zur Verfassung der Vereinigten Staatenanerkannte und Memphis endgültig Teil der Nordstaaten geworden war, zog die Stadt viele ehemalige Sklaven an. Die afroamerikanische Bevölkerung vervierfachte sich zwischen 1860 und 1870 von 4000 auf knapp 16.000 Einwohner, die Gesamtbevölkerung stieg von 22.000 auf 40.000.[4]DieEmanzipationder Schwarzen wurde in der Stadt im Rahmen des Rechts vergleichsweise weitgehend umgesetzt, das Recht, lesen zu lernen ebenso wie auch dieReligionsfreiheit. Insbesondere schwarze Kirchen wurden eine wichtige Basis für den Einfluss der Afroamerikaner. DieBeale-Street-Baptist-Churchvon Prediger Morris Henderson galt als besonders einflussreich. Ed Shaw, der wichtigste politische Führer, saß im Stadtrat der County-Commission und stieg bis zumHafenkapitänauf. Nachdem die Stadt den Sezessionskrieg besser überstanden hatte als fast alle anderen Südstaatenstädte, sorgten mehrereGelbfieberepidemien1870, 1873 und insbesondere 1878 für verheerende Auswirkungen. 1878 sorgteEl Niñofürtropisches Klimain den Südstaaten. Die Temperaturen lagen deutlich höher als sonst, die Sommersaison dauerte länger und die Niederschläge in dieser Zeit waren mehr als doppelt so hoch wie sonst. Die Hafenstädte des Südens waren direkt nach dem Krieg überfüllt mit Landflüchtlingen und Zuwanderern aus den Nordstaaten, regelmäßig wieder stattfindender globaler Schiffsverkehr brachte Krankheitserreger und infizierteStechmückenin die Gegend. Eine Gelbfieber-Seuche verbreitete sich vonNew Orleansausgehend den Flusslauf des Mississippi und seiner Nebenflüsse entlang.[4] Insbesondere jedoch aufgrund seiner miserablenAbwasserentsorgungtraf die Gelbfieber-Seuche des Jahres Memphis härter als alle anderen Städte der USA: 25.000 Einwohner verließen die Stadt. Von den 19.000 Einwohnern, die nicht flohen, steckten sich im schlimmsten Jahr 80 % mit dem Gelbfieber an, ein Viertel der Bevölkerung starb. Gemessen an der Einwohnerzahl war dies die tödlichste Epidemie, die je eine US-Stadt traf. Gleichzeitig verarmte Memphis, da insbesondere vermögende Einwohner die Stadt verließen und fast nur irisch-amerikanische Hilfsarbeiter und gerade in die Freiheit entlassene Sklaven in der Stadt blieben, die mit enormen Problemen zu kämpfen hatten.[5] 1879 erklärte die Stadt offiziell ihrenBankrottund die Stadtrechte wurden ihr aberkannt. Memphis hörte offiziell auf zu existieren und lebte nur als Steuerdistrikt Shelby County weiter. Der Staat Tennessee löste die teilweise von Schwarzen und Iren geleitete Stadtregierung auf und ersetzte sie durch ein Gremium, dem fast ausschließlich Weiße der reichen Vorkriegs-Südstaateneliten angehörten.[7] Erst in den 1880er-Jahren konnte durch ein neues Abwassersystem (das erste seiner Art weltweit) und die Einrichtungartesischer BrunnendieEpidemiegestoppt werden. Memphis gilt bis heute als eine der Städte mit dem sauberstenTrinkwasserder USA. Die Siedlungsbewegungen jedoch hatten sich dauerhaft umgekehrt. Hatte Memphis vor Ausbruch der Gelbfieberepidemien doppelt so viele Einwohner wieAtlantaund fast doppelt so viele wieNashville, zeigte sich danach ein Rückstand, den die Stadt nie wieder aufholte.[4]Stadtbevölkerung und Stadtklima veränderten sich nach der Epidemie. Während die geflohenen Weißen aus der Händlerschicht und Zuwanderer nicht wieder zurückkehrten, zogen zunehmend Weiße aus den umliegenden Gemeinden in die Stadt.[5]Aus einer kosmopolitischen Stadt, in der über 30 Prozent der Einwohner aus dem Ausland und ein großer Teil aus den gesamten restlichen Vereinigten Staaten kamen,[8]war ein Ort geworden, in dem 80 % der Einwohner in den ländlichen Gebieten um Memphis herum geboren waren.[5] Schwarze spielten auch weiterhin eine einflussreiche Rolle. Der hohe afroamerikanische Bevölkerungsanteil unterstützte eine gefestigte Schicht afroamerikanischer Ärzte, Anwälte, Banker und ähnlicher Berufe.[5]Der afroamerikanische GroßindustrielleRobert R. Churchwar vermutlich der erste schwarze Millionär der USA und gründete die erste afroamerikanische Bank. Er investierte entscheidend in den Neuaufbau der Stadt. Der ehemalige Sklave und einflussreiche Politiker derRepublikanerbaute auch dieBürgerrechtsorganisationNAACPauf und eröffnete, bei immer noch bestehender politischerSegregation, mit demChurch Parkden ersten öffentlichen Park für Afroamerikaner und mit demChurch Auditoriumeine wichtige Kulturstätte für diesen Teil der Bevölkerung.[5] Um die Jahrhundertwende herum war insbesondere die Beale Street das wichtigste soziale und kulturelle Zentrum der afroamerikanischen Gemeinschaft in den mittleren Südstaaten. Sie war berüchtigt für Alkoholkonsum, illegales Glücksspiel und anderen eher als zweifelhaft betrachteten Zeitvertreib, an ihr standen aber auch dicht gedrängt afroamerikanische Bankfilialen, Kaufhäuser und Immobilienmakler.[5]Memphis wurde wieder der weltgrößte Markt für Baumwolle. Industrielle wieNapoleon Hill,James LeeundNoland Fountainemachten Millionen in der Stadt, ebenso allerdings dieMafia. In der „Regierungszeit“ vonEdward Hull „Boss“ Crumpvon 1909 bis 1954 konnte dieser verlässlich versprechen, demjenigen bis zu 60.000 Wähler zu beschaffen, der ihm die beste Gegenleistung dafür bot. Dabei wurden Stimmen doppelt gezählt, die Stimmen nicht-registrierter, aber Crump-treuer Wähler ebenfalls mitgezählt und Wähler aus den benachbarten Staaten Mississippi und Arkansas kurzfristig in die Stadt verfrachtet. Crump selbst wurde 1909 zum ersten Mal zum Bürgermeister der Stadt gewählt. Angetreten mit dem Versprechen, dieKorruptionin der Stadt zu beenden, setzte er dieses Versprechen um, indem er all seine Konkurrenten ausschaltete. In einer Zeit, in der die USA innenpolitisch mit derProhibitionbeschäftigt war, nahm Crump Bestechungsgelder vonBordellen,Spielhallenund illegalenSaloonsan. Nachdem er von der Regierung Tennessees als Bürgermeister zum Rücktritt gezwungen wurde, ließ er sich in denKongresswählen. Crump selbst war offenerRassistund hielt die Schwarzen nicht für fähig, sich selbst zu regieren. Allerdings benötigte er die Stimmen der schwarzen Wähler. Er ernannte einige Afroamerikaner in Regierungsposten und handelte ebenso mit den Führern der schwarzen Gemeinschaft im Austausch für Wählerstimmen. Ziel dieser Verhandlungen zwischen Crump und der afroamerikanischen Elite war die weitgehende Aufhebung zahlreicher Wahlbeschränkungen, die in den restlichen Südstaaten noch bis in die Bürgerrechtsära hinein gelten sollten.[7] Die ersten schwarzenPolizistengab es allerdings erst nach 1948 und auch sie hatten nicht das Recht, Weiße festzunehmen. Nachdem Crump seine Macht gefestigt hatte, fühlte er sich zu weniger Konzessionen genötigt.[7]Strikte Rassengesetze hielten die meisten Afroamerikaner in schlecht bezahlten Hilfsarbeiterjobs gefangen. Die Arbeitslöhne waren so niedrig, dass dieTennessee General Assembly1917 dieEmigrant Agent Codeserließ, die es verboten, afroamerikanische Arbeiter aus Tennessee abzuwerben.[5] Formen des Widerstands gegen dieJim-Crow-Gesetzegab es seit dem Ende des 19. Jahrhunderts. MusikerinJulia Hookskam ins Gefängnis, da sie 1881 lautstark dagegen protestierte, nicht in der weißen Sektion des Theaters zu sitzen. 1905 führte eine große Demonstration durch den Church Park zugunsten von Mary Robinson, die sich in der Straßenbahn in die weiße Sektion gesetzt hatte und der deswegen ein Gerichtsverfahren bevorstand. Immer wieder kam es auch zu gewalttätigen Auseinandersetzungen.[5] In den späten 1940ern versuchten erstmals Afroamerikaner, Einfluss auf die Regierung der Stadt zu gewinnen. Die NAACP stellte eigene Kandidaten auf, die allerdings gegen „Boss“ Crump chancenlos waren. Durch den überraschenden Tod von Crump 1954 befand sich die schwarze Politik auch in einem Vakuum. Bisher hatte sie Ziele nur durch Kooperation mit ihm erreicht, durch seinen Tod zerfiel sein politisches System und damit war zunächst der einzige wichtige weiße Ansprechpartner für die Bewegung verloren. Seit 1960 mit dem Aufkommen derBürgerrechtsbewegungkam es zu zahlreichenSit-insundBoykottenseitens der Schwarzen und der Studenten. Gleichzeitig formierte sich in der Stadt eine nennenswerte afroamerikanischeGewerkschaftsbewegung.[5] Das politische Klima in der Stadt war zu dieser Zeit ausgesprochen konservativ und von Rassismus geprägt. So erreichte beispielsweise bei derPräsidentschaftswahl 1968der Rassist und „Südstaatenkandidat“George Wallacein der Stadt 42 % der Stimmen, der DemokratHubert H. Humphrey12 %.[5]Gleichzeitig verarmte die afroamerikanische Bevölkerung über die Jahrzehnte. Der Anteil der Afroamerikaner an den Familien mit den niedrigsten Einkommen der Stadt stieg von 59 % im Jahr 1949 auf 71 % im Jahr 1969. Ein Viertel der Innenstadtbewohner verdiente weniger als zwei US-Dollar pro Stunde, während der Anteil in vergleichbaren Städten wieNewarkbei weniger als 10 Prozent lag.[5] 1968, im Zenit der Kämpfe um die Bürgerrechtsbewegung, kam derBaptistenpastorMartin Luther Kingin die Stadt. King hatte sich in dieser Zeit schon teilweise von den rechtlichen Fragen der Diskriminierung abgewendet und sich mehr den sozialen Problemen und der weitverbreiteten schwarzen Armut angenommen. Am 1. Februar 1968 starben während eines schweren Unwetters zwei schwarze Angestellte der Müllabfuhr, als sich der Pressmechanismus einesMüllwagensvon alleine in Gang setzte. Am gleichen Tag wurden wegen des Unwetters 22 schwarze Arbeiter ohne Bezahlung nach Hause geschickt, während ihre weißen Vorgesetzten ebenfalls arbeitsfrei erhielten, allerdings mit Lohnausgleich. Zwei Wochen später begannen 1100 von insgesamt 1300 schwarzen Angestellten der öffentlichen Reinigung einen Streik für bessere Arbeitsbedingungen. Am 18. März, während der Streik immer noch andauerte, kam King in die Stadt und sprach auf mehreren Veranstaltungen. Am 28. März fand eine große Demonstration statt. Diese endete allerdings in Gewalt, als vor allem College-Studenten ihre mitgebrachten Schilder nutzten, um damit die Schaufensterscheiben von Geschäften einzuschlagen. Die Stadt erwirkte ein gerichtliches Verbot für King, Memphis zu betreten. DieSouthern Christian Leadership Conferenceverhandelte mit den Verantwortlichen, um eine Demonstration am 5. April möglich zu machen. King kehrte in die Stadt zurück. Am 4. April einigten sich die Beteiligten auf einen Protestmarsch am 8. April. Am Abend des 4. April 1968 wurde Martin Luther King auf dem Balkon desLorraine Motelserschossen. Die genauen Umstände der Tat sind bis heute ungeklärt. In dem Gebäude befindet sich heute ein Museum für Bürgerrechte. Eine filmische Schilderung des Attentats erfolgte in dem 1993 gedrehten DokumentarfilmAt The River I Stand. Folgend auf die Ermordung Kings fanden in den nächsten Jahren Aufstände und Straßenkämpfe statt. Der größte Teil des Stadtzentrums brannte in dieser Zeit nieder. Immer mehr Bewohner verließen das eigentliche Stadtgebiet, um sich im Umland neu anzusiedeln. In den 1970ern war die Innenstadt, wie viele in den USA, in einem Zustand des Verfalls begriffen. Sogar Planungen, die Beale Street abzureißen, waren schon weit fortgeschritten. Erst der öffentliche Aufschrei von Denkmalschützern führte dazu, dass die Stadt 500 Millionen Dollar bereitstellte, um diesen Stadtteil zu sanieren. 1974 wurde Harold Ford Sr. als erster direkt gewählter Schwarzer aus den Südstaaten in den US-Kongress gewählt. Erst nachdem die Wahl schon knapp verloren schien, fanden seine Anhänger acht ungeöffnete Wahlboxen, die vom ausschließlich weißen Wahlkomitee „übersehen“ worden waren und Ford den Sieg sicherten. Mittlerweile wurde die Stadt mehrfach von afroamerikanischen Bürgermeistern regiert, etwa vonWillie W. Herenton, der ab 1992 für 17 Jahre im Amt war. Seit den 1990ern wurden zahlreiche Anstrengungen unternommen, um die Attraktivität der Stadt zu steigern. Es wurden Freizeiteinrichtungen wie der Mud-Island-Vergnügungspark, dieMemphis Pyramid, das FedEx-Forum und ein neuesBaseballstadion(AutoZone Park) gebaut. Die Innenstadt bekam eine historisch aussehende, in normalem Takt verkehrendeStraßenbahn. Ende der 1990er waren auch die langanhaltenden Versuche erfolgreich, dauerhaft ein Sportteam der großen amerikanischen Profiligen zu etablieren. 2009 war Memphis auf derForbes-Listeder gefährlichsten Städte der Vereinigten Staaten erneut auf Platz zwei.[9] Obwohl die Segregation in den USA seit mehreren Jahrzehnten offiziell abgeschafft ist, spielt die Kategorie„Rasse“immer noch eine wichtige Rolle in der Lokalpolitik. Seitdem überhaupt wieder Afroamerikaner antreten, wählten bis in die 1990er Jahre hinein sowohl Afroamerikaner als auch Weiße zu über 90 % einen Kandidaten der eigenen Hautfarbe, in den meisten Wahlen lagen die Quoten bei 97 Prozent oder höher.[7]Memphis hat seit 1991 eine leichte Mehrheit schwarzer Bewohner und schwarzer Wahlberechtigter. Nachdem ein Afroamerikaner, das erste Mal seit ihrer Kollektivbesetzung 1879, im Jahr 1951 versuchte ein Wahlamt zu erringen, bildete sich eine breite weiße Gegenfront, die viele Jahrzehnte relativ erfolgreich versuchte, nur jeweils einen weißen Kandidaten zu nominieren, um ihre Stimmen nicht aufteilen zu müssen. Die Politik der Schwarzen wird maßgeblich von der Familie Ford bestimmt, die seit 1974 die Abgeordneten im lokalen Wahlkreis des Kongresses (größer als Memphis-Stadt, aber kleiner als Memphis plus Shelby County) stellt. In den Nachwehen der King-Ermordung traten Harold und sein Bruder John als entschlossene Vertreter afroamerikanischer Rechte auf und schreckten dabei nicht vor äußerst polemischen Auseinandersetzungen mit dem weißen Establishment der Stadt zurück.[7] Vorwürfe desKlientelismusähnlich wie zu Zeiten von Crump werden immer wieder laut, lassen sich aber nur bedingt erhärten. Seit 1974 nahmen Emmitt Ford, Harold Ford Sr., Harold Ford Jr., John Ford und Joe Ford Mandate und Ämter auf nationaler, staatlicher und regionaler Ebene wahr. 1974 wurden an einem Tag Harold Ford Sr. in das US-Repräsentantenhaus, sein Bruder John in den Senat des Staates Tennessee und sein anderer Bruder Emmitt in das Repräsentantenhaus desselben Staates gewählt. Als Harold Ford Sr. sich schließlich 1996 aus dem US-Kongress verabschiedete, übernahm sein SohnHarold Ford Jr.das Amt.[10] Anders jedoch als zu Crumps Zeiten haben die Fords keine feste Kontrolle über die ganze Stadt. Im Gegensatz zu Crump gibt es diverse Wahlen, bei denen Kandidaten der Fords trotz massiver Unterstützung verloren haben.[10]Schwarze Politiker in der Stadt sind entweder Pro-Ford oder Anti-Ford. Sind sie Anti-Ford, brauchen sie normalerweise weiße Unterstützung, um Wahlen zu gewinnen. Die Fords haben ein System des „Ford-Wahlscheins“ entwickelt, der regelmäßig am Abend vor Abstimmungen und Wahlen in den schwarzen Wohngegenden der Stadt verteilt wird – auf ihm sind auf einem Muster des gültigen Zettels sämtliche Markierungen so angebracht, wie sie von der Familie empfohlen werden.[11]Die genauen Details stehen dabei erst in letzter Minute fest; obwohl die Fords nominell Mitglieder der Demokratischen Partei sind, betreiben sie in erster Linie Familienpolitik und so kann es auch durchaus vorkommen, dass sie republikanische Kandidaten oder Vorschläge unterstützen. Besonders für weiße Politiker kann die Hilfe der Fords unschätzbar sein; sie haben die Möglichkeiten dazu, die weißen Politiker auch in der afroamerikanischen Gemeinschaft bekannt zu machen und sie in den ebenfalls sehr einflussreichen schwarzen Kirchen vorzustellen. Memphis wird von einem „schwachen Bürgermeister“ regiert, das heißt, die Gemeindevertretung besitzt sowohl die hauptsächlichenlegislativenals auchexekutivenBefugnisse. Die Gemeindevertretung setzt sich aus 13 Vertretern zusammen. Sieben davon werden in einem Stadtbezirk mit jeweils einem Vertreter gewählt, sechs stammen aus den zwei Wahlkreisen, die jeweils drei Vertreter bestimmen. 2004 waren von den 13 Mitgliedern des Councils sechs Afroamerikaner.Willie W. Herentonwar von 1992 bis 2009 Bürgermeister von Memphis. Nachdem er bis Anfang der 1990er die Ford-Strategie eines einzigen schwarzen Kandidaten unterstützte, um die afroamerikanischen Wählerstimmen nicht aufzusplittern, hatte er sich seitdem als politischer Gegner der Fords entwickelt.[10]1991 gewann er als erster Afroamerikaner im 20. Jahrhundert eine stadtweite Wahl und profitierte dabei davon, dass dies das erste Mal war, dass mehr Afroamerikaner als Weiße wahlberechtigt waren. Zugleich aber gelang es ihm ungewöhnlicherweise knapp über 10 Prozent der weißen Stimmen für sich zu erringen. Eine Rate, die er 1995 – allerdings ohne ernsthaften Gegenkandidaten – auf 40 % ausdehnen konnte.[7] 1999 konnte er sich in einer seiner Wahlen direkt gegen den ehemaligen Senator Harold Ford Sr. durchsetzen. Dabei profitiert Herenton davon, dass sich die Grenzen zwischen den Bevölkerungsgruppen erstmals in der Geschichte der Stadt weit genug aufgelöst haben, dass ein schwarzer Kandidat ernsthafte Stimmgewinne unter Weißen erzielen konnte.[10]Unter anderem schaffte er es in seiner Amtszeit sowohl 400 neue Polizisten einzustellen und gleichzeitig das Schulbudget um 100 Millionen Dollar zu erhöhen. Memphis investierte 1,3 Milliarden Dollar in die Wiederbelebung der Innenstadt und schuf dabei tausende neuer Jobs; insbesondere entstand ein geschäftlicher Boom bei den ethnischen Minderheiten. Trotzdem gelang es in seiner Amtszeit, den finanziellen Haushalt der Stadt zu sanieren. 2016 hatte Memphis rund 650.000 Einwohner[12], dieMetropolregion Memphisungefähr 1,3 Millionen. Memphis steht damit in der Liste der größten Städte der USA auf dem 25. Platz und nimmt in der Reihe der größten Metropolregionen des Landes den 41. Rang ein. Von den Einwohnern der Stadt bezeichneten sich 61 % alsSchwarzeund etwa 34 % alsWeiße, während sich knapp 5 % als Angehörige anderer Hautfarben klassifizierten. Die Stadt hatte damit unter den US-Großstädten den achthöchsten Anteil an Schwarzen.[13]Der Anteil der Weißen ging in der eigentlichen Stadt stark zurück (1980: 52 %; 1990: 44 %) während er in den umliegenden Landkreisen anwuchs. Die Memphis Metropolitan Statistical Area hatte 1,2 Millionen Einwohner und war damit die zweitgrößte in Tennessee. Bei Fortschreibung derzeitiger demographischer Trends wird es die erste Metropolitan Area sein, in der die Mehrheit der Bewohner schwarz ist.[13]Memphis hat mit die niedrigsten Lebenshaltungskosten einer US-Stadt. Bei einer repräsentativen Umfrage 1997 stufte ein Drittel der Einwohner seine Wohngegend in Bezug auf dieLebensqualitätals perfekt ein. Einwohnerzahlen nach dem jeweiligen Gebietsstand. Die Zahlen sind gerundet aus den zehnjährlich stattfindenden amerikanischen Volkszählungen. Religionhat in der Geschichte der Stadt immer eine große Rolle gespielt. Sie beherbergt diverseprotestantischeDenominationen, ist zum Beispiel mit derBellevue-Baptistenkircheein Zentrum der amerikanischenBaptistenund Ausgangspunkt diverser religiöser Bewegungen. Neben einigen anderen Städten wie Nashville, Dallas oder Atlanta wird Memphis auch oft alsBuckle of theBible Belt(Gürtelschnalle des Bibelgürtels) bezeichnet.[14] In Memphis befindet sich das Hauptquartier und das Verlagsgebäude derChristian Methodist Episcopal Church.[15]DieChurch of God in Christstammt ursprünglich 1907 aus Memphis und hält dort jedes Frühjahr eine „heilige Zusammenkunft“ ab.[16]Jünger ist dieChurch Uniting in Christ, die sich am 20. Januar 2002 in der Stadt formierte.[17] DieCumberland Presbyterian Churchhat seit 1978 ihr Hauptquartier in der Stadt und beherbergt hier eine ihrer zwei Ausbildungsstätten, das Memphis Theological Seminare of the Cumberland Presbyterian Church.[18]Memphis ist zudem Sitz desBistums Memphis. In den 1830er Jahren gründeten deutsche Einwanderer diejüdischeGemeinde und erbauten 1853 den ersten jüdischen Tempel der Stadt. Nach 1905 gründeten Zuwanderer ausOsteuropaneben der zuvor ausschließlichliberaleneineorthodoxejüdische Gemeinde.[19]Obwohl oftmals als Kleinhändler beginnend, ist die jüdische Bevölkerung in der Stadt vergleichsweise wohlhabend und gut integriert. Heute leben in derMetropolregion Memphisetwa 8500 Juden mit steigender Tendenz.[8] Memphis erhielt mit der 1892 gebautenFrisco Bridgedie erste Eisenbahnbrücke über den Mississippi südlich vonSt. Louisund ab 1917 mit der benachbartenHarahan Bridgedie erste Kraftverkehrsverbindung über den Fluss, die seit 2016 auch Bestandteil eines über 15 Kilometer langen Radweges nachWest Memphisist. Traditionell war Memphis eine Stadt derBaumwolle. Kaum eine Stadt hing so am Handel mit dem „Weißen Gold“. Baumwollhändler bestimmten die Geschicke der Stadt. So fanden im Jahr 1950 etwa 40 Prozent der gesamten Baumwoll-Transaktionen der USA in der Stadt statt. Unbestrittenes Handelszentrum in Memphis selbst war dieFront Street, dieBaumwollbörse(Cotton Exchange) befand sich seit 1924 an der Ecke Front Street/Union Street.[20] Auch in den Jahrzehnten nach 1945, als die Bedeutung der Baumwolle in anderen Städten zurückging, trieb sie weiterhin die wirtschaftliche Entwicklung in Memphis an. So handelte die Stadt 1959 etwa 4,5 MillionenBallenBaumwolle, dreimal so viele wie der zweitgrößte amerikanische Handelsplatz,Fresnoin Kalifornien. In ihrem Schatten gediehen ebenso andere landwirtschaftliche Erzeugnisse. Vor dem Zweiten Weltkrieg war Memphis außerdem der bedeutendste US-amerikanische Handelsplatz fürEsel. Nach dem Zweiten Weltkrieg siedelten sich großeTraktoren- undDüngerhändleran.[21] Memphis war immer eine Stadt des Handels und der Dienstleistungen, in der die Industrie nie eine dominierende Rolle spielte. Ursprünglich vor allem durch die Lage am Unterlauf des Mississippi und den der Nähe zu den Erzeugnissen der großen Plantagen der Südstaaten geprägt, hat die bedeutende Rolle als Wirtschaftsfaktor heute der Frachtflughafen eingenommen. Die Stadt ist immer noch das wirtschaftliche Zentrum der landwirtschaftlich prosperierenden südlichen Mississippi-Region sowie ein wichtiges Handelszentrum nicht nur für Baumwolle, sondern daneben auch fürSoja. Die Metropolregion von Memphis erbrachte 2016 ein Bruttoinlandsprodukt von 71,5 Milliarden US-Dollar und belegte damit Platz 48 unter den Großräumen der USA.[22]Die Arbeitslosenquote in der Metropolregion betrug 3,8 Prozent ist damit identisch mit dem nationalen Durchschnitt (Stand: März 2018).[23]Das persönliche Pro-Kopf-Einkommen liegt 2016 bei 43.498 US-Dollar, womit Memphis ein unterdurchschnittliches Einkommensniveau besitzt.[24] Die Stadt selbst bewirbt sich als Verteilzentrum Nordamerikas. In der Stadt befinden sich 90.000 Arbeitsplätze alleine in der Logistikindustrie. Wichtigster Arbeitgeber in diesem Segment und mit 30.000 Angestellten mit Abstand größter Arbeitgeber der Stadt ist das UnternehmenFedEx,[25]dessen Hauptsitz Memphis ist. Es ist außerdem dasLuftfahrt-Drehkreuzvon FedEx für die Vereinigten Staaten und der größte und wichtigste Hub, den das Unternehmen betreibt. In der Nachbarschaft von FedEx haben sich zahlreiche andere Logistikunternehmen in der Stadt angesiedelt.[4]Vor allem aufgrund der Anwesenheit von FedEx ist der 1975 fertiggestellte FlughafenMemphis International Airport– gemessen am Frachtvolumen – mit etwa 3,9 Millionen Tonnen Frachtgut 2010 derzweitgrößte Frachtflughafen der Welt(nach Passagieraufkommen dagegen nur Rang 33 in den USA). Bis 2009 lag Memphis sogar auf Rang 1, wurde jedoch 2010 von Hongkong (4,17 Mio. Tonnen Fracht) verdrängt. Auch der öffentliche Dienst spielte immer eine wichtige Rolle in der Stadt. Ende der 1990er Jahre beispielsweise waren zehn der 15 wichtigsten Arbeitgeber der Stadt öffentliche Einrichtungen.[5]Die größten dabei waren die US-Regierung und das Memphis School Board mit jeweils etwa 14.000 Mitarbeitern. 54 % der Einwohner sind in Handel und Service beschäftigt, sogar 85 % im Dienstleistungssektor insgesamt gegenüber 16 % Beschäftigten in der Industrie und im Baugewerbe. Die Arbeitslosenquote lag 1998 ebenso unter dem US-Durchschnitt (3,7 % gegenüber 4,5 %) wie das durchschnittliche Einkommen/Einwohner (22.700 US-Dollar gegenüber 24.400 US-Dollar). Ebenso ist die Stadt Hauptquartier vonInternational Paperund dem US-Marktführer im Autozubehörhandel,AutoZone. Die Stadt liegt an den beidenInterstate HighwaysI-40(Hernando de Soto Bridge) undI-55(Memphis–Arkansas Bridge). Der Hauptbahnhof, dessen Gleise heute zurCanadian National Railway(CN) gehören, wurde 1914 fertiggestellt. Er wird nur noch von einem täglichen Zugpaar, dem legendärenCity of New OrleansderAmtrakder in der RelationChicago–New Orleansbedient. Deröffentliche Personennahverkehrliegt in den Händen der kommunalen Memphis Area Transit Authority (MATA), die einDieselbusnetzund dreiStraßenbahnlinienbetreibt. Die Stadt beherbergt neun Universitäten und Colleges: Das 1962 aus Stiftungsmitteln gegründeteSt. Jude Children’s Research Hospitalgilt als eines der besten forschungsorientierten Krankenhäuser für Krebserkrankungen bei Kindern weltweit. Der Fernsehmarkt Memphis umfasst lautArbitronein Gebiet von 31 Counties in West-Tennessee, Nordwest Mississippi, Ost-Arkansas und Südost-Missouri.[13] Memphis schrieb mit dem SenderWDIARadiogeschichte. WDIA war der erste Sender, dessen Programm von Afroamerikanern geplant und gestaltet wurde. Hier begannen zum BeispielRufus ThomasundB. B. Kingihre Karrieren,Elvis Presley, der den Sender als Jugendlicher hörte, wurde nach eigener Auskunft stark von ihm beeinflusst. Während die abwechslungsreiche Radioszene maßgeblich dazu beitrug, die Musikstadt Memphis zu prägen, befinden sich heute fast alle der 30 noch vorhandenen Radiosender der Stadt im Besitz nationaler Ketten und betreiben Formatradio. Ausnahmen bilden die angeschlossenen Sender desNational Public Radios(NPR Talk und NPR Classic) undWEVL, der ein abwechslungsreiches Programm ausCajun-Musik,Rockabilly, Blues und anderem spielt. WEVL ist eng mit der Musikszene der Stadt verbunden[27]und wird von einzelnen Autoren als einer der besten Radiosender bezeichnet.[28] Die Kultur der Stadt wird maßgeblich durch die Nähe zum Mississippi-Delta bestimmt, dem Zentrum der Baumwoll- und Plantagenkultur der Südstaaten. Kulturelle Einflüsse sowohl europäischstämmiger Pflanzer und Händler wie die der afroamerikanischen Feldarbeiter trafen in der Stadt aufeinander. Eine direkte Eisenbahnverbindung in das Herz des Deltas machte Memphis schon im 19. Jahrhundert zur wichtigsten Stadt für das Delta und zum ersten Anlaufpunkt für alle, die das Delta verlassen wollten.[20] Die Stadt übte eine große Anziehungskraft auf Blues-Musiker vom Lande aus, vergleichbar mit der von New Orleans. Es bildete sich schon früh eine Club-Kultur aus, die bis heute anhält. 1909 schriebW. C. HandydenMemphis Blues, der als eines der ersten notierten Blues-Stücke der Welt gilt. Blues war zu dieser Zeit in der Stadt schon derart populär, dass Handy vom Bürgermeisterkandidaten Crump als Bandleader für dessen Wahlkampagne angestellt wurde. Eine der Ikonen des klassischen Blues, B. B. King besaß einen Club in der Beale Street, in dem er regelmäßig auftrat. ImMemphis Blueswurde das erste Mal die Bandaufteilung von zwei Gitarristen, eineRhythmus-Gitarreund eineLead-Gitarregespielt, die auch heute noch die häufigste in rock- und gitarrenlastiger Popmusik ist. Der klassische Memphis Blues beruht auf den starken christlichen Traditionen, die bei den meisten der ehemaligen Sklaven vorherrschten. Rhetorisch ausdrucksvoll bringt er sowohl das Leiden des Blues als auch spirituelle Momente zum Ausdruck. Die enge Verbindung von Blues,Gospelund christlichem Glauben zeigt sich auch anAl GreensFull Gospel Tabernacle– ein sonntäglicher Gottesdienst, bei dem Al Green singt und predigt und dabei von einem Gospel-Chor begleitet wird. Neben New Orleans undLouisville, Kentuckywar Memphis auch eine der ersten Städte, in denen sich derSkifflezu einer eigenständigen Musikform entwickelte. Daneben gilt die Stadt mit dem Studio vonSun Recordsals Ursprungsort desRock ’n’ Roll. Elvis Presley, der nebenJohnny CashoderJerry Lee Lewisbei Sun unter Vertrag stand, gab 1954 sein erstes Konzert in der Stadt. In späteren Jahren siedelte sich mitStax Musicin Memphis ein stilprägendesSoul-Label an, das nebenMotownundAtlanticdie Soul-Szene der Sechziger dominierte. Der typischeMemphis Soulwar dabei rauer und schwerer als die nördlichere Variante. Während sich inNew York CityderHip-Hopentwickelte, warenRapperaus Memphis beteiligt, als Südstaaten-Rap, sogenannterDirty Southbekannt wurde.Three 6 Mafiabrachten mitTear Da Club Up ’97den erstenCrunk-Titel in die Charts, 2006 waren sie die ersten afroamerikanischen Rapper, die einenOscarfür den besten Song gewannen. In Memphis lebten und arbeiteten: Zwei bekannte Musiker benannten sich nach der Stadt:Memphis MinnieundMemphis Slim. Ein vielfach gecovertes Musikstück vonChuck Berryheißt wie die StadtMemphis, Tennessee. Die Musik ist in der Stadt allgegenwärtig. Neben zahlreichen Straßenmusikern und den Verweisen auf Elvis Presley steht beispielsweise auchAscent of the Blues, eine 12 Meter hohe Doppelspirale aus Klavieren, Gitarren und Banjos des französischen KünstlersArmanim Stadtzentrum. Die Stadt wurde unter anderem vonBob Dylanin seinem SongStuck Inside of Mobile with the Memphis Blues Againoder vonMarc CohninWalking in Memphisbesungen. Die Historikerin Marcie Cohen Ferris schrieb: „Niemand spricht in Memphis vom Essen, ohne anBarbecuezu denken.“[29] In Memphis spielt seit 2001 ein Team der großen amerikanischen Profiligen, die Memphis Grizzlies in derNational Basketball Association. Versuche ein Team derNational Football Leaguein der Stadt anzusiedeln, sind bereits mehrfach gescheitert. WichtigeMinor-League-Teams sind dieMemphis Redbirdsim Baseball und dieMemphis RiverKingsim Eishockey. Zu Zeiten der Segregation im US-Sport spielte mit denMemphis Red Soxeines von zwei Südstaatenteams derNegro Leaguesund mit demMartin Stadioneines der wenigen Teams mit einem eigenen Stadion.[30]Tigerswerden die College-Teams der University of Memphis genannt. Memphis ist eine Traditionsstätte desWrestlings. Der bekannteste Wrestler aus Memphis warJerry „The King“ Lawler, viele andere bekannte Wrestler begannen ihre Karriere in der Stadt. Darunter sindHulk Hogan,The Undertaker,Stone Cold Steve Austin,The Rock,Mick Foley,Big Daddy V,„Macho Man“ Randy SavageundRic Flair. Derzeit beherbergt die Stadt zwei professionelle Veranstalter: Power-Pro Wrestling hält seine Veranstaltungen im Cook Convention Center ab, Memphis Wrestling gastiert im Desoto Civic Center im Vorort Southaven. Außerdem finden jährlich dieATP-Memphis- undWTA-Memphis-Tennisturniere statt. Daneben spielt in der Stadt mit denMemphis Blueseines der erfolgreichsten amerikanischenRugby-Teams. Bartlett•Germantown•Lakeland•Memphis•Millington Arlington•Collierville Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geographie 2Geschichte 2.1Indianische Besiedlung und erste Europäer 2.2Frühes 19. Jahrhundert 2.3Sezessionskrieg und danach: Wohlstand und Emanzipation 2.4Gelbfieber 2.5Jahrhundertwende und die Ära Crump 2.6Civil Rights Movement und danach 2.6.1Die Ermordung Martin Luther Kings 2.6.2Die Folgen und die langsame Erholung der Stadt 3Politik 4Bevölkerung 4.1Einwohnerentwicklung 4.2Religion 5Wirtschaft und Verkehr 6Bildung 7Medien 7.1Radio 8Kultur und Freizeit 8.1Musik 8.2Museen 8.3Sehenswürdigkeiten 8.4Regelmäßige Veranstaltungen 8.5Essen und Trinken 8.6Sport 9Persönlichkeiten 9.1Söhne und Töchter der Stadt 9.2Persönlichkeiten, die vor Ort gewirkt haben 9.3Musikbands 10Anmerkungen 11Literatur"
  },
  {
    "label": 0,
    "text": "Mode – Wikipedia Mode Inhaltsverzeichnis Soziologie Psychologie und Sozialpsychologie Modewellen Zitate Siehe auch Literatur Filme Weblinks Einzelnachweise Mode(ausfranzösischmode;lateinischmodus‚Maß‘ bzw. ‚Art‘, eigentlich ‚Gemessenes‘ bzw. ‚Erfasstes‘) bezeichnet die in einem bestimmten Zeitraum geltende Regel, Dinge zu tun, zu gestalten, zu tragen oder zu konsumieren, die sich mit den Ansprüchen der Menschen im Laufe der Zeit geändert haben. Moden sind Momentaufnahmen eines Prozesses kontinuierlichen Wandels. Mit Moden werden also in der Regel eher kurzfristige Äußerungen desZeitgeistesassoziiert. Vergleichsweise längerfristige Äußerungen des Zeitgeistes, die sich über mehrere Modewellen hinweg in positiver Bewertung halten können, gelten nicht als Mode, sondern alsKlassiker. Ganz kurzlebige Moden, die sich oft nur um ein individuelles Produkt drehen, bezeichnet man englisch alsFads. Jede neue Mode etabliert neue Verhaltens-, Denk- und Gestaltmuster. Jede neue Mode bringt damit neue Wertungen mit sich und bewertet damit auch bestehende Phänomene der menschlichen Umwelt immer wieder neu. „Mode“ wird umgangssprachlich häufig synonym mit „Kleidung“ als Verkürzung des Begriffs „Kleidermode“ verwendet. Das Adjektiv zu Mode istmodisch(„der Mode entsprechend“), im Unterschied zu „modern“, dem Adjektiv zuModerne. Umgangssprachlich wird der Begriff „modern“ oft im Sinne von „modisch“ verwandt. Beispiele für die Etablierung neuer Verhaltens-, Denk- und Gestaltmuster wären etwa die stetige Verkürzung der Rocklängen bei Frauen seit Beginn des Zwanzigsten Jahrhunderts, das Verhalten, immer mehr Haut zu zeigen bei Badekleidung und dessen gesellschaftliche Akzeptanz oder bei Männern des westlichen Kulturkreises etwa das Tragen eines Hemdes außerhalb der Hose (was früher als Schlampigkeit gedeutet wurde und heutzutage als ungezwungene Lässigkeit). Der Begriff „Mode“ beinhaltet folgende Bedeutungsaspekte: Soziologischbetrachtet drückt Mode die Zuordnung zu bestimmten Gruppen der Gesellschaft – sowie die Abgrenzung – und die Anpassung vonIndividuenin einem bestimmten Zeitabschnitt, den stetigen Wandel dieser Norm sowie die stetige Infragestellung und die stetige Auflösung bestehender Normen aus. Die heutige Verbreitung von Moden ist durch denMassenkonsumgeprägt, wobeiWerbungundMassenmedieneine wichtige Rolle spielen. Es lassen sich in einigen Bereichen auchGlobalisierungstendenzenin der Mode beobachten. Für die Verbreitung von Moden spielt die Inhomogenität der Gesellschaft eine wesentliche Rolle. Das Wechselspiel von konservativ und konformistisch eingestellten Gruppen einerseits und experimentierfreudigen, rebellischen und individualistischen Gruppen andererseits hat dabei erhebliche Bedeutung. Elemente neuer Moden werden schneller von Gruppen übernommen, die offen für Neues sind, die gerne experimentieren, die mit den bestehenden Verhältnissen unzufrieden sind, die etwas verändern wollen, die sich als anders als die große Masse erleben, die nicht in der Masse untergehen wollen und die sich als eigenständige Persönlichkeiten darstellen wollen, die sich also von ihrem Selbstverständnis gern von der Masse der Bevölkerung oder vomEstablishmentabgrenzen. Dies gilt insbesondere für Jugendliche, die sich auch äußerlich von den älteren Generationen abgrenzen wollen. Für sie sind die propagierten Moden Anregungen für die Suche nach dem eigenenStil, für die Lust anProvokationoder einfach Anregungen für das spaßmachende ästhetische Spiel. Nach und nach – je mehr die neuen Tendenzen im öffentlichen Raum erlebbar werden – übernehmen dann auch weniger innovative und experimentierfreudige Bevölkerungskreise die neue Mode, bis am Ende auch äußerst konservative und traditionalistische Milieus erreicht werden, die „mithalten“ wollen und deren Modeverhalten stärker von dem Bedürfnis nach Zugehörigkeit, insbesondere durch das Motiv „IntegrationdurchAssimilation“ geprägt wird. Für sie ist Mode eine Form vonKonformität(Konformismus) mit der Bezugsgruppe oder Gesellschaft sowie oft auch eine Form von ästhetischerAffirmationbestehender Verhältnisse. Spätestens dann sind diese Moden für die innovativeren und individualistischeren Kreise der Bevölkerung nicht mehr interessant. In Zusammenhang mit einer Psychologie der Mode werden häufig folgende Aspekte erwähnt: Es wirken eine Reihe von Grundbedürfnissen zusammen, aus denen die Modeerscheinungen psychologisch erklärt werden können: Das Grundbedürfnis nach Beachtung, um aufzufallen oder Interesse zu wecken. Das Grundbedürfnis nach Anerkennung, Bedeutung und sich selbst und anderen zu gefallen. Weiterhin wichtig sind die Bedürfnisse nach Abwechslung und Individualität, wobei letzteres mit dem Wunsch nach Konformität im Widerspruch zu stehen scheint. Dieses Erklärungsmuster greift dennoch zu kurz, denn Mode ist ein hochkomplexes gesellschaftliches Phänomen, das seine Wurzeln in sehr unterschiedlichen individuellen und kollektiven Bedürfnissen hat. Ohne die komplementären Bedürfnisse von Zugehörigkeit und Abgrenzung,KonformismusundIndividualismus,Expressionund Tarnung,Exhibitionismusund Verhüllung ist das Phänomen sicherlich nicht erklärbar. Dennoch ist das nur ein Teil der Ursachen von Mode. Unüberschaubar viele individuelle Faktoren kommen dazu. Beispielsweise die persönliche Bedeutung konkreter aktueller Modethemen und -bilder für die individuelle Persönlichkeit und die entsprechende Lebenserfahrung. In der Kleidermode wird das besonders deutlich: Kleidung, auch modische Kleidung, ist oft ein sehr persönlicher Ausdruck des individuellen Lebensgefühls, einer aktuellen Stimmung oder von Sehnsüchten, Träumen und Visionen. Insofern ist Kleidung dann auch ein alltägliches Rollenspiel oder Rollen-Einnehmen, ein Sich-Aneignenerträumter Rollen. Aber auch das ist nur ein Beispiel, das auch jenseits des Bereichs Kleidung anwendbar ist. Mit der Erforschung und Dokumentation der Kulturgeschichte von Mode und bestimmten Kleidungsstücken beschäftigen sich Modehistoriker und Modesoziologen.[1][2] UnterModewellenoderModeströmungenversteht man Erscheinungen verschiedener „Moden“, die eher kurzfristigen oderperiodischenCharakter haben. Das Wort hat einen leicht abschätzigen bishumorvollenBeigeschmack – was auf die leichte Beeinflussbarkeit und Anhängigkeit vieler Zeitgenossen anspielt. DurchkritischeStimmen können Modewellen rascher vergehen, sich aber auch verstärken. Modewellen werden häufig vonTrendsetternoder bei großen Ereignissen undVeranstaltungen„geboren“, doch können sie auchspontanentstehen. Typische Beispiele sind Philosophie und Soziologie Mode in der Karikatur Mode für Pflegebedürftige Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Soziologie 2Psychologie und Sozialpsychologie 3Modewellen 4Zitate 5Siehe auch 6Literatur 7Filme 8Weblinks 9Einzelnachweise Afrikaans Alemannisch Aragonés العربية مصرى অসমীয়া Asturianu Azərbaycanca تۆرکجه Башҡортса Žemaitėška Bikol Central Беларуская Беларуская (тарашкевіца) Betawi Български भोजपुरी বাংলা Brezhoneg Bosanski Català Cebuano"
  },
  {
    "label": 0,
    "text": "Musik – Wikipedia Musik Inhaltsverzeichnis Etymologie Begriffsgeschichte Musikgeschichte Erzeugung Medien, Technik und Wirtschaft Musikwissenschaft Musik und Gesellschaft Musikalische Semantik Musik und visuelle Künste Siehe auch Literatur Weblinks Einzelnachweise Definitionsgeschichte Historische Klassifikationen des Musikbegriffs Prähistorische Musik Schriftlose Kulturen Frühe Hochkulturen und Antike Westliche klassische Musik bis zum 19. Jahrhundert 20. und 21. Jahrhundert Akustisches Material Verarbeitung des Materials Musikalisches Werk Klangliche Erzeugung Notation Notendruck Reproduktionstechnik Audiovisuelle Medien Internet Musikindustrie Musikproduktion Das musikalische Produkt Musikalische Lebenswelten Musikalische Sozialisation Musikpädagogik Förderung von Mitgefühl und sozialem Zusammenhalt durch Musik Der Einsatz von Musik in der Psychotherapie Musiktherapie Frauen und Musik Sozialgeschichte Musik und Sprache Musikalische Symbolik Musikalische Signale Musik und Architektur Musik und bildende Kunst Enzyklopädien Lexika Musikgeschichte Musiklehre Musikwissenschaft Musikphilosophie Musikalische Semantik Sonstiges Antike Mittelalter Frühe Neuzeit 18. und 19. Jahrhundert Ab dem 20. Jahrhundert Antike Mittelalter und frühe Neuzeit Ab dem 16. Jahrhundert Lexikografie und Terminologie Musik im Alltag Subkulturen Sozialisation in Schule und Gruppe Sozialisation durch Medien Der Weg zur bürgerlichen Musikkultur Öffentliche Musikkulturen Rezeptionsformen Die Kulturindustrie Unterschiede Strukturelle Gemeinsamkeiten Synthetische Kunstformen Architektur und Raumakustik Antike und Mittelalter Von der Renaissance bis zur Aufklärung 18. und 19. Jahrhundert Kunsttheorien im 19. und 20. Jahrhundert Musikist eineKunstgattung, derenWerkeaus organisiertenKlängen[1]bestehen, die Empfindungen oder Assoziationen hervorrufen können.[2]Als Ausgangsmaterial dienenTöne, Klänge undGeräusche, sowie deren akustische Eigenschaften, wieLautstärke,Klangfarbe,TonhöheundTondauer. Musik wird gelegentlich als „hörbare Mathematik“[3]bezeichnet. Die Fähigkeit des Menschen, alsMusikintendierte Schallereignisse von anderen akustischen Reizen unterscheiden zu können, gehört zu den komplexesten Leistungen des menschlichen Gehirns.[4]So können Abfolgen unterschiedlicher Einzeltöne, die sich durch zeitliche Gestaltungsmittel, wieRhythmus,MetrumundTempozuhorizontalenTonkonstellationen zusammenschließen, alsMelodiewahrgenommen werden, während aus vokaler und instrumentalerMehrstimmigkeitvertikaleZusammenklänge aus unterschiedlichen Tonhöhen erwachsen. Wie die Sprache ist die Musik eine Universalie der menschlichen Kultur. Gelegentlich führte die Annahme der Existenz vonUniversalien der Musikwahrnehmungsogar dazu, der Musik die Eigenschaften einer „universellen Sprache“ zuzuschreiben.[5]Diese Sichtweise lässt jedoch außer Acht, dass die musikalischen Ausdrucksformen der Menschheit mindestens so vielgestaltig sind, wie die menschlichen Sprachen. So lassen sich kulturübergreifende Gemeinsamkeiten meist nur in einigenarchetypischenSituationen des menschlichen Empfindens oder Verhaltens (Wiegenlieder,Kinderlieder, Liebeswerben) nachweisen.[6] Musik alsKulturgutwar und ist immer auch Gegenstand der geistigen Reflexion, aus der dann auch die verschiedenen Disziplinen derMusikwissenschafterwachsen konnten. Dabei obliegt beispielsweise die begriffliche Erfassung, systematische Darstellung der Zusammenhänge und deren Deutung derMusiktheorie, während sich dieMusikpädagogikmit dem Lehren und Lernen von Musik befasst. Das WortMusikleitet sich überlateinischmusicaab vonaltgriechischμουσική τέχνηmousikḗ téchnē(„musische Kunst“, d. h. „Kunst derMusen, Musenkunst“, besonders „Tonkunst, Musik“). Der Ausdruck mousikḗ téchnē wurde bereits in der Antike zuμουσῐκήmousikḗverkürzt.[7][8][9] Das altgriechischeAdjektivμουσικόςmousikós(vonμοῦσαmoúsa„Muse“)[10]ist in der weiblichen Formμουσῐκήmousikḗzuerst 476 v. Chr. inPindarsersterOlympischerOdeüberliefert. Das Adjektiv floss in die lateinische Sprache ein, womusicuseinerseits Adjektiv ist („die Musik betreffend, musikalisch“, auch „die Dichtkunst betreffend, dichterisch“), andererseitsSubstantiv(„Musiker“, auch „Dichter“). Dazu kommen die lateinischen Wortbildungenmusicamit der Variantemusice(„Musenkunst“ im antiken Sinne, einschließlich der Dichtkunst) undmusicalis(„musikalisch“).[11] Das griechischeμουσῐκήmousikḗund das lateinischemusicagingen schließlich als Fachwort in dietheoretischeLiteratur ein. Von dort aus übernahmen fast alle europäischen Sprachen und dasArabischedas Wort in unterschiedlichen Schreibweisen und Betonungen. Nur in wenigen Sprachen existieren eigene Prägungen, zum Beispieltschechischundslowakischhudba,kroatischglazba[12]sowiechinesisch音乐yīnyuè,koreanisch음악ŭmak/eumak,japanisch音楽ongaku.[13] In derdeutschen Spracheerschien zunächst nur das Grundwort,althochdeutschmûsekeundmittelhochdeutschmûsik. Ab dem 15. Jahrhundert wurden Ableitungen wieMusikantodermusizierengebildet. Erst im 17. und 18. Jahrhundert änderte sich die Betonung unter dem Einfluss vonfranzösischmusiqueauf die zweite Silbe, so wie es noch heute in derdeutschen Standardsprachegültig ist.[14][15] Im Deutschen gibt es fürMusik(insbesondere im Kontext musikalischer Kompositionstätigkeit) das SynonymTonkunst. Dem entsprechenangelsächsischswēgcræft[16](wörtlich „Klangkunst“) und isländischtónlist.[17]In anderengermanischen Sprachengibt es ein Wort fürTonkunst, das wie im Deutschen neben einer Wortform vonMusikgebraucht wird: niederländischtoonkunst(nebenmuziek),[18]dänischtonekunst(nebenmusik),[19][20]norwegischtonekunst(nebenmusik)[21], schwedischtonkonst(nebenmusik).[22] Der BegriffMusikerlebte in den vergangenen Jahrtausenden mehrere Bedeutungswandlungen. Aus der Künsteeinheitμουσικήlöste sich im 4. Jahrhundert v. Chr. diemusicaheraus, deren Auffassung zunächst die einer theoriefähigen,mathematischbestimmten Wissenschaft war. Unabhängig von der übrigen Entwicklung hin zur schönen Kunst blieb diese bis ins 17., inprotestantischenKreisen auch noch bis ins 18. Jahrhundert bestehen.[23]So ist der Begriffmusicabis zum entscheidenden Bedeutungswandel, der den modernen Musikbegriff einführte, nicht allein als „Musiktheorie“ zu verstehen, er ergibt sich in seiner Definitionsvielfalt erst aus der Auffassung einzelner Epochen, ihrer Klassifikationen und Differenzierungen. Der MusikästhetikerEduard Hanslickdefinierte Musik als „Sprache, die wir sprechen und verstehen, jedoch zu übersetzen nicht imstande sind“. Die Frage, was Musik sei oder nicht sei, ist so alt wie das Nachdenken über Musik selbst. Trotz der zahlreichen historischen Versuche, zu einem allgemeinen und grundsätzlichen Musikbegriff zu gelangen, gab und gibt es keine allein gültige Definition. Die bisherigen Begriffsbestimmungen stellten jeweils einen Bestandteil des Phänomens Musik in den Mittelpunkt. Die Definitionsgeschichte ist von vielen Widersprüchen geprägt: Musik als rationale, zahlenbezogene Wissenschaft, Musik als gefühlsbetonte Kunst, Musik imapollinischen oder dionysischenVerständnis, Musik als reine Theorie oder reine Praxis – oder als Einheit beider Bestandteile. Die Musikliteratur derAntikebrachte zahlreiche Definitionen hervor, die das musikalische Material, dieTonleiter, und ihre mathematischen Grundlagen in den Mittelpunkt rückten und sie als die Natur des Tongefüges verstanden. Der sophistisch-perikleische Lehrplan[24]basierte auf den beiden Säulen μουσική (Musenkunst) und γυμναστική (Gymnastik), die eine Bildung des Geistes und des Körpers vermitteln sollten. Platon betonte ebenfalls in Politeia (III. Buch), die »musische und gymnastische Bildung [als …] Grundsätze für Bildung und Erziehung«.[25]Schon bei der Mehrzahl der Sophisten im 5. und 4. vorchristlichen Jahrhundert wurden Rechenkunst, Astronomie, Geometrie und Musik integriert, die als Einheit genannt und später im Rahmen derArtes Liberalesneben den trivialen als quadriviale Künste gelehrt wurden.[26]Die Erkenntnis derWeltseelegalt als höchstes Bildungsziel dieses antiken Lehrplans. Plato gibt in seinem oft zitiertenTimaioseinen Hinweis auf seine Vorstellung von der »Zusammenfügung der Weltseele«, die durch musikalische Schwingungsverhältnisse entstanden sei. Der MusikwissenschaftlerJacques Handschinhebt einerseits hervor, dass man aus Platos Text eine Tonleiter inklusive der exakten Größe desLimma-Intervalls konstruieren könne, allerdings müsse man diese Passagen nicht als eine musikpraktische Anweisung lesen, sondern eher als eine Musikphilosophie.[27]Aus diesem Grund unterscheidet sich der antike Musikbegriff erheblich von dem heutigen Verständnis, denn viele Jahrhunderte galt der antike erweiterte Musikbegriff vor allem als ein wichtiger Erkenntnisbereich der Philosophie.[28] Cassiodor, der zur Entwicklung derSieben freien Künsteden Beitrag einer Verbindung von antiker Wissenschaft undchristlichem Glaubenleistete, definierte Musik alsdisciplina, quae de numeris loquitur(„Musik ist Wissen, das durch Zahlen ausgedrückt wird“). Diesemlogisch-rationalen Verständnis folgtenAlkuinundRabanus Maurus.Isidor von Sevillasprach vonMusica est peritia modulationis sono cantique consistens(„Musik besteht aus der Erfahrung des klingenden Rhythmus und des Gesangs“). Dieses eher klang- und sinnenorientierte Urteil rezipiertenDominicus Gundisalvi,Robert Kilwardby,Bartholomaeus Anglicus,Walter OdingtonundJohannes Tinctoris.[29] Augustins Begriffsbestimmung erfuhr im Mittelalter zunächst durch denOdo von ClunyzugeschriebenenTraktatDialogus de musicaeine starke Veränderung. Dieser erweiterte die Anschauung um einetheologischeKomponente, indem erconcordia vocis et mentis, die „Einheit zwischen Stimme und Geist“ als zentralen Punkt des Musizierens anführte. Der Gedanke wurde vonPhilippe de Vitryaufgenommen. EineanonymeAbhandlung des Mittelalters führt ausMusica est scientia veraciter canendi(„Musik ist die Wissenschaft vom wahrhaftigen Singen“), wichtiger als theoretisches Wissen und praktische Fertigkeit sei die Aufrichtigkeit des Sängers. Dies fand sich beiJohannes de MurisundAdam von Fuldawieder.[30] Während des 15. und 16. Jahrhunderts galten weiterhin die Definitionen Augustins und Boëthius’. Parallel dazu kam eine auf die Musikpraxis bezogene Auslegung auf, die alsMusica est ars recte canendi(„Musik ist die Kunst, richtig zu singen“) populär wurde – wobei in den zahlreichen Abhandlungen auchdebite(„gebührlich“),perite(„kundig“),certe(„sicher“) oderrite(„nach Brauch oder Sitte“) auftraten. Sie erscheint u. a. beiJohann Spangenberg,Heinrich Faber,Martin Agricola,Lucas Lossius,Adam GumpelzhaimerundBartholomäus Gesius, deren musiktheoretische Leitfäden bis ins 17. Jahrhundert für den Unterricht anLateinschulenbenutzt wurden, wobei hier das Singen im Vordergrund stand. Als deutschen LeitsatzMusik ist die rechte Singekunstzitierte ihnDaniel Fridericiin seinerMusica Figuralis(1619).[30] DerRationalismusdes 18. Jahrhunderts zeigt sich in der BegriffsbildungGottfried Wilhelm Leibniz’:Musica est exercitium arithmeticae occultum nescientis se numerare animi(„Musik ist eine verborgene Rechenkunst des seines Zählens unbewussten Geistes“). Mit dem ausgehenden 18. Jahrhundert, zu Beginn derWiener Klassikund am Vorabend derFranzösischen Revolutionersetzte den rationalistischen Musikbegriff sein diametrales Gegenteil: einesubjektivistische, rein gefühlsbetonte Definition setzte sich durch. Hatten die Begriffsbestimmung zuvor Musiker wie Komponisten und Theoretiker geleistet, so lieferten die wesentlichen Definitionen aus der Künstlerperspektive während des Ineinanderfließens derÄsthetikenhin zurromantischenEinheit der Künste nunDichterwie beispielsweiseWilhelm Heinse,Novalis,Wilhelm Heinrich WackenroderundJean Paul. Das persönliche Erleben und Empfinden stand im Vordergrund.[30] So formulierteJohann Georg Sulzer: „Musik ist eine Folge von Tönen, die aus leidenschaftlicher Empfindung entstehen und sie folglich schildern.“ Als modellhaft für das gesamte Jahrhundert giltHeinrich Christoph KochsWort „Musik ist die Kunst, durch Töne Empfindungen auszudrücken“. Dies erschien kaum verändert vonGottfried WeberbisArrey von Dommer. Die bis auf die Gegenwart volkstümliche Ansicht, dass Musik eine „Sprache der Gefühle“ sei, wurde allgemein anerkannt. Der Begründer derhistorischen MusikwissenschaftJohann Nikolaus Forkeläußerte sich dergestalt, ebenso die KomponistenCarl Maria von Weber,Anton Friedrich Justus ThibautundRichard Wagner. Wagners Begriff desGesamtkunstwerksprägte die weitere Entwicklung. Für die Übergangszeit vomIdealismuszumIrrationalismuswar auffällig, dass die Musik insMetaphysischeund Transzendente erhöht wurde. So nannteJohann Gottfried Herderdie Musik eine „Offenbarung des Unsichtbaren“, fürFriedrich Wilhelm Joseph Schellingwar sie „nichts anderes als der vernommene Rhythmus und die Harmonie des sichtbaren Universums selbst“.[31] Nachklänge der rationalistischen Auffassung sind im Musikdenken des 19. Jahrhunderts gleichfalls vorhanden. Bereits 1826 hatteHans Georg Nägelidie Musik ein „bewegliches Spiel von Tönen und Tonreihen“ genannt.[31]Eduard Hanslickfand 1854 in dermusikästhetischenGrundsatzschriftVom Musikalisch-Schönenzu der prägnanten Formel, der Inhalt und Gegenstand von Musik seien nur „tönend bewegte Formen“.[32]Vor dem Streit umProgrammmusikgegenabsolute Musikwurde er damit zum Wortführer einer ästhetischen Partei. Immer noch unter dem Einfluss des 19. Jahrhunderts standErnst KurthsHinwendung zu den irrationalen Kräften der Musik in seinem SpätwerkRomantische Harmonik und ihre Krise in Wagners „Tristan“(1920): „Musik ist emporgeschleuderte Ausstrahlung weitaus mächtigerer Urvorgänge, deren Kräfte im Unhörbaren kreisen. Was man gemeinhin als Musik bezeichnet, ist in Wirklichkeit nur ihr Ausklingen. Musik ist eine Naturgewalt in uns, eine Dynamik von Willensregungen.“ Geradeso wurzelteHans PfitznersMusikdenken im Jahr 1926 noch ganz im Geist derSpätromantik, vor allem in der Sichtweise Schopenhauers: Musik sei „das Abbild des Ansich der Welt, also des Willens, indem sie dessen innerste Regungen wiedergibt“. ImStilpluralismusab derModernekann keine gültige Aussage über das Wesen der Musik mehr getroffen werden, da die Komponisten individuell über ihre ästhetischen Anschauungen befinden. Sie begründen seitdem ihre Musikdefinition auf die eigene Kompositionspraxis.[31]Arnold Schönbergbezog sich in seinerHarmonielehre(1913) auf den antiken Gedanken einermimetischenKunst, wies ihr aber zugleich den Status der höchsten und äußersten Vergeistigung zu. „Kunst ist auf der untersten Stufe einfache Naturnachahmung. Aber bald ist sie Naturnachahmung im erweiterten Sinne des Begriffs, also nicht bloß Nachahmung der äußeren, sondern auch der inneren Natur. Mit anderen Worten: sie stellt dann nicht bloß Gegenstände oder Anlässe dar, die Eindruck machen, sondern vor allem diese Eindrücke selbst. Auf ihrer höchsten Stufe befaßt sich die Kunst ausschließlich mit der Wiedergabe der inneren Natur. Nur die Nachahmung der Eindrücke, die nun durch Assoziation untereinander und mit anderen Sinneseindrücken Verbindungen zu neuen Komplexen, zu neuen Bewegungen eingegangen sind, ist ihr Zweck.“ Demgegenüber verneinteIgor Strawinskykategorisch die Ausdrucksfähigkeit von Musik. SeineneoklassizistischeDefinition knüpft an die mittelalterliche Vorstellung von Musik als einem Weltordnungsprinzip an. „Denn ich bin der Ansicht, daß die Musik ihrem Wesen nach unfähig ist, irgendetwas ‚auszudrücken‘, was es auch sein möge: ein Gefühl, eine Haltung, einen psychologischen Zustand, ein Naturphänomen oder was sonst. Der ‚Ausdruck‘ ist nie eine immanente Eigenschaft der Musik gewesen, und auf keine Weise ist ihre Daseinsberechtigung vom ‚Ausdruck‘ abhängig. Wenn, wie es fast immer der Fall ist, die Musik etwas auszudrücken scheint, so ist dies Illusion und nicht Wirklichkeit. […] Das Phänomen der Musik ist zu dem einzigen Zweck gegeben, eine Ordnung zwischen den Dingen herzustellen und hierbei vor allem eine Ordnung zu setzen zwischen dem Menschen und der Zeit.“ Nach 1945 erfolgten nur noch selten allgemeine Definitionen. Einerseits hatten sich die Bestimmungsversuche seit Beginn der Neuzeit schon immer ausschließlich auf die Kunstmusik bezogen und die Unterhaltungsmusik –Tanz-undSalonmusik,OperetteundMusical,Jazz,Pop-,Rockmusiksowie elektronische Musikrichtungen wieTechnoundIndustrialetc. – weitgehend ausgeblendet. Andererseits ging der Trend immer weiter zu Entwürfen, die einige Komponisten nur für sich selbst, teilweise nur fürEinzelwerkeunternahmen. Diese Definitionen waren bisweilen an der Verankerung im Transzendentalen orientiert, z. B. beiKarlheinz Stockhausen, bisweilen aber auch unter dem Einfluss vonHappening,Fluxus,Zenund anderen geistigen Ideen radikale Umdefinitionen bis zur „Nicht-Musik“ oder zur Idee von Musik des eigentlich Vorstellbaren, wie es z. B.John Cageausdrückte: „Die Musik, die ich bevorzuge, meine eigene oder die Musik anderer, ist das, was wir hören, wenn wir einfach still sind.“[35] Der BegriffMusikist nach neuzeitlichem Verständnis klingender und wahrnehmbarerSchall. Diese Bedeutung hat sich allerdings erst in einem Prozess ergeben, der über zwei Jahrtausende andauerte und eine Vielfalt vonKlassifikationenhervorbrachte, die das jeweilige Weltverständnis ihrer Entstehungszeit widerspiegeln. Wie die ersten Definitionen hatten auch die ersten Unterscheidungen zwischenTheorieund Praxis ihren Ursprung in derAntike. Das Begriffspaar geht aufAristoxenosim 4. Jahrhundert v. Chr. zurück. Eine weitere Differenzierung der theoretischen Bestandteile nahmPlutarchvor mit der Unterteilung inHarmonik(als Beziehung der Töne untereinander ist damit dieMelodikgemeint),RhythmikundMetrik. Während Plutarchs Einteilung noch bis ins 16. Jahrhundert gebräuchlich war, ist die Gegenüberstellung des Aristoxenos bis heute gültig. Eine weitergehende Unterteilung leisteteAristeides Quintilianus. Er führt in den theoretischen Bereich dieAkustikals Lehre vom Schall ein, in den praktischen dieMusikpädagogik. Melodik und Rhythmik rechnete er der Musikpraxis zu, die er gleichzeitig um die Lehren von dermenschlichen Stimmeund von denMusikinstrumentenerweitert. Am Übergang zumFrühmittelalterunterschiedBoethiusdie Musik in drei Teile. Der erste ist diemusica mundana, die seitPythagorasbekannte Vorstellung einer nicht hörbaren, aber alskosmologischeZahlenverhältnisse derPlanetenbahnendenkbarenSphärenmusik. Die zweite ist diemusica humana, die als göttliche Harmonie von Leib und Seele des Menschen wirkt. Die dritte ist diemusica instrumentalis, die tatsächlich erklingende und hörbare Musik – diese wiederum geschieden nach deminstrumentum naturale, d. h. die durch das „natürliche Instrument“ erzeugteVokalmusik, und deminstrumentum artificiale, also derInstrumentalmusik, die die „künstlichen Klangwerkzeuge“ hervorbringen. Um 630 ordneteIsidor von Sevilladie klingende Musik in drei Bereiche nach der Art der Tonerzeugung: erstens diemusica harmonica, die Vokalmusik, zweitens diemusica rhythmica, die Musik derSaiten-undSchlaginstrumente, drittens diemusica organica, die Musik derBlasinstrumente. Dabei gab er den Begriffen Harmonie und Rhythmus erstmals eine zweite Bedeutung, die über Plutarch hinausging. Am Ende des 8. Jahrhunderts klassifizierteRegino von Prümdie Musik neu, indem er ihre Teile zu zwei größeren Bereichen zusammenfasste. Dies ist einerseits diemusica naturalis, die durch Gottes Schöpfung erzeugte Sphären- und Leib-Seelen-Harmonie sowie die gesungene Musik, andererseits die durch den Menschen erfundenemusica artificialisder künstlichen Klangerzeuger, d. h. aller Instrumententypen. Im 9./10. Jahrhundert vereinheitlichteAl-Fārābīdie bisherigen Systematiken in das Paar Theorie und Praxis; zur Theorie rechnete er lediglich diespekulativeMusikbetrachtung, also im weiteren Sinne alleMusikphilosophie, zur Praxis sämtliche anderen Bereiche, die sich auf die aktive Musikausübung mit ihren handwerklichen Grundlagen beziehen. Die mittelalterlichen Klassifikationen wurden bis ins 17. Jahrhundert hinein rezipiert, eine Verarbeitung des Boëthius auch noch danach, so beiPietro Cerone,Athanasius KircheroderJohann Mattheson. Neben den Hauptsystematiken traten in der Literatur ab dem Mittelalter auch Klassifikationen auf, die die Einzelbereiche der Musik nach anderen Gesichtspunkten zu ordnen versuchten. Folgende Gegensatzpaare erschienen: Eine erstesoziologischeHerangehensweise war um 1300 die Unterscheidung desJohannes de Grocheo, der die Musik in drei Bereiche teilte: Im 16. Jahrhundert erschienen die Begriffemusica reservataundMusica Poetica, Ersterer als Bezeichnung für den neuen Ausdrucksstil derRenaissancemusik, Letzterer als Begriff für dieKomposition. Zusammen mit den neuen Prägungenmusica theoreticaundmusica practicaetablierte dieser sich innerhalb einer Dreiteilung nach antiken Vorbildern. Zugleich markiert er die ersten Ansätze zu einer Neubewertung desKomponisten, der bisher als handwerklich geschickter „Tonsetzer“ galt und im sozialen Gefüge nun allmählich zur kreativen Künstlerpersönlichkeit aufsteigt. Die Theoretiker des 16. Jahrhunderts, voranFriedrich Wilhelm Marpurg,Jakob AdlungundJean-Jacques Rousseau, verfolgten zunächst die antike Unterscheidung von Theorie und Praxis. Sie teilten die Theorie in vier Fächer, in Akustik, Kanonik (die Lehre vonFormenund Proportionen), Grammatik (die Lehre von denIntervallen) undÄsthetik; die Praxis teilten sie in Komposition und Exekution, also Produktion und Reproduktion des musikalischen Kunstwerks. Die gängigsten deutschsprachigen Bezeichnungen führte derSprachwissenschaftlerKaspar von Stielermit seinemWörterbuchDer Teutschen Sprache Stammbaum und Fortwachs(1691) in dieLexikografieein.SchlagwortewieKirchenmusik,KammermusikundTafelmusikwaren hier erstmals aufgeführt. Die vielfältigenKompositaauf das Grundwort-musikin Bezug aufInstrumentation(Harmoniemusik), Funktion (Filmmusik) oder Technik (Serielle Musik) nahmen hier ihren Ursprung. An dieser Stelle änderte sich auch der Sprachgebrauch, der beim Grundwort-musikstets die klingende, sinnlich wahrnehmbare Musik meinte und sich nun endgültig vom Theoriebegriff derMusicaschied. Als weiteren Beitrag zurTerminologieerarbeiteteJohann Gottfried WaltherimMusicalischen Lexikon(1732) eine große Anzahl an Definitionen wie z. B. die historischen Begriffemusica anticaundmusica modernaoder dieethnologischenmusica orientalisundmusica occidentalis.[36] Die frühesten erhaltenen Instrumente, die eigens zum Musizieren hergestellt wurden, sind dieGänsegeierflöte vom Hohlefelsund die Knochenflöten vonGeißenklösterleauf derSchwäbischen Alb, die imUrgeschichtlichen Museum Blaubeurenausgestellt sind. Sie sind mindestens 35.000 Jahre alt.[37]Die meisten Anthropologen undEvolutionspsychologensind sich jedoch darüber einig, dass die Musik schon lange vorher zum Alltag des Menschen und seiner Vorfahren gehörte. Inschriftlosen KulturenmitanimistischenundschamanistischenReligionen war Musik jahrtausendelang Teil derRitenzur Beschwörung von Geistwesen. Bis in die Gegenwart ist diesekultischeMusik von Gesang,TrommelnundTanzgeprägt. Viele Kulturen verstehen auch in der Gegenwart Musik als Einheit aus Klang, Sprache, Kult und Tanz und kennen keinen eigenen Begriff für den klanglichen Anteil dieser Einheit. In der Musik schriftloser Kulturen finden sich Anklänge an denVogelgesangmit seinen Ton- und Tongruppenwiederholungen, Tonreihen undMotiven. Melodien aus wenigen Tönen innerhalb einesTerz- bis Quartraums, die stetig wiederholt werden, könnten auf eine Nachahmung des Vogelgesangs zurückgehen.[38]Die Rhythmik ist selten anTaktschematagebunden bzw. wechselt ihre Einteilungen und Betonungen häufig, indem sie sich der melodischen Phrasierung anpasst. Sie ist jedoch nicht gestaltlos, sondernpolyrhythmischwie dietraditionelle afrikanische Musik, die vor allem bei Gesang mit begleitendenIdiophonenrhythmischePatternübereinanderschichtet. Auch der später für denJazzcharakteristischeOffbeatist zu finden, d. h. die Betonung der schwachen Taktteile.[38] Auch in den frühenHochkulturenwar die Musik inRitusundKulteingebunden. Möglicherweise wurde sie auch im normalen Alltag praktiziert. Später wurde sie zu einer autonomen Kunst. Die altorientalischenSchriftkultureninMesopotamienbegannen im 4. Jahrtausend v. Chr. mit denSumerern. Sie erfanden das ersteInstrument mit mehreren Saiten, dieLeier; diese wurde in den folgenden Jahrhunderten zu einerHarfemit vier bis zehn Saiten undResonanzkörper.[39] Imalten Ägyptenab etwa 2700 v. Chr. erweiterte sich das Instrumentarium um die Bogenharfe. In dieser Zeit entstanden auch eine weltliche Musik und die reine Instrumentalmusik. Über die Anfänge derindischen Musikim 3. Jahrtausend v. Chr. gibt es nur Vermutungen. Sie hat möglicherweise Anregungen der mesopotamischen und ägyptischen Kultur aufgenommen. Durch die Einwanderung derAriergegen 1500 v. Chr. gelangten westliche Einflüsse nach Indien. Diechinesische Musikwar bereits im Altertum weit entwickelt. Die wichtigsten Anregungen kamen vor allem aus Mesopotamien. Eigene Erfindungen waren ein Skalensystem,pentatonischeGebrauchstonleitern und eine feste Tonhöhenstimmung. Die Kompositionen waren einstimmig undhomophon. In dergriechischen Antikewar mitmousikḗbis ins 4. Jahrhundert v. Chr. eine Einheit vonPoesie, Tanz und Tonkunst gemeint. Als sich dann die Bedeutung auf die klingende Musik einengte, blieb die enge Beziehung zu Dichtung und Tanz erhalten. Die westliche klassische Musik beginnt mit derMusik des Mittelalters. Diese war stark von Zahlenordnungen geprägt. AlsArs musicabildete sie zusammen mitArithmetik,GeometrieundAstronomiedasQuadriviuminnerhalb derArtes liberales. In derMusik der Renaissancewurde erstmals der kreativen Leistung des Komponisten ein höherer Rang zugesprochen als der durch Übung erworbenen handwerklichen Meisterschaft des Musikers. Zu dieser Zeit entstanden in derKunstmusikvermehrt reine Instrumentalwerke. Die Musik des Mittelalters, die Renaissancemusik und dieBarockmusik(ca. 1600 bis 1750) werden zusammen alsAlte Musikbezeichnet. Darauf folgten die Epochen derVorklassikund derWiener Klassik(sieheEpochen der klassischen Musik). Die vorherrschende Idee des 16. bis 18. Jahrhunderts war die bereits vonAristotelesin seinerPoetikbeschriebeneMimesis, die Nachahmung der äußeren Natur bis zurTonmalereiund der inneren Natur des Menschen in derAffektdarstellung. Mit dem beginnendenRationalismusim 17. Jahrhundert setzte sich der schöpferische Aspekt durch. In derRomantikstanden das persönlich-subjektive Erleben und Empfinden und dessenmetaphysischeBedeutung im Vordergrund. DieMusik der Romantikbrachte enorme Weiterentwicklungen im musikalischen Ausdruck. Im Blick auf die Fähigkeit der Musik, außermusikalische Inhalte darzustellen, entstanden Begriffe wieabsolute Musik,Programmmusikundsinfonische Dichtung, über die erbittert gestritten wurde.[40] Die europäisch geprägte, westliche klassische Musik wird meist einfach „klassische Musik“ genannt. Dabei geraten andere Musiktraditionen aus dem Blick, die in ihrem Kulturraum eine vergleichbare Rolle spielen, zum Beispiel dieklassische arabische Musikund dieklassische indische Musik. Seit dem Ende des 19. Jahrhunderts wurde die Unterhaltungsmusik (U-Musik) immer unabhängiger. Unter anderem von der afroamerikanischen Volksmusik beeinflusst, entwickelte sie sich zu einem eigenen Zweig, der schließlichJazz,Pop-undRockmusikmit einer Vielfalt jeweils stark differenzierter Einzelgenres hervorbrachte. In der „ernsten Musik“ (E-Musik) begann um 1910 die sogenannteNeue Musik. Sie brachte den für dieModernetypischenStilpluralismushervor. Die traditionellen Grenzen von Gattungen und Stilen verschwammen, ebenso die Grenze zur U-Musik. Für die Vermischung der Musikstile stehen Begriffe wieWeltmusik,CrossoverundThird Stream.[40] Dieelektronische Musikerweiterte die Möglichkeiten der Klangerzeugung. Das musikalische Denken derPostmodernetendiert zu einem ästhetischen Universalismus, der Außermusikalisches einbezieht –multimedialoder im Sinne einesGesamtkunstwerks– oder zu neuen Denkmodellen, wie sie in Kulturen und Philosophien außerhalb desAbendlandesgewachsen sind.[40] Erfindungen wieMikrofon,Grammophon,Tonbandgerät,Lautsprecher,Audioverstärkerund vieles mehr ermöglichtenTonaufnahmenund deren Wiedergabe. Musik ist seitdem in einem zuvor unvorstellbaren Ausmaß präsent und verfügbar, was sich durchHörfunkundFernsehenund zuletzt durch diedigitale Revolutionnoch steigerte. In Kaufhäusern und Restaurants, an vielen Arbeitsplätzen und anderen Orten läuft ständigHintergrundmusik. Das Ausgangsmaterial der klingenden Musik sindSchallereignisse, d. h.TöneundGeräusche, die von Menschen erzeugt werden. Die Töne unterscheiden sich in verschiedenenParametern, nämlichTonhöhe,TondauerundLautstärke. Ferner unterscheiden sie sich in derKlangfarbe, die davon abhängt, wie sie erzeugt werden, zum Beispiel mit einerViolineoder einerFlöte. Die wahrgenommenen Töne entsprechen einer bestimmtenGrundfrequenz, wobei aberObertönemitschwingen. Das Spektrum der Obertöne und damit die Klangfarbe ist vom Musikinstrument abhängig. Außerdem enthalten die Töne einen Geräuschanteil, der ebenfalls den Klangcharakter beeinflusst, etwa ein gewisses Kratzen oder Schaben bei Streichinstrumenten (eine Ausnahme sindSynthesizer, dieSinustöneerzeugen). BeiSchlaginstrumentensteht der Geräuschcharakter im Vordergrund. Die Variabilität der Töne und Klangfarben ergibt somit ein reichhaltiges musikalisches Ausgangsmaterial. Noch größer ist die Vielfalt derKlänge, wenn mehrere Töne – oder Töne und Geräusche – gleichzeitig erklingen. Musik entsteht, indem Töne, Geräusche und Klänge mit gestalterischen Mitteln geordnet und zu einfachen oder auch komplexen klingenden Gebilden zusammengefügt werden. Zu diesen gestalterischen Mitteln zählen vor allemRhythmik,MelodikundHarmonik. Somit enthält Musik nicht nur geordnetes akustisches Material, sondern auch die musikalischen Ideen ihres Schöpfers.[41] Komponistenformen aus dem klanglichen Material in einem schöpferischen Prozess musikalische Werke. Seit demHumanismusist das „Werk“, in der Musik auchOpusgenannt, eine in sich abgeschlossene, die Zeit überdauernde individuelle Schöpfung eines Autors. Musikalische Werke, die dieser Definition entsprechen, gab es bereits imSpätmittelalter. Der Werkbegriff bildete sich erst in derFrühen Neuzeitheraus. Auf den 1533 verfassten TraktatRudimenta musicaedesNikolaus Listeniusgeht die Vorstellung des vollendeten Werks zurück(opus perfectum et absolutum), das die Lebenszeit seines Schöpfers überdauert (wenngleich es hier noch nicht auf Kompositionen beschränkt war). Im 19. Jahrhundert verfestigte sich die Vorstellung, dass zu einem Werk ein ästhetischer Gehalt und Originalität gehören. In der Moderne wurde das Kriterium der Geschlossenheit durch offene und variable Formen und die Idee eines „Work in progress“ in Frage gestellt.Zwölftonmusikundserielle Musikstanden im Widerspruch zum ästhetischen Kriterium. In derAleatorikwurde schließlich auch die Originalität des Werks relativiert.[42] Der Werkbegriff gründet sich heute vor allem auf die schriftliche Fixierung von Musik im Notentext. Ein fixiertes Werk hat einen Autor, es ist einmalig, es ist abgeschlossen und darum unveränderlich. Es behält seine Identität auch dann bei, wenn es bei der Darbietung durch verschiedene Musiker unterschiedlich klingt. Durch die schriftliche Fixierung wird es traditions- und geschichtsfähig. Die Fixierung der Werke ist Voraussetzung für Analyse, Interpretation und Bewertung sowie für Vergleiche mit anderen Werken.[42] Einen großen Anteil an der Musik hatGebrauchsmusik, die nicht verschriftlicht wird. Ihr fehlt ebenso wie derimprovisiertenMusik der Charakter eines Kunstwerks.[42] In klanglicher Hinsicht wird Musik erzeugt, wennMusiker„Musik machen“. Sänger nutzen beim Musizieren diemenschliche Stimme, Instrumentalisten spielen auf ihremMusikinstrument. Entsprechend wirdVokalmusikvonInstrumentalmusikunterschieden. Häufig wirdGesangmit Instrumentalmusik kombiniert. Schon ein einzelner Sänger kann singen und zugleich etwaGitarrespielen. InKantaten,OratorienundOpernwirken Gesangssolisten, Chor und Orchester zusammen. Medien halten die flüchtig verklingende Musik fest, machen sie für Mit- und Nachwelt verfügbar und lassen Musik erst entstehen. Sie sind als Noten eines Kunstwerks Gegenstand der historischen Forschung wie auch des Werturteils. Dabei besteht eine Wechselwirkung zwischen Medien auf der einen, Aufführungs- und Kompositionsprozess sowie Musikanschauung auf der anderen Seite; Gleiches gilt für die Technik, die zur Produktion und Reproduktion genutzt wird und im Gegenzug die Spieltechnik der Musikinstrumente beeinflusst. Produktion, Vermarktung und Vertrieb von Musikmedien sind seit den Anfängen des Druckwesens das Geschäftsziel einer ganzen Branche, die seit dem 20. Jahrhundert als Musikindustrie global operiert und ein nicht mehr zu überblickendes Angebot bereithält. Wird Musik nicht mündlich tradiert, wie es bei Volksmusik der Fall ist, so kann sie niedergelegt werden in Zeichensystemen, die der visuellen Darstellung und Klärung der musikalischen Gedanken dienen: Notationen. Eine Notation überbrückt Zeit und Raum, man kann sie aufbewahren und reproduzieren, vervielfältigen und verbreiten. Damit dient sie dazu, Einblicke in den Schaffensvorgang einesWerkszu gewähren und dessen musikalische Strukturen nachzuvollziehen. Gleichzeitig schafft sie eine der Voraussetzungen zurKompositionund Verwirklichung der Kompositionsidee, da der musikalische Gedanke in der Notation festgehalten wird. Je nach ihrerKodierung– Buchstaben, Ziffern,diskreteoder nicht diskrete grafische Zeichen – ist eine Notationsform in der Lage, Informationen mit unterschiedlicher Genauigkeit zu speichern.[43]Man unterscheidet dabei Resultatschriften, die einen bestehenden Zusammenhang verfügbar machen, von den Konzeptionsschriften, die neu erfundene Zusammenhänge festhalten. Zu den Aktionsschriften, die den Notentext für die Spieltechnik eines bestimmtenMusikinstrumentsaufbereiten, zählen z. B.TabulaturenfürOrgel- oderLautenmusik. Die heute gebräuchliche Notenschrift enthält noch vereinzelte Elemente der Aktionsschrift. Dabei kann Notation ein Werk nie vollständig in seinenParameternerfassen, so dass immer ein Spielraum bei der Ausführung und Interpretation verbleibt; diehistorische Aufführungspraxisversucht, auf Grund vonQuellendie Ausführung möglichst originalgetreu im Sinne des Komponisten und seiner ästhetischen Ansichten zu gestalten. Die ersten Notationen sind ausalten Ägyptenund demantiken Griechenlandbekannt. Die imbyzantinischenundgregorianischen ChoralverwendetenNeumenwaren in der Lage, die Melodiebewegungen der einstimmigen christlichen Musik aufzuzeichnen. Etwa seit dem 8. Jahrhundert begannen sie sich zu entwickeln. Die frühen Neumen erforderten jedoch noch immer die Kenntnis der Melodie und derrhythmischenOrdnung, sie waren eine reine Resultatschrift. Um das Jahr 1025 führteGuido von ArezzoNeuerungen ein, die bis heute in der modernen Notenschrift gültig sind:NotenlinienimTerzabstandund dieNotenschlüssel.[44] Im 13. Jahrhundert erforderte diemehrstimmigeMusik eine genauere Fixierung der Tondauern. DieModalnotationwies den einzelnen Noten feste Werte für die Dauer zu, dieMensuralnotationordnete vor dem Hintergrund des gleichzeitig entstehendenTaktsystemsdie Proportionen der Tondauern untereinander. Damit konnten Tondauern exakt dargestellt werden. Die Stimmen wurden dabei einzeln notiert, nach der Fertigstellung einer Komposition getrennt in einem Chorbuch aufgezeichnet und zur Ausführung wiederum als Einzelstimmen abgeschrieben. Die heute international gebräuchliche Standardnotation ist seit dem 17. Jahrhundert entstanden. Vor allem die präzise Aufzeichnung der Dauern erfuhr seitdem noch einige Erweiterungen. Zunächst wurde das Grundzeitmaß mitTempobezeichnungenund Taktangaben bestimmt, nach der Erfindung desMetronomsdurchJohann Nepomuk Mälzelim Jahr 1816 war es mechanisch reproduzierbar durch eine genaue Angabe der Taktschläge pro Minute. Nach dem VorbildBéla Bartóksgeben Komponisten zusätzlich die Aufführungsdauern einzelner Abschnitte in Minuten und Sekunden an. Neue Typen seit dem 20. Jahrhundert waren diegraphische Notationund Aufzeichnungsformen zur Produktionelektronischer Musik.[45] Bald nachJohannes GutenbergsErfindung desBuchdrucksmit beweglichen Lettern um das Jahr 1450 begann auch derNotendruck. Bereits für 1457 ist das erste gedruckte Musikwerk nachweisbar, noch vor 1500 entwickelteOttaviano dei Petruccidie Drucktechnik mit beweglichen Notentypen. WichtigeDruckerundVerlegerwiePierre AttaingnantundJacques Moderneveröffentlichten dieChansons,Motettenund Tanzstücke ihrer Zeit erstmals in Sammelausgaben – sie befriedigten damit die Nachfrage des Publikums nach weltlicher Musik zur Unterhaltung und zogen zugleich einen wirtschaftlichen Vorteil aus dem Verkauf hoher Stückzahlen. Damit setzte auch eine verstärkt überregionale Verbreitung von Musikstücken ein. Technische Verfahren wieTiefdruckim 16., Notenstich undLithografieim 18. Jahrhundert verbesserten die Qualität des Notendrucks erheblich und gestatteten, sowohl umfangreiche als auch grafisch kompliziert aufgebaute Notentexte wiederzugeben.Fotosatz,Offsetdruckund schließlich computergesteuerteNotensatzprogrammeerweiterten diese Möglichkeiten nochmals.[46] DieSchallaufzeichnungbegann 1877 mitThomas Alva EdisonsPhonographen. Dieser Apparat unterstützte bis in die 1930er Jahre dieMusikethnologie: allein Béla Bartók undZoltán Kodályzeichneten damit tausende von Volksliedern bei derFeldforschungin Osteuropa und Nordafrika auf. 1887 entwickelteEmil Berlinerdie ersteSchallplatteund das dazugehörigeGrammophon. Mit diesem Gerät, das bald in Serie gefertigt wurde und dieSchellackplatteals Speichermedium popularisierte, hielt die Musik aller Gattungen auch in jene Haushalte Einzug, die keineHausmusikbetrieben undArrangementsoderAuszügefür dasKlavierspielverwendeten.[46]Die Erfindung und Vermarktung der Schallplatte beeinflusste bald die Musik selbst; 1928 schlossIgor Strawinskyeinen Vertrag mit dem UnternehmenColumbia Records, um seine Werke authentisch einzuspielen.[47] Mehrere technische Schritte verbesserten die Schallplatte. 1948 setzte sichPolyvinylchloridals Fertigungsmaterial durch, das Platten mit schmaleren Rillen erlaubte, ihre Spieldauer wesentlich verlängerte und die Tonqualität erheblich steigerte, da die Abspielgeschwindigkeit von 78 auf 33 bzw. 45 Umdrehungen pro Minute sank. Die lange zuvor entwickelteStereofonieführte 1958 zur Stereo-Schallplatte und in den 1960er Jahren zum zweikanaligenRundfunk. Dies erforderte eine neue Generation vonWiedergabegeräten,Stereoanlagenwurden in großer Anzahl verkauft.High Fidelitywurde zum Standard derKlangtreue. DasDirektschnittverfahren, bei dem die Aufnahme nicht wie zuvor auf einemTonbandaufgezeichnet wurde, vergrößerte die Güte der Musikwiedergabe nochmals. Außerdem etablierte sich eine neue Berufsgruppe: derDJ. Daneben waren Tonbänder auch im privaten Bereich beliebt, insbesondere in Form vonCompact Cassettenzum Abspielen und Aufnehmen imKassettenrekorderund zum mobilen Einsatz imWalkman. DieCompact Discals Speichermedium gelangte 1982 auf den Markt. Sie steht am Anfang derdigitalenMedien, mit denen Musik in höchster Qualität bei verhältnismäßig geringem Platzbedarf gespeichert werden kann.[48]Ein weiterer Schritt zur Digitalisierung, der mit der allgemeinen Verbreitung vonComputerneinherging, war die Entwicklung vonAudioformatenwie z. B.MP3, das eineplattform- und geräteunabhängige Nutzung erlaubt und imInternetin Form von herunterladbaren Musikdateien einen weiteren Verbreitungsweg nimmt. Im Verbund mitaudiovisuellen Medienwirkt Musiksynergetisch. Ihre Verbindung mit Ausdrucksformen wieSchauspieloderTanzbesteht seit ältester Zeit in rituellem Zusammenhang. Aus der Verknüpfung mit der Dramatik ging dieOperhervor. Die Verbindung mit dem Fernsehen schafft einerseits Öffentlichkeit für und Interesse an musikalischen Inhalten, andererseits entstehen durch sie neue Genres wie dieFernsehoper. ImFilmübernimmt die Musik vielfältige Aufgaben, dramaturgische wie unterstützende, gegenüber der bildlichen Aussage. Sie intensiviert die Handlungswahrnehmung, setzt die Absichten desFilmregisseursum und trägt beim Zuschauer zu einem sinnlichen Gesamteindruck bei. In technischer Hinsicht muss sie exakt mit der optischen Information des Films synchronisiert sein. Entsprechendes gilt für den Einsatz in der Werbung. Ein weiteres Genre stellt dasMusikvideodar. Anders als in der Filmmusik ist es hier Aufgabe des Regisseurs, ein bereits existierendes Musikstück dramaturgisch zu visualisieren. Im Mix mit anderen Medien dient ein Musikvideo in der Regel dem Verkauf der entsprechenden Musik, wenngleich auch hier audiovisuelle Kunst entstehen kann.[49] Musik war bereits während der 1980er Jahre in denMailboxnetzenund imUsenetein relevantes Thema.AudiodatenkompressionundStreaming Mediasowie höhereDatenübertragungsratenmachten sie schließlich zu einem festen Bestandteil derNetzkultur. Dabei dient das Internet auf Grund seiner dezentralen Organisation nicht nur zur Information und Kommunikation, sondern schafft auch kontinuierlich neue Inhalte, die teilweise nach kurzer Zeit wieder verschwinden. Zu den Streaming-Medien zählenInternetradiosender, die entweder das terrestrisch empfangbare Programm herkömmlicher Radiosender im Internet übertragen oder aber ein eigenes, nur über das Internet verfügbares Programm anbieten. Ebenso werden Musikprogramme durchPodcastingübertragen. Eine weitere Einsatzmöglichkeit ist das Bereitstellen herunterladbarer Dateien zurMusikpromotion. Künstler,LabelsundVertriebebieten teils kostenlose, teils kostenpflichtige Downloads an, die auch durchdigitale Verwaltungin ihrer Art und Häufigkeit der Nutzung eingeschränkt sein können oder einemKopierschutzunterliegen. Einzelne Künstler bieten Werke auch ausschließlich im Internet an, so dass ein Vertrieb der „materiellen“ Tonträger entfällt. Ebenso sind kostenloseBonustrackserhältlich oder Material, das nicht auf Tonträgern erscheint. Informationen und Kommunikation über musikbezogene Themen bieten private Homepages, Fanseiten,Blogsund Nachschlagewerke in lexikalischer Form.Webportalenehmen eine bedeutende Funktion für das Musikangebot ein. Während einige Portale ein Angebot anHyperlinksauflisten, dienen andere der Vermarktung von Musik in Kooperation mit der Musikindustrie. Darüber hinaus existieren Plattformen, die Musikern gegen geringes Entgelt oder kostenfrei onlineSpeicherplatzund einContent-Management-Systemzur Verfügung stellen, um ihre selbst produzierte Musik hochzuladen und anzubieten. Sie dienen sowohl der Selbstvermarktung der Künstler als auch der Online-Zusammenarbeit und der Bildungsozialer NetzwerkedurchCommunitybildung. MitFilesharing-Programmen, die auf das Internet zugreifen, werden Musikdateien in einemPeer-to-Peer-Netzwerk zwischen den Internetnutzern getauscht. Wenn es sich dabei nicht umPrivatkopienvon urheberrechtlich geschütztem Material handelt, stellt dies eine strafwürdigeUrheberrechtsverletzungdar, die seitens der Musikindustrie bzw. einzelner Künstler bereits zu Klagen gegen Tauschbörsen und deren Kunden geführt hat, da der Verkauf durch die kostenlosen Downloads über P2P-Programme eingeschränkt würde.[50] Die Musikindustrie hat sich seit 1945 zu einem hochgradigintegrierten,global organisiertenGeschäftszweig unter der Führung einiger weniger Unternehmen aus den westlichenIndustriestaatenentwickelt. Diese beherrschen denUnterhaltungselektronik- undMassenmediensektor. Die führenden Nationen desTonträgermarktes– größtenteils der Verkauf vonCompact Discs– sind dieVereinigten Staaten,JapanundDeutschland.[51]Neben den so genanntenMajor-Labels, die den Markt im Wesentlichen beherrschen und durch Konzentration bzw. Zentralisation einAngebotsoligopolbilden, arbeitet eine große Anzahl vonIndependent-Labelsbasisnah auf Nischenmärkten. Dessen ungeachtet bestehen zwischen Majors und Independents auch wirtschaftliche Verbindungen im BereichKünstler- und Repertoirebetreuungsowie in denVertriebsstrukturen. Für Unternehmen, die zwischen beiden Kategorien stehen, hat sich der Begriff „Major Indies“ durchgesetzt. Tonträger stellen das Hauptprodukt und damit die größte Einnahmequelle der Musikindustrie dar. Die SendemedienRundfunkundFernsehenverbreiten die Produkte der Tonträgerhersteller in ihren Programmen. Dadurch ist einerseits die Musikindustrie abhängig von den Medien, die andererseits selbst auf das Angebot der Industrie angewiesen sind und durch dessen NutzungUmsatzerwirtschaften: so weitUrheberrechtundverwandte Schutzrechtefür Autoren und Künstler gelten, zahlen die Medien Gebühren für die Nutzung. DieVerwertungsgesellschaftenüberwachen diese Nutzung. Sie legen die Höhe derVergütungenfest, die an die Autoren und Interpreten zurückfließt. Von erheblicher Bedeutung für diePopularmusiksind hierMusikfernsehsender, die als Vermarktungsinstrument fürMusikvideoszum Zweck derAbsatzförderungfungieren.[52]In Einzelfällen können die Gewinne aus der Nutzung von Lizenzrechten den Tonträgerverkauf übersteigen. Diese entstehen aus dem Handel mit Fanartikeln wie T-Shirts, Postern, Postkarten oder Aufklebern.[53] Ebenso wichtig sindPrintmedien. Bis zur Massenproduktion derSchallplatteruhte das Kerngeschäft auf denMusikverlagen, die vor allemNotenherstellten undMusikalienvertrieben, heute aber vor allem die Urheberrechtsansprüche gegenüber den Medien wahrnehmen. Ferner liegt ihre Arbeit im Handel mit den Schutzrechten amgeistigen Eigentum, die bei der Vervielfältigung, Vermarktung und Verwertung von Tonträgern zu beachten sind. Der Stellenwert vonLizenzhandelundMerchandisinggewann vor allem in den 1980er Jahren an Bedeutung, als Hunderttausende von Einzelwerken der Popmusik im Rahmen von Kauf und Weiterverkauf kompletter Unternehmenskataloge mehrere hundert MillionenUS-DollarErlös erzielten. Lizenznehmer sind neben den Medien dieWerbebrancheund dieKonsumgüterindustrie. Die großenKonzerneder Tonträgerindustrie unterhalten dazu eigene Unternehmen. Neben den Tonträgerherstellern umfasst die MusikindustrieMusikproduktionsunternehmenundTonstudios,TourneeveranstalterundKünstleragenturen,Groß-undEinzelhandelsunternehmensowie Musikzeitschriften.[54] Musikproduktionist im Zeitalter der Massenmedien grundsätzlich weiterhin als arbeitsteiliger Prozess möglich. Während früher die Trennung von Komponisten und ausführenden Musikern überwog, ist durch elektronische Produktionsmittel und den Einsatz von Computern auch eine vollständige Produktion durch eine einzige Person erreichbar. Die Arbeitsteilung hat ihre Grundlage in der Übermittlung von Musik in Form von Noten,MIDI-Daten o. Ä., so dass Komponisten und Interpreten spezialisiert handeln können.[55]Während bei der Produktion von „klassischer Musik“ vorwiegend werkgetreue und künstlerisch möglichst hochwertige Aufnahmen im Vordergrund stehen, werden die produktionstechnischen Mittel in den Genres der Unterhaltungsmusik erheblich konsequenter und zielorientierter eingesetzt. Auch der Einsatz der Mittel alsEffekteum ihrer selbst willen ist zu beobachten. Gewünscht ist ein möglichst einzigartigesHörereignis.[56]Spielte im Bereich der Unterhaltungs- und besonders derFilmmusikdie Tätigkeit desArrangeursfür den Klang früher noch eine wichtige Rolle, so steht mit der zunehmenden Digitalisierung und dem Einsatz vonSampling-Technik eher dasSounddesignim Vordergrund.[57] Enge Verbindungen zu Komposition und Produktion hat derMusikinstrumentenbau. So ist beispielsweise die Geschichte derKlaviermusiknur im Zusammenhang mit der technischen Entwicklung des Instruments im 19. Jahrhundert zu verstehen; Musik wurde und wird instrumentenspezifisch komponiert, wobei technische Verbesserungen des Klaviers –Hammer- und Pedalmechanik, Klangvolumen – von den Komponisten ausgenutzt wurden. Ebenso schlugen sich die Klangverbesserungen derStreichinstrumente, dieIntonationssicherheitund Erweiterung des Tonumfangs vonHolz-undBlechblasinstrumentenin der Kompositionspraxis nieder, die höhere Anforderungen an die Instrumentalisten stellte.[58] Die Verbindung von erklingender Musik und Technik erfolgt in der Arbeit desTonmeistersbzw.ToningenieursoderTontechnikers. DemMusikproduzentenschließlich obliegt die kaufmännische, organisatorische und künstlerische Gesamtleitung.[59] Die Musikindustrie operiert im Zusammenhang kultureller Prozesse. Auch wenn sie tendenziellmonopolistischeStrukturen aufweist, sind ihre Verfahrensweisen nicht ausschließlichbetriebswirtschaftlichausgerichtet, da sie den schöpferischen Prozess des Musizierens berücksichtigen muss, um einProdukthervorzubringen. Ihre Wirtschaftsmacht entspricht nicht zugleich einer kulturellen Mächtigkeit, die die ästhetischen Wertmaßstäbe der konsumierenden Gesellschaft zu beherrschen in der Lage wäre. So hat sich seit Mitte der 1950er Jahre das Verhältnis des wirtschaftlichen Erfolgs innerhalb der angebotenen Produkte in überbetrieblicher Hinsicht trotz zunehmender Konzertverflechtung nicht verändert. Ungefähr 10 % der jährlich veröffentlichten Produktionen sind wirtschaftlich erfolgreich, wobei 3 % Gewinn erzielen und 7 % lediglich die Kosten decken; die übrigen 90 % der Veröffentlichungen verursachen Verluste. Dem imkulturwissenschaftlichenDiskursverbreiteten Urteil, die Musikindustrie betreibe eineKommerzialisierung, steht die Meinung entgegen, ihr Hauptinteresse liege weniger in der Gestalt des Werks als in der wirtschaftlichen Verwertung seiner Rechte. Dem musikalischen Produkt ist im Gegensatz zur Praxis der traditionellen Musikkultur zu eigen, dass es nur auf dem Trägermedium einer zu seiner Verwertung einmalig erbrachtenWerkleistungberuht. Dies Werk wird im Regelfall einmaligkomponiert, einmaliginterpretiert, auf dem Tonträger festgehalten und verbreitet. Es wird aus dem vorhandenenRepertoireausgewählt bzw. in der Unterhaltungsmusik häufig eigens verfasst,aufgenommen, produziert, vermarktet und im Einzelhandel vertrieben.[60]Danach erst setzt der sozialeRezeptionsprozessein. Weder Produktion noch Rezeption von Tonträgern lässt sich also strukturell mit den traditionellen kulturellen Prozessen vergleichen. Anders aber als im öffentlichen Musikleben, das dem Publikum bestehende Musikwerke anbietet, indem die Institutionen danach in einem geeigneten Repertoire suchen, geht die Musikindustrie den entgegengesetzten Weg. Sie versucht dem auf einem Tonträger präsenten Musikstück so kontrolliert wie möglich eine Rezeption aufzubauen. Das entscheidende Moment liegt in derNachfragedes Produkts, die die Entscheidungen der Verkaufspolitik bestimmt. Diese sind in den verschiedenen Sektoren (Pop, Klassik, Jazz usw.) jeweils unterschiedlich gestaltet.[61] Die Musikwissenschaft erforscht und reflektiert Musik in allen ihren Ausprägungen und Aspekten, beispielsweise die Komposition, Produktion und Rezeption von Musik sowie ihre Funktionen und Wirkungen.[62]Die Musikwissenschaft wird seit dem 20. Jahrhundert in drei Teilbereiche gegliedert, in diehistorische Musikwissenschaft, diesystematische Musikwissenschaftund dieMusikethnologie, die Musik in verschiedenen Ethnien und Kulturen erforscht und auch als Vergleichende Musikwissenschaft bezeichnet wird. Ein Teilgebiet der Musikwissenschaft ist dieMusiktheorie, die sich unter anderem mitHarmonik,Tonsatz,Formenlehreund dermusikalischen Analysebeschäftigt. Ein ähnliches Themenspektrum hat dieMusiklehre– die Bezeichnung bezieht sich auf jenen Anteil der Musiktheorie, der Musikschülern und Studenten im Unterricht vermittelt werden soll. Zur Musikwissenschaft zählt auch dieMusikpsychologie, die Themen wieMusikwahrnehmung, Musikverständnis,musikalische Sozialisationundmusikalische Begabung(auch „Musikalität“ genannt) aus psychologischer Sicht bearbeitet. Musik findet in derGesellschaftstatt. Sie steht zu ihr in stetiger und gegenseitiger Abhängigkeit und Einflussnahme: sie wird in ihrer Produktion, bei derKompositionund Aufführung von gesellschaftlichen Faktoren beeinflusst, beeinflusst bei derRezeptionwieder die Menschen und damit die Gesellschaft. Musik ist abhängig von densozialen Rollender Menschen, die sie erfinden, singen und spielen, hören, verbreiten, sammeln, kaufen, bevorzugen oder ablehnen; sie ist gleichfalls abhängig von denInstitutionen, die durch Musik andererseits erst entstehen. Überethische,ästhetischeoder auch andereWerturteilebildet sieNormenin Bezug auf das Verhalten ihr gegenüber. Sie ist in der Lage,soziale Gruppenzu konstituieren und zu verändern. Musik ist ähnlich wie Sprache ein wesentliches Element einer Kultur und damit Ausdruck der individuellen und kollektiven Identität einer Gesellschaft, Gemeinschaft oder Gruppe. Musik ist damit Träger dieser kulturellen Identität, des sensiblen kulturellen Gedächtnisses und der gewachsenen kulturellen Vielfalt einer Gemeinschaft. MusikalischeLebensweltenbedeuten das, was Menschen als selbstverständliche Umgebung und Umwelt in Bezug auf Musik erleben. Sie stellen die Weltsicht desIndividuumsdar. Sie werden durch das musikalische Alltagserleben geprägt und prägen wiederum die alltägliche Umwelt des Menschen. Die historischen und sozialen Wurzeln der Musik im Alltag sind vielfältig. Dies gilt für Gesellschaften aller Ethnien und Epochen und innerhalb dieser für zahlreiche Einzelstrukturen, da Musik einerseits eine universelle Erscheinung ist, andererseits nach innen Gesellschaften differenziert. Musik erfüllt einGrundbedürfnisnach Fürsorge und emotionaler Hinwendung. Sie gewährt Identifikation mit der sozialen Gruppe und mit sich selbst, sie befriedigt das Bedürfnis nach Heimat undkultureller Zugehörigkeit. Darum ist sie ein Bestandteil der Lebensqualität im Alltag, in dem sie ihre Bedeutung aus ihrem Gebrauch entfaltet. Die kulturellen Grundlagen der Musikverwurzelung im europäischen Kulturkreis haben sich unterschiedlich stark gehalten. Die Wurzeln der mittelalterlichen christlichen Musikkultur sind hinter dem neuzeitlichen Musikverständnis der Stände- und Fürstengesellschaft zurückgetreten, dieses hinter der bürgerlichen Musikkultur des 19. Jahrhunderts. Selbst das Kulturverständnis dieser Epoche ist nicht unverändert geblieben, es ist in anderen und neuen Formen aufgegangen, z. B. in der Event-Kultur des 20. und 21. Jahrhunderts oder in derNeuen Musik, die nur ein kleiner Ausschnitt des Publikums wahrnimmt. Dem gegenüber steht diePopkultur, die die Mehrheit des Publikums rezipiert. Die Vielfalt des musikalischen Angebots spiegelt diese Situation wider, da sie analog zu den unterschiedlichen Lebensentwürfen und den kulturellen Bedürfnissen ein Spektrum aller Musikarten umfasst. Sie alle aber können bei Menschen unterschiedlicher sozialer oder kultureller Herkunft oder aus verschiedenen Generationen dieselben Grundbedürfnisse befriedigen. Für weite Teile bleibt das Kulturmodell des Bürgertums indes noch immer vorbildhaft, da es neben der Identität stiftenden Funktion einedistinktivebesitzt, mit der sich der Bürger von anderen Gesellschaftsschichten abgrenzen kann. Andererseits war für dasProletariatdes 19. und ist für die Mittel- und Unterschicht des 20./21. Jahrhunderts die Teilhabe an diesem Kulturmodell nicht mehr unbedingt erstrebenswert. Daran haben auch die Massenmedien nichts geändert. Die Teilhabe an unterschiedlichen Musikkulturen ist unterdessen nicht mehr an bürgerliche Orte und Situationen wie einen Konzert- oder Opernbesuch gebunden, sondern lässt sich medial in jeder Lebenssituation verwirklichen.[63] Musikalische Lebenswelten führen zur Bildung vonSubkulturen. Sie ermöglichen es Menschen, ihrekognitivenund emotionalen Entwürfe erfüllend auszuleben. Dies betrifft einerseitsJugendkulturenwiePunk,Hip-HopoderGothic, andererseits zählen dazu auch Kenner und Liebhaber vonAlter Musik, Oper u. a. musikalischen Teilbereichen. Subkulturen stellen kommunikative Netze dar. Ihre Mitglieder verständigen sich, indem sie sich nicht nur primär über einen gemeinsamen Musikgeschmack definieren, sondern darüber hinaus über den Kleidungs- und Lebensstil,Gruppensprachenund gemeinsame musikbegleitete Handlungen wieMusikfestivals. Damit erzeugen und formen siealltagskulturelleSchemata. Indem sie Medien darin einbeziehen, konstruierten sie erneut Bestandteile von Lebenswelten. Musik übernimmt in diesem Regelkreis eine Funktion zurIndividualisierungdes Einzelnen und fordert ihn zur eigenensozialen Integrationauf. Sie ist mittelbar beteiligt an Neukonstruktionen von Vergemeinschaftung und schafft neue Formen der Identitätsbildung. Auch dies ist ein Anzeichen musikalischer Selbstsozialisation, zumal Jugendliche als hoch relevantes Mittel zur Identitätsfindung das aktive Musizieren nutzen, etwa durchautodidaktischesErlernen von Musikinstrumenten oder Gründen einer Band.[64] Die musikalische Vorliebe für einzelne musikalische Richtungen hängt von vielen Faktoren ab wie Alter, Geschlecht undSozialisation.[65]Die musikalische Sozialisation meint die Ausbildung von Werten, Normen und Regeln in Bezug auf Musik und die Ausbildung musikalischer Kompetenz. Musikalische Sozialisation ist wahlfreies Handeln. Zwar können bestimmte gesellschaftliche Institutionen wie Familie oder Kirche sie fördern, es wird jedoch nur in wenigen Fällen negativsanktioniert(z. B. bei Konsum und Verbreitung rassistischer oder politisch radikaler Musik).Musikpädagogikkann die Sozialisation beeinflussen, wenn der Träger ein Interesse daran zeigt und dazu in der Lage ist. In Ländern, in denen die Schulmusik eine schwache Stellung hat oder nicht existiert, ist dies schwieriger möglich oder sogar unmöglich. Eine normative Sozialisation wird durch die öffentliche Förderung von klassischer Musik und Oper – dem bürgerlichenBildungskanon– gestützt. Wo diese anRegel-,Musik-undHochschuleneine Vorrangstellung im Lehrplan besitzt, lenkt sie das Interesse der Sozialisanden auf sich.[66] Schule vermittelt zentrale Werte und Prinzipien, zu denen dieästhetische Bildunggehört. Während das gemeinsame Singen, das dieJugendmusikbewegungin den 1920er Jahren in die niederen Schultypen hineintrug, noch bis in die 1950er Jahre der hauptsächliche Berührungspunkt mit Musik gewesen war, wuchs den Schulen allgemein mit der Ausbreitung der elektronischen Medien eine starke Konkurrenz um den Zugang zur Musik. In der Regel haben Kinder bereits im Vorschulalter hinreichende Erfahrung mit Musik gesammelt, und die Schule ist nicht mehr die ersteEnkulturationsinstanz. Eine wichtige soziale Funktion von Schule besteht indessen darin, gleichaltrige Jugendliche zusammenzubringen, so dass sich Gruppen bilden. Die Mehrzahl der Jugendlichen ist in Gruppen eingebunden. Da Musikhören wenigstens als Nebenbeschäftigung zu den häufigsten Freizeitinhalten bei Jugendlichen gehört, spielt die Orientierung nach dem wahrgenommenen Musikgeschmack anderer eine Rolle beim Anbahnen sozialer Kontakte. Jugendliche nehmen Musikgeschmack als Persönlichkeitsmerkmal wahr. Innerhalb der Gruppen wird Musik häufig thematisiert. Es finden gegenseitige Anpassungen des Musikgeschmacks statt, umkognitive Dissonanzenin Bezug auf die Gruppe zu vermeiden. So entsteht im Gegenzug kollektiv geteiltes Wissen, das den Zusammenhalt der Gruppe stärkt. Analog dazu werden Abneigungen gegenüber anderen Musikstilen, d. h. gegenüber dem Musikgeschmack distanzierter Gruppen, ähnlich homogen betrachtet.[67] Die audiovisuellen Medien sind in der zweiten Hälfte des 20. Jahrhunderts zum einflussreichen Enkulturationsfaktor geworden. Dieser Wandel betrifft nicht nur die Art der Musik, die gehört wird, er betrifft vor allem die Umstände der Mediennutzung. Einerseits wird Musik weiterhin nebenbei gehört, z. B. bei den Hausaufgaben, andererseits erlauben mobile Abspielgeräte (Kofferradio,Walkman,MP3-Player) Musikkonsum an jedem beliebigen Ort. Es zeigt sich außerdem, dass vor allem Jugendliche Musik nicht in der Gruppe oder als Publikum, sondern häufig alleine hören. Dies spricht auch für einen Umgang mit Musik im Sinne einer Selbstsozialisation. Der Einflussgrad der Medien besteht in der immer leichteren Verfügbarkeit von Musik. Da Musikhören lustbetont ist, ist Mediennutzung effizient und rational. Ein Sozialisationseffekt besteht darin, dass der einzelne Mediennutzer das Angebot nicht bestimmen oder überblicken kann und fremdbestimmte Wissensstrukturen über Musik aufbaut. Ein weiterer Grund liegt insubliminalenReizen, die der Nutzer nicht oder nicht vollkommen kontrollieren kann. Dies sind vorwiegendneurophysiologischbedeutsame Effekte wieBahnungoderLernen am Modell, die eineKonditionierungdes Hörers erzeugen. Da sie auf der tiefsten Stufe der neuronalen Informationsverarbeitung ansetzen, die sich dem Bewusstsein entzieht, sind sie entsprechend wirksam und lassen sich nicht durch Aufklärung verhindern.[68] Die Musikpädagogik ist eine eng mit anderenpädagogischenBereichen verbundene Wissenschaftsdisziplin, die die theoretischen und praktischen Aspekte vonBildung,ErziehungundSozialisationin Bezug auf Musik umfasst. Sie greift einerseits die Erkenntnisse und Methoden derAllgemeinen Pädagogik, derJugendforschungundEntwicklungspsychologiesowie mehrerermusikwissenschaftlicherTeilbereiche auf, andererseits die Praxis des Musizierens und der Musikübung. Ihre Ziele sind die musikalischeAkkulturationund der reflektierende Umgang mit Musik im Sinne einerästhetischen Bildung. Die Grundgedanken der Schulmusik gingen von derJugendmusikbewegungaus, deren stärkster FördererFritz Jödevor allem das gemeinsame Singen vonVolksliedsätzenpropagierte und das praktische Musizieren vor die Musikbetrachtung stellte. Dies stieß nach 1950 u. a. beiTheodor W. Adornoauf Kritik, der die gesellschaftliche Bedingtheit des Kunstwerks und dessen kritische Funktion nicht ausreichend berücksichtigt sah. „Der Zweck musikalischer Pädagogik ist es, die Fähigkeiten der Schüler derart zu steigern, daß sie die Sprache der Musik und bedeutende Werke verstehen lernen; daß sie solche Werke so weit darstellen können, wie es fürs Verständnis notwendig ist; sie dahin zu bringen, Qualitäten und Niveaus zu unterscheiden und, kraft der Genauigkeit der sinnlichen Anschauung, das Geistige wahrzunehmen, das den Gehalt eines jeden Kunstwerks ausmacht. Nur durch diesen Prozeß, die Erfahrung der Werke hindurch, nicht durch ein sich selbst genügendes, gleichsam blindes Musizieren vermag Musikpädagogik ihre Funktion zu erfüllen.“ Die Kritik erzeugte in ihrer Folge eine Öffnung des Musikunterrichts für viele Richtungen, u. a. für dieIdeen der Aufklärungin den Schriften des PädagogenHartmut von Hentig. Vorstellungen wieKreativitätundChancengleichheitwurden ebenso wichtig wie eine kritische Wahrnehmungserziehung zum emanzipierten Verhalten in einer akustisch zunehmend überladenen Umwelt. Auch Popularmusik und Musik außerhalb des europäischen Kulturkreises spielen seitdem eine Rolle.[70]Neue Strömungen außerhalb der Schulpädagogik stellen musikalischeErwachsenenpädagogikundMusikgeragogikfür alte Menschen dar. Studien belegen, dass Musik Empathie sowie soziales und kulturelles Verständnis unter Zuhörern fördern kann.[71]Zudem vergrößern musikalische Rituale in Familien und Peer-Gruppen dort densozialen Zusammenhaltunter den Teilnehmern.[72]Interesse und Musikgeschmack sind Teile der Identität und sorgen somit gleichzeitig für ein Gemeinschaftsgefühl oder eine Abgrenzung gegenüber anderen Menschen. Somit erfüllt Musik unterschiedliche Funktionen im Aufbau zwischenmenschlicher Beziehungen.[73]Zu den Wegen, auf denen Musik diese Wirkung entfaltet, zählen nach derzeitigen Erkenntnissen auch die Gleichzeitigkeit (Synchronizität) der Reaktion auf Musik sowie Nachahmungseffekte.[74]Schon kleine Kinder entwickeln mehr prosoziales Verhalten gegenüber Menschen, die sich synchron mit ihnen bewegen.[75][76]Musik erfüllt für Kinder und Jugendliche eine weitere wichtige Sozialisationsfunktion. In vielen Situationen kann durch das Hören von Musik das eigene Empfinden aufgearbeitet und somit der Umgang mit der Realität erleichtert werden.[73] In derVerhaltenstherapiegibt es unterschiedliche Verfahren, in denen Musik als Intervention eingesetzt wird. Im Skilltraining beispielsweise nimmt Musik eine wichtige Rolle ein. Sie kann dabei als ablenkender Faktor fungieren oder durch die Fokussierung auf den auditiven Sinn als Beruhigung wahrgenommen werden. Bei der Behandlung eines Tinnitus wird Musik eingesetzt, um Ablenkung und Entspannungs- sowie Einschlaftechniken zu trainieren und damit die Symptome des Tinnitus in den Hintergrund zu rücken. Häufig kommt Musik beim Entspannungs- und Achtsamkeitstraining zur Anwendung.[77] Die Psychoanalyse nutzt Musik bewusst, um Interaktionen zwischen Patienten und Therapeuten herzustellen. Ein wichtiger Moment ist die Stille in der Pause, die nach einem Musikstück eintritt. In dieser kann das Davor nachwirken und das Danach Form annehmen. Diese Pause wird auch in der Psychoanalyse gezielt eingesetzt. Gemeinsames musikalisches Improvisieren führt zu einem Reagieren aufeinander und kann dabei helfen, Emotionen und Impulse in der Therapie in Worte zu fassen.[78] Die Musiktherapie setzt Musik im Rahmen einer therapeutischen Beziehung gezielt ein, um die seelische, körperliche und geistigeGesundheitwiederherzustellen, zu erhalten und zu fördern. Sie steht in enger Beziehung mit derMedizin, denGesellschaftswissenschaften, derPädagogik,PsychologieundMusikwissenschaft. Die Methoden folgen den unterschiedlichen (psycho)therapeutischen Richtungen wie dentiefenpsychologischen,verhaltenstherapeutischen,systemischen,anthroposophischenundganzheitlich-humanistischenAnsätzen.[79] Die Nutzung von Musik zu therapeutischen Zwecken unterliegt historisch den sich wandelnden Vorstellungen des Musikbegriffs wie der jeweiligen Vorstellungen von Gesundheit, Krankheit und Heilung.[80]Bereits bei denNaturvölkernkam derUnheil abwehrendenundmagischenKraft von Musik eine große Bedeutung zu. Sie ist imTanacherwähnt bei der HeilungSaulsdurchDavidsSpiel auf demKinnor(1 Sam16,14 ff.LUT) und in dergriechischen AntikealsKathartik, d. h. Reinigung der Seele.[81]Unterschieden werden kann zwischen der Annahme von magisch-mythischen, biologischen und psychologisch-kulturgebundenen Wirkungsmechanismen. Zu berücksichtigen ist jeweils, dass Methoden nicht ohne Weiteres außerhalb ihres jeweiligen kulturhistorischen Kontextes anwendbar sind.[82] Die heutigen Anwendungsbereiche der Musiktherapie liegen teilweise im klinischen Bereich, wiePsychiatrie,Psychosomatik,Neurologie,Neonatologie,Onkologie,Suchtbehandlungund in den verschiedenen Bereichen derRehabilitation. Arbeitsfelder finden sich aber auch in nicht-klinischen Bereichen wieHeilpädagogik, inSchulen,Musikschulen, in Einrichtungen der Alten- und Behindertenhilfe und derfrühkindlichen Förderung. Musiktherapie findet in allen Altersgruppen statt.[83] Den Beruf des Musiktherapeuten auszuüben erfordert den Abschluss in einem anerkannten Studiengang, der in zahlreichen Ländern an staatlichenHochschulengelehrt wird.[81] Die Rolle der Frau in der Musikgeschichte wie in der zeitgenössischen Musik ist Gegenstand umfangreicher Forschungen. Insbesondere durch dieZweite Frauenbewegungder 1970er Jahre[84]rückten die Beiträge von Frauen zur Musikkultur in den Fokus des Interesses. Dabei zeigte sich, dass vieles in der Vergangenheit „auf seltsame Weise verschüttet worden“ war.[85] Die weltweit bedeutendste Forschungsstätte zu diesem Themenkreis – dasArchiv Frau und Musik– befindet sich in Frankfurt am Main. Es wurde 1979 von der DirigentinElke Mascha Blankenburg(1943–2013) gegründet. Seit demHochmittelalterhat sich das musikalische Leben in sozialer wie in technischer Hinsicht vielfach gewandelt. Die Eckpunkte dieser Entwicklung sind der Übergang vomFeudalismuszurStändegesellschaft, der Aufstieg desBürgertums, schließlich das Entstehen derMassengesellschaft. In diesem Wechsel änderte sich nicht nur die Musik, sondern auch das, was als Musik angesehen wurde, und letztlich auch ihr Wesen und ihre Funktion. Die Einführung von Medien stellt den Stand der technischen Grundausstattung der Gesellschaft dar, die Musik herstellt, verbreitet und wahrnimmt. Dies sind im Wesentlichen fünf Schritte:Schrift,Buchdruck,Schallaufzeichnung,elektronischeunddigitale Medien. Sie sind jeweils dieKommunikationskanäle, unabhängig von den Inhalten und ihren sozialen Bedingungen. Erst gemeinsam mit denCodesentfalten sie produktive Kräfte. Zwischen Codes und Medien besteht eindialektischesVerhältnis. Bedienen sich die Codes bestimmter Materialien wie Papier oder elektrischer Energie, so formen sie diese in mitteilbare Information um. Zugleich müssen sich die Codes aber dem Medium anpassen. Damit sind Codes ebenso entscheidend wie die Trägermedien dafür verantwortlich, wie sich Kunst entwickelt. Diese Entwicklung ist das Ergebnis gesellschaftlicher Entscheidungen – wirtschaftlicher, politischer, rechtlicher,ideologischerund ästhetischer – für den Einsatz von Medien und Codes.[86] Die Grundlagen für die europäische Musikkultur bestanden bereits seit demSpätmittelalter. Der Wandlungsprozess setzte ein bis zur technischen Entwicklung desNotendrucksund nutzte die sich entfaltendenNotationsmöglichkeitenbis zurMensuralnotation. Diese Mittel verbesserten Produktion und Verbreitung von Musik bedeutend und erlaubten eine praktische Kontrolle; die aufkommendekontrapunktischeVerregelung der Musik, die Qualifizierung von Zusammenklängen in einem System ausKonsonanzenundDissonanzenund die Stimmführung in der sich entwickelndenPolyphonieließen sich durch eine einheitliche grafische Ordnung leichter bestimmen und prüfen. Die allgemeinen Folgen der Differenzierung waren die Rollenverteilung in die BereicheKomposition, Interpretation und Distribution. DerKomponistals autonomer Schöpfer des musikalischen Kunstwerks trat als Individuum aus der mittelalterlichen Anonymisierung heraus, während die immer komplexeren musikalischen Gebilde einer technisch angemessenen Interpretation durch denMusikerbedurften. Die Nachfrage nach Musik bestand bei Kirche undFürstenhöfenund förderte die Entstehung neuer Werke. Das Bürgertum als politisch und wirtschaftlich bestimmende Schicht schloss sich dem an. Waren im Mittelalter, z. B. beiMinnesängernundTrobadors, Komponisten immer auch Interpreten ihrer eigenen Werke und traten in einer bestimmten Funktion auf, so entstand ab derNeuzeitein arbeitsteiligerMarktmit differenzierten Berufsbildern: Komponist, Sänger, Instrumentalist, Verleger und Händler. Zunächst blieben Komponisten und Interpreten an diePatronagevon Kirche und Höfen gebunden, die Musik forderte, aber auch Musik förderte und in einMäzenatentummündete. Der Komponist stieg vom Dienstleister zumPrestigeträgerdes Fürsten auf, der ihm nicht selten durch privaten Musikunterricht verbunden war. Die Kirche bot zahlreichen Musikern Anstellung und akzeptierte ihrezunftmäßigeOrganisation in den Städten. In der bürgerlichen Gesellschaft und im beginnendenKapitalismusendlich wird der Komponist ein Subjekt auf dem freienMarkt. Er bietet seine Werke, die er ohne einen Auftraggeber schafft, einem anonymen Publikum aus Musikinteressierten an. Als vermittelnde Instanzen treten Verlage und Handel dazwischen, die die Grundlage eines neuen Industriezweigs bilden. Auf der Gegenseite förderte diese Herausbildung abstrakter Marktverhältnisse auch das „Künstlertum“, d. h. die soziale Rolle des Komponisten als einer nicht mehr in die GesellschaftintegriertenPerson, die mehr und mehr Außenseiter wird. Die parallele Entwicklung im Bereich der Unterhaltungsmusik setzte dagegen erst im späten 18. Jahrhundert ein. Bis dahin waren Volks- und Unterhaltungsmusiker sozial schlecht gestellte städtischeSpielleuteoder Spezialisten innerhalb derDorfgesellschaft. Es gab hier nach wie vor keine Arbeitsteiligkeit, nur mündliche Überlieferung der Musik und eine wenig differenzierte Funktion des Musikmachens: Volksmusiker waren in den Alltag und die Abläufe desKirchenjahrseingebunden, übernahmen aber auch die Rolle des Informationsübermittlers, etwa durch denMoritaten- undBänkelsang. Mit derIndustrialisierungkam auch in der Unterhaltungsmusik die Nachfrage nach „professioneller“ Musik. Die spezifisch bürgerlicheSalonmusikentstand ebenfalls im 19. Jahrhundert. Sie bestand größtenteils aus leichtenArrangementsvon Kunstmusik für die wohlhabenden Haushalte. Vor allem für dasKlavierund kleineHausmusikensembleswurden leicht spielbare und effektvolle Stücke hergestellt. Sie dienten als Spielmaterial für den Musikunterricht.Carl CzernysSchule der Geläufigkeitund andere Übungsmusik bildeten die Ausrüstung für den bürgerlichen Musiklehrer, der als neuer Beruf erschien. Besonders imDeutschen KaiserreichtratenGesangvereine, als Neuerung auchMännerchörehervor. Ein gedrucktes und überregional verbreitetes Repertoire an Chorwerken stiftete soziale Identität, die Sängerfeste desDeutschen Sängerbundeswurden zum Ausdruck der nationalkulturellen Identitätsbildung. Einfachere Reisemöglichkeiten, Eisenbahn und Dampfschifffahrt, begünstigten zudem diese Massenveranstaltungen. DasVirtuosentumimKonzertsaalbelebte den Musikmarkt und schuf die ersten international bekannten Stars wieFranz LisztundNiccolò Paganini. DasUrheberrecht, das bis zum Ende des 19. Jahrhunderts das expandierende Verlagswesen und Konzertleben der europäischen Länder regelte, stärkte die Komponisten. Ihre Einkommensverhältnisse besserten sich nachhaltig und sie bekamen erstmals die Kontrolle über ihre Werke und die Abgeltung aus deren Nutzung. Damit wurde der Komponistenberuf auch juristisch anerkannt und der Schwerpunkt der schöpferischen Produktion lag nun endgültig in der Hand der Künstler selbst.[87] Die Erfindung des Notendrucks half zwar, Musik schneller und weiter zu verbreiten, doch waren dieAuflagenbis zum Ende des 18. Jahrhunderts immer noch recht gering. Der Notendruck war eher ein Mittel zur Dokumentation und zur medialen Vermittlung des Musiktextes an den Interpreten. Eine breite Käuferschicht fanden Noten noch nicht. Allerdings unterstützen frei verfügbare Noten die entstehende öffentliche Musikkultur, namentlich im italienischenOpernwesenum die Mitte des 17. Jahrhunderts und im englischen Konzertwesen seit dem frühen 18. Jahrhundert. Das bürgerliche Musikleben begann sich unter deren Einflüssen sozial und wirtschaftlich zu entfalten. Zunächst hielten Gasthäuser und öffentliche Säle als Aufführungsorte her, später wurden die ersten Konzerthäuser als spezialisierte Spielstätten für die Konzertmusik errichtet – unter Mitwirkung der städtischen Verwaltungshoheit. Die Erfindung derLithografieim Jahr 1796 führte unmittelbar zu qualitativ besseren Druckergebnissen und größeren Auflagen im Notendruck. Noten erwiesen sich als profitabel und ein breiter Markt wuchs heran. Damit rückte auch derWarencharaktervon Musik in den Vordergrund. In der Mitte des 19. Jahrhunderts überstiegen die Aufführungen von Musik bereits verstorbener Komponisten erstmals diejenigen von noch lebenden. Dies ist auch ein Anzeichen für die Herausbildung eines spezifisch bürgerlichenKanonsund der ihm zugrunde liegenden wertenden Ästhetik. Im häuslichen Bereich kam dem Klavier eine besondere Rolle zu. Die wichtigsten Merkmale der europäischen Kunstmusik – Mehrstimmigkeit, Kontrapunkt,Modulationsharmonik,wohltemperierte StimmungdeschromatischenTonsystems – lassen sich auf diesem Instrument klanglich angenehm, technisch einfach und verhältnismäßig preiswert erlernen und reproduzieren. Beide, Klavier und Notendruck, wurden in der bürgerlichen Schicht zu Trägern der Musikkultur. Symptomatisch ist das 1856 vonTekla BądarzewskakomponierteGebet einer Jungfrau. Es gehört zu einer industriell gefertigten, standardisierten Musik, die später als Inbegriff für musikalischenKitschgalt. Noch offensichtlicher war die Marktentfaltung in der Popularmusik. Schon gegen Ende des 18. Jahrhunderts nahmen in den Großstädten die großen Unterhaltungsstätten für dieMittel- und Unterschicht ihren Betrieb auf:Music Hallsin England, Cafés in Frankreich, in DeutschlandRevue-Theater undBallsäle, in den USA dieMinstrel Showsund aus Frankreich übernommenenVaudevilles. Die dort gespielte Musik diente zum Tanzen und Mitsingen, nicht aber dem Kunstgenuss. Die bürgerliche Musikkultur hatte im Großen die Inhalte ihrer kirchlichen und aristokratischen Vorläufer übernommen. Sie unterschied sich aber in ihrem ideologischen Bezug. Während für die Fürsten Musik eine angenehme Zerstreuung war und die Kirche sie religiös funktionalisierte, so suchte das Bürgertum in ihrBildungundErbauungund nutzte sie zur Repräsentation. Als Mäzen trat nun der Bürger auf, vermehrt auch die öffentliche Hand. Staaten, Kommunen und Privatvereine finanzierten Bau und Unterhaltung von Opern- und Konzerthäusern. Ihr Ziel war ein öffentliches Musikleben, das dem Bürgertum selbst zu Bildung und Erziehung gereichen könne. Hier begann dieKulturpolitik. Was als kulturell wertvoll im Dienst der Allgemeinheit erachtet wurde, erhielt auch abseits von wirtschaftlichen Überlegungen Förderung. Die Werturteile fällte eine zunehmend abgeschlossene Teilkultur – Universitäten, Intellektuelle, Künstler und Kritiker – mit der Tendenz zurSelbstreferenzialität, so dass die gesellschaftliche Desintegration der Kunst weiter vorangetrieben wurde. Die Formen der bürgerlich-wertorientierten Kultur übernahmen schließlich auch die aristokratischen Herrscher. Als Gegengewicht zur bürgerlichen Sphäre wuchs die Unterhaltungsindustrie an, dieBoulevardtheaterunterhielt und als neues Berufsbild den professionellenEntertainerschuf.[88] Diese Entwicklungen hatten Folgen für die Kunstrezeption. War bis zum Beginn der Neuzeit Musik entweder funktional, z. B. mit Arbeit oder Gottesdienst verbunden, oder hatte sie als Tanz- und VolksmusikGemeinschaftgestiftet, war also der Umgang mit und der Gebrauch von Musik die herkömmliche Herangehensweise, so wandelte sich dies zurDarbietungvon Musik. Sie wurde als Kunstwerk und um ihrer selbst willen aufgenommen. Sie musste sich nicht mehr das Interesse mit anderen alltäglichen Dingen wie Arbeit oder Schlaf teilen, sondern wurde mit großer Aufmerksamkeit gehört. Diese strukturelle Hörweise, die das musikalische Kunstwerk in seiner Form und seinem Gehalt nachzuvollziehen sucht, setzt bereits theoretisches Vorwissen voraus. Damit einher ging die Unterscheidung von Kunst-Musik und „Nicht“-Kunst-Musik, d. h. die Übertragung der Werturteile auf musikalische Genres. Die Unterscheidungsfähigkeit wurde zu einem wichtigen sozialen Merkmal stilisiert; wer den bürgerlichen Kanon nicht nachvollzog, galt als ungebildet. Anders die Unterhaltungsmusik: weder empfand man sie als Kunst noch wurde sie konzentriert gehört. Man hörte sie auch, während man mit Gespräch, Essen, Tanz beschäftigt war, als Konsum einer Dienstleistung.[89] Am Ende des 19. Jahrhunderts setzte eine Wandlung der westlichen Gesellschaft ein, die die Erfindung der Schallaufzeichnung begleitete. Es entstandenMassenmedien, wieGrammophonundSchellackplatte,RundfunkundTonfilm. Mit der Erfindung desKondensatormikrofonssetzten die elektronischen Medien ein. Musik wurde nun bis zum Zweiten Weltkrieg vorwiegend medial vermittelt, durch Radio und Musikfilm. Nach dem Zweiten Weltkrieg begann sich mit dem wachsenden Wohlstand breiterer Gesellschaftsschichten dieKonsumgesellschaftzu bilden. Die Nachfrage nach elektrischem Zubehör wieTonbandgerätoderTransistorradiostieg, das Fernsehen begann sich auszubreiten. Mit der elektrischen Industrialisierung entwickelte sich erstmals eine industrielle Produktion von Musik im eigentlichen Sinne. Sämtliche Prozesse waren nun arbeitsteilig angelegt, wie es im Bereich der Unterhaltung schon zuvor gewesen war. Das bürgerliche Musikverständnis in Bezug auf Kunst hatte sich seit dem 19. Jahrhundert nicht wesentlich gewandelt. Die Musikindustrie überformte es. Sie übernahm überwiegend dieÖkonomisierungdes vergangenen Jahrhunderts, die Musikschaffenden wurden zu Arbeitnehmern einer sich konzentrierenden Industrie, die von Tonträgerherstellern und Rundfunkanstalten dominiert wurde. Diese beiden entschieden nun, welche Musik gehört und rezipiert wurde. Nach und nach kehrte sich die Erwartungshaltung des Publikums um. Die breitere Medienverfügbarkeit des Angebots, das auch in Einzelsektoren des „klassischen“ Tonträgermarktes nicht mehr zu überschauen war, führte zu einer unkonzentrierten und beschleunigten Rezeption. Dies wirkte sich aus auf die Unterhaltungsmusik, aufJazz,Pop,Rockmit ihren zahlreichen eigenständigen Strömungen wiePunk,Metal,Techno,Hip-Hop,Country,Bluesusw., die in der Spannung zwischen weiterer Standardisierung auf der einen Seite, wachsendem Innovationsdruck auf der anderen Seite entstanden.[90] Überschneidungen von Musik und Sprache sind in Teilbereichen zu finden; beide besitzen Struktur undRhetorik. Auch in der Musik gibt es eineSyntax,[91]die sich von der Syntax der Sprache jedoch erheblich unterscheidet. Semantik kommt der Musik in der Regel nur durch zusätzliche sprachliche Elemente zu oder kann durch Verschlüsselung innerhalb ihrer Schriftlichkeit entstehen. Letzteres ist aber nicht unbedingt hörbar. Musik ist daher keine Sprache, sondern nur sprachähnlich.[92] Ein Hauptunterschied besteht darin, was mit Musik ausgedrückt wird. Musik kann keineDenotatemitteilen. Sprache ist sie nur immetaphorischenSinn, sie teilt keinBezeichnetesmit. Vielmehr ist sie einSpielmit Tönen (und Tonreihen).[93] Auch einesyntaktischeOrdnung, die semantisch getragen würde, ist in der Musik nicht gegeben. Anders als in der Sprache gibt in der Musik wederlogischeVerknüpfungen noch wahre oder falscheAussagen.[91] Die Zeichensysteme von Sprache und Musik sind demnach grundlegend verschieden. Sprachesagt, Musikzeigt,[94]da sie Sinneseindrücke zuVorstellungenverarbeitet, die sie wiederum dem sinnlichen Erleben vorstellt.[95]Während die Sprache, u. a. mit Hilfe von Definitionen, aufEindeutigkeitzielt, verfolgen die Künste das entgegengesetzte Ziel: nicht die dinglichen Bedeutungen, sondern die potenziellen menschlichen Werte sind das semantische Feld der Kunst, das sich auf alle möglichenKonnotationenerstreckt.[96] Musik wird vielfach als „Sprache der Gefühle“ verstanden. Sie ist in der Lage, Emotionen,AffekteundMotivationszuständezu schildern und durch Ausdrucksmuster dem Hörer zugänglich zu machen. Allerdings sind auch diese keine sprachähnlichen Zeichen, da sie entsprechend ihrerpsychophysiologischenGrundlagen letztlich als Kontinuum in einem „emotionalen Raum“ erscheinen, d. h. nicht nur als voneinander unterschiedene Gefühlsqualitäten, sondern in Wechselwirkungen undambivalentenZuständen und Verläufen auftreten. Der Gestus ihres Ausdrucks ist keine ausdehnungslose logische Struktur – wie sie im Begriffspaar von Bezeichnendem und Bezeichnetem vorliegt –, sondern zeitlicher Natur. Er kann in sich gegliedert sein, in zeitlicher Hinsicht, aber auch durch sich überlagernde Emotionen, z. B. im Gefühlskontinuum „Freude + Trauer → Wut“. Ein Aufschwung kann bereits Trauer in sich haben oder umgekehrt. MehrereVertonungendesselben Textes werden als unterschiedlich angemessen empfunden, wie man auch umgekehrt derselben Musik mehrere Texte unterlegen kann, die jeweils mehr oder weniger passend erscheinen.[97] Eine andere Sicht auf das Verhältnis von Musik und Sprache ergibt sich aus derGenerative Theory of Tonal Music, entwickelt von dem Komponisten Fred Lerdahl und dem LinguistenRay Jackendoff.[98]Danach lässt sich zumindest die westliche Musik als ein Ensemble vier verschiedenartiger Strukturen beschreiben, die eine starke Verwandtschaft zu den Strukturen der Sprache besitzen. Auch die Beschreibung dieser Strukturen geschieht nach ähnlichen Prinzipien, nämlich mit Regeln und Wohlgeformtheitsprinzipien, wie sie in derGenerativen Grammatikauch für die Sprache angesetzt werden. MusikalischeSymbolikliegt vor, wenn Elemente der Musik mit einer außermusikalischen Bedeutung verbunden sind, auf die sie zeichenhaft verweisen. Der Hörer kann musikalische Symbole zum Teil intuitiv verstehen, während er für Symbole, die aufKonventionberuhen, ein Vorwissen um den Symbolgehalt braucht. Zum Teil ist die Symbolik von den Hörern gar nicht erkennbar. Verschiedenste Elemente der Musik können als Symbol verwendet werden: Ein musikalisches Symbol kann je nach musikalischem Kontext in verschiedener Weise zeichenhaft sein. So kann etwa das Motiv desKuckucksrufs– eine fallendeTerzoderQuarte– ein akustisches Bild für einen Kuckuck sein (im VolksliedKuckuck, Kuckuck, ruft’s aus dem Wald), der Kuckucksruf kann aber auch allgemeiner für das Naturerleben stehen (am Schluss derSzene am BachinBeethovens 6. Sinfonie) oder die Natur symbolisieren (am Anfang des Kopfsatzes vonMahlers 1. Sinfonie, in dessen Überschrift der Hinweis „Wie ein Naturlaut“ gegeben wird). In derFilmmusikwird das als „Mickey-Mousing“ bekannte Verfahren eingesetzt, das Geschehen im Film durch musikalische Effekte zu unterstreichen, die zeitlich exakt auf die bewegten Bilder abgestimmt sind.[100] Ein musikalischesSignalist Musik, die dem Zweck dient, eine Information zu übermitteln. Typische Beispiele sind Militär- undJagdsignale, zum Beispiel das JagdsignalFuchs tot. Es endet mit demHalali, mit dem das Ende der Jagd signalisiert wird. Tonsignale sind ein Sonderfall im Grenzbereich von Musik und akustischer Kommunikation.Glockengeläutoder das Signal desMartinshornswerden nicht zur Musik gerechnet. Das Klingeln des Telefons war früher nur ein Geräusch; auf heutigen Mobiltelefonen können auch Melodien oder andere Musiksequenzen als „Klingelton“ gewählt werden. Obwohl vordergründig Musik als reine Zeitkunst und transitorisch, d. h. vorübergehend erscheint gegenüber den statisch-dauerhaften RaumkünstenMalerei,Bildhauerei,Zeichnung,GrafikundArchitektur, so ist sie doch von deren räumlichen und nicht-zeitlichen Vorstellungen geprägt und hat sie ebenso mit ihren Anschauungen von Zeitlichkeit und Proportion beeinflusst. Begriffe wie „Tonraum“, „Klangfarbe“ oder „Farbton“, „hohe/tiefe“ Töne und „helle/dunkle“ Klänge und ähnlicheSynästhesieausdrücke, die Doppeldeutigkeit von „Komposition“ im musikalischen Denken und in dem der bildenden Kunst gehören zum allgegenwärtigen Beschreibungsvokabular. Die Erfahrung, dass eineakustischeWirkung wieNachhalloderEchosich erst im Zusammenhang mit dem Raum einstellt, gehört zum Urbesitz des Menschen. Es sind seit den frühesten theoretischen Auffassungen Parallelen zwischen akustischen und räumlich-visuellen Kunstformen benannt worden. Auch hinsichtlich der kunstgeschichtlichen Epochen gibt es Gemeinsamkeiten zwischen der Musik und den visuellen Künsten, etwa im Zusammenhang mit der Zeit desBarock. Da sich keine klare Abgrenzung der Epochen vornehmen lässt, werden jedoch die BegriffeFormenlehre (Musik)undKunststilverwendet. Die Vorstellung von der Verwandtschaft zwischen Musik und Architektur existiert seit derAntike. Sie beruht auf den gemeinsamenmathematischenGrundlagen. DiePythagoreerverstanden dieIntervallproportionenals Ausdruck einerkosmischen Harmonie. Musik war nach ihrer Anschauung eine Erscheinungsweise der Zahlenharmonie, die auch in schwingenden SaitenkonsonanteIntervalle ergibt, wenn deren Längen in einfachen ganzzahligen Verhältnissen stehen. Die Zahlenproportionen galten bis in die früheNeuzeitals Ausdruck von Schönheit, wie auch nur die Künste, die Zahlen, Maße und Proportionen anwenden, in Antike undMittelalterals geeignet galten, Schönes zu schaffen.VitruvsarchitekturtheoretischeSchriftDe architectura libri decemnahm ausdrücklich Bezug auf die antikeMusiktheorie, die er als Verständnisgrundlage für die Architektur bezeichnete. Die mittelalterliche Architektur griff die antiken Ideen in christlicher Sinngebung auf. DieGotikzeigte vielfach Intervallproportionen in den Hauptmaßen der Kirchenbauten. Vorbildhaft war dersalomonische Tempel, dessen Gestalt u. a.Petrus Abaelardusals konsonant ansah. Auch komplexe mathematische Phänomene wie derGoldene Schnittund dieFibonacci-Folgewurden christlich gedeutet. Sie erscheinen gleichermaßen inFilippo BrunelleschisKuppelentwurf vonSanta Maria del Fiorewie auch inGuillaume DufaysMotetteNuper rosarum flores(1436). Das Werk zur Weihe des Doms vonFlorenzweist dieselben Zahlenproportionen in Stimmverlauf und Werkstruktur auf, die die Architektur der Kuppel bestimmt hatte. Der TheoretikerLeon Battista Albertidefinierte in derRenaissanceeine Architekturlehre auf der Grundlage der vitruvianischen Proportionstheorie. Er entwickelte Idealproportionen für Raumgrößen und -höhen, Flächenunterteilungen und Raumhöhen.Andrea PalladiosQuattro libri dell’architettura(1570) systematisierte diese Proportionslehre auf der Basis vonTerzen, die inGioseffo ZarlinosLe istituzioni armoniche(1558) erstmals als konsonante Intervalle anerkannt worden waren. Damit vollzog sich ein bis in die Gegenwart reichender Wandel derHarmonik.[101] Unter dem Einfluss der neuzeitlichenMusikästhetiktrat der musikalische Zahlenbezug in der Architekturtheorie allmählich in den Hintergrund. Das Geschmacksurteil als Kriterium der ästhetischen Beurteilung setzte sich durch. Erst im 20. Jahrhundert gerieten Zahlenproportionen als architektonische und musikalische Parameter erneut in den Rang konstruktiver Elemente. Auf architektonischem Gebiet war diesLe CorbusiersModulor-System. Dessen SchülerIannis Xenakisentwickelte in der KompositionMétastasis(1953/54) die architektonische Idee in derNeuen Musik. Danach setzte er die kompositorische Gestaltung im Entwurf des Philips-Pavillons auf derExpo 58inBrüsselarchitektonisch um.[102] Unter den synthetischen Kunstformen, die nach dem Ende des universell gültigen Harmonieprinzips entstanden, wurdeRichard WagnersKonzept desGesamtkunstwerksbedeutend für das 19. Jahrhundert. Die Architektur nahm darin eine dienende Stellung zur Verwirklichung der musikalischen Idee ein. Sie hatte die praktische Raumumgebung für die Einheit der Künste, d. h. desMusikdramaszu schaffen. Wagner verwirklichte seine Ansprüche im vonOtto BrückwalderbautenFestspielhausinBayreuth. DerExpressionismusnahm im frühen 20. Jahrhundert die Kunstsynthese auf. Die zentrale Vision, den Menschen die sozialen Grenzen überwinden zu lassen, führte zu vielen Kunstentwürfen, von denen einige nie realisiert wurden. Dazu gehörteAlexander Nikolajewitsch Skrjabinskugelförmiger „Tempel“ für dasMysterium(1914), ein aus Wort, Klang, Farbe, Bewegung und Duft zusammengesetztes Offenbarungswerk. Die Kunstströmungen der zweiten Hälfte des 20. Jahrhunderts integrierten musikalische Elemente inmultimedialenFormen, in „Klang-Skulpturen“ und „Ton-Architektur“. Architektur bekam zunehmend eine zeitliche, Musik eine Raumkomponente.Karlheinz Stockhausenverband seine Vorstellungen von Raummusik in einem Kugelauditorium, das er auf derExpo ’70inOsakainstallierte. Die Hörer saßen darin auf einem schalldurchlässigen Boden, umgeben vonelektronischer Musik. Die im Raum verteiltenLautsprechererlaubten es, die Klänge im Raum zu bewegen.[103] DerMarkusdomwar einer der frühen Experimentalräume für Musik. Die Komponisten erforschten die räumliche Wirkung mehrerer Klangkörper und setzten die Ergebnisse in neuen Kompositionen um. DasAlte Leipziger Gewandhausbestand nur aus einem ausgebauten Obergeschoss. Gleichwohl erlebte es von 1781 bis 1884 die aufblühende Orchesterkultur der deutschen Romantik. Die im 16. Jahrhundert entwickelteMehrchörigkeit, die unter den europäischen Musikzentren vor allem amvenezianischenMarkusdomgepflegt wurde, nutzte die Wirkung mehrerer Ensembles im Raum.KammermusikundKirchenmusiktrennten sich nachInstrumentation, Satzregeln und Vortragsweise. Sie passten sich an die Akustik ihrer Aufführungsorte an. Dazu entwickelte die Architektur eigene Raumtypen, die der Musik gewidmet waren und ihrer Aufführung akustisch vorteilhafte Bedingungen schufen. Die erstenKammernentstanden in den fürstlichen Palästen, später in Schlössern und Stadtwohnungen. Damit änderte sich auch das Hörverhalten: Musik wurde um ihrer selbst willen gehört, frei von funktionaler Bindung und zum reinen Kunstgenuss. Das öffentlicheKonzertwesenentstand gegen Ende des 17. Jahrhunderts inLondon. Musikveranstaltungen fanden nun nicht mehr nur in Festsälen, Wirtshäusern oder Kirchen statt, sondern in eigens dafür errichtetenKonzerthäusern. Zwar fassten die Säle dieser Zeit nur einige Hundert Hörer, hatten noch keine feste Bestuhlung und dienten neben der Musik auch allerlei Festanlässen, sie wiesen aber bereits eine erheblich verbesserteRaumakustikauf, in der dieOrchestermusikzur Geltung kam. Vorbildhaft war der erste Bau desLeipzigerGewandhauses(1781). Nach seiner Gestaltung als schmaler und langer Kastensaal mit Bühnenpodium und ebenemParkettentstanden im 19. Jahrhundert viele weitere Säle, die das kulturell interessierteBürgertumals Stätten der Musikpflege nutzte.[104] Insbesondere diesinfonischenWerke derromantischen Musikmit ihrer vergrößerten Orchesterbesetzung profitierten von den Konzerthäusern. Die Akustik dieser Säle verband Klangfülle mitDurchhörbarkeit; die schmale Bauform führte zu starkerReflexiondes Seitenschalls, das im Verhältnis zur Innenfläche große Raumvolumen optimierte dieNachhallzeitauf ein Idealmaß von anderthalb bis zwei Sekunden. Die Größe der Säle – sie fassten nun ungefähr 1.500 Hörer – folgte daraus, dass sichAbonnementskonzerteals Teil des städtischen Kulturlebens etabliert hatten. Die bedeutendsten Konzertstätten dieser Epoche sind derGroße SaaldesWiener Musikvereins(1870), dasNeue Gewandhausin Leipzig (1884) und dasAmsterdamerConcertgebouw(1888). Neue technische Möglichkeiten und die Notwendigkeit, Säle durchgehend wirtschaftlich zu bespielen, veränderten die Architektur in der Moderne.FreitragendeBalkone, künstlerisch gestaltete Säle in asymmetrischer oder Trichterform und ein Fassungsvermögen von bis zu 2.500 Plätzen prägten die Konzerthäuser im 20. Jahrhundert. DiePhilharmonieinBerlinund dieRoyal Festival Hallin London waren zwei bedeutende Vertreter neuer Bautypen. Letztere war der erste Konzertsaal, der nach akustischen Berechnungen erbaut wurde. Seit den 1960er Jahren ist ein Trend zu verzeichnen, Säle mit variabler Akustik zu bauen, die sich für verschiedene Musikarten eignen.[105] Die mannigfachen Beziehungen zwischen Musik und bildender Kunst zogen sich historisch gleichermaßen durch die theoretische Betrachtung beider Künste wie durch die praktische Arbeit, die sich in wechselseitigen Beeinflussungen niederschlug. Zunehmend bezogen bildende Künstler und Komponisten die andere Kunst in ihr Schaffen ein, bildeten projektbezogene Arbeitsgemeinschaften oder schufen gemeinsam multimediale Werke. Etliche Werke der Malerei fanden Eingang in die Musik:Hunnenschlacht (Liszt),Bilder einer Ausstellung,Die Toteninsel. Dieses Bild vonArnold BöcklinregteMax Reger,Sergei Wassiljewitsch Rachmaninowund andere Komponisten zusinfonischen Dichtungenan. Dem stehen Komponistenporträts und unzähligeGenrebilderMusizierender gegenüber, die auch derIkonografieals Forschungsmaterial dienen. Nicht immer sind Parallelentwicklungen festzustellen. Nur Teile der Stilgeschichte fanden eine Entsprechung in der Gegenseite. Kunsthistorische Begriffe wieSymbolismus,ImpressionismusoderJugendstilsind weder klar voneinander abzugrenzen noch ohne Weiteres auf die Musik übertragbar. Wenn beispielsweise ein Vergleich zwischen den BildgestaltenClaude Monetsund der „impressionistischen“ MusikClaude Debussysgezogen und durch das Zerfließen der Form oder der Darstellung der Atmosphäre erklärt wird, so steht dies im Widerspruch zu Debussys Ästhetik. Ebenso sind parallele Erscheinungen wie dieNeue Sachlichkeitnicht eindimensional zu erklären, sondern nur aus ihren jeweiligen Tendenzen; während sie in Kunst und Literatur eine Abgrenzung zumExpressionismuswar, wandte er sich in der Musik gegen dieRomantik.[106] Das antike Künsteverhältnis trennte klar diemetaphysischenAnsprüche von Musik und bildender Kunst. Die Doppelgestalt vonμουσική– einerseits Kunst, andererseits die geistige Beschäftigung mit ihr – wurde als ethisches und erzieherisches Gut geschätzt. Die Malerei allerdings galt als schlecht und nur Schlechtes erzeugend, wiePlatonsPoliteiaausführt, da sie nur einenachahmendeKunst sei. Der Unterschied, der keinesfalls ästhetisch zu verstehen ist, beruhte auf derpythagoreischenLehre, die die Musik als Widerspiegelung der kosmischen Harmonie in Gestalt derIntervallproportionenverstand. So fand die Malerei in denArtes liberalesfolgerichtig keinen Platz. Diese Ansicht hielt sich bis in die Spätantike. Derbyzantinische Bilderstreitstellte die heftigste politisch-religiös motivierte Kunstablehnung dar, die jedoch die Musik nicht berührte: diese fand als überformtes Symbol der göttlichen Weltordnung aus Maß und Zahl Eingang in den christlichen Kult und in dieLiturgie. Das Mittelalter schrieb diese Trennung fest und fügte die bildende Kunst in den Kanon derArtes mechanicae.[107] LeonardosVitruvianischer Menschdefinierte den Proportionsbezug der Künste neu. Nicht mehr die Musik, sondern die Malerei war Leitkunst der Renaissance; nicht mehr diekosmischen, sondern dieKörperproportionenwaren das Bezugssystem. Die Aufwertung der Malerei zur schönen Kunst begann in der Renaissance mit dem Hinweis auf die kreative Leistung der bildenden Künstler. Sie wurde zwar weiterhin unter die Musik gestellt, die in Leon Battista Albertis Kunsttheorie Modell zum Modell für die Architektur wird, sie stand andererseits schon über derPoesie. Eine erste Verwissenschaftlichung der Malerei unternahmLeonardo da Vinci, für den sie die Musik übertraf, da ihre Werke dauerhaft sinnlich erfahrbar sind, während Musik verklingt. Dieser Prozess setzte ein vor dem Hintergrund des weltlichenHumanismus, der der Kunst weder eine staatsphilosophische noch eine religiöse Bedeutung zumaß. DasZeitalter der Aufklärungstellte den Menschen als das betrachtende und empfindende Subjekt endgültig in den Mittelpunkt. Aus dieser Positionierung von autonomer Kunst gegenüber der Wissenschaft entwickelte sich das Kunstverständnis, das bis in die Gegenwart vorherrscht. Die Künste entwickelten in der Folge jeweils eigene ästhetische Theorien.[107] SeitAlexander Gottlieb BaumgartensAesthetica(1750/58) rückt Kunst selbst in die Nähe der Philosophie bzw. wird als eigene philosophische Disziplin betrachtet. Damit verlor die Musik ihre Sonderstellung innerhalb der Künste und wurde in die schönen Künste eingegliedert, die ihre Rangordnung durch eigene Ästhetiken neu bestimmten.Immanuel KantsKritik der Urteilskraft(1790) rechnete sie auch den angenehmen Künsten zu, d. h. sie ist nun als Schöne Kunst der Malerei über-, als angenehme Kunst ihr aber untergeordnet, weil sie mehr Genuss als Kultur bedeutet. Ein grundlegender Umschwung geschah in derromantischenÄsthetik, die eine Verschmelzung der Künste und Kunstideale anstrebte. Sinnfällig wird dies inRobert SchumannsParallelisierung der Kunstanschauungen. „Der gebildete Musiker wird an einer Raphael’schen Madonna mit gleichem Nutzen studiren können wie der Maler an einer Mozart’schen Symphonie. Noch mehr: dem Bildhauer wird jeder Schauspieler zur ruhigen Statue, diesem die Werke jenes zu lebendigen Gestalten; dem Maler wird das Gedicht zum Bild, der Musiker setzt die Gemälde in Töne um.“ Einzigartigkeit stellte die Musik indes inArthur SchopenhauersRückgriff auf die Antike dar; inDie Welt als Wille und Vorstellung(1819) verneint er ihre mimetischen Eigenschaften.[109] Im späten 19. Jahrhundert folgte die Begründung vonKunstgeschichteundMusikwissenschaftan dengeisteswissenschaftlichenFakultäten. Die Musik war damit auch in ihrer wissenschaftlichen Betrachtung von bildender Kunst und Architektur geschieden. Die symbolistische und impressionistische Malerei, Musik und Literatur und die beginnendeAbstraktionveränderten das Verhältnis durch zunehmende Reflexion der Künstler über die benachbarten Künste, das auch Aspekte des eigenen Schaffens einbezog. Ein romantisches Musikverständnis prägtePaul Gauguin: „Denken Sie auch an den musikalischen Part, der künftig in der modernen Malerei die Rolle der Farbe einnehmen wird. Die Farbe ist genauso Schwingung, wie die Musik zu erreichen in der Lage ist, was das Allgemeinste und dabei doch am wenigsten Klare in der Natur ist: ihre innere Kraft.“ Henri Matissebeschrieb seinen Schaffensprozess als musikalisch. Mit der SchriftÜber das Geistige in der Kunst(1912) lösteWassily Kandinskydie Goethe’sche Kritik ein, in der Malerei fehle „schon längst die Kenntnis desGeneralbasses[…] [und] an einer aufgestellten, approbierten Theorie, wie es in der Musik der Fall ist“.[111]Er deutete sie als prophetische Äußerung, die eine Verwandtschaft der Künste ankündige, besonders der Musik und der Malerei. Dies ist gemeint als Rückbezug auf das kosmologische Prinzip, wie es den Pythagoreern zur metaphysischen Begründung der Musik erschien, und zugleich als seine Ausweitung auf die bildende Kunst.Paul Kleeschloss sich in seinenBauhaus-Vorlesungen dahin gehend an, dass er die musikalischeTerminologiezur Erklärung der bildenden Kunst verwandte. Später entwarf er eineKunstpoetikauf der Basis musiktheoretischer Fragen. Das Verhältnis von Musik und bildender Kunst nach 1945 wuchs aus ästhetischen Theorieansätzen. Im Vordergrund stand eine systematische Klassifizierung der beiden Künste.Theodor W. Adornotrennte sie aus seiner Sicht notwendigerweise in Musik als Zeitkunst und Malerei als Raumkunst. Grenzüberschreitungen sah er als negative Tendenz. „Sobald die eine Kunst die andere nachahmt, entfernt sie sich von ihr, indem sie den Zwang des eigenen Materials verleugnet, und verkommt zum Synkretismus in der vagen Vorstellung eines undialektischen Kontinuums von Künsten überhaupt.“ Er erkannte an, dass die KünsteZeichensystemeseien und von gleichem Gehalt sind, dessen Vermittlung sie Kunst sein lasse. Die Unterschiede hielt er allerdings für bedeutender als die Gemeinsamkeiten. FürNelson Goodmanstellten sich die Probleme der Kunstdifferenzierung alserkenntnistheoretisch, so dass er an Stelle einer ästhetischen generell eineSymboltheoriesetzen wollte. Als Grenze des Ästhetischen zum Nicht-Ästhetischen betrachtete er den Unterschied vonExemplifikationundDenotation: während die bildende Kunst autographisch sei, da ihre Werke nach dem Schaffensprozesssind(worin sich auch Original undFälschungunterscheiden) ist die zweiphasige Musik allographisch, denn ihrenotiertenWerke erfordern erst eine Aufführung – wobei diese Unterscheidung sich nur auf Kunst erstreckt.[113] Wörterbücher und Terminologie Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie 2Begriffsgeschichte 2.1Definitionsgeschichte 2.1.1Antike 2.1.2Mittelalter 2.1.3Frühe Neuzeit 2.1.418. und 19. Jahrhundert 2.1.5Ab dem 20. Jahrhundert 2.2Historische Klassifikationen des Musikbegriffs 2.2.1Antike 2.2.2Mittelalter und frühe Neuzeit 2.2.3Ab dem 16. Jahrhundert 2.2.4Lexikografie und Terminologie 3Musikgeschichte 3.1Prähistorische Musik 3.2Schriftlose Kulturen 3.3Frühe Hochkulturen und Antike 3.4Westliche klassische Musik bis zum 19. Jahrhundert 3.520. und 21. Jahrhundert 4Erzeugung 4.1Akustisches Material 4.2Verarbeitung des Materials 4.3Musikalisches Werk 4.4Klangliche Erzeugung 5Medien, Technik und Wirtschaft 5.1Notation 5.2Notendruck 5.3Reproduktionstechnik 5.4Audiovisuelle Medien 5.5Internet 5.6Musikindustrie"
  },
  {
    "label": 0,
    "text": "New Orleans – Wikipedia New Orleans Inhaltsverzeichnis Geografie Demographie Geschichte Hurrikane Kultur Wirtschaft und Infrastruktur Persönlichkeiten Filme und Serien, deren Schauplatz New Orleans ist (Auswahl) Städtepartnerschaften Literatur Weblinks Einzelnachweise Klima Kolonialzeit Erwerb durch die Vereinigten Staaten Aufstieg zum Zentrum der Südstaaten der Vereinigten Staaten New Orleans im 20. Jahrhundert Hurrikan Katrina 2005 Stadtveränderung durch die Folgen der Katastrophe Gustav 2008 Friedhöfe Historische Gebäude Musik und Feste Küche Sport Hafen Unternehmen Bundesbehörden und militärische Einrichtungen Schifffahrt ÖPNV und Eisenbahn Straßenbahnen Autoverkehr Flughafen Medien Internet Bildung Söhne und Töchter der Stadt Persönlichkeiten mit Bezug zur Stadt New Orleans(amerikanisches Englisch:[ˈnuːˈɔrlɪns, nuˈɔ:rlɪns]ⓘ, lokal: [ˈnɔrlənz]) ist mit 383.997 Einwohnern[2](2020) die größte Stadt imBundesstaatLouisianain denVereinigten Staaten. Sie ist ein Industriezentrum mit einem bedeutendenHafenamMississippi River. Die Stadt ist bekannt für ihrelokale kreolische Kücheund ihre historische Altstadt, dasFrench Quarter, wo sich zahlreiche Gebäude im spanischen und französischen Kolonialstil befinden. Außerdem gilt New Orleans als die „Wiege desJazz“. Die Stadt liegt im Delta des Mississippi River und hat eine Fläche von 907,0 km², wovon 467,6 km² auf Land und 439,4 km², also 48,45 % auf Wasser entfallen. Wegen ihrer Lage zwischen dem Mississippi im Süden und demLake Pontchartrainim Norden, die ihr einen Stadtumriss in Form einer Mondsichel gegeben hat, erhielt New Orleans die BezeichnungCrescent City(Mondsichelstadt). Am Westufer gegenüber der Altstadt liegt der StadtteilAlgiers. Bei der Gründung der Stadt bebauten französische Siedler ein kleines, etwas höher gelegenes Stück Land, das heute alsFrench Quarterbekannt ist. Das Gebiet des heutigen New Orleans ist erst vor 2500 Jahren aus vom Fluss angeschwemmtenSedimentenentstanden. Es ist größtenteils ein mehrere hundert Meter tieferSumpf, der sich unter dem Druck seines eigenen Gewichtes verdichtet und bei Ausbleiben der Sedimentablagerung weiter unter denMeeresspiegelsinken wird. Seitdem der Mississippi von den Franzosen und anschließend vomUnited States Army Corps of Engineerseingedeicht wurde, sinkt das Gebiet von New Orleans um etwa 8 mm pro Jahr. 70 % der Stadtfläche liegen bis zu 1,6 m unterhalb des Meeresspiegels, wobei der angrenzende 1839 km² große Lake Pontchartrain über die Wasserstraße Rigolets Strait und dieLaguneLake Borgne mit demGolf von Mexikoverbunden ist. Ab 1910 legte Albert Baldwin Wood die Stadt, die von Sümpfen umringt ist, mit zahlreichen großen Pumpen trocken. Der auf der südlichen Seite des Mississippis nahegelegene Naturpark Barataria Preserve in Marrero zeigt Reste der ursprünglichen Landschaft im Mississippi-Delta. Ein Drainagesystem von mehreren hundert Kilometern Länge durchzieht heute New Orleans und entwässert über 22 Pumpstationen bei starkem Regen die gesamte Stadt. Durch die Trockenlegung konnte New Orleans um erhebliche Flächen erweitert werden, sie führte jedoch auch zu einer ausgedehnten Absenkung des Terrains. Heute ist die Stadt im Norden von einem 5 bis 6 Meter hohenDeichsowie im Süden von einem 9 Meter hohen Deich gegen Wassereinlauf geschützt.[3] Die ersten Einwohner von New Orleans waren eine Mischung kanadischer Grenzbewohner, Handwerker und Truppen derJohn Law’s Gesellschaft des Westens, die das Gebiet beherrschte, bis es 1731 an die französischen Kronkolonie zurückging, sowie Häftlingen, schwarzer und indianischer Sklaven. 1727 erreichten 88 aus Pariser Gefängnissen befreite Frauen die Stadt, begleitet von acht ursulinischen Nonnen, um sich in der Stadt niederzulassen. Die unerschrockenen Nonnen ließen sich in der heutigen Chartres Street nieder; auch ein Konvent wurde etwas weiter unten auf der Straße gebaut. Dieses Gebäude, dessen Bau 1745 begonnen wurde und das sich in der Nähe des heutigenFrench Marketbefindet, ist das einzige erhaltene Gebäude aus der Zeit der französischen Herrschaft. Französische Abenteurer und andere Europäer folgten bald den ersten Siedlern. Der BegriffKreole, in Französisch-Westindien geprägt und gebraucht, wurde in Louisiana als Bezeichnung einer Person eingeführt, die dort als reiner Franzose geboren wurde. Die Abstammung und der Begriff gehen auf das spanische Wortcriollozurück, das die erste Generation von spanischen Eltern in der neuen Welt bezeichnete. Im Jahr 1755 kam eine Gruppe vonCajunsin das Gebiet. Sie waren Nachfahren französischer Kolonisten, die sich inAkadien, der späteren kanadischen ProvinzNova Scotia, niedergelassen hatten und dortvon den Briten vertriebenworden waren. Die Spanier übernahmen 1762, vier Jahre nach dem Abkommen vonFontainebleaudie Kontrolle über die Stadt. Nach Unterdrückung einer Revolte ließen sie sich nieder und prägten die Stadt mit ihrer Architektur und Lebensart. Vom 19. bis zum frühen 20. Jahrhundert gab es eine große Einwanderungswelle in die Vereinigten Staaten, die Deutsche, Iren und Italiener nach New Orleans brachte. Trotz des französischen Namens der Stadt wird kaum noch Französisch gesprochen, seit 1968 wird aber seitens des Bundesstaates Louisiana die Wiederbelebung der französischen Sprache gefördert (sieheFranzösisch in Louisiana). Die Stadt New Orleans hatte bis zur Evakuierung am 28. August 2005 durch die Folgen von Hurrikan Katrina nach Angaben desUnited States Census Bureau454.863 Einwohner (Stand 1. Juli 2005). Am 1. Juli 2006 waren es mit 223.388 nur noch etwa die Hälfte und im Juli 2007, fast zwei Jahre nach der Katastrophe, war die Zahl lediglich auf 239.124 Einwohner gestiegen. Es stand zu befürchten, dass es in den kommenden Jahren kaum wesentlich mehr werden, da ein sicherer Hochwasserschutz in den tiefliegenden Kernzonen mit einem finanziell vertretbarem Aufwand kaum vorstellbar ist und es sich viele ehemalige Bewohner finanziell nicht leisten können zurückzukehren. Für Stadtplaner eröffnete es neue Gestaltungsmöglichkeiten auf den zerstörten (und verlassenen) Wohnbauflächen. Nach den Ergebnissen derVolkszählung im Jahr 2010ergab sich eine Steigerung der Einwohnerzahl auf 343.829. Die Bevölkerungsdichte betrug 478 Einwohner pro Quadratkilometer. In der Stadt lebten 28,05 Prozent Weiße; 67,25 ProzentAfroamerikaner; 0,20 Prozent Ureinwohner; 2,26 Prozent Asiaten und 3,06 Prozent Hispano-Amerikaner. Im Großraum New Orleans, der Metropolitan Area, lag die Bevölkerungszahl am 1. Juli 2005 bei 1.313.787. Am 1. Juli 2006 betrug sie mit 1.024.678 rund 300.000 Personen weniger. Die folgende Übersicht zeigt die Einwohnerzahlen nach dem jeweiligen Gebietsstand. Von 1769 bis 1800 und von 2005 bis 2007 handelt es sich um Schätzungen des United States Census Bureau, in den jeweils vollen Jahrzehnten von 1810 bis 2010 umVolkszählungsergebnisse. LautJakob/Schorb (2008) schwanken die Schätzungen der Einwohner für den März 2008 zwischen dem City Council, die von 300.000 Rückkehrern ausgehen und den Bundesbehörden, die von 240.000 Rückkehrern ausgehen. Die unterschiedlichen Einschätzung führen die beiden Soziologen auf die jeweils unterschiedlichen Interessen zurück: „Die Höhe der Bundeszuschüsse an die finanzschwache Kommune bemisst sich nämlich an der Höhe der Einwohnerzahl.“[5] Am 1. Juli 2013 lag die Einwohnerzahl der Stadt wieder bei 378.715, verglichen mit 223.388 exakt 7 Jahre zuvor. Das Wachstum der Stadt scheint sich fortzusetzen, zwischen der Volkszählung 2010 und dem 1. Juli 2013 lag der Anstieg bei 10,1 Prozent, verglichen mit 2,1 Prozent für ganz Louisiana.[6] New Orleans wurde 1718 von dem FranzosenJean-Baptiste Le Moyne de Bienvillegegründet und erhielt zu Ehren des Regenten von Frankreich,Philipp II., Herzog von Orléans, den NamenLa Nouvelle-Orléans. 1722 wurde die Stadt die Hauptstadt der KolonieLouisiana. Seit spätestens 1721 wurden in unmittelbarer Nähe zu New Orleans deutschsprachige Kolonisten von der französischen Kolonialverwaltung angesiedelt.[7]1762 ging die Kolonie im Rahmen des geheimenAbkommens von FontainebleauanSpanien, was imPariser Frieden 1763bestätigt wurde; bis 1766 wurde jedoch kein spanischerGouverneureingesetzt. Am Karfreitag, dem 21. März 1788 wurde die Stadt durch einen Großbrand beinahe völlig vernichtet. 856 der rund 1100 Gebäude der Stadt fielen den Flammen zum Opfer. Nach nur sechs Jahren des Wiederaufbaus zerstörte am 8. Dezember 1794 ein erneuter Brand mehr als 200 Häuser imFrench Quarter. Im Jahr 1795 gewährte Spanien den Vereinigten Staaten imPinckney-Vertragdas Recht auf Niederlassung und erlaubte den Amerikanern, den Hafen der Stadt zu nutzen. Im Rahmen des 1800 vonNapoleon BonapartediktiertenAbkommens von San Ildefonsoging das 1763 Spanien zugesprochene Gebiet westlich desMississippizurück an Frankreich. 1803 verkaufte Napoleon im sogenanntenLouisiana Purchasedie gesamteKoloniefür 15 Mio. US-Dollar an die Vereinigten Staaten unter PräsidentThomas Jefferson. New Orleans, das zu dieser Zeit etwa 10.000 Einwohner hatte, wurde offiziell am 20. Dezember 1803 den Vereinigten Staaten übergeben. Der in Paris unterzeichnete Vertrag wurde imCabildo-Gebäude am Jackson Square, dem vormals spanischen Rathaus der Stadt – heuteLouisiana State Museum– öffentlich verkündet. In einer Parade zogen die französischen Soldaten aus und die amerikanischen ein, am Fahnenmast auf dem Platz wurde die Flagge gewechselt. Von seinen frühen Tagen an war New Orleans bekannt für seine kosmopolitische und polyglotte Bevölkerung und die zahlreichen Kulturen, die dort existierten. Die Stadt wuchs schnell mit Einflüssen aus Frankreich, Amerika und der kreolischen Karibik. Während desBritisch-Amerikanischen Kriegeswollten die Briten die Stadt erobern, sie wurden aber von den von GeneralAndrew Jacksonangeführten Truppen einige Meilen flussabwärts am 8. Januar 1815 zurückgeschlagen (Schlacht von New Orleans). Die Stadtbevölkerung verdoppelte sich in den 1830ern und 1840ern, und New Orleans wurde die größte Stadt imamerikanischen Südenund außerhalb des „Atlantic Seaboards“. Bis 1849 war New Orleans dieHauptstadtdes Staates Louisiana, später wieder von 1865 bis 1882. Der Hafen war vor allem für den Sklavenhandel wichtig, obwohl New Orleans gleichzeitig die Stadt mit den meisten schwarzen Personen Nordamerikas war. Die Bedeutung der Stadt nahm zu, als die Regierung 1838 eine lokaleMünzeneben den südlichen Münzprägeanstalten vonCharlotte (North Carolina)undDahlonegaeinrichtete. Im Gegensatz zu den beiden anderen Münzen wurden in New Orleans nebenGold- auchSilber-Münzengeprägt, was zur Stellung als wichtigste Prägeanstalt im Süden beigetragen haben könnte. Im Jahr 1853 war New Orleans von einer schwerenGelbfieber-Epidemie betroffen, bei der fast 8000 Menschen starben.[8][9] ImAmerikanischen Bürgerkriegwurde New Orleans bereits am 28. April 1862 durch eine von AdmiralDavid Glasgow Farragutgeleitete Schiffsoperation von derUnionkampflos erobert, wodurch ein Großteil der historischen Gebäude erhalten blieb. Ein wichtiger Anziehungspunkt für Touristen aus aller Welt im späten 19. und frühen 20. Jahrhundert war der berühmte RotlichtbezirkStoryville. New Orleans ist für seine kreolische und seineVoodoo-Kultur bekannt, die mit demSklavenhandelausAfrikanachAmerikagekommen war. Ebenso bekannt ist die Stadt für ihre vielfältigenMusikrichtungen. So entstand hier als Synthese aus verschiedenen Musiktraditionen – auch den damals das Straßenbild prägendenStreet Bands– seit der Jahrhundertwende derJazz, der hier vor allem in den frühen 1920ern seine große Blüte erlebte. Zur selben Zeit wurden zur Modernisierung des Stadtbilds die alten gusseisernen Balkone in der Canal Street entfernt, in den 1960er Jahren ersetzte die Stadt dieCanal Streetcar Line(Straßenbahn) durchBusse. Beide Maßnahmen wurden in den 1990er Jahren wieder rückgängig gemacht. 1927 trat der Mississippi auf einer Fläche von etwa 70.000 km² über die Ufer und machte mehrere 100.000 Menschen obdachlos. Auslöser fürdie Flutwar ein lang andauernder Regen. Er begann im August 1926 und dauerte bis in den Frühling 1927. Damals sprengte man die Deiche bei der 18 km entfernt gelegenen Stadt Poydras auf einer Länge von 450 m, um New Orleans vor den Wassermassen zu schützen. 1965 erlebte New Orleans seine bis dahin schlimmste Hurrikankatastrophe. Der HurrikanBetsysetzte einen Großteil der Stadt unter Wasser und machte tausende Bewohner New Orleans’ und Louisianas obdachlos. New Orleans liegt im Einzugsgebiet vonHurrikanen. Bei Hurrikanen handelt es sich um tropische Stürme, die sich zwischen Mai und Oktober über demAtlantischen Ozeanbilden. Der bisher verheerendste,Hurrikan Katrina, traf die Stadt am 29. August 2005 und sorgte in Zusammenhang mit einer auf den Hurrikan zurückgehenden Flutkatastrophe für die fast vollständige Verwüstung der Stadt. Am 29. August 2005 wurde New Orleans vonHurrikan Katrinaheimgesucht, der einer der vier schwersten Hurrikane war, die in den Vereinigten Staaten jemals gemessen wurden. Obwohl New Orleans nur von Ausläufern des Hurrikans in Mitleidenschaft gezogen wurde, brachen die Wände zweier Kanäle, worauf das Wasser desLake Pontchartraindie Stadt fast vollständig überschwemmte. Auch die großen Wasserpumpen der Stadt fielen aus. Unterspülte Gebäude brachen zusammen, was die Situation deutlich verschlimmerte. Wasser, Müll und Schutt spülten in den als Notunterkunft genutztenSuperdome. Dieser war Ziel jener gewesen, die die Stadt nicht rechtzeitig verlassen hatten und dort auf die Verteilung von Wasser und Essen durch die Armee warteten. Zusätzlich verschlechterten Wasser- und Stromausfall die Bedingungen im Innern des Stadions. Am 30. August 2005 brachen auch die Wände des Industrial Canal, der den Lake Pontchartrain im Norden der Stadt mit dem Mississippi südlich des Zentrums verbindet, und Wasser lief in die am tiefsten gelegenen Stadtviertel. Einen Tag später waren mehrere Stadtteile der Jazzmetropole erneut überflutet. Das betraf dort besonders die Sozialbausiedlungen der „Big Four“, so genannte Public Housing Projects, die zwischen 1941 und 1955 in den tiefergelegenen Gebieten in damals als Wohnorte unbeliebten Teilen des Zentrums erbaut wurden. Das Wasser stand in Ninth Ward zwei Meter hoch[10]in den Straßen und dreiviertel aller Bewohner mussten fliehen oder wurden evakuiert.[11] Als das Ausmaß der Überflutung deutlich wurde, beschloss die Regierung am 31. August, die Stadt – und damit auch den Superdome – komplett zu evakuieren, zuerst mit Bussen und später mit einerLuftbrücke. Ziel war zunächst der Astrodome vonHouston, dessen Kapazitäten jedoch bereits nach einem Tag ausgeschöpft waren. Nach einigen Tagen des Notstandes wurde derAusnahmezustand, das Kriegsrecht und der Gesundheitsnotstand ausgerufen. DerLouis Armstrong New Orleans International Airport, der täglich etwa 300 Flugzeuge abfertigt, konnte nach wenigen Tagen wieder in Betrieb genommen werden. Da er etwas westlich der Stadt im weitgehend verschont gebliebenen VorortKennerliegt, wurden Landebahn und Tower durch den Hurrikan nicht zerstört, allerdings standen Bodenradar und Befeuerung der Landebahn zunächst nicht mehr zur Verfügung. Am 5. September 2005 wurde bekannt gegeben, dass der Deich zum Lake Pontchartrain repariert sei, jedoch das Abpumpen des Wassers mindestens drei Monate dauern werde. An einigen Deichen halfen vom deutschenTechnischen Hilfswerkzur Verfügung gestellte Hochleistungspumpen. Am 6. September 2005 ordnete BürgermeisterRay Nagindie Zwangsevakuierung der letzten in der Stadt verharrenden Einwohner an. Jeder, der nicht am Wiederaufbau beteiligt war, musste die Stadt verlassen, da die Gefahr von Seuchen bestand. Da der Damm entlang des Mississippi gehalten hatte, waren die höher gelegenen historischen Stadtteile am Flussufer, einschließlich desFrench Quarter, von den Überflutungen verschont geblieben. Am 20. September 2005 wurde New Orleans erneut evakuiert, weil befürchtet wurde, dass derHurrikan Ritadie Stadt in den nächsten Tagen überqueren würde, woraufhin die wenigen Rückkehrer die Stadt wieder verließen. Mitte Oktober 2005 war die Stadt wieder trockengelegt. Doch musste die Bevölkerung am 18. Oktober auf eine weitere möglicheEvakuierungvorbereitet werden: DerHurrikan Wilma, dessen Entwicklung zunächst ungewiss war, zog allerdings überFloridagenAtlantik. Während der Ruf nach einem schnellen Wiederaufbau New Orleans mit entsprechender staatlicher Unterstützung lauter wurde und der politische Druck auf die Verantwortlichen wuchs, mehrten sich auch Zweifel, ob New Orleans nicht aufgegeben werden sollte, da in Zukunft mit einer Verschlechterung der Situation zu rechnen sei. Die Stadt entschied sich für eine geringere Bevölkerungsdichte vor allem in den Sozialwohnungsgebieten (Public Housing Projects). Im Rahmen dieserGentrifikationwurden diese Public Housing Projects zum Großteil abgeriegelt und nicht mehr saniert bzw. abgerissen. Vor allem in der Innenstadt wurden diese Gebiete an Immobilienfirmen ausgeschrieben und dünner besiedelte Wohngebiete mit gemischten Einkommen errichtet bzw. geplant. Die Naturkatastrophe traf vor allem Schwarze und sozial benachteiligte Bewohner. John Logan, Stadtforscher von der Brown University in Rhode Island, analysierte das Datenmaterial der amerikanischen Katastrophenschutzbehörde FEMA. Danach waren Afroamerikaner, Arme, Mieter und Arbeitslose „weit überproportional“ von den Folgen des Hurrikans betroffen. In den zerstörten Gebieten lebten zu etwa 75 Prozent Afroamerikaner, während in den intakten Gebieten überwiegend Weiße lebten. Logan kommt zu dem Schluss, dass die Risikoverteilung zwischen wirtschaftlich starken und schwachen Bewohnern ungleich war. Arme und Schwarze hätten auch über weniger Mittel und Möglichkeiten verfügt, nach New Orleans zurückzukehren und ihre Häuser wieder aufzubauen. Schon vor der Katastrophe war klar, dass die tieferliegenden Gebiete der Stadt – Wohngebiete der Schwarzen und der Armen – die gefährdetsten Teile sein würden.[12]Der JournalistJordan Flahertystellt 2016 hierzu fest: „New Orleans ist heute kleiner, wohlhabender und weißer als vor dem Sturm“.[13] Ende August 2008 bedrohteHurrikan Gustavdie Stadt. Am 31. August wurde ihre Evakuierung angeordnet. Die Autobahn wurde kurzfristig zu einer Einbahnstraße stadtauswärts umgestaltet, so dass tausende Bürger den sicheren Weg Richtung Norden und Nordosten antreten konnten. Der Hurrikan bewegte sich jedoch weiter westlich auf die Küste zu und New Orleans blieb weitgehend verschont. Es zeigte sich, dass die Behörden (im BesonderenFEMA) nun ein besseres Vorbeugungsmanagement praktizierten, auch wenn die Herausforderungen und Ausmaße nicht mit denen von Hurrikan Katrina verglichen werden können. Das bekannteFrench Quarter, auch „Vieux Carré“ genannt, das noch aus der französischen und spanischen Zeit stammt und an den Mississippi, die Rampart Street, Basin Street, die Canal Street und die Esplanade Avenue grenzt, wurde 1721 von Ingenieur Adrian de Pauger entworfen. Vor allem der French Market (Französischer Markt), ein Platz, an dem dieChoctaw-Indianer handelten, mit demCafé du Monde, das berühmt für seinenZichorienkaffeeund seineBeignetsist, ist ein ebenso großer Anziehungspunkt für Touristen wie die legendäreBourbon Street, auf der allabendlich Partys bis in den Morgen gefeiert werden. Am nördlichen Ende des French Quarters liegt im StadtviertelTremederLouis Armstrong Park. Der Jackson Square liegt im Zentrum der gitterförmigen Anlage des Vieux Carre gegenüber dem Fluss, dessen Wasserpegel heute deutlich über dem Niveau der historischen Ansiedlung verläuft und durch einen Damm kanalisiert ist. Früher war der Platz als Paradeplatz als der „Place d' Armes“ (Waffenplatz) bekannt. Soldaten exerzierten vor der Stadtkirche, die von den Hauptquartieren des spanischen Stadtrats flankiert wurden. Nachdem die Spanier die Gemeinde übernommen hatten, tauften sie den Platz „Plaza de Armas“. Die Bezeichnung Jackson Square entstand im 19. Jahrhundert, als eine ReiterstatueAndrew Jacksonsauf der Platzmitte aufgestellt wurde. Am Jackson Square steht auch die 1727 gegründete, nach dem Stadtbrand von 1788 in den Jahren 1789 bis 1794 neu errichtete und ab 1850 erneuerte römisch-katholischeSt.-Louis-Kathedrale. Sie wird flankiert von den ab 1791 erbauten spätbarocken GebäudenCabildo(links) undPresbytere(rechts), beide heute Teile desLouisiana State Museum. Der ab 1832 angelegteGarden District(zwischen St. Charles Avenue, 1st Street, Magazine Street und Toledano Street, rund um die Prytania Street) weist eines der am besten erhaltenen Ensembles von eleganten Wohnhäusern derAntebellum-Architekturdes frühen und mittleren 19. Jahrhunderts auf. Wegen der tiefen Lage von New Orleans und dem damit verbundenen sehr feuchten Boden gibt es in der Stadt keine herkömmlichen Friedhöfe, weil Seuchen befürchtet werden. Seit 1830 werden die Toten inMausoleenbeerdigt; diese „Cities of the Dead“ sind Touristenattraktionen. Die Stadt entwickelte ihre eigene Art der Begräbnisse: Ein traditionellesJazz Funeralwird von einerMarching Bandbegleitet, die traurige, auf das Jenseits ausgerichtete Musik (Hymnen) auf dem Weg zur Beerdigung hin und fröhliche, weltliche Musik (Hot Jazz) auf dem Weg zurück spielt. Dieses Motiv wird in dem Film „James Bond 007 – Leben und sterben lassen“ aufgegriffen. Im French Quarter befindet sich in der Dumaine Street 632 die historischeMadame John’s Legacy. Das dreiteilige Anwesen wurde 1970 insNational Register of Historic Placesaufgenommen und wegen seines prägenden kreolischen Architekturcharakters imNational Historic Landmarkeingetragen.[14][15] In der Canal Street 423 befindet sich das historischeOld Post Office and Custom House, das 1974 ebenfalls in das National Register of Historic Places aufgenommen wurde. Seit 1914 besteht derAudubon Zoo. In angrenzenden Stadtteilen, die das alte French Quarter zuerst ergänzten, wieFaubourg Marigny,Bywater,Treme, ist das lokaltypischeShotgun Housevorherrschend. Diese idyllisch wirkenden Viertel waren vor demHurricane Katrinateils ärmlich, erfahren aber seit dem Abwandern zahlreicher Arbeitsloser der meist schwarzen Bevölkerung in boomende Zentren wieHouston(Texas) eine rascheGentrifizierung. New Orleans war immer ein Zentrum der Musik. Es vermischte europäischen Stil mit dem ausLateinamerikaund afroamerikanischen Kulturen. Vor allem derJazzmit Blechbläsern hat seine Wurzeln in New Orleans. Eine der bekanntesten Musikclubs fürNew-Orleans-Jazzist diePreservation Hallim French Quarter. Die Stadt ist ebenfalls bekannt fürRhythm and Blues, der denRock ’n’ Rollmaßgeblich prägte, und auchCajun-undZydeco-Musik ist vielerorts zu hören. Ende der 1980er Jahre prägten einige lokale Metal-Bands einen eigenenDoom-Metal-Stil, der später alsSludgebekannt wurde und häufig alsNOLA-Soundumschrieben wird.[16][17] Die bekannteste Musikveranstaltung ist dasJazz and Heritage Festival, kurz Jazz Fest, das an zwei aufeinanderfolgenden Wochenenden Ende April und Anfang Mai auf der Pferderennbahn Fairgrounds stattfindet. Auf vielen Bühnen treten bekannte und unbekannte Musiker verschiedener Genres auf, neben Jazz sind auchRock,Bluesund Zydeco zu hören. Ein Stammgast war zum Beispiel der in der Stadt geboreneDr. John, aber auchNellyundBrian Wilsonwaren hier schon zu hören. Weiterer Höhepunkt im Musikkalender der Stadt ist das French Quarter Festival. Mitte April werden hier für ein Wochenende im French Quarter, hauptsächlich in der Bourbon Street und in der Royal Street sowie auf dem Jackson Square, Bühnen aufgebaut, auf denen lokale und internationale Bands kostenlose Konzerte geben. Auch das Satchmo SummerFest zieht viele Jazzfreunde an; das nach dem bekanntesten Sohn der Stadt,Louis Armstrongbenannte Fest findet jedes Jahr am ersten Augustwochenende statt. Neben den Konzerten, ähnlich dem French Quarter Festival, gibt es diverse Ausstellungen zum Thema Jazz und auch eine Jazzmesse. Die bekannteste Feier in New Orleans ist derKarneval, der seinen Höhepunkt in einemUmzug, demMardi Grasfindet (französischeBezeichnung für denletzten der Karnevalstage). Während der Karnevalszeit gibt es mehrere große Umzüge in und um New Orleans, verschiedeneKrewes(Karnevalsvereine) gestaltenFloats(gezogene Wagen) und Trucks, fahren damit durch die Straßen und werfen Beads, bunte Plastikketten. Lila, Grün und Gold sind die offiziellen Farben des Mardi Gras, die 1872 vom Karnevalskönig Rex festgelegt wurden. Dieser „König“ stellt einen Seitenhieb auf die historischeAlte Welt, ihreMonarchenund deren Bevormundung der Bevölkerung dar.[18] New Orleans ist berühmt für seine Esskultur. ZahlreichekreolischeundCajun-GerichtewieCrawfish Étouffée,JambalayaundGumbosind sehr beliebt. Die Spezialitäten reichen vonPo’ boy(lokale Aussprache vonpoor boy= „armer Junge“; eine Art belegteBaguettes) undMuffuletta-Sandwiches überBananas Fosterbis zuPompano en Papillote,Golf-Austernund anderen Meerestieren. Eine weitere Spezialität istRed beans and rice; das herzhafte Eintopfgericht wird traditionell montags serviert. Bekannt sind auch dieBeignets, eine Art Krapfen, welche auf französisch-kreolische Siedler aus dem 18. Jahrhundert zurückgehen.[19] Die oft verwechselten Begriffe „kreolisch“ und „cajun“ in Bezug auf die lokalen Speisen und Kulturen bedeuten etwas Unterschiedliches.[20][21]Beide Kulturen basierten zunächst auf französischen Einwanderern, doch die späteren Kreolen kamen aus Europa und vermischten sich bald mit Spaniern, Indianern und Schwarzen, die Cajuns dagegen sind Einwanderer aus Nordostkanada, vor allemNova Scotia, und vermischten sich lange Zeit kaum mit der übrigen Bevölkerung. Ihr Siedlungsgebiet blieb außerdem weitgehend auf Südwestlouisiana beschränkt. Entsprechend entwickelte sich die heutige kreolische Küche durch Vermischung aller Speisentraditionen sämtlicher Einwanderer, die Cajun-Küche nahm nur zögerlich neue Elemente auf. New Orleans beherbergt eine Vielzahl von preisgekrönten Restaurants wie beispielsweise Antoine’s, Arnaud’s, Brennan’s oder Commander’s Palace. Der in den USA durch zahlreiche Kochsendungen und auch in Deutschland durchtv.gustobekannte KochKevin Beltonführt die New Orleans School of Cooking, eine Kochschule für Spezialitäten der Stadt.[22] New Orleans ist die Heimat von zwei bekannten Teams nordamerikanischer Profiligen. DieNew Orleans Pelicansspielen in der BasketballligaNational Basketball Association(NBA) und tragen ihre Heimspiele imSmoothie King Centeraus. In der American-Football-LigaNational Football League(NFL) ist die Stadt mit denNew Orleans Saintsvertreten. Die Saints spielen imCaesars Superdomeund gewannen denSuper Bowl XLIV. New Orleans war bislang zehnmal Austragungsort des Super Bowls, zuletzt 2013 beimSuper Bowl XLVII. Im Jahr 2024 sollte derSuper Bowl LVIIIim Superdome stattfinden, das Spiel wurde jedoch aufgrund der Terminkollision mit demMardi Grasin New Orleans in dasAllegiant StadiuminParadise,Nevadaverlegt. DasArena-Football-TeamNew Orleans VooDoobestand von 2004 bis 2015. Zuvor spielten 1991 und 1992 dieNew Orleans Nightin der Stadt. Die FußballmannschaftNew Orleans Jesterstritt in derNational Premier Soccer League(NPSL) an. Im Hochschulsport ist New Orleans mit denTulane Green Wavein derAmerican Athletic Conference(AAC) derNational Collegiate Athletic Association(NCAA) vertreten. Seit 1965 findet derMardi Gras Marathonstatt. New Orleans betreibt mit demHafen New Orleanseinen der größten und verkehrsreichsten Häfen der Vereinigten Staaten und die Metropolregion New Orleans ist ein Zentrum der maritimen Industrie.[23]Ebenso entfällt ein erheblicher Teil der Ölraffinerie- und Petrochemieproduktion der USA auf diese Region, die als Wirtschaftsstandort für die Onshore- und Offshore-Erdöl- und Erdgasproduktion dient. Zudem entwickelt sich New Orleans seit Beginn des 21. Jahrhunderts zu einem Technologiezentrum.[24][25]Daneben ist die Metropolregion New Orleans ein wichtiges regionales Zentrum der Gesundheitsbranche und verfügt über einen kleinen Fertigungssektor. In der Innenstadt, die zahlreiche Touristen anzieht, ist ein schnell wachsenderKreativwirtschaftssektorangesiedelt. Die wirtschaftliche Entwicklung der Metropolregion wird vonGreater New Orleans, Inc.gefördert, welches die Aktivitäten desDepartment of Economic Developmentund verschiedener Agenturen für die wirtschaftliche Entwicklung der Region koordiniert. Die Metropolregion von New Orleans erbrachte 2022 eine Wirtschaftsleistung von 74 Milliarden US-Dollar.[26]Mit einem BIP pro Kopf von 55.167 Dollar im Jahr 2021 zählt New Orleans zu den ärmeren der 100 größten Städte des Landes.[27] New Orleans begann als strategisch günstig gelegener Handelsstützpunkt und ist nach wie vor ein wichtiger Verkehrsknotenpunkt und Zentrum für den Schiffshandel. Der Hafen von New Orleans ist gemessen am Frachtvolumen der fünftgrößte in den Vereinigten Staaten und nach demHafen Südlouisianader zweitgrößte im Bundesstaat. Gemessen am Frachtwert ist es der zwölftgrößte in den USA. Der Hafen von South Louisiana, ebenfalls in der Gegend von New Orleans gelegen, ist im Hinblick auf Massengutumschlag der weltweit verkehrsreichste. Zusammen mit dem Hafen von New Orleans bildet es volumenmäßig das viertgrößte Hafensystem. Viele Schiffbau-, Schifffahrts-, Logistik-, Speditions- und Rohstoffmaklerunternehmen haben ihren Sitz entweder im Großraum New Orleans oder sind vor Ort präsent. Beispiele hierfür sind Intermarine, Bisso Towboat, Northrop Grumman Ship Systems, Trinity Yachts,Expeditors International, Bollinger Shipyards, IMTT, International Coffee Corp, Boasso America, Transoceanic Shipping, Transportation Consultants Inc., Dupuy Storage & Forwarding und Silocaf. Die größte Kaffeerösterei der Welt, betrieben vonFolgers, befindet sich inNew Orleans East. Insgesamt werden ein Drittel der Kaffeeimporte der USA im Hafen von new Orleans umgeschlagen.[28] Die Stadt liegt in der Nähe desGolfs von Mexikound seiner zahlreichenBohrinseln. Mehrere Energieunternehmen haben regionale Hauptsitze in der Region, darunterRoyal Dutch ShellundENI Petroleum.Chevronverlegte in Folge Hurrikan Katrinas seine Zentrale für den Golf von Mexiko von New Orleans nachCovington, LA.[29]Daneben haben weitere Energieerzeuger und Ölfelddienstleistungsunternehmen ihren Hauptsitz in der Stadt. MitEntergy, einem Energieversorger und Spezialist für den Betrieb vonKernkraftwerken, hat einFortune-500-Unternehmenseinen Sitz in New Orleans.[30]Nach Hurrikan Katrina verlor die Stadt 2013 ihr anderes Fortune-500-Unternehmen,Freeport-McMoRan, als dieses seiney Kupfer- und Goldexplorationseinheit mit einem Unternehmen aus Arizona fusionierte und diese Abteilung nachPhoenixverlegte.[31]Die Tochtergesellschaft McMoRan Exploration hat ihren Hauptsitz weiterhin in New Orleans. Zu den Unternehmen mit bedeutenden Niederlassungen oder Hauptsitzen in New Orleans gehören: Pan American Life Insurance, Pool Corp,Rolls-Royce Group, Newpark Resources,AT&T, TurboSquid, iSeatz,IBM, Navtech, Superior Energy Services, Textron Marine & Land Systems,McDermott International, Pellerin Milnor,Lockheed Martin, Imperial Trading, Laitram,Caesars Entertainment, Stewart Enterprises, Edison Chouest Offshore, Zatarain's, Waldemar S. Nelson & Co., Whitney National Bank,Capital One, Tidewater Marine,Popeyes Louisiana Kitchen,WSP Global, MWH Global,CH2M Hill, Energy Partners Ltd, The Receivables Exchange, GE Capital und Smoothie King. Bundesbehörden und das Militär betreiben in New Orleans bedeutende Einrichtungen. DasUnited States Court of Appeals for the Fifth Circuitist in einem Gerichtsgebäude in der Innenstadt ansässig. DieMichoud Assembly FacilityderNASAbefindet sich in New Orleans East und hat mehrere Mieter, darunter Lockheed Martin undBoeing. Es handelt sich um einen großen Fertigungskomplex, in dem unter anderem die externen Treibstofftanks für dieSpace Shuttles, die erste Stufe derSaturn-V-Raketeund die integrierte Fachwerkstruktur derInternationalen Raumstationproduziert wurden und jetzt für den Bau desSpace Launch Systemsder NASA verwendet wird. Die Raketenfabrik liegt imNew Orleans Regional Business Park, in dem sich auch das vomLandwirtschaftsministerium der Vereinigten StaatenbetriebeneNational Finance Centerund das Crescent-Crown-Vertriebszentrum befinden. Weitere große staatliche Einrichtungen sind dasNaval Information Warfare Systems CommandderUnited States Navy, das sich im Forschungs- und Technologiepark derUniversity of New OrleansinGentillybefindet, dieNaval Air Station Joint Reserve Base New Orleansund das Hauptquartier der Marine Force Reserves in Federal City inAlgier. New Orleans liegt am Knotenpunkt desGulf Intracoastal Waterway, des Mississippis und desMississippi River-Gulf Outlet Canal, einem 106 km langen und 10,8 m tiefen Kanal vom Innenhafen New Orleans zum Golf. DerÖPNVwird von derRegional Transit Authority New Orleans, kurz RTA betrieben, es werden Bus- und Straßenbahnlinien angeboten. Außerdem gibt es noch einen Anschluss der EisenbahngesellschaftAmtrak(Bahnknotenpunkt) und zahlreiche Verbindungen der ÜberlandbusgesellschaftGreyhound. Straßenbahnengehören auch zum Flair von New Orleans. Am bekanntesten ist die Linie St. Charles mit ihren grünen Wagen, die 1924 von Perley-Thomas gebaut wurden. Diese Linie verbindet seit 1893 New Orleans mit der Vorstadt Carrollton. Entlang der Strecke durch die St. Charles Avenue kann man im Stadtteil Garden District zahlreiche herrschaftliche Villen aus dem 19. Jahrhundert sehen. Auch zwei private Universitäten befinden sich hier, dieTulane Universityund dieLoyola University. Eine weitere Strecke ist die Riverfront-Linie, auch bekannt als „Ladies in Red“, die 1988 wieder in Betrieb genommen wurde. Sie läuft parallel zum Fluss von der Canal Street ins French Quarter. 2004 wurde die Linie Canal Street wiedereröffnet. Sie bedient unter anderem den City Park mit dem KunstmuseumNew Orleans Museum of Artdurch einen Abzweig in die North Carrollton Avenue. Die Strecke, nach derTennessee Williamssein bekanntes DramaA Streetcar Named Desire(Endstation Sehnsucht)benannte, wurde bereits 1948, ein Jahr nach der Veröffentlichung, eingestellt. Die Stadt hat mit derInterstate 10einen direkten Autobahnanschluss, welcher bis an die Bundesstaaten Kalifornien an der Westküste sowie Florida an der Ostküste reicht. Über die im Westen der Stadt beginnendeInterstate 55gibt es eine direkte Verbindung in nördlicher Richtung bis Illinois. Nördlich der Stadt befindet sich mit derInterstate 12eine Umgehung für den Fernverkehr um die Stadt. Im Zentrum von New Orleans existiert ein gut ausgebautes Autobahnsystem, zu dem neben der Interstate 10 auch dieInterstates 310,510und610sowie dieU.S. Highways 61und90gehören. DieCrescent City Connectionist die einzige Brücke über den Mississippi im Stadtgebiet und gleichzeitig die letzte Brücke über den Strom vor seiner Mündung. New Orleans hat mehrere Flughäfen, der größte ist derLouis Armstrong New Orleans International Airport(MSY), der im VorortKennerliegt. Im Mai 2012 gab Advance Publications, Inc. als Herausgeber der ZeitungThe Times-Picayune(Pulitzer-Preisim Jahr 2006 für die Berichterstattung zumHurrikan Katrina)[32]bekannt, dass das Blatt zukünftig nicht mehr täglich erscheinen werde. Von 2012 bis 2014 war die gedruckte Version der Zeitung nur mittwochs, freitags und sonntags erhältlich.[33]2014 kehrte man zum täglichen Andruck zurück. Am 13. Dezember 2019 wurden nach dem Feststellen einer massivenCyberattackealle Server der Stadtverwaltung heruntergefahren und derAusnahmezustanderklärt. Auf einigen Rechnern der Stadt wurdeRansomwaregefunden.[34] New Orleans ist auch ein Zentrum für höhere Bildung: Über 50.000 Studenten sind an den elf Einrichtungen der Region eingeschrieben, die zwei- und vierjährige Studienabschlüsse verleihen. Die Tulane University, eine der 50 besten Forschungsuniversitäten, befindet sich in Uptown. New Orleans listet folgende fünfzehnStädtepartnerschaftenauf:[37] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geografie 1.1Klima 2Demographie 3Geschichte 3.1Kolonialzeit 3.2Erwerb durch die Vereinigten Staaten 3.3Aufstieg zum Zentrum der Südstaaten der Vereinigten Staaten 3.4New Orleans im 20. Jahrhundert 4Hurrikane 4.1Hurrikan Katrina 2005 4.2Stadtveränderung durch die Folgen der Katastrophe 4.3Gustav 2008 5Kultur 5.1Friedhöfe 5.2Historische Gebäude 5.3Musik und Feste 5.4Küche 5.5Sport 6Wirtschaft und Infrastruktur 6.1Hafen 6.2Unternehmen 6.3Bundesbehörden und militärische Einrichtungen 6.4Schifffahrt 6.5ÖPNV und Eisenbahn 6.6Straßenbahnen 6.7Autoverkehr 6.8Flughafen 6.9Medien 6.10Internet 6.11Bildung 7Persönlichkeiten"
  },
  {
    "label": 0,
    "text": "New York City – Wikipedia New York City Inhaltsverzeichnis Geographie Bevölkerung Geschichte Politik Kultur und Sehenswürdigkeiten Wirtschaft und Infrastruktur Militär Panoramen Siehe auch Literatur Weblinks Einzelnachweise Geographische Lage Geologie Stadtgliederung Klima Auswirkungen des Klimawandels Bevölkerungsentwicklung Bevölkerungsgruppen und Herkunft Sprachen Präkoloniale Geschichte 17. Jahrhundert 18. Jahrhundert 19. Jahrhundert 20. Jahrhundert 21. Jahrhundert Stadtregierung Städtepartnerschaften Theater Musik Museen Bauwerke Parks Sport Regelmäßige Veranstaltungen Kulinarische Spezialitäten Wirtschaft Wärmeversorgung Fernverkehr Nahverkehr Medien Bildung Freiheitsstatue und Liberty Island Flatiron Building Chrysler Building Empire State Building Brookfield Place Rockefeller Center Weitere Wolkenkratzer Brooklyn Bridge Gebäude in SoHo Kirchen Grand Central Terminal New York Public Library Dakota-Apartmenthaus Central Park Battery Park Bronx Zoo Poe Cottage Governors Island High Line Hafen Straßen Flughäfen Zugverbindungen Straßen, Taxis Öffentlicher Nahverkehr Fähren und Brücken Printmedien Rundfunk und Fernsehen New York City(AE: [nuːˈjɔɹk ˈsɪɾi], offiziellCity of New York, kurzNew York, AbkürzungNYC) ist eineWeltstadtan derOstküstederVereinigten Staaten. Sie liegt imBundesstaatNew Yorkund ist mit rund 8,8 Millionen Einwohnern die bevölkerungsreichste Stadt des Landes.[4][5][6] DieMetropolregion New Yorkmit 20 Millionen Einwohnern[7]ist einer der bedeutendstenWirtschaftsräumeund Handelsplätze der Welt, Sitz vieler internationaler Konzerne und Organisationen wie derVereinten Nationensowie wichtigerSee- und Binnenhafenan der amerikanischen Ostküste und dem Hudson. Die Stadt genießt mit ihrer großen Anzahl an Sehenswürdigkeiten, den 500 Galerien, etwa 200 Museen, mehr als 150 Theatern und mehr als 18.000 Restaurants Weltruf in den Bereichen Kunst und Kultur und verzeichnet jedes Jahr etwa 50 Millionen Besucher, davon knapp 12 Millionen aus dem Ausland.[8][9]LautForbes Magazineist New York City die Stadt mit den höchstenLebenshaltungskostenin den Vereinigten Staaten sowie eine der teuersten Städte weltweit.[10] Die Stadt wird allgemein zu den wichtigsten Zentren der weltweitenDiplomatiegezählt.[11]Zusammen mitGenf(IKRK,ISO,WHO, europäischer UNO-Sitz etc.) zählt New York City zu den wenigen Städten der Welt, die als Sitz einer der gemeinhin als wichtigsten erachteteninternationalen Organisationenfungieren, ohne jedoch Hauptstadt eines Landes zu sein. Nachdem 1524Giovanni da Verrazzano[12]und 1609Henry Hudsondie Gegend des heutigen New Yorks erforscht hatten, siedelten ab 1610 niederländische Kaufleute an der Südspitze der InselManna-Hattaund bald darauf an der Westspitze vonLong Island, dem heutigen Brooklyn. Der Legende nach kauftePeter Minuit1626 den Einheimischen, wahrscheinlichLenni-Lenape-Indianern, die Insel „Manna-hatta“ für Waren im Wert von 60 Gulden ab.[13]Die Siedlung erhielt den NamenNieuw Amsterdamund war zunächst Hauptstadt der KolonieNieuw Nederland, bis sie 1664 von den Briten erobert wurde und die Stadt den seither gültigen Namen bekam.[14]Ihr Aufstieg zur Weltstadt begann 1825 mit der Fertigstellung desEriekanals. Die Stadt ist einePrincipal CityderNew York–Newark–Jersey City, NY–NJ–PAMetropolitan Statistical Area. Die Region erbrachte 2024 eine Wirtschaftsleistung von 1,992 Billionen US-Dollar. Unter den Städten der Welt belegt sie damit denzweiten RanghinterTokiound wäre als eigener Staat gezählt unter den 20 größten Volkswirtschaften der Welt.[15] 40.712777777778-74.005833333333 New York City hat sich von der InselManhattanaus ausgedehnt, auf der sich heute das Stadtzentrum befindet. Die Stadt liegt auf 40,42° nördlicher Breite und 74,00° westlicher Länge an derOstküste der Vereinigten Staatenin der New York Bay und in unmittelbarer Nachbarschaft zuJersey City. Im Stadtgebiet liegt der Hauptarm der Mündung desHudson Riverin denAtlantischen Ozeansowie derEast Rivergenannte Nebenarm dieser Mündung. Das Gelände, auf dem New York errichtet wurde, erhebt sich durchschnittlich sechs Meter über denMeeresspiegel. Die Stadtränder New Yorks und seiner Nachbarstädte verzahnen sich innerhalb ausgedehnter Vorstadtgebiete. Das eigentliche Stadtgebiet hat eine Landfläche von 785,6 km² (Berlin 891,85 km²), das gesamte durchgängig bebaute Gebiet (dieAgglomeration) erstreckt sich über 8683,2 km². DieNew York–Newark–Jersey City, NY–NJ–PA Metropolitan Statistical Areaumfasst 17.405 km², die etwas weiter gegriffeneNew York–Newark, NY–NJ–CT–PA Combined Statistical Area30.671 km² (das BundeslandBrandenburg29.478,61 km²). Manhattan steht auf Felsgrund, der es ermöglicht, auch höhere Bauwerke zu errichten. Es ist von zahlreichen Naturhäfen umgeben, die durch Schiffe mit großem Tiefgang angelaufen werden können. Der Hudson öffnet den wichtigsten natürlichen Weg ins Hinterland und ermöglicht es, über denMohawk Riverbzw. denLake Champlainzu denGroßen Seenim Nordwesten bzw. demSankt-Lorenz-Stromim Norden zu gelangen. Südlich des Gebiets, das stärker durchEisbergegefährdet ist, haben sich die Hauptmassen der europäischen Einwanderer angesiedelt. Dank dieser geographischen Voraussetzungen konnte New York eine herausragende Stellung in der US-amerikanischen und derWeltwirtschafterringen.[16] Das Gebiet um die Stadt New York liegt am nördlichen Ende derAtlantischen Küstenebeneim Grenzbereich zumNewark-Beckenund zu den NördlichenAppalachen. Oberflächlich finden sich im Stadtgebiet weit überwiegendglazialeAblagerungen desPleistozäns. So zieht sich küstenparallel von der Südspitze vonStaten Islandund weiter nach Nordosten quer durchBrooklynundQueens(Long Island) eineEndmoräneder Wisconsin-Kaltzeit (entspricht zeitlich derWeichsel-KaltzeitNord-Mitteleuropas), die sogenannte Harbour Hill Moraine. Sie repräsentiert den maximalen Vorstoß desLaurentidischen Eisschildsim jüngeren Abschnitt der Wisconsin-Kaltzeit (Woodfordium) vor ca. 22.000 Jahren.[17]An die Endmoräne schließt sich küstenseitig einSandergebiet(sowie u. a. in Gestalt derRockaway-Halbinselein schmaler Streifenkiesig-sandigerholozänerKüstenablagerungen) und küstenabgewandt eineGrundmoränean. DiequartärenAblagerungen sind jedoch oft nur wenige Meter mächtig und insbesondere nordwestlich des Endmoränenzuges auf Staten Island, inManhattanund in derBronxstehen Gesteine desmetamorphenGrundgebirgesrelativ oberflächennah oder sogar oberflächlich an. Dieser nur wenige Kilometer breite Grundgebirgsausläufer, der sich von Norden zwischen das Newark-Becken und die Küstenebene schiebt, wird auchManhattan Pronggenannt (als Gegenstück zum Reading Prong westlich des Newark-Beckens inNew JerseyundPennsylvania).[18]Das Grundgebirge gehört regionalgeologisch zu denAppalachenund besteht auskristallinen Schiefern(vor allemGranat-Glimmerschiefer),Marmorund verschiedenenGneisen. Es handelt sich umkambro-ordovizischemarine Sedimente, die im Ordovizium während einer der frühen Phasen derKaledonischen Orogenesei. w. S. (TakonischeundSalinischePhase) intensiv gefaltet und (teils unter partieller Aufschmelzung) metamorph überprägt wurden (Hartland Formation, Manhattan Formation, Inwood Marble[19]), sowie um ihrproterozoisches(„grenvillisches“) Grundgebirge (Fordham Gneiss).[18]Die Gesteine der Manhattan Formation sind dabei von besonderer Bedeutung für die New Yorker Skyline, denn sie bilden den standfestenBaugrundfür die Wolkenkratzer vonLower Manhattan. Die Grundgebirgseinheiten formen eineSattelstrukturmit NNE-SSW-streichenderFaltenachse.[18][20]Aufgrund ihrer lithologisch sehr ähnllichen Ausprägung wurde die Hartland Formation (Hartland Schist) ursprünglich als stratigraphisch höchstes Schichtglied der Manhattan Formation (Manhattan Schist) ausgeschieden, wird mittlerweile aber alsallochthoneEinheit, die während der Orogenese aus größerer Entfernung tektonisch an ihre heutige Position transportiert und auf die anderen Einheitenüberschobenwurde, interpretiert. Demnach repräsentierte sie einen vom großen SüdkontinentGondwanaabstammenden Kontinentalsplitter (perigondwanisches Terran), der dem damaligen „Ur-Nordamerika“ (Laurentia) angegliedert wurde.[18][20]Zwar lagert auch die Manhattan Formation mit Überschiebungskontakt auf dem Inwood Marble, aber sie gilt lediglich als parautochthon, d. h. repräsentiert einfach einendistalerenAblagerungsraum des laurentischenKontinentalrandes. Ebenfalls als allochthon gilt der Staten Island Serpentinite, ein großerSerpentinitkörper. Aufgrund seiner vergleichsweise hohen Verwitterungs- und Erosionsresistenz baut er die höchste Erhebung auf Staten Island, Todt Hill, auf. Der Staten Island Serpentinite bildet das südliche Ende des Manhattan Prongs. Auf ihn greifen (unterhalb des Quartärs) Gesteine desMesozoikumsüber. Im Nordwesten sind dies die basalenArkosender Füllung des Newark-Beckens (Doswell*und/oder Stockton Formation derNewark-Supergruppe, untereObertrias), im Südwesten und Südosten, im Bereich der Atlantik-Küstenebene, die schwach verfestigten, überwiegend tonigenmarinen Sedimenteder unterenOberkreide.[21]Während das Newark-Becken nur auf Staten Island Anteil am New Yorker Stadtgebiet hat, wo sich nach Nordwesten bis zur Grenze zuNew JerseynochSand-,Silt-undTonsteineder Passaic Formation („Brunswick Formation“, höhere Obertrias) anschließen, die von der Doswell/Stockton Formation durch einen mächtigenfrühjurassischenDolerit-Lagergang, den Palisade Diabase, getrennt werden, stehen im Untergrund von Brooklyn und Queens, im Bereich der Atlantik-Küstenebene, schwach verfestigtesiliziklastischemarine Sedimente der unteren und oberen Oberkreide großflächig an. Diese sind oberflächennah durch die pleistozäne Gletschertätigkeit deformiert und in die Endmoräne eingeschuppt.[17] Das Stadtgebiet ist in fünf Stadtbezirke (Boroughs) geteilt, von denen jeder einem Kreis (County) des BundesstaatesNew Yorkentspricht. JedesBoroughuntersteht einemBorough President. Manhattan(New York County) hat 1.601.948 Einwohner (Stand 2011[23]) und eine Landfläche von 59,5 km². Der Stadtbezirk besteht hauptsächlich aus der InselManhattan Island, die vomHudson Riverim Westen, vomEast Riverim Osten und vomHarlem Riverim Nordosten umflossen wird, sowie aus weiteren kleineren Inseln, darunterRoosevelt Island,Belmont Island,Governors Islandund einem kleinen Stück vom Festland,Marble Hill. Marble Hill war bis zum Bau desHarlem River Ship Canalim 19. Jahrhundert noch Teil vonManhattan Island. Brooklyn(Kings County) hat 2.532.645 Einwohner und ist damit der bevölkerungsreichste Stadtteil New Yorks. Er liegt im Südosten der Stadt, am westlichen EndeLong Islandsund hat eine Ausdehnung von 182,9 km². Die Stadt Brooklyn wurde nach der niederländischen StadtBreukelenbenannt und 1898 nach New York City eingemeindet. Queens(Queens County) hat 2.247.848 Einwohner und ist mit einer Fläche von 282,9 km² der größte der fünf Stadtteile von New York. Er liegt im Westen der InselLong Island. Queens wurde am 1. November 1683 gegründet, als die Kolonie New York in Countys eingeteilt wurde. 1898 wurde der Stadtteil nach New York eingemeindet. Dort liegen auch die zwei größten Flughäfen New Yorks, derJohn F. Kennedy International Airportund derLaGuardia Airport. DieBronx(Bronx County) hat 1.392.002 Einwohner und eine Ausdehnung von 108,9 km². Die früher eigenständige Stadt ist seit dem 1. Januar 1874 der nördlichste Teil von New York. Sie wurde nach dem ersten Siedler in diesem Gebiet, dem aus dem heutigenSchwedenausgewandertenJonas Bronck, benannt. Zur damaligen Zeit wurde, um einen Besuch auf seiner Farm anzukündigen, gesagt: „We are going to the Broncks“; heute wird der Name zwar anders geschrieben, aber das Voranstellen des Artikels wurde beibehalten, also „The Bronx“. Außerdem ist die Bronx der einzige Stadtteil New Yorks, der auf dem Festland liegt. Staten Island(Richmond County) hat 470.467 Einwohner und eine Landfläche von 151,5 km². Es liegt südwestlich der Insel Manhattan und westlich des aufLong Islandgelegenen Stadtteils Brooklyn. MitBrooklynist die Insel über diemautpflichtigeVerrazzano-Narrows Brückeverbunden, mit Manhattan durch die kostenloseStaten Island Ferry. Im Westen und Norden wird Staten Island durch den schmalenArthur Killund denKill van Kullvom Bundesstaat New Jersey getrennt. Nach New Jersey führen dieGoethals Bridge, dieBayonne Bridgeund dieOuterbridge Crossing. Der höchste Punkt der Insel, derTodt Hill, ist auch zugleich der höchste Punkt der Stadt New York. Früher wurde auf Staten Island der ganze Müll der Stadt deponiert, was bis heute teilweise für Probleme mit Geruchsbelästigung sorgt. Auch die Trümmer des World Trade Centers wurden nach Staten Island gebracht. Die durchschnittliche Jahrestemperatur beträgt 12,5 °C und die mittlere jährlicheNiederschlagsmenge1056,4 Millimeter.[24]Der wärmste Monat ist der Juli mit durchschnittlich 24,7 °C und der kälteste der Januar mit −0,4 °C im Mittel.[24]Die Niederschlagsmenge ist recht gleichmäßig über das Jahr verteilt; der meiste Niederschlag fällt im Monat April mit 99,1 Millimeter im Durchschnitt, der wenigste im Oktober mit 73,2 Millimeter im Mittel.[24]Die Stadt befindet sich damit in dergemäßigten Klimazone. Das Wetter von New York wird überwiegend von den kontinentalen Landmassen im Westen beeinflusst. Die Sommer sind im Allgemeinen tropisch warm bis heiß und die Winter kalt. Laut derKlimaklassifikation nach Köppen und Geigerbefindet sich New York City in derwarmen, feucht gemäßigten Klimazone.[25] Aufgrund des städtischenWärmeinseleffektsdurch asphaltierte Straßen und hohe Gebäude ist die Temperaturen hier um 5,3 °C höher als im Umland. Die Temperatur steigt im Sommer, insbesondere im Juli und August, oft über 30 °C, dazu kommt eine hoheLuftfeuchtigkeitwas zuSchwüleführt, mit teils heftigen Niederschlägen, oft auch Gewittern, was das Sommerwetter oft recht unangenehm macht. Im Winter können die Werte auf unter −10 °C fallen und auch +16 °C erreichen.[26]Gelegentlich bringt Meeresluft (theNor'easter– der Nordostwind) Feuchtigkeit vomAtlantischen Ozeansowie starke Winde und heftige Regen- oder Schneefälle. Einer der schwersten und folgenreichsten dieser Schneestürme war derGreat Blizzard von 1888mit einer Schneehöhe von rund 51 cm und etwa 100 Toten allein in New York City. Die bisherige Rekordschneehöhe seit Beginn der Wetteraufzeichnungen wurde mit 68 cm im Februar 2006 gemessen.[27] Mit durchschnittlich 2540Sonnenstundenpro Jahr hat die Stadt ein sonniges Klima und auch die Winter sind oft sonnig.[28] Dietiefsteje gemessene Temperatur liegt bei −26 °C am 9. Februar 1934 und die höchste bei 41 °C am 9. Juli 1936.[29] DieWassertemperaturenan der New Yorker Küste schwanken zwischen durchschnittlich 4 °C und 24 °C.[30] New York ist von derglobalen Erwärmungbedingt durch einen möglich steigendenMeeresspiegelund die steigende Gefahr vonSturmflutenbetroffen. In Modellrechnungen wird davon ausgegangen, dass infolge derKlimakriseder Wasserspiegel in New York bis zum Jahr 2100 um 1,80 Meter ansteigen könnte und die Anzahl von schweren Stürmen und Sturmfluten deutlich ansteigen dürfte. Prognosen gehen davon aus, dass im Jahr 2050 etwa 37 % der Gebäude an Manhattans Südspitze von Sturmfluten bedroht sein werden und 2100 20 % der Straßen in diesem Gebiet täglich überflutet sein werden. Eine Strategie derAnpassung an die globale Erwärmungist die Verstärkung der 850 km langen Küstenlinie.[31] New York City ist mit 8,5 Millionen Menschen die größte Stadt der Vereinigten Staaten und eine der größten Städte der Welt. Die Einwohnerzahl hat sich seit Anfang des 20. Jahrhunderts verdoppelt. Seit 1825 ist die Stadt durch die Lage amAtlantischen Ozeanund den Wasserweg desHudson Riversins Inland der Anlaufpunkt für Einwanderer aus der ganzen Welt. Die weitere Entwicklung machte New York zur größten Industriestadt und zur Finanzmetropole. Die Bevölkerung der Stadt teilt sich in verschiedene Schichten auf. Die Oberschicht lebt überwiegend etwas außerhalb der Stadt beziehungsweise im teuren Stadtteil Manhattan. In New York gibt es zahlreiche Maßnahmen dessozialen Wohnungsbaus, und für sozial schwache Menschen und Familien verwaltet die städtische WohnungsbaugesellschaftNYCHAinsgesamt 178.000 Sozialwohnungen, in denen rund 400.000 Menschen leben.[32] In einer Rangliste der Städte nach ihrerLebensqualitätbelegte New York City im Jahre 2018 den 45. Platz unter 231 untersuchten Städten weltweit.[33] Im Jahr 2019 lebten in der Stadt New York 8.336.817 Einwohner.[34]Die Bevölkerungsdichte beträgt 10.356 Einwohner je km². DieAgglomerationNew York–Newarkweist 18.351.295 Einwohner bei einer Bevölkerungsdichte von 5.319 Einwohnern je km² auf (Stand: 2010).[35]In derMetropolregionNew York–Northern New Jersey–Long Islandleben 18.897.109 Menschen (1. April 2010),[7]was bei einer Fläche von 17.405 km² einer Bevölkerungsdichte von 1.086 Einwohnern je km² entspricht. DieNew York–Newark–Bridgeport Combined Statistical Areahat sogar 22.085.649 Einwohner (1. April 2010)[36]und damit eine Bevölkerungsdichte von 720 Einwohnern je km². Die folgenden Übersichten zeigen die Einwohnerzahlen nach dem jeweiligen Gebietsstand. Bis 1775 handelt es sich um Schätzungen, von 1790 bis 2010 um Volkszählungsergebnisse desUnited States Census Bureau. Die Zusammensetzung der Bevölkerung New Yorks ist sehr heterogen und spiegelt alle Einwanderungswellen der amerikanischen Geschichte wider. Die größte Gruppe sind mit 35 % nicht-hispanische Weiße (Kaukasier). Zwei von fünf Einwohnern sind nicht in den Vereinigten Staaten geboren worden; die meisten der europäischstämmigen Einwanderer stammen ausItalien(8 %),Irland(5 %) undDeutschland(3 %). Etwa 972.000 Einwohner sind dabeijüdischen Glaubensoderjüdischer Abstammung, die somit 12 % der Gesamtbevölkerung ausmachen. In Manhattan beträgt der Anteil sogar etwa 20 %. New York City ist damit die Stadt mit der größten jüdischen Gemeinde der Welt, da hier mehr Juden leben, als ganzJerusalemEinwohner hat. Der Anteil der nicht-hispanischen Schwarzen bzw.Afroamerikanerbeträgt 24 % und ist ebenso wie der der Weißen entsprechend dem gesamtamerikanischen Trend rückläufig. Hingegen bildenHispanics(Latinos) mit insgesamt 27 % inzwischen die zweitgrößte Bevölkerungsgruppe. DieAsiatensind die am stärksten wachsende Gruppe und machen mittlerweile 12 % der Bevölkerung New Yorks aus. Afroamerikaner, Weiße, Asiaten und Hispanics konzentrieren sich jeweils tendenziell in bestimmte Stadtquartiere. So ist Manhattan (Downtown und Midtown bis zum Central Park) mit Ausnahme von Chinatown überwiegend ein Wohngebiet der Weißen, im nordwestlichen Brooklyn leben überwiegend Schwarze, ebenso wie in den Straßenzügen nördlich des Central Parks (Harlem); in den nordwestlichen Bezirken von Queens, südlich von La Guardia, siedeln überwiegend Latinos und vereinzelt Asiaten; in der Bronx wohnen kaum Weiße, sondern mehrheitlich Hispanics und zum Teil Schwarze und im Süden von Brooklyn (Coney Island) leben insbesondere russischstämmige Menschen.[41]In den letzten Jahren hat es hier allerdings teilweise gegenläufige Tendenzen gegeben; so beträgt der Anteil der Afroamerikaner unter den Einwohnern von Harlem inzwischen weniger als 50 %, da Latinos und Weiße in das lange Zeit fast ausschließlich von Schwarzen bewohnte Viertel gezogen sind. 40,6 % der Einwohner sind nicht in den Vereinigten Staaten geboren, davon 19,2 % in Lateinamerika (ohne Puerto Rico), 9,4 % in Asien und 6,4 % in Europa. Die beiden folgenden Tabellen stellen die statistischen Daten zu den Bevölkerungsgruppen (raceundethnicity) der Einwohner New Yorks im Einzelnen dar. Bei allen Daten handelt es sich um Eigenangaben der Befragten. 1) Die American Community Survey ist einerepräsentativeStudie desU.S. Census BureaumitstichprobenhafterBefragung der Bevölkerung, um aktuellere Daten zu gewinnen. Gut die Hälfte der Bevölkerung spricht nurEnglischzuhause, während fast ein Viertel der Einwohner über keine sehr guten Englischkenntnisse verfügt. Die zweitwichtigste Sprache istSpanisch, das von 24 % der New Yorker zuhause verwendet wird. Die übrigen Sprachen machen zusammen 23 % aus. 1) inkl.spanisches Kreolisch; 2) inkl.PatoisundCajun; 3) inkl.französisches Kreolisch Vor Ankunft der ersten europäischen Siedler war das Gebiet des heutigen New Yorks vonAlgonkin-Völkern bewohnt, unter diesen dieLenni Lenape, deren Heimat sich von Staten Island über Manhattan, die Bronx und den westlichen Teil Long Islands bis in das untere Hudsontal erstreckte.[42] Erste Entdeckungsreisen in das Gebiet des heutigen New York fanden 1524 durchGiovanni da Verrazzanound 1609 durchHenry Hudsonstatt. Ab 1610 begannen niederländische Kaufleute einen lukrativen Fellhandel mit den dort lebenden Indianern. Am 27. März 1614 erhielt die neu gegründeteCompagnie van Nieuwnederlant(Neuniederland-Kompanie) von denGeneralstaatenein Monopol für den Handel in dem Gebiet. Im Oktober 1618, zehn Monate nach Ablauf des Handelsmonopols, bewarb sich die Kompanie um eine neueCharta. Damals wurde aber von den Generalstaaten bereits die Gründung einer neuen Kompanie, derNiederländischen Westindien-Kompanie(WIC), erwogen. Am 3. Juni 1621 erhielt die WIC von derRepublik der Sieben Vereinigten Provinzeneinen Freibrief für den alleinigen Handel in Amerika. Die Kolonisation begann 1624, als sich 30 niederländische, wallonische und französische Familien[43]auf der Insel Manhattan und in der Gegend desDelawareansiedelten. Der Legende nach kaufte 1626Peter Minuitden Einheimischen, wahrscheinlich ein Zweig derLenni-Lenape-Indianer, die die Insel „Manna-hatta“ nannten, das Eiland für 60 Gulden ab.[13]Die neu gegründete Siedlung erhielt den NamenNieuw Amsterdamund wurde zur Hauptstadt der KolonieNieuw Nederland.[14]In der Siedlung herrschten bald chaotische Verhältnisse. Unter der Herrschaft von korrupten Gouverneuren nahm die Kriminalität enorm zu. 1647 entschloss sich die Niederländische Westindien-Kompanie, wieder für Ordnung zu sorgen. Diese Aufgabe solltePetrus Stuyvesantübernehmen. Während seiner 17-jährigen Amtszeit als Gouverneur wurden das erste Krankenhaus, das erste Gefängnis und die erste Schule gebaut. Als Schutz vor Überfällen ließ er 1652 im Norden der Stadt quer über die Insel eine Mauer errichten, die später der dort verlaufenden Straße ihren Namen geben sollte, derWall Street. Am 2. Februar 1653 erhielt Nieuw Amsterdam die Stadtrechte. Am 8. September 1664 ergab sich die Stadt kampflos einer vonRichard Nicollsgeführten Flotte derRoyal Navy.[14]Die Engländer benannten die Stadt und die KolonieNew Yorknach deren damaligem BefehlshaberJames,Herzog von York, dem BruderCharles’ II. von England, der später selbst König wurde. 1667 gaben die Niederländer alle Ansprüche auf die Kolonie imFrieden von Bredaauf, in dem ihnen dafür die Rechte anSurinamezugesichert wurden. Im folgenden Dritten Englisch-Niederländischen Krieg nahmen die Niederländer 1673 durchCornelis Evertsendie Kolonie kurzzeitig wieder ein, bevor sie das Land endgültig durch die Unterzeichnung desVertrags von Westminsteram 19. Februar 1674 abgeben mussten. Um 1700 war die Zahl derLenapein der New Yorker Bevölkerung auf 200 Personen geschrumpft.[44]1703 hatten 42 % der Haushalte Sklaven,[45]und etwa 40 % der Bevölkerung New Yorks waren Sklaven.[46]1712 kam es zu einemAufstand afrikanischer Sklaven, der schnell und äußerst brutal niedergeschlagen wurde. 1741 kam es zu mehreren Bränden, und es breitete sich eine Massenhysterie aus. Schwarze wurden beschuldigt, in einer Verschwörung mit armen Weißen die Stadt abbrennen zu wollen. 13 Schwarze wurden lebendig verbrannt und vier Weiße und 18 Schwarze wurden gehängt.[47] 1754 wurde dieColumbia Universityunter dem Namen King’s College von KönigGeorge II.inLower Manhattangegründet.[48]Erst nach der amerikanischen Revolution wurde sie umbenannt. DerStamp Actvon 1765 und andere Maßnahmen führten zu Unmut in der Bevölkerung, und dieSons of Libertylieferten sich zwischen 1766 und 1776 Gefechte mit den Briten. New York war aktiv an der Unabhängigkeitsbewegung beteiligt. 1776 war die Stadt für kurze Zeit dasHauptquartierGeorge Washingtons, wurde dann aber von denBritenbesetzt. Im gleichen Jahr zerstörte ein Feuer große Teile New Yorks. Die Britischen Besatzer verließen die Stadt erst 1783, nachdem die amerikanische Unabhängigkeit auch von europäischen Staaten einschließlich Großbritannien anerkannt wurde. 1785 verwüstete abermals eine Brandkatastrophe weitere Bereiche der Stadt. Von 1788 bis 1790 war New YorkHauptstadtderVereinigten Staaten. George Washington wurde 1789 auf dem Balkon derFederal Hallvon New York als erster Präsident vereidigt. In den wirtschaftlich schwierigen Zeiten nach dem Krieg gründeten Wertpapierhändler am 17. Mai 1792 dieNew York Stock Exchange. Am 13. April 1796 erreichteder erste Elefant in Amerikaden New Yorker Hafen. 1797 wurdeAlbanyanstelle von New York zur Hauptstadt des BundesstaatsNew Yorkbestimmt und ist es bis heute geblieben. Anfang des 19. Jahrhunderts wuchs die Stadt schneller als je zuvor. 1811 beschlossen die Stadtplaner mit demCommissioners’ Plan, die ganze Insel Manhattan, von der nur die Südspitze bebaut war, mit einem rasterförmigen Straßennetz zu überziehen. Bis auf wenige Ausnahmen – die bedeutendste ist derBroadway– wurde dieser Plan konsequent umgesetzt. Ein Wendepunkt in der Geschichte der Stadt war die Fertigstellung desEriekanals1825. Dieses Bauwerk war durch den Gouverneur von New York,DeWitt Clinton, gegen erhebliche politische Widerstände durchgesetzt worden. Durch ihn wurde eine Verbindung zwischen New York, denGroßen Seenund damit demMittleren Westengeschaffen. Schnell wurde die Stadt zum größten Warenumschlagsplatz an der amerikanischen Ostküste. Gleichzeitig veränderte sich das städtebauliche Bild. Bisher prägten sogenannte „Brownstone Houses“ (meist zwei- bis viergeschossige Häuser ausBacksteinen) das Stadtbild. Mit der Expansion und dem trotzdem knapp werdenden Wohnungsraum führte man große mehrstöckige Apartmenthäuser ein. Diese opulent gestalteten Wohnhäuser, wie man sie noch heute unter anderem in derUpper West Sideam Broadway Ecke70thbis74th Streetsehen kann, verfügten über luxuriöse Ausstattung, bis hin zu zentralen Dinnerräumen, in denen sich die Bewohner auch zu gesellschaftlichen Anlässen trafen. Um das Vertrauen in den neuen Standort zu stärken, siedelten die Städteplaner bewusst beeindruckende Institutionen in den neuen Stadträumen an, von denen heute viele zu denNational Historic Landmarksvon New York gehören. Ein Beispiel dafür ist das monumentale Bankgebäude Broadway Ecke73rd Streetoder dieCarnegie Hallan der Ecke57th Streetund7th Avenue. Durch das große Bevölkerungswachstum, die Armut und das Fehlen einer Kanalisation in den Slums wurden jedoch trotz des StadtaufstiegsEpidemienbegünstigt. So kam es 1832 und 1849 zuCholera-Epidemien, 1837 verbreitete sichTyphusund 1842Fleckfieber.[49][50]DieWirtschaftskrise von 1837hatte verheerende Wirkungen, ein Drittel der Arbeiter und zehn Prozent der Gesamtbevölkerung waren zu dieser Zeit arbeitslos, und es kam mehrfach zu Unruhen.[51] Mitte des 19. Jahrhunderts begannen die Planungen für einen großen Stadtpark, den sogenanntenCentral Park. Die Bauarbeiten dafür begannen 1858 und waren 1866 größtenteils abgeschlossen. In der zweiten Hälfte des 19. Jahrhunderts nahm der Strom der Einwanderer stetig zu.Iren,Italiener,Deutsche, alle kamen in der Hoffnung auf ein besseres Leben, doch die meisten verbrachten viele Jahre inSlumswieFive PointsundBowery. Konflikte entluden sich teilweise gewaltsam wie in denDraft Riots, die die Stadt in das größte Chaos ihrer Geschichte stürzten. Wegen des starken Wachstums musste die Verwaltung geändert werden. 1898 schlossen sich die heutigen fünf Stadtbezirke Manhattan, Brooklyn, Richmond (heute Staten Island), Queens und Bronx zuGreater New Yorkzusammen. Teile der Bronx gehörten schon ab 1874 zum Stadtgebiet von New York. Brooklyn war vor dem Zusammenschluss bereits eine der größten Städte des Landes. Bis heute haben sich die einzelnen Stadtteile innerhalb der Stadtbezirke sowohl in der Verwaltung als auch im Selbstverständnis ihrer Bewohner eine gewisse Eigenständigkeit bewahrt. So ist beispielsweiseLittle Italydie Bezeichnung für ein Stadtviertel, das vor allem von italienischstämmigen Amerikanern bewohnt wird. In der ersten Hälfte des 20. Jahrhunderts wurde die Stadt zu einem Zentrum der Industrie und des Handels. In den „wilden Zwanzigern“ geriet New York in einen Börsenrausch, der am 24. Oktober 1929, demSchwarzen Donnerstag, ein jähes Ende fand. Die Wirtschaftskrise traf New York hart, und die unfähige,korrupteStadtregierung von BürgermeisterJimmy Walkerwar völlig überfordert und die Stadt überschuldet. DieArbeitslosenquotestieg auf über 25 %; die Menschen verloren neben der Arbeit auch ihre Wohnungen. Die Wende kam durch die von BürgermeisterFiorello LaGuardianach seiner Wahl 1933 aufgelegten Hilfs- und Bauprogramme. Anfang des 20. Jahrhunderts entstanden auch die erstenWolkenkratzer, zum Beispiel dasWoolworth Building(1913), dasChrysler Building(1930) und dasEmpire State Building(1931), die zu Wahrzeichen der Stadt wurden. Nach Kriegseintritt der USA in denZweiten Weltkriegwar New York ab September 1942 Ausgangspunkt der wichtigen SC- undHX-GeleitzügenachGroßbritannien. Die beiden wichtigen transatlantischen Konvoiserien bestanden bis Mai 1945. Nach dem Krieg ging es nach einer kurzen optimistischen Phase schnell bergab. DieMittelschichtzog in die Vororte, und die Industrie wanderte ab. Wie viele andere US-Städte litt auch New York in den 1960er Jahren unter Rassenunruhen. Durch neue Wellen von schlecht ausgebildeten Einwanderern (vor allem ausPuerto Rico) und durch die Abwanderung der traditionellen Industrie stieg die Zahl der von der Sozialfürsorge abhängigen Bevölkerung drastisch. Durch die seit den 1950er Jahren andauernde Abwanderung der Mittelschicht hatte die Stadtverwaltung immer weniger Steuermittel für immer umfangreichere Ausgaben zur Verfügung. Infrastrukturen wie die U-Bahn verfielen, die Stadt konnte elementaren Aufgaben wie Schneeräumung teilweise nicht mehr nachkommen. Von den 70er bis in die 90er Jahre herrschte in New York hohe Arbeitslosigkeit (8 %), boomender Handel mit billigen Drogen und hohe Kriminalität mit einer extrem hohen Mordrate. Wegen Geldknappheit der Stadt wurden 50.000 städtische Mitarbeiter entlassen, darunter auch viele Polizisten. 1975 musste die Stadt ihrenBankrottauf der Grundlage vonChapter 9erklären. Die Menschen flohen vor der Kriminalität aus der Stadt, Ende der 70er Jahre waren es fast eine Million. 1964, im Jahr derWeltausstellungin New York,[52]hatte die Mordrate bei 550 Toten gelegen, 1972 verdreifachte sich der Wert auf 1691 Morde. 1990 wurde die höchste Mordrate mit 2245 Toten gezählt. Durch Verstärkung der Polizeikräfte sank 1998 die Mordrate auf 630 Tote, 2017 und 2018 halbierte sich der Wert auf unter 300 Morde.[53]Am 13. Juli 1977 gab es einen 25-stündigenStromausfall, wobei es zu Plünderungen und Bränden kam. Dieser Stromausfall machte den heruntergekommenen Zustand der Stadt für alle deutlich sichtbar.[54] Der relativ günstig zu habende Wohnraum (dazu wurden oftmalsleerstehende Industriebautenadaptiert) in Vierteln wie derLower East SideundBrooklynzog von Mitte der 1970er bis weit in die 1980er Jahre eine neue Generation von Künstlern, Musikern undIndependent-Filmemachernan. In dieser Zeit entstanden völlig neue Musikrichtungen wieHip-HopoderNo Wave, so unterschiedliche Künstler wieJim Jarmusch,MadonnaoderKeith Haringstarteten hier ihre Karrieren. Ed Kochsanierte während seiner Amtszeit als Bürgermeister (1978–1989) das städtische Budget. Im Wirtschaftsaufschwung der 1980er Jahre nahm die Wall Street wieder eine führende Rolle in der Finanzwelt ein. In den 1990er Jahren erzielte der populäre BürgermeisterRudy Giulianimit der sogenanntenNulltoleranzstrategieerheblichen Erfolg, die Kriminalitätsrate fiel drastisch, auch unter Beteiligung der zivilen SchutzorganisationGuardian Angels. Insgesamt ging die Zahl der Verbrechen zwischen 1990 und 2007 um 77 % zurück,[55]so dass New York City nicht einmal mehr zu den 200 amerikanischen Städten mit der höchsten Kriminalitätsrate zählte. Kritiker wandten allerdings ein, dass Giuliani das Verbrechen nicht beseitigt, sondern lediglich ins Umland abgedrängt habe. Aufgrund der massiv verbesserten Sicherheit und der wirtschaftlichen Erholung wurde es wieder modern, in New York zu leben. Dadurch stieg allein in den 1990er Jahren die Wohnbevölkerung von 7,3 Millionen auf gut 8 Millionen an. Am11. September 2001wurde dasWorld Trade Center(WTC) durch einenAnschlagdes islamistischen TerrornetzwerkesAl-Quaidazerstört. In einer konzertierten Aktion lenkten Terroristen zwei von insgesamt vier entführten Passagierflugzeuge in die Zwillingstürme, die in der Folge kollabierten. Der Anschlag forderte insgesamt 2997 Todesopfer und werden alsZäsurin der Geschichte der Auseinandersetzung zwischen derwestlichenund derarabischen Weltgedeutet. Es dauerte bis Mai 2002, bis die Aufräumarbeiten am sogenanntenGround Zero, dem Areal, auf dem das WTC gestanden hatte, beendet werden konnten. Das World Trade Center war bereits 1993 durch einenBombenanschlag in der Tiefgarage des Komplexeserschüttert worden (mit sechs Toten und hunderten Verletzten). AufGround Zerowurde von 2006 bis 2014 dasOne World Trade Centergebaut, das nun mit 541 Metern das höchste Bauwerk der USA ist. Ebenfalls wurde eineGedenkstätteerrichtet, die im Mai 2014 eröffnet wurde.[56]An den letzten Gebäuderesten des WTCs vorbei können die Besucher in ein unterirdisches Museum gelangen, das an die Opfer und die Zerstörung erinnern soll. Ein neuerBahnhof am World Trade Centerwurde im Februar 2014 teileröffnet. Am 3. März 2016 wurde dieOculusgenannte Haupthalle des Bahnhofs eröffnet. Seit den Anschlägen gilt für ganz New York permanent eine erhöhte Alarmbereitschaft; die Polizeipräsenz wurde noch einmal stark erhöht. Die Lebenshaltungskosten in Manhattan sind seit 2001 massiv gestiegen; sie gelten als die mit weitem Abstand höchsten in den USA. Ende des Jahres 2002 kamen mehr Touristen nach New York als in den Jahren vor den Terroranschlägen. DerParteitagderRepublikanischen Parteifand 2004 imMadison Square Gardenin New York statt. Ende Oktober 2012 richtete derHurrikan Sandyin New York erhebliche Schäden an. Im Stadtgebiet wurden 47 Personen durch die Auswirkungen des Sturms getötet.[57]Weil die Sturmflut mit einerSpringflutzusammentraf, erreichte der Pegel imBattery Parkan der Südspitze Manhattans einen neuen Höchststand, der die bisherige Höchstmarke beim Durchzug vonHurrikan Donna1960 übertraf. In der Folge wurden zum ersten Mal seit über 100 Jahren mehrere Tunnel derNew York City Subwayüberflutet. DerHudson Rivertrat vorübergehend über die Ufer und überschwemmte etliche Straßen.[58]DieNew York Stock Exchangeblieb erstmals seit 1888 (Großer Schneesturm) zwei Tage in Folge wegen eines Unwetters geschlossen.[59] An der Spitze der Stadtverwaltung von New York steht der Bürgermeister(Mayor), der von der Bevölkerung für eine Amtszeit von vier Jahren gewählt wird. Er ernennt ihm verantwortliche Commissioners als Leiter der Verwaltungsteile (Beigeordnete für dieDepartements). DerRat der Stadt New York(Council)besteht aus 51 Mitgliedern und wird ebenfalls alle vier Jahre gewählt. Sowohl der Bürgermeister, als auch die Mitglieder des Councils können nur für drei aufeinanderfolgende Amtszeiten gewählt werden, sie können sich dann nach vier Jahren aber erneut zur Wahl stellen. New York City hat eigene Zuständigkeiten für Schulen, Strafvollzug, Bibliotheken, öffentliche Sicherheit, Erholungsangebote, Wasserver- und -entsorgung und öffentliche Wohlfahrt.[60] Seit dem 1. Januar 2022 istEric Adams(Demokrat) neuer Bürgermeister von New York. Er folgte aufBill de Blasio, der die Geschicke der Stadt von 2014 bis 2021 gelenkt hat.[61]de Blasio wurde am 5. November 2013 gewählt.[62]Nach mehr als zwei Jahrzehnten stand damit wieder ein Demokrat an der Spitze der New Yorker Stadtverwaltung. Bill de Blasio lösteMichael Bloombergnach zwölfjähriger Amtszeit ab. Vom 1. Januar 2002 bis 31. Dezember 2013 war Bloomberg 108. Bürgermeister von New York. Er ist bekannt als Gründer des Informationsdienstleistungs-, Nachrichten- und MedienunternehmensBloomberg L.P.mit Hauptsitz in New York. Im Jahre 2001 gewann er die Bürgermeisterwahl und trat die Nachfolge vonRudy Giulianian. Um nicht in den Vorwahlen antreten zu müssen, wechselte derDemokratBloomberg vor der Wahl die Partei und trat denRepublikanernbei; ab 2007 war er parteilos. 2005 wurde er mit 58,5 % der abgegebenen Stimmen wiedergewählt. Sein HerausfordererFernando Ferrervon der Demokratischen Partei erhielt 38,7 %. 2009 wurde Bloomberg erneut mit 50,6 % wiedergewählt, während Ferrer auf 46 % der Stimmen kam. Bloombergs Vorgänger Rudy Giuliani war vom 1. Januar 1994 bis 31. Dezember 2001 der 107. Bürgermeister von New York und wurde vor allem als rigoroser Verbrechensbekämpfer und durch sein Auftreten nach den Terroranschlägen am 11. September 2001 auf das World Trade Center populär. Im Rennen um die Nachfolge des scheidenden StadtoberhauptsEd Kochunterlag Giuliani 1989 als Kandidat der Republikanischen Partei und derLiberalen Parteinoch dem DemokratenDavid Dinkins, der als erster Schwarzer New Yorker Bürgermeister wurde. Fünf Jahre später kandidierte Giuliani erfolgreich gegen Amtsinhaber Dinkins in einer Wahl, die die Stadt nach ihren ethnischen Gruppierungen teilte. Giuliani profitierte dabei auch von der Unterstützung des damals gerade sehr unpopulären US-PräsidentenBill Clintonzu Gunsten des Demokraten Dinkins. 1997 wurde Giuliani von den Wählern mit großer Mehrheit im Amt bestätigt. In seiner ersten Amtsperiode (1994–1997) ging Giuliani das Problem der Kriminalität durch eine strikte „Law and Order“-Politik an. Er führte eine sehr offensive Polizeiüberwachung ein, die zu einer starken Abnahme der Fallzahlen in fast allen Verbrechenskategorien führte. Giuliani setzte in New York die sogenannteNulltoleranzstrategieum. Durch diese Politik wandelte sich das Bild New Yorks in den Augen der Touristen und der Bürger. Der Eindruck einer verbrechensgeplagten, verarmtenMetropole, wie er in den beiden Jahrzehnten vor seiner Amtszeit existierte, wich dem einer pulsierenden, sicheren Weltstadt. Es gab wenigerGraffitiin den U-Bahnen und einen Wirtschaftsaufschwung, weil sich die Menschen auch nachts im Freien wieder sicher fühlten. Kritiker bemängelten allerdings, dass die verstärkte Polizeipräsenz in NYC nicht nur zu einem Abdrängen der Kriminalität ins bis dahin sichere Umland, sondern auch zu mehr Misstrauen bei den Schwarzen und anderen Minderheiten gegenüber den Ordnungshütern geführt habe. Zu den bekannten Zwischenfällen mit Polizeibrutalität während Giulianis Amtszeit zählen der Tod des unbewaffnetenAmadou Diallound die Gewaltanwendung in Polizeigewahrsam gegenüber Abner Louima. New York hat die folgendenStädtepartnerschaften:[63] New York besitzt zahlreiche architektonische Sehenswürdigkeiten, 500 Galerien, etwa 200 Museen, mehr als 150 Theater, zahlreiche Kaufhäuser und über 18.000 Restaurants. Die Stadt gilt als kulturelles und künstlerisches Zentrum derOstküste der USAund darüber hinaus in diesen Bereichen als weltweit prägend. Viele kulturelle Sehenswürdigkeiten befinden sich insbesondere in Manhattan. Hier bestimmen vor allem in Downtown und Midtown Wolkenkratzer das Stadtbild. Das im Jahr 1902 eröffnete dreieckigeFlatiron Buildingwar der erste Wolkenkratzer der Stadt. Weitere sind unter anderem dasWoolworth Buildingvon 1913, das im Art-déco-Stil erbauteChrysler Buildingvon 1930 und der im Jahr 1939 fertiggestellte Gebäudekomplex desRockefeller Centermit demNBC-Studio. Zu den meistbesuchten Monumenten der Welt zählt dasEmpire State Buildingaus dem Jahr 1931. Etwa 3,5 Millionen Besucher blicken jedes Jahr von dessen Aussichtsplattform auf New York hinunter. Das Stadtbild zeichnen auch viele Kirchen, zum Beispiel die im Jahr 1879 fertiggestellteSaint Patrick’s Cathedral, dieCathedral Church of Saint John the Divine(Baubeginn 1892), dieTrinity Churcham Ende der Wall Street, die „United Synagogue of Conservative Judaism“ und die „Armenian Apostolic Church of America Eastern Prelacy“. Das Wahrzeichen New Yorks ist dieFreiheitsstatueaus dem Jahr 1886 aufLiberty Island, südlich von Manhattan. Weitere bedeutende Bauwerke sind der wichtigste Bahnhof von New York, der im Jahr 1913 eröffneteGrand Central Terminal, dieBrooklyn Bridge, die Manhattan und Brooklyn verbindet, die ArenaMadison Square Gardenund dieCarnegie Hall, eine Konzerthalle. AmEast Riverin Manhattan liegt der Gebäudekomplex derVereinten Nationen. Zu den touristischen Attraktionen zählen weiterhinBrooklyn Heights, ein altes Wohnviertel, sowieGreenwich Villagemit seinem Triumphbogen und demJefferson Market Courthouse, in dem sich eine Bibliothek befindet,Ground Zero, der Platz an dem das World Trade Center stand – es war bei seiner Fertigstellung im Jahr 1973 mit 417 Metern das höchste Gebäude der Welt – undEllis Island, die ehemaligeEinwanderer-Aufnahmestelle imHudson River. Bekannte Hotels sind unter anderem das „Regent“ in der Wall Street (eröffnet 1842), das „Peninsula“ an der Ecke5th Avenueund55th Street(eröffnet 1900), „The Plaza“ an der Ecke5th AvenueundCentral Park(eröffnet 1907 und mittlerweile in Eigentumswohnungen umgewandelt), dasWaldorf Astoriain derPark Avenue(eröffnet 1930), das „Carlyle“ in der76th Street(eröffnet 1931) und das „Four Seasons“ in der57th Street(eröffnet 1993). DerNational Park Serviceweist für New York City 116National Historic Landmarksaus.[69]923 Bauwerke und Stätten der Stadt sind imNational Register of Historic Places(NRHP) eingetragen (Stand 19. November 2018).[70] Das Zentrum des Theaterlebens in New York ist derBroadwaymit Musicalaufführungen für jeden Geschmack. Unter dem Broadway versteht man das Theaterviertel amTimes Squarezwischen der41st Streetund53rd Streetund zwischen derSixthundNinth Avenue. In diesem Viertel gibt es etwa 40 große Theater und ungefähr 1500 sogenannte „Off-Broadway“ und „Off-off-Broadway“-Aufführungen pro Jahr in kleineren Theatern. Das erste Theater, welches vom vorherigen Theaterviertel amHerald Squarezum Times Square umzog, war dasEmpire Theater. Charles Frohman ließ im Jahr 1893 das Gebäude mit etwa 1000 Sitzplätzen erbauen. Es lag direkt gegenüber dem im Jahr 1883 eröffneten Opernhaus der „Metropolitan Opera“, an dessen altem Standort zwischen der39thund40th Street– EckeBroadway. Der Broadway erlebte zwischen den Jahren 1910 und 1930 eine Vielzahl von Theater-Neugründungen. Das älteste ununterbrochen bespielte Theater aus dieser Zeit ist das am 2. November 1903 eröffnete „Lyceum“. Es hieß ursprünglich: „New Lyceum Theatre“, da das vorherige Lyceum Theater aus dem Jahr 1885 in der45th Street, Ecke Broadway, wegen des Neubaus abgerissen worden war. Es war auch das erste Theater, das elektrisches Licht im gesamten Gebäude besaß, eingebaut vonThomas Alva Edison. In den 1980er Jahren gab es ein „Theatersterben“ am Broadway, aber durch städtebauliche Maßnahmen, Einsparmaßnahmen und die Verpflichtung großer Namen ist die Krise heute (2008) überwunden. Im Jahr 1988 entschied die New York Landmarks Preservation Commission, den meisten historischen Theatergebäuden den Statushistoric siteszu verleihen. Große Broadwayerfolge warenCats,42nd Street,Les Misérables,The Lion KingundThe Producers. In der Nähe desCentral Parksliegt unter anderem das „Metropolitan Opera House“, kurzMET, das Zuhause der Metropolitan Opera Company und desAmerican Ballet Theatre. Es gehört zum Komplex des „Lincoln Centerfor the Performing Arts“, wie auch unter anderem die „Avery Fisher Hall“, Spielstätte des New York Philharmonic Orchestra, das „New York State Theater“, welches Aufführungen derNew York City Operaund desNew York City Balletszeigt, die „Carnegie Hall“ und die „Juilliard School“. Die „Radio City Music Hall“ wartet mit wechselnden Veranstaltungen auf. New York ist nebenLos Angeles,New OrleansundChicagoeines der bedeutendsten Zentren der Musik in den Vereinigten Staaten. Neben zahlreichen Aufführungsorten für klassische Musik u. a. in derMetropolitan Operabefinden sich dort zahlloseJazz- undRockclubs. New York ist seit den späten 1920er Jahren immer wieder Ausgangspunkt und Zentrum verschiedenster musikalischer Entwicklungen gewesen. Mit dem vor allem von Afroamerikanern bewohnten Stadtteil Harlem ist New York seit den 1920er Jahren ein Brennpunkt afroamerikanischer Kultur. Nicht nur derSwing,BebopundFree Jazzhatten hier ihren Ausgangspunkt. AuchDisco,Punkrock, undNew Wavesind mit New York verbunden. Bis heute ist New York die unumstrittene „Hauptstadt“ des Jazz. Darüber hinaus hat dieHip-Hop-Kulturhier ihren Ursprung, die inzwischen zu einem der wichtigsten kulturellen Exportartikel des Landes geworden ist und im Jahr 2005 zum New Yorker Kulturerbe erklärt wurde. Zu den bekannten naturwissenschaftlichen, historischen und technischen Museen in New York gehören das „American Craft Museum“, das „Brooklyn Museum“ und das „Intrepid Sea-Air-Space Museum“. Das „American Museum of Natural History“ (an der EckeCentral Park Westund79th Street) ist eines der größten Naturkundemuseen der Welt. Auf fünf Stockwerken wird nahezu die komplette Geschichte der Menschheit abgedeckt, von derSteinzeitbis insWeltraumzeitalter. Die Sammlung ist derart umfangreich, dass ein Tag für den Besuch des Museums kaum ausreichend ist. Über 30 Millionen Muster und Exponate hält es für seine Besucher bereit. Dazu gehören der 563Karatschwere „Stern von Indien“, der größte jemals gefundeneSaphir, ein lebensgroßerBlauwal, das 19,2 Meter lange einstämmigeZedern-Kriegskanu derHaida-Indianerund viele Dinosaurierskelette, um nur eine kleine Auswahl zu nennen. Zum Komplex gehört auch das „Rose Center for Earth and Space“ mit dem „Hayden Planetarium“. Das „American Museum of Natural History“ verfügt über ein eigenesIMAX-Kino und eine eigeneU-Bahn-Haltestelle. Vor dem Haupteingang in Richtung Central Park befindet sich einTheodore-Roosevelt-Denkmal. Zu den bekannteren Museen für Bildende Kunst und Design gehören das „Metropolitan Museum of Art“ (Exponate vom altägyptischen Tempel bis zur modernen Kunst, eröffnet 1880), das „Museum of Modern Art“ (Museum für moderne Kunst, kurz:MoMA, eröffnet 1929) und seine AußenstelleP, S. 1.inQueens, die „Frick Collection“ (eröffnet 1935) und das „Whitney Museum of American Art“ (eröffnet 1966), des Weiteren eine Zweigstelle des „National Museum of the American Indian“,[71]das „Cooper-Hewitt National Design Museum“, das „American Museum of the Moving Image“, das „International Center of Photography“, die „Pierpont Morgan Library“. Das „MoMA“ gilt als das weltweit bedeutendste Museum für die Gegenwartskunst der westlichen Welt und wurde 2004 nach umfangreichen Erweiterungen in einem neuen Gebäude wiedereröffnet. Einige der oben genannten Museen bilden an der5th Avenuein derUpper East SidedieMuseum Mile. Der Spiralbau des „Solomon R. Guggenheim Museum“ (10715th Avenue) wurde nach Plänen des berühmten ArchitektenFrank Lloyd Wrightentworfen und ähnelt im Aufbau einem Schneckenhaus. Es ist Lloyd Wrights einziges Gebäude in New York, wurde im Jahre 1959 eröffnet und beherbergt eine Sammlung von Kunst des 20. Jahrhunderts sowie zeitgenössischerMalereiundBildhauerei, darunter Klassiker vonWassily Kandinsky,Pablo PicassoundPaul Klee. Nicht weit entfernt vonLiberty Islandmit derFreiheitsstatueliegtEllis Island, einst die erste Kontrollstelle für über zwölf Millionen Einwanderer. Unter dem NamenGibbert Islanddiente die Insel den Engländern als Straflager für Piraten. 1892 errichtete man dort einen Vorposten der Einwanderungsbehörde, der vor allem den gewaltigen Zustrom von Immigranten aus Süd- und Osteuropa bewältigen sollte. Als der Vorposten 1954 geschlossen wurde, stand auf der Insel, die durch Landaufschüttung vergrößert worden war, ein gewaltiger Komplex. In dem mit Türmen versehenen Zentralgebäude befindet sich heute das „Ellis Island Immigration Museum“. New York ist mit rund 5.800 Gebäuden mit mehr als zwölf Etagen eine Stadt der Hochhäuser undWolkenkratzer. Das fünfzigsthöchste Gebäude New Yorks misst über 200 Meter, womit es in Europa bereits zu den höchsten zählen würde. Die Zwillingstürme desWorld Trade Centerswaren bis zu ihrer Zerstörung bei denTerroranschlägen am 11. September 2001die höchsten Gebäude der Stadt, der etwas höhere Nordturm maß 417 Meter (mit Antenne 527 Meter) und der Südturm 415 Meter. DieFreiheitsstatue(Statue of Liberty), umgangssprachlich auchMiss LibertyoderLady Libertygenannt, wurde am 28. Oktober 1886 vor dem New Yorker Hafen zur Begrüßung von Heimkehrern und Neuankömmlingen eingeweiht. Sie ist ein GeschenkFrankreichsan die USA und sollte ursprünglich 1876 zur Hundertjahrfeier deramerikanischen Unabhängigkeitserklärungvon 1776 vollendet werden. Die Statue steht aufLiberty Islandund gehört zusammen mitEllis IslandzumStatue of Liberty National Monument. Von derUNESCOwurde sie zumWeltkulturerbeerklärt. Die Statue wurde vonFrédéric-Auguste Bartholdientworfen undGustave Eiffel, der Konstrukteur desEiffelturms, konzipierte das innere Eisenskelett. Sie hat eine Höhe von 46 Meter ohne Sockel, mit Sockel erreicht ihre Höhe 93 Meter. Ihr Äußeres besteht aus einem Kupfermantel, der von einem inneren Eisengerüst gestützt wird. Im Laufe der Zeit hat das Kupfer eine grünePatinaschichtgebildet. Der steinerne Sockel beherbergt ein Museum und wurde auf einem sternförmigenForterrichtet, das der ehemaligen Befestigungsanlage am gleichen Ort nachempfunden wurde. DieFreiheitsgöttinsteht mit einem Fuß auf zerbrochenen Ketten, die die Sklaverei symbolisieren. Sie hält in ihrer linken Hand eine Tafel mit der InschriftJULY IV MDCCLXXVI– 4. Juli 1776 –, das Datum der amerikanischen Unabhängigkeitserklärung. Mit ihrer rechten Hand hält sie eine Fackel mit einer goldbeschichteten Flamme in die Höhe. Die Statue ist mit einer siebenstrahligen Krone geschmückt, in der sich 25 Fenster befinden. Die sieben Strahlen symbolisieren diesieben Meereund Kontinente und die 25 Fenster symbolisieren die 25 Edelsteine der Welt. In das Podest ist das GedichtThe New ColossusvonEmma Lazarus, eine Anspielung auf denKoloss von Rhodos, eingraviert. Liberty Island („Freiheitsinsel“) ist eine kleine, unbewohnte Insel in derUpper New York Bay, der ursprüngliche Name lauteteBedloe’s Island. Am 3. August 1956 benannte Eisenhower die Insel um. Sie ist im Besitz der Bundesregierung der USA und wird vomNational Park Serviceverwaltet und gepflegt. Die Insel liegt 600 Meter entfernt vom Liberty State Park inJersey City, New Jersey. VomBattery Parkin Manhattan ist die Insel 2,6 Kilometer entfernt. Obwohl die Insel nicht Territorium von New Jersey ist, befindet sie sich auf der New Jersey zugewandten Seite der Grenzlinie zwischen den Bundesstaaten New Jersey und New York. Sie bildet daher eineExklaveNew Yorks in den Gewässern New Jerseys. Die Entfernung zur nahe gelegenen Insel Ellis Island, die einen ähnlichen Status hat, beträgt ungefähr 1,6 Kilometer. Das im Jahr 1902 fertiggestellte „Flatiron Building“ (1755th Avenue) an der Kreuzung vonBroadway,Fifth Avenueund23rd Streetwar mit 91 Metern Höhe zwar nie das höchste Gebäude der Stadt, doch war es von Anfang an ein Touristenziel. Es wurde nach Plänen des ArchitektenDaniel Burnhamgebaut. Der eigenwillige dreieckige Grundriss gab dem Gebäude den Namen „Flatiron Building“ (= „Bügeleisen-Gebäude“). Die Form des ältesten noch erhaltenenWolkenkratzersNew Yorks hat zur Folge, dass starke Winde auf der Straße entstehen. Auf Grund seiner Größe fällt das Gebäude heute nicht mehr auf, obwohl es nach seinem Bau mit den zwanzig Stockwerken alles Umliegende überragte. Bereits kurz nach seiner Fertigstellung diente das Gebäude als Motiv für eine der bekanntesten Aufnahmen des amerikanischen FotografenAlfred Stieglitz. Eines der markantesten Bauwerke der Stadt ist dasChrysler Buildingin derLexington Avenue. DasArt-déco-Gebäude besitzt eine glänzende, abgesetzte Turmspitze aus rostfreiem Stahl mit Bögen und Dreiecksfenstern und hat einschließlich der Turmspitze eine Höhe von 319 Metern. Bis zum Dach misst es 282 Meter. Es wurde 1930 vom ArchitektenWilliam Van Alenim Auftrag des AutomobilfabrikantenWalter Percy Chrysler(1875–1940) entworfen. Für ein Jahr war es das höchste Gebäude der Welt, dann wurde das Empire State Building fertiggestellt. DasEmpire State Buildingist ein Art-déco-Wolkenkratzer in derFifth Avenuein Midtown. Es ragt 381 Meter bis zum Dach, einschließlich der Antenne sogar 443 Meter, in den Himmel. Von seiner Fertigstellung 1931 bis 1972 war es das höchste Gebäude der Welt. Vom 11. September 2001 bis zumRichtfestdesOne World Trade Centermit 541 Metern Höhe am 10. Mai 2013 war es wieder dashöchste Gebäude in New York City. Seit der Fertigstellung haben etwa 120 Millionen Besucher das Panorama der Stadt von der Besucherplattform im 86. Stock aus besichtigt. Im 102. Stockwerk, der obersten Etage, ist eine weitere Aussichtsplattform. Diese ist jedoch im Inneren des Gebäudes gelegen. Zu besonderen Feiertagen und Anlässen erstrahlt die Turmspitze in verschiedenfarbigem Lichterglanz. Das Gebäude spielte auch in mehreren bekannten Kinofilmen eine bedeutende Rolle, darunterKing Kong und die weiße Frau,Independence DayundDie große Liebe meines Lebens. DasBrookfield Place(bis 2014World Financial Center) befindet sich inLower Manhattanim StadtteilBattery Park CityamHudson Riverin direkter Nachbarschaft zumWorld Trade Center. Entworfen wurde dieser Komplex von Cesar Pelli & Associates. In den vier Türmen habenAmerican Express,CIBC World Markets,Dow Jones,Merrill Lynchund weitere bedeutende Firmen ihren Hauptsitz. Den Mittelpunkt bildet dasWinter Garden Atrium, in dem hohePalmenaus der Sonora-Wüste inArizonastehen. Des Weiteren befinden sich im Komplex Restaurants, Geschäfte und eine Piazza, die eine Aussicht zum Yachthafen am Hudson River bietet. Die verschiedenen kulturellen Veranstaltungen und Vorführungen sind kostenlos. Der Wintergarten und andere Teile wurden durch den Terroranschlag auf das World Trade Center am 11. September 2001 stark in Mitleidenschaft gezogen. Die Reparaturarbeiten im Wintergarten sind im September 2002 abgeschlossen worden. 1933 fertiggestellt, befindet sich das Center in Midtown Manhattan zwischen der 48. und 51. Straße. 19 einzelne Gebäude bilden den Komplex. Unter anderem der US-amerikanische Fernsehsender NBC ist dort beheimatet.[72] Die Aussichtsplattform „Top of the Rock“ ist eine von aktuell 4 Outdoor-Aussichtsplattformen auf Manhattan (One World Trade Center,Empire State Building,The Edge). Der 2009 fertiggestellteBank of America Towerist das neunthöchste Gebäude. Der 366 Meter hohe Wolkenkratzer gehört zu einer neuen Generation Hochhäusern in New York, die sehr modern und umweltfreundlich gebaut sind. Dazu gehört auch derNew York Times Tower, neuer Hauptsitz derNew York Times, der 2007 fertig wurde und mit 319 Metern genau so hoch wie das Chrysler Building ist. Das höchste Gebäude der Vereinigten Staaten ist das am 3. November 2014 fertiggestellteOne World Trade Center(ehemals Freedom Tower) im neuenWorld Trade Center.[73]Mit 541 Metern überragt dieses Gebäude das Empire State Building weit, ebenso ist das One World Trade Center über 120 Meter höher als seine Vorgänger: DieZwillingstürmedes altenWorld Trade Center, gebaut von 1966 bis 1972/73, zerstört während derAnschläge vom 11. September 2001, waren 417 Meter hoch. DerHearst Tower, vonNorman Fosterentworfen, mit einer eigenwilligen Fassade ist das erste Hochhaus, bei dessen Bau besonders ökologische Gesichtspunkte im Vordergrund standen. Kennzeichnend amGebäude der New York Life Insurance Company(51Madison Avenue) ist das goldene Pyramidendach. Der ArchitektCass Gilbertentwarf dasneugotischeGebäude 1926 (Fertigstellung 1928). DasParamount Buildingin 1501 Broadway wurde 1927 vonRapp and Rappentworfen. Auffällig ist die nach oben gestufte Spitze, die eine Uhr und ein Globus krönen. Das im internationalen Stil errichteteSeagram Building(375Park Avenue) wurde 1958 eröffnet. Der Block aus bronzenem Glas und Stahl ist nach Plänen des deutschen ArchitektenMies van der Roheerrichtet worden. Das NobelrestaurantFour Seasonsbefand sich ebenfalls in diesem Gebäude. DasDeutsche Bank Center(früher Time Warner Center, 10Columbus Circle) besteht wie einst das World Trade Center aus Zwillingstürmen. Seit seiner Fertigstellung 2004 ist es der erste Gebäudekomplex, dessen Bauarbeiten nach den Terroranschlägen 2001 abgeschlossen wurden, und eines der modernsten Zentren. So befinden sich im Deutsche Bank CenterBüros, einEinkaufszentrum,Fernsehstudios, eineKonzerthalle, einTheatersowie einHotelund Eigentumswohnungen. Der MultimillionärDonald Trumpließ einen Büro- und Apartmentturm der Luxusklasse, denTrump Tower(7255th Avenue), im Stil der Postmoderne erbauen. Über sechs Stockwerke hoch erstreckt sich das prunkvolleAtrium. In den unteren Stockwerken des Gebäudes befinden sich Boutiquen, Juweliere und weitere exklusive Geschäfte für die gehobenen Ansprüche, in den oberen Etagen liegen Apartments. DasUN-Hauptquartierin der1st Avenuezwischen der42ndund48th Streetbesteht aus mehreren Gebäuden, Straßen und Parks. 1952 konnten die Vereinten Nationen ihren Hauptsitz beziehen, nachdemJohn D. Rockefeller II.(1874–1960) 8,5 Millionen US-Dollar für den Kauf des Grundstückes gespendet hatte und weil dieUSAzinsfrei Geld liehen. Die Verwaltung befindet sich im 39-stöckigen grünen Glashochhaus, und im geschwungenen General Assembly Building sind der Saal der Vollversammlung und der Besuchereingang zu finden. Seit 1931 zählt das im Art-déco-Stil errichteteWaldorf Astoria(301 Park Avenue) zu den großen Luxushotels in New York. In den Zwillingstürmen befinden sich pompöse Zimmer für Wohlhabende und berühmte Persönlichkeiten. Die Lobby des Gebäudes ist mitMarmorsäulen,BronzeundMahagoniausgestattet. DasWoolworth Building(322 Broadway) wurde 1913 vollendet und war bis 1930 das höchste Gebäude der Welt. Der vom Architekten Cass Gilbert entworfene, elegantegotischeWolkenkratzer wird von einem Pyramidendach gekrönt und ist mit vielen Tierornamenten verziert. Neben dem Hauptquartier der F.W. Woolworth Company befanden sich Geschäfte und ein Restaurant im Hochhaus.[74] In den 2010er Jahren veränderte sich Manhattan am südlichen Ende desCentral Parksmit dem Bau derBillionaires’ Rowoptisch deutlich. Dort entstanden unter anderem schlanke, mehr als 300 Meter hohe „Superwolkenkratzer“ wie das Gebäude432 Park Avenue. Unweit desTimes Squareist im Westen Manhattans mit denHudson Yardsein weiteres neues Bauensemble entwickelt worden. Dieses erstreckt sich zwischen 7th Avenue, 12th Avenue, West 43rd Street und West 30th Street. DieBrooklyn Bridgewurde 1883 vollendet und war zur damaligen Zeit die längsteHängebrückeder Welt. Sie überspannt denEast Riverund war die erste Brücke, dieManhattanmitBrooklynverband. Der IngenieurJohn Augustus Roebling(1806–1869) konstruierte dieses Wunder der Technik, erlebte jedoch die Fertigstellung nicht. Sein Sohn vollendete das Werk. Um zu prüfen, ob die Brücke große Gewichte tragen kann, wurde der ZirkusBarnummit zahlreichenElefantenhinübergeschickt. Vom höher gelegenen Gehweg sind die Skyline von Manhattan in der Ferne und die gotischen Bögen der Brückenpfeiler aus nächster Nähe zu sehen. Fast direkt unter der Brücke befindet sich das edle „River Café“ im Stadtteil Brooklyn. Im StadtviertelSoHo(South of Houston Street) befinden sich zahlreiche Galerien, Antiquitätengeschäfte, Cafés und Museen, zu denen die Außenstelle des „Guggenheim Museum“, das „New Museum of Contemporary Art“ und das „Museum of African Art“ gehören. Sehenswert sind die Lagerhäuser aus den Jahren 1860 bis 1890 mit ihren schön verzierten, gusseisernen Fassaden. Das Viertel „Cast-Iron Historic District“ wurde unter Denkmalschutz gestellt. Zahlreiche von diesen Gebäuden werden heute alsLoftsoder Ateliers genutzt. SoHo hat sich zu einem beliebten Bezirk entwickelt und beherbergt eine große Ansammlung an Gusseisenarchitektur. Das Gusseisen entdeckte man auf der Suche nach Möglichkeiten, Gebäude schnell und relativ kostengünstig zu bauen. So benutzte man anstelle von schwerem Mauerwerk Eisenträger zum Abstützen der Stockwerke und gewann dadurch Raum für größere Fenster und vor allem für Fassaden. Fast jede Idee eines Architekten ließ sich mit Gusseisen verwirklichen. Die Baumeister gestalteten SoHos Fabriken mit barocken Balustraden undRenaissance-Säulen. Auch dieReligionspielt in der Architektur New Yorks eine große Rolle. In Uptown befinden sich der „Biblische Garten“, verschiedeneSkulpturenund die noch unvollendeteCathedral of St. John the Divinemit ihrer Mischung aus romanischen und gotischen Stilelementen. Die Bauarbeiten an der Cathedral of Saint John the Divine begannen 1892, wurden bei Kriegsausbruch 1939 unterbrochen und Anfang der 1990er Jahre trotz zahlreicher Kontroversen und Geldsorgen sporadisch wiederaufgenommen. Erst zwei Drittel der Kathedrale sind fertiggestellt. Sollte sie jemals wie geplant fertiggestellt werden, wäre sie nach heutigem Stand die größte Kirche der Welt und groß genug, umNotre-Dame de Parisund dieKathedrale von Chartresin sich aufzunehmen. In Uptown steht auch dieRiverside Churchmit dem 120 Meter hohen Glockenturm. Die Architekten waren Charles Collens und Henry C. Pelton, der Bau dauerte von 1927 bis 1933 und Geldgeber warJohn D. Rockefeller II. In Manhattan steht die KircheSt. Thomas. Die Bauarbeiten an der neogotischenSt. Patrick’s Cathedralin derFifth Avenueund50th Streetwurden 1888 beendet. Ihr Erbauer James Renwick hat alle Details der gotischen Stilrichtung sorgfältig zusammengetragen. Am westlichen Ende der Wall Street, am Broadway zwischen Rector und Church Street, steht dieneugotischeTrinity Church, auf deren Friedhof viele bekannte Persönlichkeiten der US-Gründungsgeschichte ruhen. 1846 nach Plänen des Architekten Richard Upjohn errichtet, war sie 50 Jahre lang das höchste Bauwerk der Stadt. Die reformierteMiddle Collegiate Churchwurde 1892 im neugotischen Stil erbaut. Im Zentrum von Midtown wurde an der Ecke East42nd Street/Park Avenuezwischen 1903 und 1913 vom Architektenteam Warren & Wetmore ausMinnesotaderGrand Central Terminalerrichtet. Das Bauwerk verbindet einerseits die Romantik des Reisens und andererseits die Historie eines prächtigen Bahnhofsgebäudes aus der damaligen Zeit. Dank dem Einsatz berühmter Persönlichkeiten New Yorks wieJacqueline Kennedy Onassiswurde der Bahnhof vor dem Abriss gerettet und zu einem Wahrzeichen der Stadt ernannt. Nachdem der Grand Central Terminal lange nur zur Durchreise genutzt worden war, präsentiert er sich nach umfangreicher Renovierung in den 1990er Jahren heute mit vielen exklusiven Geschäften und Restaurants. Die Kosten für die Renovierung beliefen sich auf über 200 Millionen Dollar. Die zwölfstöckige Bahnhofshalle ist 142 Meter lang, 50 Meter breit und 46 Meter hoch. An der Gewölbedecke funkeln über 2.500 Sterne inTierkreiskonstellationen, und die riesigen Fenster sorgen für sonnendurchflutete Hallen. DieNew York Public Libraryist eine der führenden Bibliotheken der USA und eine der drei öffentlichen Bibliotheken in New York. Der Entwurf der New York Public Library in derFifth Avenue, zwischen der40thund42nd Street, erfolgte 1897 von den ArchitektenCarrère and Hastings. 1911 im Beaux-Arts-Stil errichtet, bietet die Bibliothek Platz für mehr als sieben Millionen Bücher und 10.000 Zeitschriften. Ihr Status als einer der weltweit führenden Bibliotheken wird belegt durch den Besitz von beispielsweise einerGutenberg-Bibel, einer Ausgabe derPhilosophiae Naturalis Principia Mathematicaund der handgeschriebenen Unabhängigkeitserklärung vonThomas Jefferson. Die ersten Bücher stammen vonJohann Jakob Astor(1763–1848). Das Luxus-ApartmenthausThe Dakota(auchDakota-Buildinggenannt) an der Ecke72nd StreetundCentral Park Westwurde vonHenry J. Hardenberghzwischen 1880 und 1884 entworfen und in einer damals noch ärmlichen Gegend gebaut. Es ist eines der wenigen noch erhaltenen Beispiele desneugotischen Baustilsin New York. Hardenbergh war auch der Architekt des berühmtenNew York Plaza Hotels. Bauherr des „Dakota“ warEdward Clark, der Inhaber desSinger Nähmaschinen-Konzerns, der jedoch die Fertigstellung nicht mehr erlebte. Der Name soll während der Bauphase entstanden sein, weil das Grundstück damals so weit außerhalb des bebauten Stadtgebietes lag, dass man scherzhaft behauptete, es befinde sich bereits im Stammesgebiet derDakota, eines Indianervolks, dessen Siedlungsgebiet im Norden der USA lag. Als Signum ist über dem Haupteingang die Natursteinplastik eines Indianerkopfes eingelassen. Insgesamt gibt es im Wohnhaus 65 Luxus-Suiten, in denen Künstler wieJudy Garland,Leonard Bernstein,Boris KarloffundJohn Lennongelebt haben. Das ehemalige Mitglied derBeatleswurde 1980 direkt vor dem „Dakota“ erschossen. Seine WitweYoko Onowohnt dort noch heute. Das Gebäude kann nur von außen besichtigt werden. Siehe auchListe der Parks in New York Cityfür eine Übersicht aller Parks in der Stadt. Der Central Park wurde 1853 als Landschaftspark eingerichtet und wird seitdem auch als Volkspark genutzt. Er erstreckt sich heute auf einer Länge von vier Kilometern von der59th Streetbis zur110th Streetund auf 750 Meter Breite zwischen derFifthund derEighth Avenueund wird auch diegrüne LungeNew Yorks genannt. Mit etwa 340Hektarnimmt er etwa fünf Prozent der Bodenfläche Manhattans ein. Er ist der größte Park der Stadt und einer der größten der Welt. In den vergangenen Jahrzehnten ist der Central Park zu einer imposanten Parkanlage angewachsen. ZahlreicheJoggerundInline-Skaternutzen die asphaltierten Straßen im Park zum Sporttreiben. Im Sommer gehen viele New Yorker an den Wochenenden in den Central Park, um sich bei einem ausgiebigenPicknickzu erholen. Im Süden des Parks befinden sich unter anderem ein Zoo und ein Baseballplatz und in der Mitte ein großer See, das „Jacqueline-Kennedy-Onassis-Reservoir“, sowie das „Metropolitan Museum of Art“. In den 1990er Jahren noch sollte das Gebiet oberhalb der110th Streetwegen seiner Nähe zum StadtteilHarlemund dessen ehemals hoher Kriminalitätsrate gemieden werden. Inzwischen zählt New York allerdings zu den sichersten Großstädten der USA. Der Battery Park ist eine Parkanlage auf der Südspitze Manhattans. Der am Battery Park gelegene Hafen dient als Ausgangspunkt für die Fähren nachEllis Island, zur Freiheitsstatue, nachStaten Islandsowie im Sommer auch nachGovernors Island. In der Parkanlage selbst befinden sich neben dem Denkmal des schwedischen Ingenieurs und ErfindersJohn Ericssonnoch zahlreiche weitere Denkmäler. Der Park hat seinen Namen von denGeschützen, die einst hier postiert waren, um den Hafen zu verteidigen. Durch Verlandung und Aufschüttung ist die heutige Küstenlinie jedoch etwas vorgerückt. Die Landaufschüttung entstand mit Hilfe des Erdaushubs beim Bau desWorld Trade Centers. Der Bronx Zoo entstand 1899. Mit mehr als 300 Hektar Fläche ist er der größte Zoo von New York und der größte in einer Stadt befindliche Zoo in den USA. Zum Zoo wird auch der New York Botanical Garden gezählt, der nördlich anschließt. Im Bronx Zoo findet man viele Tierarten, die in freier Wildbahn schon ausgestorben sind. Edgar Allan Poesletztes Wohnhaus ist heute eine liebevoll restaurierte und allgemein zugängliche Gedenkstätte. Er lebte hier im damaligen Vorort Fordham von 1846 bis 1849. Hier starb seine Frau und CousineVirginia Eliza Clemm Poe, das Vorbild seinerAnnabel Lee. Poe Cottage wurde 1913 um 140 Meter von seinem ursprünglichen Platz verschoben. Es liegt dem Poe Park gegenüber an der Kingsbridge Road in der Bronx. Das südlich vonManhattangelegene Governors Island wurde in den letzten Jahren vom National Park Service zu einem neuen Freizeitpark im Herzen New Yorks ausgebaut. Der historische nördliche Teil mit dem Castle Williams und dem Fort Jay ist bereits seit einigen Jahren in den Sommermonaten für die Öffentlichkeit zugänglich (die Fähre liegt unmittelbar neben der Staten Island Ferry). Von der Insel hat man einen guten Blick auf dieFreiheitsstatue, Ellis Island,New Jersey, die Südspitze Manhattans und die Hafenanlagen vonBrooklyn. Auf einer ehemaligenHochbahntrasseim Westen vonManhattanentsteht seit 2006 eine Parkanlage. Der erste Abschnitt wurde der Öffentlichkeit am 8. Juni 2009 zugänglich gemacht, der zweite folgte am 7. Juni 2011. Der letzte Teil zwischen30thund34th Streetbefindet sich noch in Planung. Die Stadtverwaltung investierte 50 Millionen US-Dollar in das Projekt; weitere Baukosten werden durch Spenden gedeckt.[75] Zu den bekannteren der zahlreichen professionellen Sportmannschaften in derNew York Metropolitan Areazählen unter anderem: Die 1901 gegründeten New York Yankees, derenYankee Stadiumim StadtteilBronxliegt, sind seit den 1920er Jahren der erfolgreichste Klub des amerikanischen Profisports. Sie gewannen dieWorld Series, die höchste Trophäe des amerikanischenBaseballs, bisher 27 Mal, zuletzt 2009. Damit hat der Klub die meisten Titel aller Mannschaften in den vier großen ProfiligenMLB,NFL,NBAundNHLgesammelt. Berühmt sind besonders die Meisterschaftswettbewerbe gegen die damaligenBrooklyn Dodgers, die heutigen San Francisco Giants und die New York Mets, dieSubway Seriesgenannt werden. In New York City fanden fünfSchachweltmeisterschaftenganz oder teilweise statt: Die erste offizielleWeltmeisterschaft 1886zwischenWilhelm SteinitzundJohannes Hermann Zukertortwurde am 11. Januar 1886 dort eröffnet, acht Jahre später verlor Steinitz seinen Titel1894gegenEmanuel Lasker, nachdem der erste Teil des Titelkampfes erneut in New York City ausgetragen worden war. Vor der Teilung der Schachwelt fand dieSchachweltmeisterschaft 1990zwischenGarri KasparowundAnatoli Karpowin New York City und Lyon statt. Fünf Jahre später duellierten sich Kasparow undViswanathan Anandim Südturm des World Trade Center1995. Im November 2016 war New York City Schauplatz derSchachweltmeisterschaft 2016zwischenMagnus CarlsenundSergei Karjakin. Das zu diesem Zweck errichteteNassau County International Cricket Stadiumin New York City war einer der Austragungsorte beimT20 World Cup 2024. Zu den vielen jährlichen Festivitäten in New York gehört daschinesische Neujahrsfest, es beginnt im Januar oder Februar und dauert zehn Tage.Saint Patrick’s Day, der irische Nationalfeiertag, wird jedes Jahr am 17. März mit einem großen Festumzug auf derFifth Avenuegefeiert.Christopher Street Day, die Festtage der Schwulen und Lesben, die hier ihren Ursprung haben, werden jeden Sommer in New York und rund um die Welt gefeiert, und dieSteubenparadezieht jedes Jahr am dritten Samstag im September durch die Fifth Avenue bis zumCentral Park. Weitere Veranstaltungen sind derColumbus Day, ein großer Festumzug jedes Jahr am zweiten Montag im Oktober, dieMacy’s Thanksgiving Day Parade, die seit 1924 jedes Jahr zehntausende Besucher anzieht und seit einigen Jahren im Fernsehen übertragen wird, dieUS OpenTennis Championships in Flushing Meadows, Queens, derNew-York-City-Marathonvon Staten Island bis zum Central Park und die Silvesternacht am Times Square. Die Vielzahl und Verschiedenheit seiner Bewohner spiegelt sich auch in der Küche New Yorks wider. Die Stadt besitzt etwa 17.300 Restaurants, die Speisen aus aller Welt anbieten. Die Restaurants der verschiedenen Bevölkerungsgruppen warten unter anderem mit italienischen,koscheren, asiatischen und indischen Speisen auf. Die sogenannten Delis sind eine New Yorker Institution und bieten ein reichhaltiges Angebot; ihr zentrales Element ist ein heißes und kaltesBuffet. Zu den typischen New Yorker Speisen gehörenBagels,Pancakes,Cheesecake(Käsekuchen),Waldorfsalat,Pizza,HotdogsundBurger, aber auchSoul FoodundSushi. Eine besondere Spezialität aus derjüdischenTradition ist das sogenannteKnisch(in etwa Kartoffeltasche), das an fast jedemDelierstanden werden kann. Einige international bekannte Köche und Gastronomen wieMario Batali,David Bouley,Daniel Boulud,Alain Ducasse,Thomas Keller,Nobu Matsuhisa,Danny Meyer,Masa TakayamaundJean-Georges Vongerichtenbetreiben Restaurants in New York. Der renommierte RestaurantführerGuide Michelin, von dem eine Ausgabe eigens für New York vorliegt, vergibt darin mehr Auszeichnungen als in jeder anderen getesteten Stadt außerParis. Die Rolle New Yorks als globales Finanzzentrum[76]wirkt sich auf die Unternehmen und Bewohner der Stadt und der Region aus. Bill Hyers, Berater des ehemaligen Bürgermeisters von New YorkBill de Blasio, sagte 2014, dass der wirtschaftliche Druck zu einer kollektiven Besorgnis führe, die in der ganzen Stadt beinahe wiePheromonezu riechen sei. „Es gibt 40 Milliardäre; 400.000 Millionäre. Das heißt, dass es acht Millionen Menschen gibt, die in dieser sehr teuren Stadt leben, arbeiten und überleben müssen.“[77] DieMetropolregionNew York ist einer der bedeutendsten Wirtschaftsräume der Erde und gehört mitLondonundTokiozu den größtenFinanzplätzender Welt. Zu den wichtigsten Wirtschaftsbereichen zählen unter anderem diechemischeund die elektrotechnische Industrie, dieDruckindustrie, dieTextilindustriesowie der Dienstleistungsbereich. Im Süden von Manhattan (Lower Manhattan,Downtown) liegt das Hauptgeschäftszentrum mit dem Finanzbezirk umWall Streetund Broad Street. Hier befindet sich auch dieNew York Stock Exchange(NYSE), die größte Wertpapierbörse der Welt. Ihr Grundstein wurde am 17. Mai 1792 gelegt, als 24Brokerund Händler das Buttonwood-Abkommen unterzeichneten. Damals wurden gerade einmal fünf Wertpapiere in New York gehandelt: dieAktienvon zwei Banken und drei Staatsanleihen, die im Jahr 1790 ausgegeben worden waren. Heute sind an der NYSE etwa 2.800 Firmen gelistet (460 davon haben ihren Sitz außerhalb der Vereinigten Staaten) und das Handelsvolumen beträgt im Durchschnitt etwa 45 MilliardenUS-Dollarpro Tag (Stand: November 2004). Weitere wichtige Börsen in New York City sindNASDAQ,New York Mercantile Exchange(größte Warenterminbörse der Welt),American Stock Exchangeund New York Board of Trade. ZahlreicheWeltkonzernehaben ihren Hauptsitz in New York. Dazu gehören unter anderem dieAltria Group(weltweit einer der größten Hersteller von Tabak, Nahrung und Getränken),American International Group(größter Versicherungs- und Finanzdienstleistungskonzern der Welt),Pfizer(weltweit zweitgrößtes Pharmaunternehmen),Sony Music(zweitgrößte Plattenfirma der Welt),Bristol-Myers Squibb(Pharmakonzern),Jetblue Airways(Fluggesellschaft),DC Comics(Comicverlag) undEstée Lauder(Kosmetikkonzern) sowieSteinway & Sons(Flügelfabrikant). Weiterhin ist die Stadt der Sitz vieler Anwaltskanzleien von internationalem Renommee. New York ist ein bedeutender Medienstandort. Die Stadt ist Sitz der globalen MedienkonzerneTime WarnerundViacom, mehrerer Großverlage, Musikfirmen, Produktionsstudios und der Zentralen oder Teilzentralen von vier großen amerikanischen Fernseh-, Film- und Radionetzwerken:ABC,CBS,NBCundFOX. In Brooklyn und der Bronx befinden sich ausgedehnteHafenanlagen, Lagerhäuser und Betriebe der verarbeitenden Industrie. Die Hafenanlagen(Waterfront)im Stadtteil Bronx haben eine Länge von etwa 130 Kilometern. Über den FlughafenJohn F. Kennedy Internationalim New Yorker StadtteilQueenswerden über 50 % der Luftfracht in andere Staaten abgewickelt, die FlughäfenLaGuardiaundNewark Liberty Internationaldienen überwiegend dem Inlandverkehr. Da der Hafen im BundesstaatNew Jerseyeinen großen Teil des Frachtverkehrs von New York übernommen hat, verliert der Passagier- und Frachtverkehr im gesamten Stadtgebiet immer mehr an Bedeutung. DerTourismusspielt für die Weltstadt gleichfalls eine sehr bedeutende Rolle. So registriert New York City jährlich fast 60 Millionen Gäste, davon über 12 Millionen aus dem Ausland (hauptsächlich aus Kanada und Großbritannien), darunter 632.000 Besucher aus Deutschland. Dabei profitiert New York City durch die Ausgaben der Urlauber in Höhe von rund 42 Milliarden US-Dollar (alle Tourismus-Angaben Stand 2015).[78] Im späten 19. Jahrhundert suchten die Stadtoberen nach einer Alternative, als Kohle- und Holzöfen die Luft verpesteten. Dies war der Beginn des Ausbaus des ab 1882 zwischenzeitlich vonConsolidated Edisonbetriebenen größten Fernwärmenetzes der USA. Für 80 Prozent der älteren Wohnhäuser Manhattans, als auch für Wolkenkratzer wie das Empire State Building, dient Dampf als Wärmequelle, der im Winter als Teil des Stadtbildes aus Schächten aufsteigen kann.[79]Auch das 2014 eröffnete One World Trade Center wird mit Dampf geheizt. Mit Dampf waren laut einem Bericht der NGO Urban Green Council 2023 rund 70 Prozent der Apartments in der Stadt chronisch überheizt, da die Regulierung der Heizwärme oft kaum möglich sei.[80]2022 wurde für die städtischen Wohnungen der Zubau von 30.000 Wärmepumpen angekündigt.[81] Der Hafen von New York, sowohl der Naturhafen in der Upper Bay als auch die Hafenanlagen rundum, nimmt eine große Fläche an der Ostküste ein. Er gehört nur zum Teil zum Stadtgebiet und liegt zum anderen Teil inNew Jersey. Die beiden Nachbarstaaten haben zur Koordinierung ihrer Interessen eine gemeinsame Hafenbehörde (Port Authority), diePANYNJ, und dieWaterfront Commissionof New York Harbor (WCNYH) gebildet. Nach dem Güterumschlag ist der Hafen der drittgrößte, nach seiner Fläche aber der größte Hafen der USA. Er dient als internationalerTiefsee- sowie über denHudsonauch alsBinnenhafenfür Teile der USA (Nordosten) und den OstenKanadas. New Yorks heutige wirtschaftliche Bedeutung hängt mit seiner Nutzung für den Warenumschlag und historisch mit demPersonentransportauf Schiffen zusammen. Mit der Fertigstellung desEriekanalswurde 1825 ein günstiger Transportweg zu denGroßen Seenerschlossen und New York stieg zum wichtigsten Handelshafen der Ostküste auf. Außerdem war und ist New York ein Hauptort derEinwanderungin die Vereinigten Staaten. Millionen von Einwanderern landeten hier und passierten seine Anlagen, von denenEllis Islandeine relativ neue ist. Nach der früher bedeutenderen Einwanderung dominiert heute derTourismusdie Personenschifffahrt im New Yorker Hafen. Der derzeit einzigeTransatlantiklinerim Liniendienst, dieQueen Mary 2, legt amBrooklyn Cruise Terminalan. New York ist durch zahlreicheAutobahnenmit dem Rest des Landes verbunden. Für die Anfahrt mit dem Auto gibt es verschiedene Möglichkeiten: DieRoute 495, die in New York zurInterstate 495wird, führt vonNew Jerseykommend durch denLincoln Tunnelund aus Richtung Osten durch den Queens-Midtown Tunnel nach Midtown Manhattan. Aus Richtung Südwesten führen dieInterstate HighwaysI-95(New Jersey Turnpike) und dieI-78durch denHolland Tunnelzur Canal Street beziehungsweise Spring Street in der Nähe vonSoHoundTribeca. Aus Richtung Norden erreichen derI-87(New York State Thruway) und der I-95 die Ringstraßen Manhattans. Besonders an den Tunneln und Brücken kommt es immer wieder zu Staus, auch weil dort in der RegelMautkassiert wird. New York war 2021 laut einer Studie die staureichste Stadt in den Vereinigten Staaten.[82] In New York befinden sich zweiFlughäfen:John F. Kennedy International Airportund derLaGuardia Airport. Die beiden Flughäfen liegen in Queens, wobei LaGuardia überwiegend für Inlandsflüge genutzt wird. Darüber hinaus gibt es noch denNewark Liberty International Airport, der zwar am nächsten an Manhattan liegt und der älteste Flughafen in der Region New York ist, sich jedoch inNewark,New Jerseybefindet. Die Flughäfen befördern pro Jahr insgesamt über 90 Millionen Fluggäste mit über einer Million Flügen. DerFlughafen Teterborowird durch seine Nähe zur Stadt bevorzugt für den Geschäftsreiseflugverkehr genutzt. In New York befinden sich drei wichtige Bahnhöfe für denZugverkehr:Grand Central Terminal,Grand Central MadisonundPennsylvania Station. Die beiden Erstgenannten liegen auf der East Side in Midtown, Penn Station dagegen auf der West Side. Alle drei Bahnhöfe besitzen Umsteigemöglichkeiten zu zahlreichen Bus- und U-Bahnlinien. Im Grand Central Terminal (oft auchGrand Central Stationgenannt) enden gegenwärtig diePendlerzügeder „Metro-North Commuter Railroad“ in RichtungWestchester County,Putnam County,Dutchess County,Fairfield CountyundNew Haven County. Der Grand Central Terminal wurde am 2. Februar 1913 als Kopfbahnhof eingeweiht und ist seitdem der größte Bahnhof der Welt – er verfügt über 44 Bahnsteige, an denen 67 Gleise enden. Der Bahnhof liegt auf zwei Ebenen, 41 Gleise enden auf der oberen, 26 auf der unteren Ebene. An der New York Pennsylvania Station, kurz „Penn Station“, halten die Fernzüge mehrerer Eisenbahngesellschaften, unter ihnen auch die Züge derAmtrak, und dient des Weiteren als Endstation für die Pendlerzüge derLong Island Rail Road(LIRR) vonLong Island. Die Penn Station ist wie die Überland-BusbahnhöfePort Authority Bus Terminalund „George Washington Bus Station“ ein Knotenpunkt für mehrere U-Bahnlinien. Der 43 Meter tief unter dem Grand Central Terminal gelegene und 2023 eröffnete eigenständigeTunnelbahnhofGrand Central Madison ist neben der Penn Station die zweite Endstation der nach Manhattan führenden Pendlerzüge der Long Island Rail Road. Der für die East Side von Manhattan zuständige Kopfbahnhof besitzt zwei Ebenen mit acht Gleisen. Die Straßen von New York haben eine Länge von 10.200 Kilometern. Das gitterförmige Straßennetz in Manhattan ist nummeriert und unterteilt in Ost und West (mit der Trennung an der5th Avenue). Die Ausnahme bildet hier das unterste Downtown, wo dieses Gittersystem endet. In Brooklyn und Queens gibt es ähnliche Gitternetze, die aber historisch bedingt eine eher unregelmäßige und von Brüchen gekennzeichnete Struktur aufweisen. Bezüglich der benutzten Verkehrsmittel ist New York eine sehr unamerikanische Stadt, denn die meisten Bewohner benutzen öffentliche Verkehrsmittel. Auf den Straßen Manhattans fahren nur etwa 50 Prozent Privatfahrzeuge – das Bild prägen vor allem die über 12.000 gelbenTaxis(„Yellow Cabs“). Grund sind unter anderem hohe Parkgebühren sowie hohe Gebühren für Brücken und Tunnel. DasNew Yorker U-Bahn-Netz, das in weiten Streckengebieten einen 24-Stunden-Service bietet, ist eines der größten der Welt. Der erste Tunnelabschnitt wurde am 28. Oktober 1904 von der Interborough Rapid Transit (IRT) eröffnet. Rund 6.000 Wagen der U-Bahn verkehren auf 27 Linien mit 476 Bahnhöfen. Das Netz hat eine Länge von 407,2 Kilometer – davon sind 393,3 Kilometer für den öffentlichen Verkehr bestimmt. 371,1 Kilometer werden von derNew York City Transit Authority(NYCTA) betrieben und 22,2 Kilometer von derPort Authority Trans-Hudson(PATH). Letztere verkehrt zwischen Manhattan und New Jersey. Die Subway befördert wochentäglich 4,5 Millionen und jährlich 1,5 Milliarden Fahrgäste. Täglich werden 7.400 Zugfahrten durchgeführt. Für den Pendlerverkehr in RichtungLong Islandist dieLong Island Rail Road(LIRR) zuständig. Die Vorortszüge vonNew Jersey Transittransportieren Pendler von New Jersey nach New York. AufStaten Islandverkehrt dieStaten Island Railway; die Insel ist durch die Verrazzano-Narrows-Brücke mit Brooklyn und durch dieStaten Island Ferrymit Manhattan verbunden. Zusätzlich existiert ein gut ausgebautesBusnetz. In der Stadt verkehren über 4.000 Busse auf 235 Linien mit über zwei Millionen Fahrgästen an Werktagen (jährlich 666 Millionen). Zwischen dem 8. Oktober 1921 und dem 26. Juli 1960 fuhrenTrolleybussein New York. Am 26. November 1832 eröffnete dieNew York and Harlem Railroaddie erstePferdestraßenbahnder Welt in der Stadt. Ab 1893 wurde dasStraßenbahnnetzteilweise elektrifiziert, das Verbot von Oberleitungen und die technisch aufwendigeren Unterleitungen sorgten dafür, dass New York einen der letzten Pferdebahnbetriebe der USA hatte. Der Großteil des Netzes wurde so schon vor 1922 eingestellt. 1956 fuhr über die George-Washington-Brücke die letzte Straßenbahn aus Jersey City, wo sie seit dem 15. April 2000 wieder verkehrt, nach Manhattan. Mit demInterborough Expresssoll ab der zweiten Hälfte der 2020er Jahre eine Stadtbahnstrecke entstehen, die in Brooklyn und Queens als Tangentialverbindung die U-Bahn ergänzen sowie schlecht abgedeckte Gebiete besser erschließen soll.[83]Zur Verbesserung der Anbindung Staten Islands gibt es eine Reihe von Vorschlägen, die nicht mit Nachdruck verfolgt werden, etwa eine Straßenbahnstrecke in Staten Island mit einer optionalen Verknüpfung mit derHudson-Bergen Light Rail.[84][85] In New York werden 29 ständige und saisonaleFährlinienbetrieben. Die kostenloseStaten Island Ferryverbindet Manhattan und Staten Island. Zusätzlich verkehren zahlreiche kommerzielle Fähren über den Hudson sowie verschiedene Fähren auf dem East River, die die Stadtbezirke Brooklyn, Queens, Bronx und Manhattan miteinander verbinden, darunter dieNYC Ferrymit sechs Routen. In New York City gibt es 2.027Brücken. Die längste Brücke ist die Verrazzano-Narrows-Brücke zwischen Staten Island und Brooklyn. Die Brücken nach Manhattan über den East River am Nordostufer heißen (von Nord nach Süd) Throgs Neck Bridge, Bronx Whitsstone Bridge, die Eisenbahnbrücke von der Randalls Wards bzw. Park Ave Third Ave Bridge, Willis Ave Bridge,Triborough Bridge,Queensboro Bridge,Williamsburg Bridge,Manhattan BridgeundBrooklyn Bridge. Am Westufer gibt es ganz im Norden dieBear Mountain Bridge, dann dieTappan Zee Bridge(Dewey Thruway) und im Zentrum dieGeorge Washington Bridgeüber den Hudson. Außer diesen Hauptbrücken von/nach Manhattan gibt es in ganz New York neben vielen normalen auch 25 bewegliche Brücken: zwei Einziehbrücken, sieben Schwenkbrücken, vier Hebebrücken und zwölf Zugbrücken. Die Brücken und Tunnel der Stadt werden von demNew York City Department of Transportationunterhalten, für einige Maut-Brücken und Tunnel ist dieMTA Bridges and Tunnelszuständig. In New York erscheint eine Vielzahl von Tages- und Wochenzeitungen. Zu den größten Tageszeitungen gehören dieNew York Times,Post,Daily Newsund dasWall Street Journal. DieTimeserscheint überregional. Sie gilt als seriös und politischliberalbislinksliberal. Gegründet 1851, wurde sie mit 91Pulitzer-Preisenausgezeichnet, weit mehr als jede andere Tageszeitung. Das erstmals 1889 erschieneneWall Street Journal, das sich insbesondere Wirtschaftsthemen widmet, gilt politisch alskonservativ, ist eine der auflagenstärksten Zeitungen der Vereinigten Staaten und bietet eigene europäische und asiatische Ausgaben. Weitere regional erscheinende Tageszeitungen sindNewsday,AM New York,Metro New YorkundStaten Island Advance. Außerdem gibt es eine große Zahl von Blättern, deren Berichterstattung sich beispielsweise auf Bevölkerungsgruppen, Bezirke oder Viertel der Stadt konzentriert oder in den Sprachen der ursprünglichen Herkunftsländer der Einwohnergruppen erscheinen. Zu den wichtigsten regionalen Wochenzeitungen und -zeitschriften gehörenNew York Observer,New York Press,Village VoiceundTime Out NY. Alle sechs Tage erscheintStreet Newsund einmal im MonatBIGNew. DieVillage Voice, gegründet 1955, war die erste und ist die bekannteste der alsAlternative weekliesbezeichneten Publikationen. Die Printausgabe wurde 2017 eingestellt. Die Onlineausgabe erscheint aber weiter. Zu den in New York verlegten landesweiten Zeitschriften zählenTime,Newsweek,The New Yorker,Vogue,Vanity FairundArchitectural Digest. New York ist Sitz globaler Medienkonzerne (Time Warner,Viacom) sowie großer Fernseh- und Radionetzwerke (ABC,CBS,FOX,NBC). Dutzende New Yorker und zahlreiche nationale sowie internationale Radio- und Fernsehstationen sind über Kabel und Satellit zu empfangen. Rund 10.000 Journalisten berichten aus der Stadt in die ganze Welt über Politik, Wirtschaft und Kultur. Über 3000 Film- und Serienproduktionen, darunter DutzendeBlockbuster, wurden in der Stadt und Umgebung gedreht. Viele bekannte Unterhaltungssendungen undTalkshowswerden in der Stadt aufgezeichnet. DerLate-Night-ShowModeratorJimmy Fallonempfängt seine Gäste in derTonight Show. Die seit 1975 existierendeSaturday Night LiveShow sendet ebenfalls aus Manhattan. Weitere hier produzierte Shows und Sendungen sind unter anderemThe Late Show with Stephen Colbert,Inside the Actors Studio,The Daily Show,Good Morning America,Last Week Tonight with John Oliver,The Today ShowsowieRed EyeundLive with Kelly. Die SenderMTVundComedy Centralsind ebenfalls in der Stadt beheimatet. Zu den vielen hervorragenden Hochschuleinrichtungen zählen unter anderem dieColumbia University(eröffnet 1754), dieCooper Union for the Advancement of Science and Art, dieCity University of New York, dasBarnard College, dieNew York University, das 1887 eröffnetePratt Institute, dieFordham University, dieNew School, dieRockefeller University, dieJuilliard Schoolfür Musik, Tanz und Schauspiel, dieManhattan School of Music(eröffnet 1917) sowie dasCulinary Institute of America(eröffnet 1946). Die Stadt New York betreibt auch eine Reihe vonweiterführenden Schulenzur Förderung vonhochbegabten Schülern. Zu den ältesten und renommiertesten dieser Schulen zählen dabei dieBronx High School of Science, dieStuyvesant High Schoolund dieBrooklyn Technical High School. DieColumbia-Universitätliegt inMorningside Heights, gleich nördlich von derUpper West Side, im StadtteilManhattanund gehört zu der sogenannten „Ivy League“, den Elite-Universitäten im Nordosten der USA (wieYale,Princeton,Harvardund andere). Sie wurde 1754 als King’s College unter königlichem Erlass von KönigGeorg II.gegründet und ist die älteste Höhere Schule im StaatNew Yorksowie die fünftälteste des Landes. Columbia gilt als eine der angesehensten Universitäten der Welt. DieNew York University(NYU) ist eine weltbekannte Elite-Universität. Sie ist die größte private Universität der Vereinigten Staaten mit insgesamt 39.408 Studenten (Herbst 2004) – davon 20.212 im Grundstudium, 15.884 im Hauptstudium und 3.312 Doktoranden. 4.000 der Studenten kommen aus über 100 Ländern. Der Campus der Universität liegt imGreenwich Villagein Manhattan. Die Universität besteht aus 14 Fakultäten und Colleges. Sie wurde am 21. April 1831 von einer Gruppe prominenter New Yorker, unter ihnen der ehemalige US-Finanzminister Albert Gallatin, gegründet und ist renommiert für ihre Fakultäten der Wirtschaftswissenschaften, Volkswirtschaft, Jura, Medizin, Informatik, Mathematik, Philosophie, Politikwissenschaften und der Neurowissenschaften. In den Rankings desU.S. News & World Reportund anderen Publikationen sind die Fakultäten der Universität regelmäßig unter den Top 25 zu finden, so beispielsweise die NYUStern School of BusinessdieSchool of Law, dieRobert F. Wagner School of Public Service. DieTisch School of the Artsist eines der berühmtesten Zentren für Musik-, Theater-, Regie- und Bühnenausbildung. Die Fakultät für Philosophie ist weithin als eine der besten des Landes bekannt. Die Aufnahme an der Universität ist, abhängig von der Fakultät, sehr selektiv. Sie wurde in Umfragen desPrinceton Reportunter College-Bewerbern als die „Nr.-1-Traumschule“ bezeichnet und hat 2004 und 2005 unter allen nordamerikanischen Universitäten den größten Bewerbungsandrang bewältigt. Zu den bedeutendsten Bibliotheken der Stadt gehören dieNew York Public Librarymit etwa zehn Millionen Büchern und dasSchomburg Center for Research in Black Culture. In New York City befindet sich Fort Hamilton, das die einzige aktiv wachhabende Einrichtung desUS-Militärsinnerhalb der Stadt ist.[86]1825 in Brooklyn an der Stelle einer kleinenBatteriegegründet, die während derAmerikanischen Revolutiongenutzt wurde, ist es eines der ältesten noch heute betriebenen militärischen Forts.[87]Heutzutage dient Fort Hamilton als Hauptquartier der Nordatlantik-Division desUnited States Army Corps of Engineersund des New-York-City-Rekrutierungs-Bataillons. Es beherbergt auch die 1179. Transport-Brigade, die 772. aeromedizinische Versorgungsstaffel und eine militärische Musterungsanstalt. Weitere früher aktive militärische Einrichtungen, die immer noch für militärisches Training oder Reserve und Einsätze derNationalgardein der Stadt genutzt werden, sindFort Wadsworthin Staten Island und Fort Totten in Queens. Bronx|Brooklyn|Manhattan|Queens|Staten Island Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geographie 1.1Geographische Lage 1.2Geologie 1.3Stadtgliederung 1.4Klima 1.5Auswirkungen des Klimawandels 2Bevölkerung 2.1Bevölkerungsentwicklung 2.2Bevölkerungsgruppen und Herkunft 2.3Sprachen 3Geschichte 3.1Präkoloniale Geschichte 3.217. Jahrhundert 3.318. Jahrhundert 3.419. Jahrhundert 3.520. Jahrhundert 3.621. Jahrhundert 4Politik 4.1Stadtregierung 4.2Städtepartnerschaften 5Kultur und Sehenswürdigkeiten 5.1Theater 5.2Musik 5.3Museen 5.4Bauwerke 5.4.1Freiheitsstatue und Liberty Island 5.4.2Flatiron Building 5.4.3Chrysler Building 5.4.4Empire State Building 5.4.5Brookfield Place 5.4.6Rockefeller Center"
  },
  {
    "label": 0,
    "text": "Oper – Wikipedia Oper Inhaltsverzeichnis Abgrenzungen Geschichte Form Aufführungspraxis der Oper Siehe auch Literatur Weblinks Einzelnachweise Oper und Schauspiel Oper und Ballett Oper und Operette/Musical Vorgeschichte Ursprung 17. Jahrhundert 18. Jahrhundert 19. Jahrhundert Jahrhundertwende Frühes 20. Jahrhundert Zweiter Weltkrieg Zeit nach 1945 Nummernoper Durchkomponierte Großform Opera seria und Opera buffa „Hoher“ Stil „Niederer“ Stil Große Oper – Kammeroper Gattung oder bloß Untertitel? Weitere Sonderformen Repertoire Sprache der Aufführungen Bücher Fachzeitschriften Antike Mittelalter Renaissance Florentiner Camerata Monteverdi Italien Paris Deutsches Sprachgebiet England Allgemeine Entwicklung Pasticcio Nummernoper Opera buffa Entwicklung der Opera buffa zur Opera semiseria England Frankreich Deutscher Sprachraum Opernreform Verschwinden der Kastratenpartien Allgemeine Entwicklung Deutscher Sprachraum Frankreich Russland Böhmen Italien Frankreich Wiener Schule Weitere Entwicklungen im Deutschen Sprachraum Allgemeine Entwicklung Weitere bedeutende Opernkomponisten des 20. und 21. Jahrhunderts Instrumentalmusik Geschlossene lyrische Formen Handlungsbetonte Passagen und Nummern AlsOper(von italienischopera in musica, „musikalisches Werk“) bezeichnet man seit 1639[1]eine um 1600 (mit Beginn des Barockzeitalters) entstandenemusikalische GattungdesTheaters. Ferner werden auch dasOpernhaus(die Aufführungsstätte oder produzierende Institution) oder die aufführendeKompagniealsOperbezeichnet. Eine Oper besteht aus derVertonungeinerdramatischenDichtung, die von einemSängerensemble, einem begleitendenOrchestersowie manchmal von einemChorund einemBallettensembleausgeführt wird. Neben dem Gesang führen die DarstellerSchauspielundTanzauf einerTheaterbühneaus, die mit den Mitteln vonMalerei,Architektur,Requisite,BeleuchtungundBühnentechnikgestaltet ist. Die Rollen der Darsteller werden durchMaskeundKostümeoptisch verdeutlicht. Als künstlerische Leitung betätigen sichDirigentenfür das Musikalische,Regisseurefür die Personenführung sowieBühnen– undKostümbildnerfür dieAusstattung. Im Hintergrund unterstützt sie dieDramaturgie. Die Oper wird mit Tanz,MusicalundOperetteunter dem BegriffMusiktheaterzusammengefasst.[2]Die Grenzen zu verwandten Kunstwerken sind fließend und definieren sich in jeder Epoche, meist auch im Hinblick auf bestimmte nationale Vorlieben, immer wieder neu. Auf diese Art bleibt die Oper als Gattung lebendig und erhält immer wieder neue Anregungen aus den verschiedensten Bereichen des Theaters. Schauspielein dem strengen Sinne, dass auf der Bühne nur gesprochen würde, sind in derTheatergeschichteselten. Mischformen aus Musik, Rezitation und Tanz waren die Regel, auch wenn sich zu manchen Zeiten Literaten und Theaterleute um eine Rettung oder Reform des Schauspiels bemüht haben. Seit dem 18. Jahrhundert sind Mischformen zwischen Schauspiel und Oper aus den verschiedenen Spielarten derOpéra-comiquehervorgegangen, wieBallad Opera,SingspieloderPossemit Gesang. Die Singspiele Mozarts werden der Oper zugerechnet, diejenigenNestroysgelten als Schauspiele. Auf der Grenze bewegen sich z. B. auch die Werke vonBrecht/Weill, derenDreigroschenoperdem Schauspiel näher steht, währendAufstieg und Fall der Stadt Mahagonnyeine Oper ist. Sich dem Schauspiel völlig unterordnende Musik bezeichnet man alsSchauspielmusik. Eine verbreitete, dem Schauspiel verwandte Theaterform seit dem Beginn des 19. Jahrhunderts war dasMelodram, das heute nur noch im populären Film gegenwärtig ist. Es hatte mit seinen Abenteuerstoffen großen Einfluss auf die Oper in jener Zeit. Stellenweise enthielt esHintergrundmusikals Untermalung der Bühnenhandlung (weniger des gesprochenen Texts). Darauf bezieht sich der heute noch bekannte BegriffMelodram. Eine solche Untermalung findet sich zum Beispiel in MozartsIdomeneo, Ludwig van BeethovensFidelio, in WebersDer Freischütz(in der Wolfsschluchtszene) und inHumperdincksKönigskinder. In französischer Tradition war der Tanz seit dem Barock in die Oper integriert. Das klassischeBallettlöste sich im 19. Jahrhundert mühevoll aus dieser Verbindung, aber inneoklassizistischenWerken des 20. Jahrhunderts, beispielsweise vonIgor StrawinskyoderBohuslav Martinů, bestätigt sich die Verwandtschaft von Oper und Ballett erneut. Auch die italienische Oper war nicht frei von Tanz, wenn auch der Tanz nicht im gleichen Maß dominierte. Heute werden die Ballette undDivertissementsder Repertoirewerke meist aus den Partituren gestrichen, sodass der Eindruck einerSpartentrennungentsteht. Das Genre derOperetteund verwandter Formen wie derZarzuelagrenzt sich als Weiterentwicklung aus demSingspieldurch die gesprochenen Dialoge, aber auch durch dessen vorherrschenden Unterhaltungsanspruch und das vorrangige Bemühen um Popularität oder kommerziellen Erfolg von der ab der Mitte des 19. Jahrhunderts zunehmend durchkomponierten Oper ab. Diese Abgrenzung entstand erst im ausgehenden 19. Jahrhundert: Als die „komische Oper“ vom „niederen“ zum „hohen“ Genre geworden war, bildete sich die Operette als neues „niederes“ Genre. Ähnliches gilt für dasMusical, die Weiterentwicklung des populären Musiktheaters in den Vereinigten Staaten. Operette und Musical sind gleichwohl in nicht geringerem Maße Kunstformen als die Oper. Bereits imTheater der griechischen Antikeverband man szenische Aktion mit Musik. Die Oper der Neuzeit berief sich immer wieder auf dieses Vorbild und konnte es, weil von der Aufführungspraxis wenig überliefert ist, auf unterschiedlichste Weise deuten. EinChor, der sang und tanzte, hatte eine tragende Rolle, indem er dasDramainEpisodengliederte oder auch die Aufgabe hatte, die Handlung zu kommentieren. Die Römer pflegten eher dieKomödieals dieTragödie.Mimusund später Pantomimus hatten einen hohen Musikanteil. Durch die Zerstörung der römischen Theater im 6. Jahrhundert und dieBücherverluste in der Spätantikesind viele Quellen darüber verloren gegangen. Jedoch werden seit Beginn des 20. Jahrhunderts zahlreiche antike Bauten, insbesondere Amphitheater und Theaterbauten, für Opernaufführungen genutzt. Die bekanntesten sind dasThéâtre AntiqueinOrange(mit Unterbrechungen seit 1869), dieArena di Verona(seit 1913), dasOdeon des Herodes Atticusin Athen (seit den 1930er Jahren), dieThermen des Caracallain Rom (seit 1937) und derRömersteinbruch St. Margarethen(seit 1996). ImHochmittelalterentstand ausgehend vom Gottesdienst derOstermesseeine neue Tradition gesungener Handlung. Dasgeistliche Spielfand zunächst in der Kirche, im 13. Jahrhundert dann alsPassionsspieloderProzessionsspielaußerhalb der Kirche statt. Beliebte Themen waren das biblische Oster- und Weihnachtsgeschehen, auch mit komödiantischen Einlagen. Die Melodien sind oft überliefert, der Einsatz von Musikinstrumenten ist wahrscheinlich, aber selten belegbar. Im höfischen Bereich gab es weltliche Stücke wieAdam de la HallesmelodienreichesJeu de Robin et de Marion(1280). Die Zeit desKarnevals, die später zur traditionellen Opernsaison wurde, bot seit dem 15. Jahrhundert Gelegenheit zu musikalisch-theatralischen Aktionen, die von den damals größten europäischen Städten in Italien ausgingen:Intermedien, Tanzspiele, Masken- und Triumphaufzüge gehören zur städtischen Repräsentation in der italienischenRenaissance. DasMadrigalwar die wichtigste Gattung derVokalmusikund verband sich oft mit Tänzen. Der Königshof in Frankreich gewann im 16. Jahrhundert gegenüber Italien an Bedeutung. DasBallet comique de la reine1581 war eine getanzte und gesungene Handlung und gilt als bedeutender Vorläufer der Oper. Ein früher Versuch in Deutschland, eine dramatische Handlung mit singenden Protagonisten in einem Bühnenbild aufzuführen, ist die Aufführung vonOrpheus und Amphionauf einerSimultanbühneanlässlich derJülichschen HochzeitvonJohann Wilhelm von Jülich-Kleve-Bergmit MarkgräfinJakobe von BadeninDüsseldorf1585. Als möglicher Komponist der nicht überlieferten Musik wirdAndrea Gabrieligenannt. Die Musik sei so schön gewesen, „daß es denselben / so dazumahl nit zugegen gewesen / und solchen Musicum concentum & Symphoniam gehört haben / onmüglich zu glauben.“ Die Handlung war freilich primär eineAllegoreseim Sinne einesFürstenspiegels. Die Oper im heutigen Sinn entstand Ende des 16. Jahrhunderts inFlorenz. Eine wichtige Rolle in der Entstehungsgeschichte spielte dieFlorentiner Camerata, einakademischer Gesprächskreis, in dem sich Dichter (z. B.Ottavio Rinuccini), Musiker, Philosophen, Adelige und ein Kunstmäzen – zunächst übernahmGraf Bardidiese Rolle, späterGraf Corsi– zusammenfanden. DieseHumanistenversuchten, das antikeDramawiederzubeleben, an dem ihrer Meinung nach Gesangssolisten, Chor und Orchester beteiligt waren. Nach denPastoraldramendes 16. Jahrhunderts wurde dasLibrettogestaltet und mit den musikalischen Mitteln der Zeit in Musik gesetzt.Vincenzo Galileigehörte dieser Gruppe an. Er entdeckte (heute verlorene)HymnendesMesomedesund schrieb einTraktatgegen dieniederländische Polyphonie. Dies war ein deutlicher Beweis für den gewünschten musikalischen Stil, den damals neuenSologesangmit Instrumentalbegleitung. Textverständlichkeit derVokalmusikwar für die Florentiner Camerata das Wichtigste. Eine klare, einfache Gesangslinie wurde zum Ideal erklärt, der sich die sparsameGeneralbass-Begleitung mit wenigen und leisen Instrumenten wieLauteoderCembalounterzuordnen hatte. Großartig ausgearbeitete melodische Einfälle waren unerwünscht, um den Inhalt der Worte nicht durch den Gesang zu verschleiern. Man sprach sogar von einer „nobilesprezzaturadel canto“ (Giulio Caccini:Le nuove musiche, 1601), einer „noblen Verachtung des Gesangs“. Diese Art des Singens nannte manrecitar cantando, rezitierenden Gesang. Die Schlichtheit und Beschränkung desrecitar cantandosteht im Gegensatz zur vorherrschendenPolyphoniemit ihren komplexen Ton- und Textschichtungen. Mit derMonodie, wie man diesen neuen Stil in Anlehnung an die Antike nannte, sollte das Wort wieder zu seinem vollen Recht kommen. Es entwickelte sich eineTheorie der Affekte, die durch den gesungenen Text transportiert werden konnten. Zur Monodie der einzelnen Gesangsstimme gesellten sich Chöre inMadrigalformoder alsMotette. Das Orchester spielte dazwischenRitornelleundTänze. Als erstes Werk der Gattung Oper giltLa DafnevonJacopo Peri(Uraufführung 1598) mit einem Text vonOttavio Rinuccini, von der nur einzelne Fragmente erhalten geblieben sind. Weitere bedeutende Werke aus der Anfangszeit sind PerisEuridice(1600) als älteste erhaltene Oper, sowieEuridice(1602) undIl Rapimento di Cefalo(1602) vonGiulio Caccini. Stoffe dieser frühen Opern entnahm man derSchäferdichtungund vor allem dergriechischen Mythologie.Wunder,Zauberund Überraschungen, dargestellt durch aufwändigeBühnenmaschinerie, wurden zu beliebten Bestandteilen. Besondere Beachtung fandClaudio Monteverdiserste OperL’Orfeo(1607). Sie wurde anlässlich des Geburtstags vonFrancesco IV. Gonzagaam 24. Februar 1607 in Mantua uraufgeführt. Hier sind im Vergleich zu seinen Vorgängern erstmals ein reicheres Instrumentarium (wenngleich es in derPartiturmeist nur angedeutet ist), ausgebauteHarmonik,tonmalerisch-psychologische und bildhafte Ausdeutung von Worten und Figuren sowie eine die Personen charakterisierendeInstrumentationzu hören.Posaunenwerden zum Beispiel für die Unterwelt- und Todesszenen eingesetzt, Streicher bei Schlafszenen, für die Hauptfigur Orfeo kommt eineOrgelmit Holzregistern (organo di legno) zum Einsatz. Monteverdi erweitert die Gesangslinie desrecitar cantandozu einem mehr arienhaften Stil und gibt den Chören größeres Gewicht. Seine SpätwerkeIl ritorno d’Ulisse in patria(1640) undL’incoronazione di Poppea(1643) sind in Hinblick auf ihre Dramatik Höhepunkte der Operngeschichte. Noch in dieser letzten Oper Monteverdis,L’incoronazione di Poppea, findet man den Prolog durch dreiallegorische Figurendargestellt, in derFortunadieVirtù(Tugend) verspottet. Die übrige Handlung spielt in der irdischen Welt um den römischen KaiserNero, dessen ungeliebte GattinOttaviaundPoppea, die Gattin des PrätorsOttone. Diese wird Neros Gattin und Kaiserin. Neros brutaler Charakter wird von einemKastratenund entsprechend virtuoser Musik dargestellt, Ottone wirkt dagegen weich, und Neros würdiger Lehrer und BeraterSenecabekommt die Bassstimme zugewiesen.Belcanto-Gesang undKoloraturreichtumwerden für den Adel und für Göttergestalten eingesetzt, für die übrigen Personen schlichtere Ariosi und Lieder. 1637 wurde dasTeatro San CassianoinVenedigals erstes öffentliches Opernhaus eröffnet. In schneller Folge entstanden neue Spielstätten, und Venedig wurde mit seiner „venezianischen Oper“ zum Opernzentrum Norditaliens. Historische Darstellungen verdrängten bald die mythischen Stoffe, wie in der OperL’incoronazione di Poppea(1642), die noch den Namen Claudio Monteverdis trägt, wobei die Forschung seitAlan Curtisdarüber diskutiert, ob es sich vielmehr um einPasticciohandle, das sich den berühmten Namen zu Nutze machte.[3] Das Publikum dieser Opern setzte sich vornehmlich aus Angehörigen der nichtadeligen Stände zusammen. Den Spielplan bestimmte der geldgebendeAdelauf Grund des Publikumsgeschmacks. Die aus den Akademien hervorgegangene Oper wurde in diesem Zusammenhang kommerzialisiert und vereinfacht, das Orchester reduziert. DieDa-capo-Ariemit vorangestelltemRezitativprägte für lange Zeit den Sologesang, Chöre und Ensembles wurden gekürzt. Verwechslungen und Intrigen bildeten das Grundgerüst der Handlungen, die mit komischen Szenen der beliebten Nebenfiguren angereichert wurden.Francesco CavalliundAntonio Cestiwaren die bekanntesten venezianischen Opernkomponisten in der auf Monteverdi folgenden Generation. Die SchriftstellerGiovanni Francesco BusenelloundGiovanni Faustinigalten als stilbildend und wurden häufig nachgeahmt. Zum zweiten, stärker vom Geschmack der Aristokratie geprägten Opernzentrum Italiens wurde seit den 1650er Jahren die Großstadt Neapel. Als Begründer der neapolitanischen Oper gilt der KomponistFrancesco Provenzale. In der folgenden Generation wurdeAlessandro Scarlattizum Vorreiter derneapolitanischen Schule. DieLibrettistenerhielten ihr Geld durch den Verkauf von Textbüchern, die zusammen mit Wachskerzen zum Mitlesen vor der Vorstellung verteilt wurden. Lange Zeit blieb die Literatur desRenaissance-HumanismusVorbild der italienischen Operntexte. Opern wurden nur zu bestimmten Spielzeiten (ital.:stagione) gegeben: während des Karnevals, von Ostern bis zur Sommerpause sowie vom Herbst bis zum Advent. In der Passions- und Adventszeit wurden stattdessenOratoriengespielt. InRomerhielten nicht nurMaschineneffekteund Chöre ein größeres Gewicht, sondern auch geistliche Stoffe. InParisentwickelteJean-Baptiste Lullyzusammen mit seinem LibrettistenPhilippe Quinaulteine französische Variante der Oper, deren herausragendstes Merkmal neben den Chören dasBallettist. Lully verfasste eine französische Version von CavallisL’ercole amante(1662), in die er Ballette einfügte, die größeren Beifall fanden als die Oper.Cadmus et Hermione(1673) wird als ersteTragédie lyriqueangesehen und blieb modellhaft für die nachfolgenden französischen Opern. Die aus Italien importierte Oper wurde von der Tragédie lyrique zurückgedrängt. Dennoch versuchten Lullys NachfolgerMarc-Antoine CharpentierundAndré Campra, französische und italienische Stilmittel zu verbinden. Ausgehend von italienischen Vorbildern, entwickelte sich bereits gegen Mitte des 17. Jahrhunderts eine eigenständige Operntradition innerhalb des deutschen Sprachgebietes, die auch die Verwendung deutschsprachiger Libretti mit einschließt. Die erste Oper eines „deutschen“ Komponisten war 1627 die (verschollene)DafnevonHeinrich Schütz, der die Musikform der Oper bei seinem Studienaufenthalt 1609–1613 in Italien kennengelernt hatte. 1644 entstand die erste erhaltene deutschsprachige Oper vonSigmund Theophil Stadennach einem Libretto vonGeorg Philipp HarsdörfferDas geistlich Waldgedicht oder Freudenspiel, genannt Seelewig, einpastorales Lehrstückin starker Nähe zum moralisierendenSchuldramader Renaissance. Kurz nach demDreißigjährigen Kriegetablierten sich auch im deutschsprachigen Raum Opernhäuser zunehmend als zentrale Versammlungs- und Repräsentationsorte der führenden Gesellschaftsschichten. Eine zentrale Rolle spielten dabei die führenden Fürsten- und Königshäuser, die sich zunehmend eigeneHoftheatersamt der zugehörigen Künstler leisteten, die in der Regel auch für die (wohlhabende) Öffentlichkeit zugänglich waren. So erhielt München sein erstes Opernhaus 1657, Dresden 1667. Bürgerliche, d. h. durch Städte und/oder private bürgerliche Akteure finanzierte „öffentliche und populäre“ Opernhäuser wie in Venedig existierten hingegen lediglich inHamburg(1678), Hannover (1689) undLeipzig(1693). Im bewussten Gegensatz zum durch italienischsprachige Opern dominierten Betrieb an den „adligen“ Häusern, setzte insbesondere die HamburgerOper am Gänsemarktals ältestes bürgerliches Opernhaus Deutschlands bewusst auf deutschsprachige Werke und Autoren. SoHändel,Keiser,MatthesonundTelemann. Jene etablierten bereits ab Beginn des 18. Jahrhunderts unter Verwendung deutschsprachiger Libretti von Dichtern wieElmenhorst,Feind,HunoldundPosteleine eigenständige deutschsprachige Opern- undSingspieltradition. Die Bedeutung Hamburgs für die Entwicklung einer eigenständigen deutschsprachigen Operntradition unterstreichen auch die beiden zeitgenössischen Schriften zur Theorie der Oper: Heinrich ElmenhorstsDramatologia(1688) und Barthold FeindsGedancken von der Opera(1708). InEnglandverbreitete sich die Oper erst relativ spät. Die vorherrschende musikalische Theaterform in der Zeit desElisabethanischen Theaterswar dieMasque, eine Kombination aus Tanz, Pantomime, Sprechtheater und musikalischen Einlagen, bei denen der vertonte Text meist nicht in unmittelbarem Zusammenhang mit der Handlung stand. Im Anschluss an das puritanische Verbot von Musik- und Theateraufführungen von 1642 begründete erst dieStuart-Restaurationab 1660 wiederum ein Theaterleben, in das die Oper integriert wurde. Ein in jeder Hinsicht singuläres Werk istHenry Purcellsknapp einstündige OperDido and Aeneas(Uraufführung vermutlich 1689, Libretto:Nahum Tate). Der Komponist greift darin Elemente der französischen und der italienischen Oper auf, entwickelt jedoch eine eigene Tonsprache, die sich vor allem dadurch auszeichnet, dass sie sehr eng am Text bleibt. Chorpassagen und tänzerische Abschnitte stehen den ariosen Passagen der Hauptfiguren gegenüber, die fast ohne arienartige Formen auskommen. Die wechselnden Stimmungen und Situationen werden mit musikalischen Mitteln genau wiedergegeben; die Schlussszene, wenn die karthagische KöniginDidoaus unglücklicher Liebe zu dem trojanischen HeldenAeneasan gebrochenem Herzen stirbt, gehört zum Bewegendsten der Opernliteratur. Im Laufe des 18. Jahrhunderts bilden sich zwei Operntypen heraus: Neben der etabliertenOpera seriaals vorwiegend vom Repräsentations- und Legitimationsbedürfnis des Adels getragene Form, die mehrheitlich auf mythologischen oder historischen Stoffen basiert und deren Personal aus Göttern, Halbgöttern, Heroen, Fürsten sowie deren Geliebten und ihrer Dienerschaft besteht, entwickelt sich um 1720 dieOpera buffamit zunächst grobschlächtig komischen Handlungen, die sich zu bürgerlich-sentimentalen entwickeln. Eine Konkurrenz zu den italienischen Opern bilden in Frankreich einerseits die höfischeTragédie lyrique, mit ihrem im Vergleich zu älteren italienischen Opern volleren Instrumentarium, und andererseits dieOpéra-comique, die vomPariser Jahrmarktstheaterherstammt. Diese Gattungen regen auch außerhalb Frankreichs Opernaufführungen in der eigenen Landessprache an, als einheimisches Gegengewicht zu den allgegenwärtigen italienischen Gesangsvirtuosen. Stilprägend wurde die im zweiten Viertel des 18. Jahrhunderts von Italien ausgehende Tendenz, aus dem ursprünglichenDramma per musicaein Arienkonzert bzw. eineNummernopermit festgelegtem Inhalt und Musik zu machen. Eine weitere zentrale Entwicklung während der ersten Hälfte des 18. Jahrhunderts ist die Einteilung der auf fünf Teile angewachsenenDa-capo-Arienmit der Abfolge AA'–B–AA' in spezifische Untergruppen: Der Star des Abends konnte zudem eine virtuoseAria baule(„Koffer-Arie“) einschieben, die mit der Handlung nichts zu tun hatte. Solche Arien konnten leicht vertauscht oder mehrfach eingesetzt werden. DerBelcanto-Gesang wurde zu einer Präsentation virtuoser Gesangstechniken, die extreme Spitzentöne, geschmeidige Triller und weite Sprünge umfassten. Weil im 18. Jahrhundert das Konzept derWerktreuenoch nicht etabliert war und Auftraggeber und Publikum stets neue, noch nie gehörte Opern wünschten, und weil vielen Opernkompanien häufig nur begrenzte Ressourcen an Instrumentalisten und Sängern zur Verfügung standen, bestand eine weitverbreitete Aufführungspraxis des 18. Jahrhunderts darin, Arien und Ensembles aus verschiedenen Werken je nach vorhandener Besetzung möglichst wirkungsvoll zusammenzustellen und eine solche Abfolge musikalischer Nummern mit neuen Texten und einer neuen Handlung zu unterlegen. Diese Art von Opern nannte manPasticcio; ein Opernpasticcio konnte sowohl aus der Feder eines einzigen Komponisten stammen, der vorhandene Nummern aus früheren Werken wiederverwendete, als auch aus Werken verschiedener Komponisten zusammengesetzt sein. Diese Praxis führte dazu, dass Handlung und Stimmung einer Opernaufführung bis zum Ende des 18. Jahrhunderts – an einigen Aufführungsorten auch bis in die 1830er Jahre hinein – nicht festgelegt waren und ständigen Anpassungen, Wandlungen und Veränderungen unterlagen. Die Praxis des Pasticcio bedeutete, dass bis zum Beginn des 19. Jahrhunderts kaum eine Aufführung des gleichen Werks musikalisch oder inhaltlich einer vorhergehenden glich. Das daraus folgende Handlungschaos – erzeugt von der Strategie, unterschiedlichen Erwartungen zugleich gerecht zu werden – stieß die italienischen LibrettistenApostolo ZenoundPietro Metastasioab. Als Gegenmaßnahme verzichteten sie ab den späten 1730er Jahren zunehmend auf überflüssige Seitenhandlungen, mythischeAllegorienund Nebenfiguren und bevorzugten stattdessen eine klare, nachvollziehbare Handlung und Sprache. Damit schufen sie die Grundlage für einen „ernsteren“ Operntypus jenseits der bis dahin üblichen Aufführungspraxis derOpera seria. Das zu diesem Zweck entwickelte Handlungsschema verwickelt die Hauptfiguren nach und nach in ein scheinbar unlösbares Dilemma, das sich zum Schluss durch einen unverhofften Einfall zum Guten wendet (lieto fine). Auch dichterisch leiteten beide Autoren eine Erneuerung der Oper ein. Gegen die Beliebigkeit desPasticcionummerierten sie die musikalischen Teile, wodurch deren Austausch erschwert wurde. So trugen sie wesentlich zur Herausbildung derNummernopermit ihrer festgelegten Abfolge bei. Als in sich geschlossenes Werk mit stringenter Handlung konnte sich die Oper nunmehr gegenüber demSchauspielbehaupten. Die Gattung derOpera buffaentstand gleichzeitig in Neapel und Venedig als zumeist heiterer und lebensnaher Operntypus. Einerseits gab es selbstständige musikalische Komödien, andererseits die komischenIntermezzizurOpera seriaAnfang der 1730er Jahre, aus derApostolo ZenoundPietro Metastasiodie komischen Elemente ausgeschlossen hatten, sodass sie auf Einlagen zwischen den Akten beschränkt werden mussten. Als stilprägende Werke gelten die OperLo frate ’nnamoratovonGiovanni Battista Pergolesi, uraufgeführt am 28. September 1732 imTeatro dei Fiorentiniin Neapel, und die ab Mitte der 1740er Jahre in Venedig uraufgeführten WerkeBaldassare Galuppis, die in enger Zusammenarbeit mitCarlo Goldonientstanden. Inhaltlich schöpfte die Opera buffa aus dem reichen Fundus derCommedia dell’arte. Die Handlungen waren oft Verwechslungskomödien, deren Personal aus einem adligen Liebespaar und zwei Untergebenen, oft Magd und Diener, bestand. Letztere können im Unterschied zur Opera seria als Hauptakteure auftreten, womit sich ein bürgerliches und subbürgerliches Publikum identifizieren konnte. Die Opera buffa wurde aber auch von der Aristokratie geschätzt, die ihre Provokationen kaum ernst nahm. Seit Mitte des 18. Jahrhunderts begann eine Verlagerung der Komik in der Opera buffa auf alltagsweltliche und gegenwartsbezogene Handlungen, in denen Adlige nicht mehr unangreifbar waren. MozartsDon Giovanni(1787) wurde zunächst als Opera buffa angesehen und erst im 19. Jahrhundert uminterpretiert, als das Schicksal der bürgerlichen Verführten ernst genommen und der adlige Verführer als Schurke betrachtet werden konnte. Ausdruck dieser Veränderungen ist die Weiterentwicklung der Opera buffa zum Typus derOpera semiseriaEnde des 18. Jahrhunderts, weil ein bürgerliches Publikum sich auf der Bühne nicht mehr verlacht sehen wollte. Die Alltagsnähe der Opera buffa und ihres französischen Gegenstücks, derOpéra-comique, besaß in der zweiten Hälfte des 18. Jahrhunderts soziale Sprengkraft. Damit im Zusammenhang stand der von 1752 bis 1754 in Frankreich ausgetrageneBuffonistenstreit.Jean-Jacques Rousseauschätzte den bürgerlich geprägten „heiteren“ Operntypus mehr als die Tragédie lyrique der Hocharistokratie. Seine Verurteilung der französischen Oper zu Gunsten der italienischen führte zu wütenden Reaktionen. Im englischen Sprachraum wurdeGeorg Friedrich Händel(anglisiert George Frideric Handel) zu einem der produktivsten Opernkomponisten (mehr als 45 Opern). Sein Wirken inLondonhatte nicht den gewünschten geschäftlichen Erfolg, u. a. wegen der starken Konkurrenz des berühmten KastratenFarinelli, der in der rivalisierenden Operntruppe sang, und ruinöser Gagen für die engagierten Primadonnen. Im 20. Jahrhundert sind vor allemAlcina,Giulio CesareundSersewieder in die Spielpläne gekommen, in den letzten Jahrzehnten auch viele andere Händel-Opern (u. a.Ariodante,Rodelinda,Giustino). Nachdem im Zuge derAlte-Musik-Bewegung diehistorische Aufführungspraxisimmer besser erforscht worden war, entstanden auch an den großen Opernhäusern stilbildende Produktionen unter Mitwirkung vonBarock-Spezialisten. Frankreichs Pendant zur in Paris umstrittenenOpera buffawurde dieOpéra-comique. Die Rezitative wurden durch gesprochene Dialoge ersetzt. Auch dieses Modell fand im Ausland Erfolg. Die neue Einfachheit und Lebensnähe schlägt sich auch in kleineren Arietten undnouveaux airs, die im Unterschied zu den allseits bekanntenVaudevillesneu komponiert wurden, nieder. 1752 erlebte Frankreich eine neue Konfrontation zwischen der französischen und der italienischen Oper, die unter dem NamenBuffonistenstreitin die Geschichte einging.Giovanni Battista PergolesisOperLa serva padrona(deutsch:Die Magd als Herrin) war der Anlass dafür. Gegen die Künstlichkeit und Stilisierung der herkömmlichen französischen Adelsoper waren vor allemJean-Jacques RousseauundDenis Diderot, die sich gegen die Kunst und StilisierungRameauszur Wehr setzten. Rousseau verfasste neben der bewusst einfach gestalteten OperLe devin du village(deutsch:Der Dorfwahrsager) auch ein preisgekröntes Traktat mit dem TitelDiscours sur les sciences et les arts(1750), in dem er ein von Wissenschaft und Kultur unverdorbenes Leben zum Ideal erklärt. Weitere Musikartikel schrieb er für die berühmte umfassendeEncyclopédieder französischen Aufklärung. Der Buffonistenstreit ging schließlich zu Ungunsten der italienischen Operntruppe aus, die aus der Stadt vertrieben wurde. Somit war der Streit zwar vorläufig beendet, an Beliebtheit stand die Grand opéra aber immer noch hinter der Opéra comique zurück. Die Schließung der Oper am Gänsemarkt im Jahr 1738 führte zu einer weiteren Stärkung des zu diesem Zeitpunkt bereits dominanten italienischsprachigen Opernbetriebs im deutschen Sprachraum. Dennoch etablierte sich – ausgehend vom Hamburger Vorbild – ab Mitte des 18. Jahrhunderts zunehmend die Praxis bei Aufführungen französischer und italienischer Opern die Rezitative ins Deutsche zu übersetzen und – aus vorwiegend musikalischen Gründen – lediglich bei den Arien die Originalsprache beizubehalten. Auch wurden ab Mitte des 18. Jahrhunderts der Verkauf oder die Verteilung gedruckter Erläuterungen und Übersetzungen nicht-deutschsprachiger Werke in deutscher Sprache an das Publikum mehr und mehr üblich. Um 1780 setzt mit dem WerkWolfgang Amadeus Mozartsschließlich eine bis weit ins 19. Jahrhundert reichende Entwicklung ein, die zur zunehmenden Verdrängung des bis dahin dominierenden Italienischen zugunsten deutschsprachiger Werke und Aufführungen in deutscher Übersetzung führte. Dabei fand Mozart seinen ganz eigenen Weg, mit der Tradition der italienischen Oper umzugehen. Er reüssierte bereits in jugendlichen Jahren mehrfach in Italien (u. a. mitLucio SillaundMitridate, re di Ponto) und komponierte mitIdomeneo(1781), einer ebenfalls auf Italienisch geschriebenenOpera seria, für München sein erstes Meisterwerk. Auf diese Form sollte er mitLa clemenza di Tito(1791) kurz vor seinem Tod nochmals zurückkommen. Nach den SingspielenBastien und Bastienne,Zaide(Fragment) undDie Entführung aus dem Serail(mit dieser 1782 uraufgeführten Oper gelang es ihm, sich in Wien als freier Komponist zu etablieren) schaffte er es in seinemFigaro(1786) und mehr noch imDon Giovanni(1787), Opera seria und Opera buffa einander wieder anzunähern. Neben den zuletzt genannten entstand 1790 als drittes Werk in kongenialer Zusammenarbeit mit dem LibrettistenLorenzo Da PonteCosì fan tutte.In derZauberflöte(1791) verband Mozart Elemente der Oper mit jenen desSingspielsund des lokal vorherrschendenAlt-Wiener Zaubertheaters, das seine Wirkung besonders aus spektakulärenBühneneffektenund einer märchenhaften Handlung bezog. Dazu kamen Ideen und Symbole aus derFreimaurerei(Mozart war selbst Logenmitglied). Mozart-Opern (und insbesondere dieZauberflöte) gehören bis heute zum Standardrepertoire eines jeden Opernhauses. Er selbst bezeichnete die Oper als „Große Oper in 2 Akten“. Der ebenfalls sowohl in Italien wie auch in Wien tätigeChristoph Willibald Gluckleitete mit seinen OpernOrfeo ed Euridice(1762) undAlceste(1767), in denen er Elemente der ernsten Oper aus Italien und Frankreich mit der realistischeren Handlungsebene derOpera buffakombinierte, eine umfassendeOpernreformein. Der konsequent klar und logisch aufgebaute Handlungsablauf, gestaltet vonRanieri de’ Calzabigi, kommt dabei ohne komplexe Intrigen oder Verwechslungsdramen aus. Die Zahl der Protagonisten schrumpft. Oberstes Ziel ist eine größere Einfachheit und Nachvollziehbarkeit der Handlung. Dabei ordnet sich Glucks Musik vollständig Dramaturgie und Text unter, charakterisierte Situationen und Personen und stand nicht für denbelcanto-Gesang an sich. Durchkomponierte oder strophisch gestaltete Lieder ersetzten die Da-capo-Arie. Dadurch wurde eine neue Natürlichkeit und Einfachheit erreicht, die hohlem Pathos und Sängermanierismen entgegenwirkte. Der Chor schaltete sich getreu dem antiken Vorbild aktiv in die Handlung ein. Die Ouvertüre bezieht sich auf die Handlung und steht nicht mehr als abgelöstes Instrumentalstück vor der Oper. Italienisches Arioso, französisches Ballett und Pantomime, englisches und deutsches Lied sowie Vaudeville wurden in die Oper integriert, nicht als nebeneinanderstehende Einzelstücke, sondern als neuer klassischer Stil. Glucks ästhetische Ideen wurden von seinem SchülerAntonio Salieriim späten 18. Jahrhundert zu einer neuen Blüte gebracht. Besonders bedeutend sind die OpernLes Danaïdes,TarareundAxur, re d’Ormus. Weiterer Ausdruck der größeren Alltagsnähe derOpera buffaund der durchChristoph Willibald Gluckangeregten Neuerungen derOpernreformist die in der 2. Hälfte des 18. Jahrhunderts einsetzende Praxis auf hohe Kastratenpartien für Männerpartien zugunsten realistischerer Stimmlagen zu verzichten. Neben der bewussten Abgrenzung von der stark durch dasVirtuosentumder Kastraten geprägten Opernkultur derOpera seriades Adels, spielten hierfür nicht zuletzt Kostengründe eine entscheidende Rolle. DaImpresariosmit der Opera buffa auf ein weniger zahlungskräftiges bürgerliches und sub-bürgerliches Publikum zielten, waren die horrenden Kosten für dieGageeines bekanntenKastratenkaum zu erwirtschaften. Die hieraus folgende Identifikation der Virtuosenkultur der Kastratenpartien mit der durch den Adel geprägten kostspieligen Tradition der Opera seria erklärt auch das Verschwinden der Kastraten aus dem Opernbetrieb nach dem Ende desAncien Régimeund dem hierdurch bedingten Aufstieg der durch die „natürlichere“ Stimmbesetzung der Opera buffa undOpera semiseriageprägten bürgerlichen Schichten zur auch in Sachen Oper führenden Gesellschaftsschicht des 19. Jahrhunderts. Im ersten Viertel des 19. Jahrhunderts verschwinden zunehmend die durch denGeneralbassbegleiteten Rezitative zugunsten einer ausnotierten Orchesterfassung. Neben der bis dahin noch führenden italienischen Oper und den französischen Operntypen treten nach und nach andere nationale Opernformen auf, so zuerst in Deutschland. DieFranzösische Revolutionund der AufstiegNapoleonszeigten ihre Auswirkungen auf die Oper am deutlichsten beiLudwig van Beethovenseinziger OperFideliobzw.Leonore(1805, 1806 und 1814). Dramaturgie und musikalische Sprache orientierten sich deutlich anLuigi CherubinisMédée(1797). Die Handlung beruht auf einem „fait historique“ vonJean-Nicolas Bouilly, das 1798 vonPierre Gaveauxunter dem TitelLéonore, ou L’amour conjugalkomponiert worden war; die Ideale der französischen Revolution bilden daher auch den Hintergrund von Beethovens Oper.Fideliokann zum Typus der „Rettungsoper“ gezählt werden, in der die dramatische Errettung eines Menschen aus großer Gefahr der Gegenstand ist. Formal ist das Werk uneinheitlich: der erste Teil ist singspielhaft, der zweite mit dem groß angelegten Chorfinale erreicht symphonische Durchschlagskraft und nähert sich demOratorium. Nach derZauberflöteund demFideliobrauchte die deutsche Produktion mehrere Anläufe, um schließlich in der Romantik eine eigene Opernsprache zu entwickeln. Eine der wichtigsten Vorstufen hierzu liefertenE. T. A. Hoffmannmit seiner romantischen OperUndineundLouis Spohrmit seiner Vertonung desFaust(beide 1816). Carl Maria von Weberwar es schließlich, der aus der Tradition desSingspielsmit viel dramatischem Farbenreichtum im Orchester die deutsche Oper in Gestalt desFreischützim Jahr 1821 gebührend aufleben ließ. Sein wegen des schlechten Textbuches kaum gespieltes WerkOberon(London 1826) maß dem Orchester so viel Bedeutung zu, dass sich später namhafte Komponisten wieGustav Mahler,Claude DebussyundIgor Strawinskyauf ihn beriefen. Weitere Komponisten der deutschen Romantik waren die als Opernkomponisten kaum bekannten HochromantikerFranz Schubert(Fierrabras, komponiert 1823,UA1897), dessen Freunde ihm keine kongeniale Textvorlage liefern konnten, undRobert Schumann, der mit der Vertonung des unter Romantikern beliebtenGenoveva-Stoffs nur eine Oper (1850) vorlegte. Ferner zu nennen sindHeinrich Marschner, der mit seinen Opern um übernatürliche Ereignisse und Naturschilderungen (Hans Heiling, 1833) großen Einfluss aufRichard Wagnerausübte,Albert Lortzingmit seinen Spielopern (u. a.Zar und Zimmermann, 1837, sowieDer Wildschütz, 1842),Friedrich von Flotowmit seiner komischen OperMartha(1847) und schließlichOtto Nicolai, der mit denLustigen Weibern von Windsor(1849) etwas „italianità“ in die deutsche Oper trug. Richard Wagnerschließlich formte die Oper so grundlegend nach seinen Ideen um, dass die oben genannten deutschen Komponisten neben ihm schlagartig verblassten. MitRienzi(1842) erlebte der bis dahin eher glücklose Wagner seinen ersten Erfolg in Dresden; er wurde später vonDer fliegende Holländer(1843) noch übertroffen. Wegen seiner Verwicklung in dieMärzrevolutionvon 1848 inDresdenmusste Wagner für viele Jahre ins Exil in die Schweiz. Sein späterer SchwiegervaterFranz Liszttrug durch die Uraufführung desLohengrin(1850) inWeimardazu bei, dass Wagner trotzdem weiterhin in Deutschland präsent war. Mit der Unterstützung des jungen bayerischen KönigsLudwig II.konnte Wagner schließlich den lang gehegten Plan desRing des Nibelungenverwirklichen, für den er eigens dasBayreuther Festspielhauserbauen ließ, in dem bis heute nur seine Werke gespielt werden. Die grundlegende Neuerung Wagners bestand in der vollständigen Auflösung der Nummernoper. Tendenzen zur durchkomponierten Oper zeigten sich schon in WebersFreischützoder inRobert Schumannsselten gespielterGenoveva(1850). Konsequent vollendet wurde diese Entwicklung erst durch Wagner. Daneben behandelte er Singstimmen und Orchesterpart grundsätzlich gleichberechtigt. Das Orchester begleitet also nicht mehr den Sänger, sondern tritt als „mystischer Abgrund“ in vielfältige Beziehung zum Gesungenen. Die Länge von Wagners Opern verlangt Sängern und Zuhörern viel Konzentration und Ausdauer ab. Die Themen seiner – mit Ausnahme einiger Frühwerke sowie derMeistersinger– durchweg ernsten Opern, deren Libretti er sämtlich selbst verfasste, sind häufig Erlösung durch Liebe,Entsagungoder Tod. InTristan und Isolde(1865) verlegte Wagner das Drama weitgehend in den psychischen Innenraum der Hauptfiguren, den er dann mit seiner Musik ausleuchten konnte – die äußere Handlung der Oper ist dagegen ungewöhnlich ereignisarm. Der Gestaltung dieses „ozeanischen“ Innenraums diente auch die Harmonik, die mit dem „Tristan-Akkord“ die bis dahin gültigen harmonischen Regeln in den Hintergrund rückte und damit in die Musikgeschichte einging. Musikalisch zeichnen sich Wagners Opern sowohl durch seine geniale Behandlung des Orchestersatzes, die auch auf die symphonische Musik der Zeit bis hin zuGustav Mahlerstarken Einfluss ausübte, aus, als auch durch den Einsatz wiederkehrender Motive, der sogenanntenLeitmotive, die sich mit Figuren, Situationen, einzelnen Begriffen oder auch mit bestimmten Ideengehalten verbinden. Mit demRing des Nibelungen(komponiert 1853–1876), dem wohl bekanntesten Opernzyklus in vier Teilen (daher auch schlicht „die Tetralogie“ genannt) mit etwa 16 Stunden Aufführungszeit insgesamt, schuf Wagner eine monumentale musikdramatische Verwirklichung seiner in der SchriftOper und Drama(1852) entwickelten Reform der überkommenen Oper. Das BühnenweihfestspielParsifalwar die letzte seiner Opern, die die Musikwelt in zwei Lager spalteten und sowohl Nachahmer (Engelbert Humperdinck,Richard Straussvor seinerSalome) als auch Skeptiker – insbesondere in Frankreich – hervorriefen. In Frankreich herrschte zunächst die in der zweiten Hälfte des 18. Jahrhunderts entwickelte Form derOpéra-comiquevor.Daniel-François-Esprit Aubergelang mit seiner OperLa muette de Portici(1828) deren Titelheldin von einer stumm bleibenden Ballerina dargestellt wurde, der Anschluss an dieGrand opéra(„große Oper“). Der DramatikerEugène Scribewurde zu deren maßgeblichem Librettisten. In der Grand opéra traten neben den Verwicklungen der operntypischen Liebesgeschichte vor allem historisch-politische Motive in den Vordergrund, wie es deutlich inRossinisletzter OperGuillaume Tell(1829) vorgeprägt ist. Der erfolgreichste Vertreter der Grand Opéra warGiacomo Meyerbeer, mit seinen WerkenRobert le diable(1831),Les Huguenots(1836) undLe prophète(1849), die jahrzehntelang und noch bis ins beginnende 20. Jahrhundert hinein, im internationalen Repertoire gespielt wurden. Andere bedeutende Beispiele sindLa Juive(„Die Jüdin“, 1835) vonHalévy,DonizettisDom Sébastien(1843), oderVerdisDon Carlos(1867). Etwa ab 1850 vermischten sich Opéra comique und Grand opéra zu einer neuen Opernform ohne Dialoge.Georges Bizetschrieb 1875 sein bekanntestes BühnenwerkCarmennoch als Opéra comique, deren Rezitative erst postum vonErnest Guiraudhinzugefügt wurden. Wenn die „realistische“ Handlung und der Ton des Werks nicht zu einer Grand opéra passen, so steht wiederum das tragische Ende, das bei der Uraufführung zunächst für einen Misserfolg sorgte, im Widerspruch zur Opéra comique. Weitere Beispiele für die Vermischung von Opéra comique und Grand opéra sindCharles GounodsFaust(1859) – hier wird zum ersten Mal der BegriffDrame lyriqueverwendet – undJacques OffenbachsLes contes d’Hoffmann(Hoffmanns Erzählungen, 1871–1880). Schließlich trat auch Russland mit seinen ersten Nationalopern auf den Plan, genährt durch den Import anderer Erfolge aus dem Westen.Michail Glinkakomponierte 1836 die Oper Жизнь за царя (Schisn sa zarja,deutsch:Ein Leben für den Zaren,in der Sowjetunion zuIwan Sussaninumbenannt). Das Werk hat ein russisches Sujet, ist aber musikalisch noch stark in westlichen Einflüssen verhaftet. Seine bekannteste OperRuslan und Ljudmilaübte großen Einfluss auf die folgenden Generationen russischer Komponisten aus.Modest Mussorgskilöste sich mitBoris Godunow(1874) nach einem Drama vonAlexander Puschkinendgültig von westlichen Einflüssen. AuchBorodinsFürst Igor(1890) führte Glinkas Erbe weiter.Pjotr Tschaikowskistand zwischen den russischen Traditionen und denen der westlichen Welt und entwarf mitEugen Onegin(1879) undPique Dame(1890) Liebesdramen mit bürgerlichem Personal, die beide ebenfalls auf einer Vorlage von Puschkin beruhen. InBöhmenwarenBedřich SmetanaundAntonín Dvořákdie meistgespielten Komponisten der Prager Nationaloper, die mit SmetanasLibuše(1881) im neuenNationaltheaterin Prag ihren Anfang nahm.Die verkaufte Braut(1866) desselben Komponisten wurde zum Exportschlager. Dvořaks OperRusalka(1901) verknüpfte volkstümliche Sagen und deutsche Märchenquellen zu einer lyrischen Märchenoper.Bohuslav MartinůundLeoš Janáčekführten ihre Bestrebungen weiter. Letztgenannter Komponist ist in seiner Modernität in den letzten Jahrzehnten wiederentdeckt worden und hat vermehrt die Spielpläne erobert. WährendDas schlaue Füchslein(1924) noch immer meist in der deutschen Übersetzung vonMax Brodgegeben wird, werden andere Werke wieJenůfa(1904),Káťa Kabanová(1921) oderVěc Makropulos(1926) immer häufiger in der tschechischsprachigen Originalversion aufgeführt; das ist insofern wichtig, da Janáčeks Tonsprache sich eng an diePhonetikundProsodieseiner Muttersprache anlehnt. Italien verfiel ab dem Jahr 1813, in dem seine OpernTancrediundL’italiana in Algeriaufgeführt wurden, dem jungen und überaus produktivenBelcanto-KomponistenGioachino Rossini.Il barbiere di Siviglia(1816),La gazza ladra(dt.Die diebische Elster) undLa Cenerentola(beide 1817) nach dem Aschenputtel-Märchen vonCharles Perraultsind bis heute im Standardrepertoire der Opernhäuser zu finden. Federnder Rhythmus und eine geistreich-brillante Orchestrierung sowie eine virtuose Behandlung der Singstimme ließen Rossini zu einem der beliebtesten und verehrtesten Komponisten Europas werden. Die bis dato noch üblichen improvisierten Verzierungen der Sänger schrieb Rossini dezidiert in seine Partien hinein und unterband damit ausufernde Improvisationen. Eine neue formale Idee verwirklichte er mit seinerscena ed aria, die den starren Wechsel von Rezitativ und Arie auflockerte und doch das Prinzip der Nummernoper aufrechterhielt. Daneben hat Rossini auch eine ganze Reihe vonOpere seriegeschrieben (z. B. seinenOtello, 1816, oderSemiramide, 1823). 1824 ging er nach Paris und schrieb wichtige Werke für die Opéra. Eine politischeGrand opéraverfasste er über Wilhelm Tell (Guillaume Tell, 1829), die in Österreich verboten und an verschiedenen europäischen Orten in entschärfter Fassung mit anderen Haupthelden aufgeführt wurde. Rossinis jüngere Zeitgenossen und Nachfolger kopierten zunächst seinen koloraturenreichen Stil, bis vor allemVincenzo BelliniundGaetano Donizetties schafften, sich mit einem eigenen, etwas schlichteren, ausdrucksvollen und romantischeren Stil von dem übermächtigen Vorbild zu emanzipieren. Bellini war berühmt für die ausdrucksvolle und ausgefeilte Deklamation seiner Rezitative und die „unendlich“ langen und ausdrucksvollen Melodien seiner Opern, wieIl pirata(1827),I Capuleti e i Montecchi(1830),I puritani(1835),La sonnambula(1831), und vor allemNorma(1831). Die Titelpartie dieser Oper mit der berühmten Arie „Casta diva“ schrieb Bellini, genau wie die Amina inLa sonnambula, der großen SängerinGiuditta Pastaauf den Leib. Die Norma ist so anspruchsvoll, dass sie nur von ganz wenigen großen Sängerinnen gesungen und interpretiert werden kann, sie wurde durch die historische Interpretation vonMaria Callaswieder der Vergessenheit entrissen. Der wenige Jahre ältere Donizetti war ein ungemein fleißiger Komponist, der neben Bellini und vor allem nach dessen frühzeitigem Tode (1835) zum erfolgreichsten italienischen Opernkomponisten aufstieg. Seinen ersten großen Durchbruch hatte er mitAnna Bolena(1830), deren Titelpartie ebenfalls von der Pasta kreiert und von der Callas wiederentdeckt wurde. Dagegen istLucia di Lammermoor(1835) mit der berühmten koloraturreichen Wahnsinnsszene nie ganz aus dem Repertoire verschwunden und hält sich neben den heiteren OpernL’elisir d’amore(1832),Don Pasquale(1843), undLa fille du régiment(1840) konsequent auf den Spielplänen der Opernhäuser. Die weit gespannten Melodiebögen Bellinis machten starken Eindruck auf den jungenGiuseppe Verdi. Seit seiner dritten OperNabuccogalt er alsNationalkomponistfür das immer noch von denHabsburgernbeherrschte Italien. Sein Chor„Va, pensiero, sull’ ali dorate“(1842) entwickelte sich rasch zur heimlichen Nationalhymne des besetzten und zerteilten Landes. Musikalisch zeichnet Verdis Musik eine betonte Rhythmik aus, über der sich einfache, oft extrem ausdrucksstarke Melodien entwickeln. In seinen Opern, bei denen Verdi mit untrüglichem Theaterinstinkt auch oft selbst am Textbuch mitwirkte, nehmen Chorszenen zunächst eine wichtige Stellung ein. Verdi verließ zunehmend die traditionelle Nummernoper; ständige emotionale Spannung verlangte nach einer abwechslungsreichen Durchmischung der einzelnen Szenen und Arien. MitMacbethwandte sich Verdi endgültig von der Nummernoper ab und ging seinen Weg der intimen Charakterschilderung von Individuen weiter. MitLa traviata(1853, nach dem 1848 erschienenen RomanDie KameliendamevonAlexandre Dumas d. J., der um die authentische Figur der KurtisaneMarie Duplessiskreist) brachte er erstmals einen Gegenwartsstoff auf die Opernbühne, wurde von derZensurjedoch gezwungen, die Handlung aus der Jetztzeit zu verlegen. Verdi vertonte häufig literarische Vorlagen, etwa vonFriedrich Schiller(z. B.Luisa MillernachKabale und LiebeoderI masnadierinachDie Räuber), Shakespeare oderVictor Hugo(Rigoletto). Mit seinen für Paris geschriebenen Beiträgen zurGrand Opéra(z. B.Don Carlos, 1867) erneuerte er auch diese Form und nahm mit dem spätenOtelloElemente von Richard Wagners Musikdrama auf, bis er mit der überraschenden KomödieFalstaff(1893; Dichtung in beiden Fällen vonArrigo Boito) im Alter von 80 Jahren seine letzte von fast 30 Opern komponierte. Als seine populärsten Opern geltenLa traviata(1853) undAida(1871). Nach dem Abtreten Verdis eroberten die jungenVeristen(ital.vero= wahr) in Italien die Szene. Ungeschönter Naturalismus war eines ihrer höchsten ästhetischen Ideale – dementsprechend wurde von säuberlich verfassten Versen Abstand genommen.Pietro Mascagni(Cavalleria rusticana,1890) undRuggero Leoncavallo(Pagliacci,1892) waren die typischsten Komponisten aus dieser Zeit.Giacomo Pucciniwuchs hingegen an Ruhm weit über sie hinaus und ist bis heute einer der meistgespielten Opernkomponisten überhaupt.La Bohème(1896), ein Sittengemälde aus dem Paris der Jahrhundertwende, der „Politkrimi“Tosca(1900, nach dem gleichnamigen Drama von Victorien Sardou) und die fernöstlicheMadama Butterfly(1904), mit der unvollendetenTurandot(Uraufführung posthum 1926) noch um ein weiteres an Exotismus gesteigert, sind vor allem wegen ihrer Melodien zu Schlagern geworden. Puccini war ein eminenter Theatraliker und wusste genau für die Stimme zu schreiben; die Instrumentierung seiner meist für großes Orchester gesetzten Partituren ist sehr differenziert und meisterhaft.[4]Zurzeit wird der damals sehr populäre italienisch-deutsche KomponistAlberto Franchetti, trotz dreier Welterfolge (Asrael,Christoforo ColomboundGermania) zwischendurch fast vergessen, zaghaft wiederentdeckt. Einem anderen musikdramatischen Ideal verpflichtet als die Veristen war der gleichzeitig tätigeAlfredo Catalani, dessen beim Publikum sehr beliebten Werke auch mit fantastischen Elementen durchsetzt sind. Seine letzte und heute bekannteste Oper,La Wallynach dem RomanDie Geier-WallyvonWilhelmine von Hillern, wurde am 20. Januar 1892 imTeatro alla Scalain Mailand uraufgeführt. Claude Debussygelang es schließlich, sich vom Einfluss des Deutschen zu befreien, und schuf mitPelléas et Mélisande1902 eines der nuanciertesten Beispiele für die von Wagner übernommeneLeitmotivtechnik.Maurice MaeterlincksTextvorlage bot viel an mehrdeutigen Symbolismen an, die Debussy in die Orchestersprache übernahm. Die Gesangspartien wurden fast durchweg rezitativisch gestaltet und boten der „unendlichen Melodie“ Wagners mit dem „unendlichen Rezitativ“ ein Gegenbeispiel. Eine der raren Ausnahmen, die dem Hörer eine gesangliche Linie darbieten, ist das schlichte Lied der Mélisande, das wegen seiner Kürze und Schmucklosigkeit kaum als echte Arie angesehen werden kann. NachRichard Strauss, der mitSalomeundElektrazunächst zum spätromantischen Expressionisten wurde, sich dann allerdings mitDer Rosenkavalierwieder früheren Kompositionsstilen zuwendete und mit einer Reihe von Werken bis heute viel gespielt wird (z. B.Ariadne auf Naxos,Arabella,Die Frau ohne SchattenundDie schweigsame Frau), schafften es nur noch wenige Komponisten, einen festen Platz im Repertoire der Opernhäuser zu finden. Stattdessen wurden (und werden) eher die Werke der Vergangenheit gepflegt. Die Aufnahme eines zeitgenössischen Werkes in das Standardrepertoire bleibt die Ausnahme. Alban Berggelang dies dennoch mit seinen OpernWozzeck,der freitonal angelegt wurde, undLulu,die sich ganz der Zwölftonmusik bedient. Die zuerst Fragment gebliebeneLuluwurde vonFriedrich Cerhafür die Pariser Aufführung unterPierre BoulezundPatrice Chéreauin ihrer dreiaktigen Gestalt vollendet. Von beiden Opern hat insbesondereWozzeck,bei dem Gehalt des Stücks und musikalische Vision zu einer Einheit finden, inzwischen weltweit in unzähligen Inszenierungen an großen wie kleineren Bühnen Eingang in das vertraute Opernrepertoire gefunden und eine unbestrittene Stellung erobert. Durchaus ähnlich verhält es sich mitLulu,die jedoch wegen ihres im Werk angelegten Aufwands oft nur von größeren Bühnen bewältigt werden kann. Sie inspiriert allerdings regelmäßig wichtige Interpretinnen wieAnja Silja,Evelyn Lear,Teresa StratasoderJulia Migenes. VonArnold Schönbergwerden regelmäßig dasMonodramErwartung– die erste Oper für eine einzige Sängerin – sowie das vom Komponisten bewusst unvollendet hinterlassene, höchste Ansprüche an den Chor stellende WerkMoses und Aronaufgeführt.Erwartung,bereits 1909 entstanden, doch erst 1924 inPragmitMarie Gutheil-Schoderunter der Leitung vonAlexander von Zemlinskyuraufgeführt, bewies in den dem Zweiten Weltkrieg folgenden Jahren eine spezifische Faszination gleichermaßen für Sängerinnen (besondersAnja SiljaundJessye Norman) wie für Regisseure (z. B.Klaus Michael Grübermit Silja 1974 in Frankfurt;Robert Wilsonmit Norman 1995 bei denSalzburger Festspielen). 1930 begann Schönberg die Arbeit anMoses und Aron,die er 1937 abbrach; nach der szenischen Uraufführung in Zürich 1957 hat diese Oper international zumal seit den 1970er Jahren in zahlreichen Aufführungen ihre besondere Bühnentauglichkeit bewiesen. Interessant ist ferner, dass Moses sich über die gesamte Oper hinweg eines Sprechgesangs bedient, dessen Tonhöhe vorgezeichnet ist, Aron dagegen singt. Ansonsten hinterließ dieWiener Schulekeine weiteren Spuren im Standardrepertoire. Musikalisch musste sich allerdings jeder moderne Komponist mit derZwölftonmusikauseinandersetzen und entscheiden, ob er auf ihrer Grundlage weiter arbeitete oder eher in tonalen Bahnen dachte. Hans Pfitznergehörte zu den bedeutendsten Komponisten der ersten Jahrhunderthälfte, die bewusst an den tonalen Traditionen festhielten. Sein Opernschaffen zeigt gleichermaßen Einflüsse Richard Wagners und frühromantischer Komponisten, wie Weber und Marschner. Pfitzners Musik wird zum großen Teil von linear-polyfonem Denken bestimmt, die Harmonik bewegt sich zwischen schlichter Diatonik und bis an die Grenzen der Tonalität gehender Chromatik. Von Pfitzners Opern ist die 1917 uraufgeführte Musikalische LegendePalestrinaam bekanntesten geworden. Er schrieb außerdem:Der arme Heinrich,Die Rose vom Liebesgarten,Das Christ-ElfleinundDas Herz. Franz Schrekerschuf 1912 mitDer ferne Klangeinen der großen Opernerfolge vor demZweiten Weltkrieg, geriet jedoch später in Vergessenheit, als derNationalsozialismusseine Werke aus den Spielplänen verdrängte. Nach vielen früheren Versuchen begann erst in den 1980er Jahren die wirklich tief greifende Wiederentdeckung dieses Komponisten, die neben Neuinszenierungen vonDer ferne Klang(Teatro La Fenice1984,Wiener Staatsoper1991) auch Aufführungen vonDie Gezeichneten,Der SchatzgräberoderIrrelohezeitigte. Eine wesentliche Rolle in Schrekers Musik spielen stark ausdifferenzierte Klangfarben. Die chromatische Harmonik Wagners erfährt bei Schreker eine nochmalige Intensivierung, die nicht selten die tonalen Bindungen bis zur Unkenntlichkeit verwischt. Ähnlich wie Schreker erging es dem WienerAlexander von Zemlinskyund dem BrünnerErich Wolfgang Korngold, deren Werke es nach 1945 ebenfalls schwer hatte. Seit den 1980er Jahren gelang es beiden Komponisten, wieder einen Platz im internationalen Repertoire zu erlangen, Zemlinsky mitKleider machen Leute,besonders aberEine florentinische Tragödie,Der ZwergundDer König Kandaules,Korngold mitDie tote Stadt. Auch das Schaffen vonWalter Braunfelswurde von den Nationalsozialisten verboten und erfährt erst seit Ende des 20. Jahrhunderts wieder verstärkte Aufmerksamkeit. Mit seiner OperDie Vögelwar Braunfels in den 1920er Jahren einer der meistgespielten Komponisten auf deutschen Opernbühnen. An seinen Werken fällt ihre stilistische Vielseitigkeit auf: BietetPrinzessin Brambillaeinen auf dieCommedia dell’artezurückgreifenden Gegenentwurf zum Musikdrama der Wagnernachfolge, zeigenDie Vögelden Einfluss Pfitzners. Mit den späteren OpernVerkündigung,Der Traum ein LebenundJeanne d’Arc – Szenen aus dem Leben der Heiligen Johannanähert Braunfels sich der Tonsprache des späteren Hindemith an. Zu den in den 1920ern erfolgreichsten Komponisten der jungen Generation zählteErnst Krenek, ein Schüler Schrekers, der zunächst mit in freier Atonalität gehaltenen, expressionistischen Werken für Aufsehen sorgte. Ein Skandalerfolg wurde 1927 seine OperJonny spielt auf, die Elemente des Jazz aufgreift. Sie ist ein typisches Beispiel für die damals entstandene Gattung der „Zeitoper“, die ihre Handlungen dem stark vom Wechsel unterschiedlicher Moden bestimmten Alltag der damaligen Zeit entnahm. Kreneks Musik wurde von den Nationalsozialisten später als „entartet“ abgelehnt und verboten. Der Komponist emigrierte in die USA und brachte es bis 1973 auf über 20 Opern, in denen sich die wechselvolle Entwicklung der Musik des 20. Jahrhunderts exemplarisch widerspiegelt. DerZweite Weltkriegbezeichnete einen großen Einschnitt in der Geschichte Europas und Amerikas, der sich auch auf die musikalische Welt auswirkte. In Deutschland wurden kaum noch Opern mit modernen Klängen gespielt und gerieten immer mehr ins Abseits. Ein bezeichnendes Beispiel hierfür bildetPaul Hindemith, der in den 1920ern mit Werken wie der OperCardillacals musikalischer „Bürgerschreck“ galt, nach 1930 aber schließlich zu einem gemäßigt modernen Stil neoklassizistischer Prägung gefunden hatte, dem u. a.Mathis der Maler(aus Teilen dieser Oper stellte der Komponist eine viel gespielte Sinfonie zusammen) zuzurechnen ist. Trotz des Stilwandels bekam Hindemith die Ablehnung deutlich zu spüren, daAdolf Hitlerpersönlichen Anstoß an seiner 1929 vollendeten OperNeues vom Tagegenommen hatte. Schließlich wurden auch Hindemiths Werke mit dem Etikett „entartet“ versehen und ihre Aufführung verboten. Hindemith ging, wie andere Künstler und Komponisten vor und nach ihm, 1938 ins Exil. Die Zeit nach 1945 ist durch eine deutliche Internationalisierung und Individualisierung des Opernbetriebes gekennzeichnet, welche die bis dahin übliche Unterteilung in nationale Traditionen kaum mehr sinnvoll erscheinen lässt. Die Oper wurde immer stärker von individuellen Einflüssen der Komponisten abhängig als von allgemeinen Strömungen. Die ständige Präsenz der „Klassiker“ des Opernrepertoires ließ die Ansprüche an moderne Opern steigen, und jeder Komponist musste seinen eigenen Weg finden, um mit der Vergangenheit umzugehen, sie fortzuführen, zu verfremden oder mit ihr zu brechen. Im Folgenden entstanden immer wieder Opern, die die Grenzen der Gattung sprengten und zu überwinden trachteten. Auf musikalischer wie textlicher Ebene verließen die Komponisten zunehmend bekanntes Terrain und bezogen die Bühne und die szenische Aktion in den – oft genug abstrakten – musikalischen Ablauf mit ein. Kennzeichen für die Erweiterung der visuellen Mittel im 20. Jahrhundert sind die zunächst handlungsbegleitenden, später selbstständigeren Videoprojektionen. In der zunehmenden Individualisierung der Musiksprache lassen sich in der Oper der zweiten Hälfte des 20. Jahrhunderts dennoch Strömungen erkennen: zum einen dieLiteraturopern, deren Dramaturgie sich zu großen Teilen an der Tradition ausrichtet. Dazu werden aber mehr und mehr aktuelle Stoffe und Libretti verwendet. Dennoch sind zwei wegweisende Werke dieser Zeit ausgerechnet Opern, die Klassiker der Literatur als Grundlage verwenden, nämlichBernd Alois ZimmermannsOperDie SoldatennachJakob Michael Reinhold LenzundAribert ReimannsLearnachWilliam Shakespeare. Weitere Beispiele für die Literaturoper wären ReimannsDas Schloss(nachKafka) undBernarda Albas Haus(nachLorca). Zunehmend werden auch politische Stoffe vertont, beginnend mitLuigi NonoundHans Werner Henze; ein jüngeres Beispiel istGerhard RosenfeldsOperKniefall in WarschauüberWilly Brandt, deren Uraufführung 1997 in Dortmund allerdings bei Publikum wie Presse gleichermaßen wenig Wirkung zeigte und keine Folgeproduktionen zeitigte. Können schonLuigi NonosWerke aufgrund ihrer experimentellen Musiksprache nicht mehr als Literaturoper kategorisiert werden, so wird auch die Dramaturgie der Opernvorlage auf ihre experimentellen Möglichkeiten hin ausgelotet. Der BegriffOpererfährt daher eine Wandlung in der zweiten Hälfte des 20. Jahrhunderts, viele Komponisten ersetzen ihn durchMusiktheaterodermusikalische Szenenund verwenden den Begriff Oper nur für explizit mit der Tradition verbundene Werke. In den Werken experimenteller Komponisten ist nicht nur ein kreativer Umgang mit Text und Dramaturgie zu entdecken, auch die Bühne, die Orchesterbesetzung und nicht zuletzt die Musik selbst überwindet konservative Muster, das Genre ist hier nicht mehr klar eingrenzbar. Zudem werden neue Medien wie Video, Elektronik eingesetzt, aber auch das Schauspiel, Tanz und Performance halten Einzug in die Oper. Eine ganz eigene Stimme im zeitgenössischen Musiktheater verkörpert ein anderer italienischer Komponist:Salvatore Sciarrino. Er schafft mit seinem Interesse an Klangfarben oder auch der Stille in der Musik, z. T. im Rückgriff auf Kompositionstechniken derRenaissance(z. B. in seiner OperLuci mie traditricivon 1998 über das Leben des Madrigal-KomponistenCarlo Gesualdo) unverwechselbare Werke. Benjamin Brittenließ das moderne England auf den internationalen Opernbühnen Einzug halten. Von seinen überwiegend tonalen Opern sindA Midsummer Night’s Dream, basierend auf dem SchauspielWilliam Shakespeares,Albert Herring,Billy BuddundPeter Grimesam bekanntesten. Immer wieder zeigte sich Brittens Vorliebe und Talent zur Klangmalerei insbesondere in der Darstellung des Meeres. DieDialogues des Carmélites(Gespräche der Karmelitinnen, Uraufführung 1957) vonFrancis Poulencgelten als eines der bedeutendsten Werke des modernen Musiktheaters. Grundlage bildet der historische Stoff derMärtyrinnen von Compiègne, die 1794 unter den Augen des Revolutionstribunals singend zum Schafott schritten, nachdem sie sich geweigert hatten, ihre Ordensgelübde zu brechen. Auf Poulenc geht auch die zweite bekannt gewordene Oper für eine einzige Sängerin zurück: InLa voix humainezerbricht die schlicht als „Frau“ bezeichnete Person an der Untreue ihres Geliebten, der ihr per Telefon den Laufpass gibt.Luciano Berioverwendete inPassaggiozu der weiblichen Hauptfigur „Sie“ auch einen kommentierenden Chor. Der KomponistPhilip Glass, derMinimal Musicverhaftet, verwendete fürEinstein on the Beachkeine zusammenhängenden Sätze mehr, sondern Zahlen,Solfège-Silben, Nonsens-Worte. Entscheidend war die Darstellung der Geschehnisse auf der Bühne. 1976 entstandEinstein on the Beach, der erste Teil einer Trilogie, in der auchSatyagrahaundAkhnatenvertreten sind – Hommagen an Persönlichkeiten, die die Weltgeschichte veränderten:Albert Einstein,Mahatma Gandhiund den ägyptischen PharaoEchnaton. Glass’ Arbeiten haben besonders in Verbindung mit den als kongenial empfundenen Inszenierungen von Robert Wilson oderAchim Freyergroße Publikumswirksamkeit bewiesen. Mauricio KagelsBühnenwerke sind ebenso oft Werke über Musik oder Theater an sich, die am ehesten als „Szenisch-musikalische Aktion“ zu klassifizieren ist – die Musik ist kaum festgelegt, da Kagel sich der freien Improvisation seiner Interpreten überlässt, die auf Nicht-Instrumenten (Reißverschlüssen, Babyflaschen etc.) spielen oder sie ungewöhnlich benutzen, bedeutungslose Silben singen oder Handlung und/oder Musik per Zufall oder durch improvisierte Lesart entstehen lassen. Mit Witz übte Kagel dabei hintersinnige Kritik an Staat und Theater, Militär, Kunstbetrieb usw. Skandale erregte sein berühmtestes WerkStaatstheater, in dem die verborgenen Mechanismen desselben an die Oberfläche gekehrt werden. Luigi Nonoverwendete seine Musik dagegen, um politische und soziale Missstände anzuklagen. Besonders deutlich wird dies inIntolleranza 1960, wo ein Mann auf einer Reise zu seiner Heimat Demonstrationen, Proteste, Folterungen,Konzentrationslager, Gefängnishaft und Missbrauch bis hin zu einer Überschwemmung erlebt und schließlich feststellt, dass seine Heimat dort ist, wo er gebraucht wird. Ein sehr produktiver Komponist war der 2003 mit demPremium Imperialeder Japan Art Foundation (sog.Nobelpreis der Kunst) ausgezeichneteHans Werner Henze. Er stand von Anfang an im Konflikt mit den teilweise dogmatisch ausgerichteten herrschenden Strömungen der zeitgenössischen Musik in Deutschland (StichwortDarmstadtbzw.Donaueschingen, s. o.), griffserielleTechniken auf, wandte jedoch auch ganz andere Kompositionstechniken bis hin zurAleatorikan. Am Beginn seines Opernschaffens stand seine Zusammenarbeit mit der DichterinIngeborg Bachmann(Der junge Lord, 1952, und die Kleist-AdaptionDer Prinz von Homburg, 1961). DieElegie für junge Liebende(1961) entstand mitW. H. Audenund Chester Kallman, den Librettisten vonStrawinskysOperThe Rake’s Progress. Später vertonte er Libretti vonEdward Bond(The Bassarides, 1966, undThe English Cat, 1980). Sein WerkL’Upupa und der Triumph der Sohnesliebewurde 2003 bei den Salzburger Festspielen uraufgeführt. Henze, der seit vielen Jahrzehnten in Italien lebte, hat viele jüngere Komponisten nachhaltig gefördert und beeinflusst. Seit 1988 gibt es in München die von ihm gegründeteBiennale für Neues Musiktheater. Karlheinz Stockhausenvollendete 2005 seine 1978 begonnene HeptalogieLICHT. Mit seinem Hauptwerk hinterließ er ein religiöse Themen behandelndes, monumentales Opus, bestehend aus sieben Opern, die jeweils für einen Wochentag stehen. Die ersten Opern erlebten in Mailand ihre Uraufführung(Donnerstag,Samstag,Montag), inLeipzigwurdenDienstagundFreitagzum ersten Mal gespielt. In seiner Gesamtheit wurde das insgesamt 29 Stunden Musik umfassende komplexe Werk nicht zuletzt wegen der immensen organisatorischen Schwierigkeiten noch nicht aufgeführt. Aufmerksamkeit erregte in Deutschland 1996 die OperDas Mädchen mit den SchwefelhölzernvonHelmut Lachenmann. Sie basiert auf der bekannten Weihnachtsgeschichte vonHans Christian Andersen. Auf eigenwillige Weise und mit teilweise neuartigen Instrumentaltechniken setzt Lachenmann hier das Gefühl der Kälte in Klang um. Nach der Statistik vonOperabasesind die fünf meistaufgeführten lebenden Opernkomponisten in den fünf Spielzeiten von 2013/14 bis 2017/18 die AmerikanerPhilip Glass,Jake Heggie, der EngländerJonathan Dove, der Niederländer Leonard Evers, und der EngländerThomas Adès. Als meistaufgeführte deutsche Komponisten nennt OperabasePeter Lundan 8.,Marius Felix Langean 11.,Wolfgang Rihman 14.,Ludger Vollmeran 17., undAribert Reimannan 23. Stelle.[5] SeitHumperdincksMärchenoperHänsel und Gretelhaben Opernkomponisten immer wiederKinderoperngeschrieben, wie z. B.Henze(Pollicino, 1980),Oliver Knussen(Wo die wilden Kerle wohnen, 1980 und 1984) undWilfried Hiller(Tranquilla Trampeltreu,Norbert Nackendick,Der Rattenfänger,Eduard auf dem Seil,WolkensteinundDer Goggolori). Opern sind von einer Formenvielfalt geprägt, die durch konventionelle Kompositionsstile ebenso wie durch individuelle Lösungen der Komponisten bestimmt wird. Deshalb gibt es keine allgemeingültige Formel für ihre Struktur. Grob gesehen, kann man jedoch eine Entwicklung von derNummernoperüber viele verschiedene Mischformen bis hin zur durchkomponierten Oper gegen 1900 feststellen. Von der Barockzeit bis in die Romantik hinein ist die Oper eine Aneinanderreihung in sich geschlossener Musikstücke („Nummern“), die durchRezitativeoder (imSingspiel) gesprocheneDialogemiteinander verbunden werden und eine durchgängigeHandlungdarstellen. Wie auch das Schauspiel kann eine Oper inAkte, inBilder,Szenenbzw.Auftrittegegliedert sein. Die musikalischen Bestandteile der Oper sind vielfältig: Die Trennung der Nummern und die Abgrenzung zwischen Rezitativ und Arie wurden im 19. Jahrhundert in Frage gestellt. Ab 1825 verschwand allmählich dasSecco-Rezitativ, an seine Stelle trat in der italienischen Literatur das Prinzip vonscena ed aria, das beiGiuseppe Verdidie Akte zu einem größeren musikalischen Ganzen formt.Richard Wagnerpropagierte ab der Mitte des Jahrhunderts den Verzicht auf die Nummernstruktur zugunsten einesdurchkomponierten, auf der Grundlage vonLeitmotivengeformten Ganzen. Für Wagners Opern hat sich der BegriffMusikdramadurchgesetzt, das Stichwort „Unendliche Melodie“ steht für ein kontinuierliches Fortschreiten der musikalischen und emotionalen Entwicklung, das sich nach seiner Auffassung gegen musikalische Tanzformen durchsetzen sollte. Seine OperTristan und Isolde(1865) bezeichnete Wagner als „Handlung in Musik“, was an die ursprünglichen Opernbegriffe „favola in musica“ oder „dramma per musica“ erinnern sollte. Die durchkomponierte Form wurde im späten 19. Jahrhundert allgemein bevorzugt, auch beiJules MassenetoderGiacomo Puccini, und blieb das vorherrschende Modell der frühenModernebis zumNeoklassizismus, der mit brüchigen Strukturen und mit Rückbezügen auf Formen der frühen Operngeschichte experimentierte. Auch in sich abgeschlossene Teile aus durchkomponierten Opern werden in Konzerten aufgeführt, wie etwa viele Arien aus Puccini-Opern. Als Meister der durchkomponierten Großform gilt Richard Strauss, der dies insbesondere in den EinakternSalome,ElektraundAriadne auf Naxosunter Beweis stellte. Im 20. Jahrhundert griffen viele Komponisten wieder auf das Nummernprinzip zurück, zum BeispielZoltán Kodály,Igor StrawinskioderKurt Weill. Die Nummernoper besteht außerdem inOperetteundMusicalweiter. In der Geschichte der Oper gab es zumeist einen „hohen“ und einen „niederen“ Stil, frei nach der antiken Unterscheidung zwischenTragödieundKomödie. Nicht immer bedeutet dies jedoch eine Grenze zwischen ernst und lustig. Der „hohe“ Stil kann sich über den „niederen“ auch einfach durch antike Stoffe erheben oder durch adlige Figuren oder durch eine „literarisch“ ernst zu nehmende Vorlage oder durch „schwierige“ (bzw. bloßdurchkomponierte) Musik. All diese Anhaltspunkte für das Wertvollere wurden im Lauf der Geschichte angegriffen. Dabei gab es Gattungen, die den Gegensatz abzuschwächen versuchten wie die Opera semiseria. Solange die Oper noch im Stadium des Experiments war, wie bis zu Beginn des 17. Jahrhunderts, war eine Trennung noch nicht nötig. Sie ergab sich erst, als Opernaufführungen zur Gewohnheit wurden, und zwar aus sozialen Gründen: Die ernste Oper enthielt aristokratisches Personal und „hohe“ politische Symbolik, die komische hatte bürgerliche Figuren und „unwesentliche“ alltägliche Handlungen zum Thema. Allmählich trennten sichOpera seriaundTragédie lyriquevon ihren komischenIntermezzi, aus denenOpera buffaundOpéra-comiquehervorgingen. Diese Trennung wurde erst am Ende des 18. Jahrhunderts aufgebrochen: Weil die Bürger in der für sie bestimmten „niederen“ Operngattung nicht mehr komisch (also lächerlich) dargestellt werden wollten, wurde das Komische oft ins Sentimentale abgebogen und aufgewertet. Daher sind „komische Opern“ oft nicht lustig. Nach derFranzösischen Revolutionlöst sich dieStändeklauselauf, und auch bürgerliche Opern durften „ernst“ sein. Somit ergaben sich im 19. Jahrhundert andere Abgrenzungen zwischen Tragödie und Komödie als im 18. Jahrhundert. Ein Sammelbegriff sowohl für tragische als auch für komische Werke ist das italienischeDramma per musica, wie die Oper in ihrer Anfangszeit betitelt wurde. Ein Beispiel für eine frühe ernste Oper istIl ritorno d’Ulisse in patriavonClaudio Monteverdi. Der seriöse Anspruch resultiert aus dem Rückgriff auf antike Theaterstoffe – insbesondere Tragödien – undepischeHeldendichtungen. Sie wurden seit dem späteren 18. Jahrhundert von jüngeren historischen Sujets verdrängt. Im Italien des 19. Jahrhunderts wurde der Begriff Dramma in der ZusammensetzungMelodrammaverwendet und nicht mehr auf das antike Drama bezogen. SowohlBellinistragische OperNormaals auch die komödiantische OperL’elisir d’amorevonGaetano Donizettiwurden so genannt. Als fester Begriff etablierte sich dieOpera seriaerst im 18. Jahrhundert. Mischformen oder tragikomische Inhalte waren mit dieser Titelbezeichnung ausgeschlossen.HändelsOperRadamistoist ein typisches Werk. Als Antipode zu Italien verlieh Frankreich seiner eigenen Form der Opera seria den TitelTragédie lyrique, wesentlich geprägt durchJean-Baptiste Lullyund dasBallettam HofeLouis’ XIV., später durchJean-Philippe Rameau. Nach der Französischen Revolution etablierte sich allmählich dieGrand opéraals bürgerliche ernste Oper. Dazu zählenLes HuguenotsvonGiacomo Meyerbeer, auch weniger erfolgreiche Werke wieLes TroyensvonHector Berlioz. Das durchkomponierteMusikdramades reiferenRichard Wagner(Der Ring des Nibelungen)hatte großen internationalen Einfluss. Französische Komponisten jener Zeit wieMassenetsetzten dagegen eher auf einen durchsichtigen und gesanglichen Opernstil, für den die BezeichnungDrame lyriqueverwendet wurde. NochDebussyverwendete diesen Begriff für seine OperPelléas et Mélisande. Schon immer konnten Opernstoffe vonRomanen,NovellenoderBühnenwerkenherstammen. Die italienische Oper des 18. Jahrhunderts verstand sich als in Musik gekleideteLiteratur. Seitdem die Musik die absolute Vorherrschaft erlangt hat, also seit dem späten 19. Jahrhundert, nennt man ausgesprochen literarische OpernLiteraturoper.Death in VenicevonBenjamin Brittennach der Vorlage vonThomas Mannist eine recht getreue Umsetzung des literarischen Stoffes in Musik. DieOpera buffaist die Urform der heiteren Oper.PergolesisLa serva padronagalt um die Mitte des 18. Jahrhunderts als das maßgebliche Beispiel. Ein spätes Beispiel istIl barbiere di SivigliavonGioachino Rossini. Die ausnehmend heiteren Opern waren oft geringer angesehen als die sentimentalen. Ihre Stoffe stammen aus dem Volkstheater und von derPosse, stark beeinflusst durch die italienischeCommedia dell’arte. Aus der frühen Opera buffa geht die französischeOpéra-comique (Werkgattung)hervor, die vor der Revolution zur Oper eines zunehmend selbstbewussten Bürgertums wird. Zunächst verstand man hierunter eher einLiederspiel(Vaudeville). Doch der musikalische Anteil wurde immer größer und begann zu überwiegen. Aus der Opéra-comique ist das deutschsprachigeSingspielentstanden. Das Singspiel trägt oft volkstümlich-bürgerlichen Charakter, ist geprägt von einfachenLied- bzw.Rondo-Formen und verwendet stattRezitativengesprochene Dialoge, gelegentlich auchMelodramenzwischen den musikalischen Nummern. Der Hof sprach Französisch. Das Problem der deutschen Oper war im 18. und zum Teil noch im 19. Jahrhundert, dass sie als volkssprachliche Oper zur „niederen“ Gattung gehörte und sich behaupten und emanzipieren musste.Die Entführung aus dem SerailvonWolfgang Amadeus Mozartist eines der bekanntesten Singspiele mit dieser Zielsetzung. Mozart bedient sich für die Arien auch komplexerer musikalischer Formen. Das im Auftrag von KaiserJoseph II.zur Etablierung einesNationalsingspielsgeschaffene, 1782 am WienerBurgtheateruraufgeführte Werk war für die Entwicklung der deutschen Oper von entscheidender Bedeutung. Paris war im 19. Jahrhundert führend für die Operngeschichte, und auch die Italiener wie Rossini und Verdi kamen hierher. DieOpéra-comique, die im Haus derOpéra-Comiqueaufgeführt wurde, blieb auch gegenüber der neu entstandenen, durchkomponiertenGrand opéra, die in derOpérazur Aufführung kam, zweitrangig – weniger von ihrer musikalischen als von ihrer sozialen Bedeutung her. Aus den erwähnten Gründen musste sie nicht unbedingt einen heiteren Inhalt haben. Ein auch im deutschen Sprachgebiet bekanntes Beispiel einer komisch-rührseligen Opéra-comique istDer Postillon von LonjumeauvonAdolphe Adam. Eine Gruppe von formal noch als Opéra-comique zu bezeichnenden Werken nach 1860 verstärkte den sentimentalen Grundcharakter (etwaMignonvonAmbroise Thomas). Ein sentimentaler Einschlag findet sich auch in einigen komischen Opern von Rossini(La Cenerentola). Eine Erneuerung der Opéra-comique gelang mitCarmenvonGeorges Bizet, deren Dramatik in die Richtung derVerismo-Oper weist. Bei ihr war – abgesehen von den proletarischen Figuren – das Reißerische ein Merkmal des „niederen“ Stils. Auch die „Größe“ kann ein Zeichen für hohen oder niederen Stil sein. Zuweilen findet sich der Begriff „Große Oper“ als Untertitel eines Werkes. Damit wird zum Beispiel gesagt, dass das Orchester und der Chor in großer Besetzung spielen und singen sollten, oder dass die Oper ein abendfüllendes Werk mit integriertem Ballett ist. Dies sind Opern, die nur in einem größeren Theater zur Aufführung kommen und sich vom Repertoire der fahrenden Truppen unterscheiden konnten. Als Beispiel für eine „Große Oper“ istManonvonJules Massenetzu nennen. Der BegriffKammeroperbezieht sich dagegen auf ein mit geringem Personal realisierbares Werk. Die Anzahl der Sänger ist in der Regel nicht mehr als fünf, das Orchester wird auf einKammerorchesterbegrenzt. Dies konnte aus der Not materielle Armut hervorgehen und damit auf das „niedere“ Genre verweisen oder im Gegenteil die größere Exklusivität und Konzentration eines „höheren“ Genres bedeuten. Auch die Bühne ist oftmals kleiner, was zu einer intimeren Atmosphäre beitragen kann, die für die Wirkung des Werkes von Vorteil ist. Beispiele dafür wärenAlbert HerringvonBenjamin Brittenoder „Les Larmes de couteau“ vonBohuslav Martinů. Manche Opernkomponisten wehrten sich auch gegen die Einordnung in Gattungstraditionen oder bezeichneten ihre Werke in bewusster Relation zu diesen mit bestimmten Untertiteln. WagnersTristan und Isoldeträgt zum Beispiel die Bezeichnung „Handlung in Musik“,Luciano Berioverwendete für sein WerkPassaggioetwa den Begriff „messa in scena“ (‚Inszenierung‘).George Gershwinbeschrieb sein WerkPorgy and Bessals „An American Folk Opera“. Um sich von klischeehaften Vorstellungen abzugrenzen, bevorzugen moderne Komponisten oft alternative Bezeichnungen wie etwa „azione scenica“ (Al gran sole carico d’amorevon Luigi Nono) oder „azione musicale“ (‚musikalische Handlung‘,Un re in ascoltovon Luciano Berio). AuchPeter Tschaikowskisbekannte OperEugen Oneginwurde vom Komponisten „Lyrische Szenen“ genannt. Richard Geppertschrieb 2016 die deutscheRockoperFreiheitmit den musikalischen Ausdrucksmitteln und Instrumenten derRockmusik.[6] Vereinzelt gibt es Beispiele für Opern – darunterJohn Coriglianos1991 uraufgeführtes WerkThe Ghosts of Versailles–, die bezogen auf die Formselbstreferenziellsind, indem sie selbst wiederum Schauspiel oder Oper enthalten.[7] Aufgrund der nicht immer leichten Abgrenzbarkeit der Gattung Oper von anderen musikalischen Gattungen und Genres und der Praxis desPasticciosist eine Aussage zum Gesamtumfang des Opern-Repertoires mit zahlreichen Schwierigkeiten behaftet. Aktuelle Auflistungen gehen von ca. 5800 bis 6000 bekannten Werken aus. Rechnet man die nicht unerhebliche Anzahl verschollener und verlorener Werke, insbesondere des 18. und frühen 19. Jahrhunderts mit ein, dürfte eine Gesamtzahl von ca. 60.000 Opern realistisch sein.[8] Die große Menge an Werken macht es Theatern und Opernhäusern nicht einfach, eine Auswahl zu treffen, die einem hohen Anspruch genügt und auch genügend Publikum findet. Abhängig von der Größe des Theaters und dem vorhandenen Budget wird vonIntendantundDramaturgiefür jede Sparte des Theaters (Schauspiel, Musiktheater, Ballett, Kinder- und Jugendtheater,Puppentheateretc.) einSpielplanerarbeitet, der dem Haus und seinen Mitarbeitern angepasst ist. Der Spielplan geht auf die regionalen Eigenheiten und Aufführungstraditionen des Ortes ein – zum Beispiel durch open air-Festspiele, Weihnachts- oder Neujahrskonzerte – weist aber auch auf aktuelle Strömungen des Musiktheaters hin, indem auch zeitgenössische Werke aufgeführt werden. Je nach Größe des Hauses werden verschiedene Opern in einerSpielzeitneu inszeniert. Die erste öffentliche Darbietung einer neuen Oper nennt manUraufführung, die erste öffentliche Darbietung einer Oper in einer neuen InszenierungPremiere. Nach und nach hat sich ein praxiserprobter, mehr oder weniger enger Kanon an Opern herausgebildet, die regelmäßig auf dem Spielplan stehen. Etwa 150 Opern bilden diesen nicht festgeschriebenen Kanon im Kern. Entsprechend hat sich das Interesse vor allem desFeuilletonsvon den vielfach bereits bekannten Werken hin zu derenInterpretationverlagert, wobei vor allem dieInszenierungin den Vordergrund rückt. Das Publikum verbindet seine Lieblingsopern oft mit bestimmten Traditionen, die zum Teil auch inKonventionenerstarrt sind, und reagiert auf radikale Deutungsansätze (Regietheater) kontrovers. Bis zur Mitte der 1960er Jahre wurden Opern zumeist in der jeweiligen Landessprache des Aufführungsortes aufgeführt. So wurden Verdi-Opern in Deutschland in deutscher Sprache und Wagner-Opern in Italien in italienischer Sprache gesungen, wie auch Radio- und Fernsehaufzeichnungen belegen. Bereits zuvor gab es jedoch Theater, die Opern in der jeweiligen Originalsprache aufführten, etwa dieMetropolitan Operain New York. Auch dieSalzburger Festspielezeigten Opern stets ausschließlich in der Originalsprache. Aufgrund eines Vertrages mit derMailänder Scala, bei dem sich italienische Sänger verpflichteten, auch an derWiener Staatsoperzu singen, führteHerbert von Karajan1956 an der Wiener Staatsoper das Prinzip ein, Opern in der Originalsprache aufzuführen. Mit seiner Begründung, die Einheit von Wort und Musik gehe bei Übersetzungen in eine andere Sprache verloren, wurden Opern allmählich immer mehr in ihrer ursprünglichen Form aufgeführt. Auch der Schallplatten und Sänger-Markt, der sich zunehmend internationalisierte, trug entscheidend zu dieser Entwicklung bei. In der DDR gab es hingegen weiterhin eine große Tradition von Übersetzungen, jedoch wurde mit neuen Übertragungen (z. B.Walter Felsenstein,Siegfried Schoenbohm) versucht, den Inhalt des Originals genauer, sprachlich gelungener und vor allem musikalisch passender umzusetzen. Heute werden in fast allen großen Opernhäusern Opern in der Originalsprache aufgeführt und dazu simultanÜbertiteleingeblendet. An vielen kleineren Theatern, vor allem im Osten Deutschlands, gibt es noch Aufführungen in deutscher Sprache. Auch gibt es in einigen Städten (z. B. Berlin, München, Wien) mehrere Opernhäuser, von denen eines Opern in Übersetzungen aufführt, wie etwa dieVolksoper Wien, dieKomische Oper Berlin, dasStaatstheater am Gärtnerplatzin München, oder in London dieEnglish National Opera. Hin und wieder gibt es auch eine autorisierte Übersetzung (wie im Falle der OpernLeoš Janáčeks, deren deutscher Text von Janáčeks FreundMax Brodstammt, so dass auch der deutsche Text als original gelten darf). Schwierig gestaltet sich die Aufführung in Originalsprache auch immer dann, wenn Dialoge in dem Werk vorkommen. Hier gibt es auch Mischformen, das heißt, gesprochene Texte werden übersetzt, gesungene erklingen jedoch in Originalsprache. Im BereichSingspiel,Operette,Musicalist daher die übersetzte Musiktheateraufführung weit verbreitet. Für die exakte Übersetzung aus einer Fremdsprache ist am Theater die Dramaturgie zuständig. Wenn die Sprachkenntnisse derKorrepetitorenvertieft werden sollen, werden auch spezialisierte Coaches für eine Fremdsprache hinzugezogen. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Abgrenzungen 1.1Oper und Schauspiel 1.2Oper und Ballett 1.3Oper und Operette/Musical 2Geschichte 2.1Vorgeschichte 2.1.1Antike 2.1.2Mittelalter 2.1.3Renaissance 2.2Ursprung 2.2.1Florentiner Camerata 2.2.2Monteverdi 2.317. Jahrhundert 2.3.1Italien 2.3.2Paris 2.3.3Deutsches Sprachgebiet 2.3.4England 2.418. Jahrhundert 2.4.1Allgemeine Entwicklung 2.4.2Pasticcio 2.4.3Nummernoper 2.4.4Opera buffa 2.4.5Entwicklung der Opera buffa zur Opera semiseria 2.4.6England 2.4.7Frankreich 2.4.8Deutscher Sprachraum 2.4.9Opernreform 2.4.10Verschwinden der Kastratenpartien 2.519. Jahrhundert 2.5.1Allgemeine Entwicklung 2.5.2Deutscher Sprachraum"
  },
  {
    "label": 0,
    "text": "Papierflieger – Wikipedia Papierflieger Inhaltsverzeichnis Formen von Papierfliegern Geschichte Flugverhalten Physik Wettbewerbe Rekorde Motorisierung Literatur Weblinks Einzelnachweise Faltflieger Papierflugzeuge Material Gestaltung Abwurf Umgebung Werfer Gleiter Red Bull Paper Wings Guinnessbuch der Rekorde Red Bull Paper Wings Origami-Flieger Andere EinPapierflieger(auch „Papierschwalbe“ genannt) ist ein ausPapierhergestelltes Flugobjekt. Er besitzt keinen eigenen Antrieb und wird im Regelfall durch Werfen gestartet. Allerdings ist es auch möglich, ihn mit einerModellraketeoder einemKatapultzu starten. Die Beschäftigung mit Papierfliegern ist heute vor allem einHobby, wird aber mitunter auch für die Simulation von Flugeigenschaften oder für andere experimentelle Zwecke benötigt. Ein wesentliches Merkmal des Papierfliegers sind seine meist dünnen Tragflächen.Profile, wie sie im Flugzeugbau üblich sind, eignen sich aus physikalischen Gründen nicht für Papierflieger. Die nötige Stabilität muss daher auf anderem Weg erreicht werden. Nach den Techniken ihrer Herstellung werden Papierflieger inFaltfliegerundPapierflugzeugeunterteilt. Faltflieger werden in der Regel aus einem, manchmal auch aus mehreren Blatt Papier hergestellt. Die Verwendung von Klebstoff oder die Beschädigung des Papiers sind verpönt, Klebeband oder zusätzliche Anbaumassen wie Briefklammern kommen hingegen vor. Wenn Faltflieger ausschließlich durch Falten entstehen und keine zusätzlichen Applikationen enthalten, werden sie auchOrigami-Flieger oder Oriplane genannt. Kleine Flugzeuge, die aus Papier bestehen, basieren auf zum Teil aufwendigemFlugmodellbau. Hier wirdPapier,KartonoderPappelediglich als Werkstoff verwendet. Papierflugzeuge sollten zu mehr als 75 % aus Papier bestehen. Die Idee, Papierspielzeuge zu basteln, stammt ausChina, wo schon vor 2000 JahrenDrachenfliegereine beliebte Unterhaltungsmöglichkeit darstellten. Die erste Erwähnung von Origami-Objekten in China stammt jedoch erst aus dem 17. Jahrhundert. In Europa giltLeonardo da Vincials Urvater der Papierfliegerei. In seiner Nachfolge haben viele Pioniere der Luftfahrt mit Papierfliegern experimentiert. Überliefert sind beispielsweise Modelle von SirGeorge Cayley,Alphonse PénaudundOtto Lilienthal. Die erste Veröffentlichung über Papierflieger ist wohl das Buch „Model Gliders“ von E. W. Twining von 1909. Das erste deutschsprachige Buch mit dem Titel „Das kleine Buch vom Papierflugzeug“, Autor Gerhard Katz, erschien 1953. Im Jahr 1967 fand der vonHoward Luck Gossagefür die Zeitschrift „Scientific American“ organisierte „1st International Paper Airplane Competition“ statt. Darauf folgte im Jahre 1985 der „2nd Great International Paper Airplane Contest“. Beide Wettbewerbe sind Bestandsaufnahmen der Papierflieger dieser Zeit. Trotz zahlreicher älterer Quellen ist nicht sicher, wo genau viele der heute bekanntenModelleund Designs entstanden sind. Der Flug eines Papierfliegers wird durch folgende Aspekte beeinflusst: Das Verhalten von Papierfliegern in der Luft lässt sich mit guter Näherung mittels Schulmathematik voraussagen. Je nach Konstruktion bewegen sich Papierfliegerballistischund/oderaerodynamischdurch die Luft. Reine Ballisten nennt manWerfer, alle übrigen werdenGleitergenannt. Ein reiner Werfer bewegt sich auf einerballistischen Kurvedurch die Luft. Eine gute Näherung für seine Flugbahn ist dieWurfparabel, welcher er exakt folgen würde, wenn es den Luftwiderstand nicht gäbe. Auch ein Gleiter lässt sich ballistisch starten. Nach dem Erreichen des Scheitelpunktes der ballistischen Kurve wird seine Bewegung jedoch zumaerodynamischenFlug. Gleitflieger verhalten sich im aerodynamischen Flug ähnlich wie richtige Flugzeuge oder Vögel. Im Gegensatz zu diesen sind Papierflieger aber klein und bewegen sich mit geringer Geschwindigkeit, weshalb ihre Tragflächen wie bei Insektenlaminarund nichtturbulentumströmt werden. Eine spezielle Form des Gleiter ist der sogenannte Spazierflieger, englisch auch walk along glider genannt. Bei diesem Papierflugzeugtyp ist die Fluggeschwindigkeit so gewählt, dass sie in etwa der Gehgeschwindigkeit eines Menschen entspricht. Erzeugt dieser Mensch im Nachlaufen mit seinen Händen, einem Brett oder ähnlichem einen Aufwind, welcher mit der Sinkgeschwindigkeit des Gleiters korrespondiert, so fliegt dieser vor der Person in konstanter Höhe. Durch Veränderung von Stärke und Richtung des Aufwindes lässt sich der Spazierflieger steuern[1]. Das Werfen von Papierfliegern hat vor allem beiKindernTradition, doch Anfang des 21. Jahrhunderts etablierte sich das Papierfliegen auch alsSportart. Vor allem beiStudentenkommt die Sportart, bei der es sowohl auf das Physische als auch auf das Geistige ankommt, sehr gut an. Im Laufe der Jahre hat sich vor allem ein Wettbewerb herauskristallisiert, dieRed Bull Paper Wings. Zwar gibt es im Papierfliegen keine offiziellenWeltmeisterschaften, doch die Red Bull Paper Wings nennen sich auch World Championships. Sie finden seit 2006 alle 3 Jahre früh im Mai in Salzburg imHangar 7statt. Für Papierflieger gibt es keine offiziellen Weltmeisterschaften. Dennoch existieren verschiedene Rekorde, welche allerdings unter unterschiedlichen Wettbewerbsbedingungen mit zum Teil stark voneinander abweichenden Regeln aufgestellt wurden. Untereinander sind diese Leistungen deshalb nur schwer vergleichbar. ImGuinnessbuchder Rekorde sind folgende Daten aufgeführt: DieRed Bull Paper Wingsführen als Rekordhalter: Mit einem sogenannten „Smart-Modul“-Antrieb, der an Papierflieger auf Basis vonA4-Format-Papierbögen geklemmt werden kann, können Papierflieger motorisiert und ferngesteuert werden. Das Antriebsmodul besteht aus einer dünnen Stange (dem Rumpf), die am hinteren Ende einen elektrisch angetriebenenDruckpropellerund einSeitenruderträgt und vorn einen Empfänger mit integriertem Akku und einerSteuerelektronik. Es wird perBluetoothmit einer für das Antriebsmodul entwickeltenAppfürSmartphonesgekoppelt. Die App wertet dieLagesensorendes Smartphones aus und überträgt die Veränderungen als Steuerbefehle an das Antriebsmodul.[8] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Formen von Papierfliegern 1.1Faltflieger 1.2Papierflugzeuge 2Geschichte 3Flugverhalten 3.1Material 3.2Gestaltung 3.3Abwurf 3.4Umgebung 4Physik 4.1Werfer 4.2Gleiter 5Wettbewerbe 5.1Red Bull Paper Wings 6Rekorde 6.1Guinnessbuch der Rekorde 6.2Red Bull Paper Wings 6.3Origami-Flieger 6.4Andere 7Motorisierung 8Literatur 9Weblinks 10Einzelnachweise Afrikaans العربية Беларуская (тарашкевіца) Български Brezhoneg Català Čeština Dansk"
  },
  {
    "label": 0,
    "text": "Paris – Wikipedia Paris Inhaltsverzeichnis Geografie Geschichte Hoheitssymbole Gesellschaft Politik Kultur und Sehenswürdigkeiten Wirtschaft und Infrastruktur Persönlichkeiten Siehe auch Literatur Dokumentarfilme Weblinks Einzelnachweise Lage Klima Geologie Seine Inseln Hügel Stadtgliederung Antike Mittelalter Neuzeit Demografie Einwanderung Religionen Stadtregierung Stadtrat(Conseil de Paris) Städtepartnerschaften Theater Museen Bauwerke Grünflächen Film Sport Regelmäßige Veranstaltungen Gastronomie Einzelhandel Sehenswürdigkeiten in der Umgebung Wirtschaft Verkehr Wissenschaft und Bildung Ehrenbürger Söhne und Töchter der Stadt Persönlichkeiten, die vor Ort gewirkt haben Photographie-Bücher Paris – Stadt des Exils Paris im Zweiten Weltkrieg Paris nach dem Ende des Zweiten Weltkriegs Brücken Plätze und Straßen Weltliche Bauwerke Kirchen Promenaden, Parks und Gärten Friedhöfe Sportveranstaltungen Sportstätten Straßen-Fernverkehr Flughäfen Eisenbahn Nahverkehr Luftqualität Antike Mittelalter Frühe Neuzeit 17. Jahrhundert 18. Jahrhundert 19. Jahrhundert 20. Jahrhundert 21. Jahrhundert Mittelalter Neuzeit Paris(deutsch:[paˈʁiːs]ⓘ; französisch:[paʁi]ⓘ) ist dieHauptstadtderFranzösischen Republik, Hauptort derRegionÎle-de-FranceundGlobalstadt. Mit rund 2,11 Millionen Einwohnern ist Paris dieviertgrößte StadtderEuropäischen Union(EU). Der Großraum ist mit über 12,5 Millionen Menschen die größteMetropolregionder EU.[1][2][3][4] Mit einer vergleichsweise kleinen Stadtfläche von 105,40 Quadratkilometern ist Paris mit 20.054 Einwohnern pro Quadratkilometer die am dichtesten besiedelteGroßstadtEuropas. Das zusammenhängend bebaute städtische Siedlungsgebiet(Unité urbaine de Paris)ist 2845 Quadratkilometer groß und geht somit weit über diepolitische Grenzeder Stadt Paris hinaus. 2015 zählte die Unité urbaine de Paris 10.706.072 Einwohner, was einer Bevölkerungsdichte von 3763 Einwohnern je Quadratkilometer entspricht und womit Paris zu denMegastädtenzählt.[5]Paris ist das politische, wirtschaftliche sowie kulturelle Zentrum deszentralistischorganisierten Frankreichs und mit vier Flughäfen und sechs Kopfbahnhöfen dessen größter Verkehrsknotenpunkt. Teile des Seineufers zählen heute zumUNESCO-Welterbe. Die Stadt ist Sitz derUNESCOund darüber hinaus derOECDund derICC. Sehenswürdigkeiten wie derEiffelturm, dieKathedrale Notre-Dameoder derLouvremachen die Stadt zu einem beliebten Touristenziel. Mit rund 16 Millionen ausländischen Touristen pro Jahr ist die Stadt hinterLondonundBangkokeine der meistbesuchten Städte weltweit.[6] Das heutige Paris entwickelte sich seit dem 3. Jahrhundert v. Chr. aus derkeltischenSiedlung „Lutetia“ auf derÎle de la Cité. Später errichteten die Römer an der Seine eine Stadt, die im 6. Jahrhundert zunächst eine Hauptresidenz desFränkischen Reicheswurde. Eine Blütezeit der Kunst und Kultur erlebte Paris im 16. Jahrhundert unterFranz I.Durch denAbsolutismus, insbesondere unterLudwig XIV.im 17. Jahrhundert, wurde die Stadt um zahlreiche barocke Gebäude und Prachtstraßen bereichert und so zu einem beispielhaften Muster für barocken Städtebau. Obwohl die Königsresidenz 1682 nachVersaillesverlegt wurde, blieb Paris aufgrund seiner politischen und wirtschaftlichen Bedeutung das Zentrum des Landes. Mit derFranzösischen Revolutionkam ab 1789 eine welthistorische Bedeutung zu. DieIndustrialisierungführte im 19. Jahrhundert zu einem enormen Bevölkerungszuwachs, sodass 1846 erstmals die Grenze von einer Million Einwohnern überschritten wurde. In den folgenden Jahrzehnten bekam die Stadt durch die sogenannteBelle Époqueund sechs Weltausstellungen weltweite Beachtung. Das Stadtgebiet hat eine Fläche von 105,40 Quadratkilometern. Das entspricht ungefähr der Fläche vonKoblenzoder vonGelsenkirchenund weniger als 12 Prozent der FlächeBerlins. Hierbei handelt es sich aber nur um die Fläche der Kernstadt. Die Metropolregion erstreckt sich über eine Bodenfläche von 14.518 Quadratkilometern. Das entspricht etwa der FlächeSchleswig-Holsteins. Die Stadt liegt im Zentrum desPariser Beckens, in einer Höhenlage von 28 bis 130 Metern (durchschnittlich65m). Die Seine verlässt, je nach Wasserstand, in25mdas Stadtgebiet. Paris ist umgeben von den beiden großenStadtwäldernBois de BoulogneundBois de Vincennes, die der Bevölkerung als Naherholungsgebiete dienen. Paris befindet sich in dergemäßigten Klimazone. DieJahresmitteltemperaturbeträgt 10,8 Grad Celsius und die durchschnittliche Jahresniederschlagsmenge 649,6 Millimeter. Der wärmste Monat ist der Juli mit 18,4 Grad Celsius im Mittel, der kälteste der Januar mit durchschnittlich 3,5 Grad Celsius. Der meiste Niederschlag fällt im Mai mit 65,0 Millimetern im Mittel, der wenigste im August mit durchschnittlich 43,0 Millimetern. Seit 1873 finden in Paris regelmäßige meteorologische Messungen statt. Die tiefste bisher festgestellte Temperatur betrug −23,9 Grad Celsius und stammt vom 10. Dezember 1879. Der Wärmerekord liegt bei 42,6 Grad Celsius und wurde am 25. Juli 2019 imParc Montsourisgemessen. Der bis dahin höchste Lufttemperaturwert betrug 40,4 Grad Celsius und war am 28. Juli 1947 ebenfalls im Parc Montsouris gemessen worden.[7] Das Pariser Becken bildet eine großeSchichtstufenlandschaft. Schüsselförmig liegen hier die Schichten desMesozoikumsund desPaläogens(früherAlttertiär) ineinander und sind von der Abtragung zu einer weit gespannten Stufenlandschaft ausgearbeitet worden, deren Stufen sich jeweils nach außen richten. Nur im östlichen Teil herrschen am Abfall dieser Stufen gegen dieSaône-FurchetektonischeBruchlinien vor. Sie bewirken die steilen Abfälle desPlateaus von Langresund derCôte d’Or(bis 636 Meter), die berühmte Weinbaugebiete sind, da sie im Regenschatten derLeeseitegrößere Sonnenscheindauer haben und zudem noch die Vorteile der Südexposition genießen. Eine gewisse Ungleichförmigkeit besteht insofern, als die Schichtenfolge im nordöstlichen Teil vollkommener ist als im Westen. Die etwas stärkere Heraushebung des Ostflügels hat auch allgemein größere Höhenunterschiede und eine markantere Herausbildung der Stufen mit sich gebracht. Beckeneinwärts ragt als bedeutende Stufe die derEozänen-Kalke auf, in deren Innerem dieÎle-de-France, das Ballungsgebiet von Paris, eingebettet liegt. DieSeineverbindet Paris mitBurgundim Landesinneren und mit demÄrmelkanalan der Nordküste. Der hier leichte Übergang über sie war der wichtigste Faktor für die Entstehung und Entwicklung der Stadt, die auf der größten der seinerzeit zahlreichen Seineinseln ihren Ursprung hat. Sie spaltet die Stadt in zwei ungleiche Uferhälften, das nördliche Ufer, das grob betrachtet dem Handel und Finanzen gewidmete rechte Ufer(Rive Droite)und die südliche Stadthälfte am linken Ufer(Rive Gauche), die mit demQuartier Latinals Viertel der Intellektuellen angesehen wird und als Wohngegend gefragt ist. Seit 1991 ist dasSeineufer von Pariszwischen derPont de Sullyund den BrückenPont d’Iéna(rechtes) undPont de Bir-Hakeim(linkes Ufer) mit 365 Hektar FlächeWeltkulturerbe. DieÎle de la Citéim Herzen der Stadt wurde in der Antike besiedelt und ist damit der älteste Teil der Hauptstadt. 1584 ließHeinrich III.drei der westlichen Inselspitze vorgelagerte kleine und sumpfige Inseln untereinander verbinden und gliederte sie der größeren an. Damit wuchs die Fläche im Laufe der Jahrhunderte von ursprünglich 8 auf insgesamt 17 Hektar an. So konnte ein „königlicher“ Platz, diePlace Dauphine, mit einer einheitlichen Saumbebauung entstehen und aus dem Verkauf der Häuser das Geld zum Bau einer Brücke beschafft werden, welche die Verbindung zu den beiden Seineufern herstellt. DiePont Neuf(deutsch „Neue Brücke“) ist heute die älteste der in Paris erhaltenen Brücken. Auch dieÎle Saint-Louis, die kleinere der nebeneinander liegenden Seineinseln, ist eine Zusammenfügung von zwei Inselchen, derÎle aux Vachesund derÎle Notre Dame. Im Gegensatz zu ihrer großen Schwester, derCité, blieb sie bis zum Anfang des 17. Jahrhunderts unbebaut. Im Jahre 1614 beauftragteLudwig XIII.den Bauunternehmer Christophe Marie mit der Erschließung des Geländes. Marie schüttete den Seinearm zu, umfasste die beiden kleinen Inseln mit einer Kaimauer und ließ Brücken zu den Flussufern errichten. Ab etwa 1618 wurde das Gelände zunächst mit Häusern für Handwerker und Kaufleute bebaut, ab 1638 auch mit luxuriösen Stadtpalästen für hohe Würdenträger. Die Bebauung mit geraden Straßen folgte einem festen Grundplan, der noch heute erkennbar ist. Die frühereÎle des Cygnes(Schwaneninsel) wurde 1773 mit demChamp de Mars, dem Manöverfeld der Militärschule, verbunden. Ihr Name ging auf dieÎle aux Cygnesüber, einen im Jahr 1825 künstlich in der Seine angelegtenDamm, auf dem unter anderem eineKopie der Freiheitsstatuesteht. Der Damm entstand als Fundament für eine auffällige Brücke, diePont de Bir-Hakeim, deren unteres Niveau die Stützen für den darüber gelegenenViaduktderMetroaufzunehmen hatte. Die höchste natürliche Erhebung innerhalb der Stadtgrenzen ist der Hügel(Butte)Montmartremit einer Höhe von 129 Metern. Auf den Hügel fährt dieStandseilbahnFuniculaire de Montmartre. Der am Nordhang angelegte Weinberg ist, seit auch imParc Georges BrassensimParc de Bellevilleund imParc de BercyWein wächst, nicht mehr der einzige von Paris. Paris wurde im Jahre 1790 Verwaltungssitz desDépartements Seinemit der Ordnungsnummer 75 und ist seit der Neugliederung der Départements der RegionÎle-de-Franceim Jahre 1968 gleichzeitig Stadt undDépartement. Abgesehen von der geografischen Gliederung inRive Droite,Rive Gaucheund „Inseln“ ist Paris in Stadtbezirke (Arrondissements, abgekürztArrdt.bzw. Arrt) und Viertel(Quartiers)unterteilt. Der FlussSeineteilt die Stadt in einen nördlichen (Rive Droite, „rechtes Ufer“) und einen südlichen Teil (Rive Gauche, „linkes Ufer“); administrativ ist sie in 20 Stadtbezirke(Arrondissements)unterteilt. Seit dem 11. Juli 2020 sind das 1., 2., 3. und 4. Arrondissement verwaltungsrechtlich in einem einzigen Sektor namensParis Centrezusammengefasst.[8] Die 20 nummerierten Stadtbezirke tragen die Postleitzahlen 75001 bis 75020 und durchziehen Paris spiralförmig von innen nach außen. Die Spirale beginnt im historischen Stadtkern, der Gegend um denLouvre, dasPalais Royalund dasForum des Halles, und endet nach zweieinhalb im Uhrzeigersinn verlaufenden Umdrehungen im Osten der Stadt, dem Arrondissement desFriedhofs Père-Lachaise. Jedem Arrondissement steht ein Bürgermeister(maire d’arrondissement)vor, der im Bürgermeisteramt seines Bezirkes(mairie d’arrondissement)residiert (außer für die ersten vier Arrondissements, die ab 2020 imSecteur Centrezusammengefasst sind und von einem einzigen Bürgermeister verwaltet werden). Jeder Bezirk untergliedert sich seinerseits in Viertel, französischQuartiers. Die französische Schreibweise der Nummerierungen folgt dem Schema mitOrdinalzeichen. So erfolgt die Schreibweise der Ordinalzahl 1 für das 1. Arrondissement mit einem hochgestellten „er“ (1er), der Ordinalzahl 2 und den Folgezahlen mit einem hochgestellten „e“ (2e) für das 2. Arrondissement usw. Auch die Schreibweise mitrömischen Zahlenist gängig (z. B.000001Ier,000007VIIe,000016XVIe). Der antike Name der Stadt warLutetia(auch: Lutezia). Lutetia entwickelte sich seit Mitte des 3. Jahrhunderts v. Chr. aus derkeltischenSiedlungLutetiades Stammes derParisiiauf der Seine-Insel, die heuteîle de la Citéheißt. Erstmalige schriftliche Erwähnung fand der Name Lutetia 53 v. Chr. im sechsten Buch vonJulius CaesarsDarstellung desgallischen KriegesDe bello Gallico.[9] Als dieRömersich im Jahr 52 v. Chr. nach einem ersten gescheiterten Anmarsch zum zweiten Mal der Stadt näherten, zündeten dieParisiiihren Hauptort Lutetia an und zerstörten die Brücken, bevor sie in Stellung gingen. Die siegreichen Römer überließen ihnen die Insel und bauten auf dem linken Ufer der Seine in dominanter Lage auf dem späterMontagne Sainte-Genevièvegenannten Hügel eine neue römische Stadt auf. Dort entstanden Thermen, ein Forum und ein Amphitheater. Die Stadt wurde im römischen Reich alsCivitas ParisiorumoderParisiabekannt, blieb aber im besetzten Gallien zunächst recht unbedeutend. Im 4. Jahrhundert setzte sich der heutige Name der Stadt durch.[10] Vom Namen Lutetia leitet sich der Name des 1905 entdeckten chemischen ElementsLutetiumab. Im 5. Jahrhundert wurde die römische Herrschaft durch dieMerowingerbeendet. Im Jahre 508 wurde Paris Hauptstadt des Merowingerreiches unterChlodwig I.(466–511). Danach wurde Paris unter einem seiner Söhne zur Hauptstadt eines fränkischenTeilkönigreichs. Während der Karolingerherrschaft überfielen dieNormannenwiederholt die Stadt. DieKapetingermachten Paris zur Hauptstadt Frankreichs.Philipp II. Augustus(1165–1223) ließ die Stadt befestigen. 1190 wurde eine Mauer am rechten Ufer der Seine und im Jahre 1210 ein Wall am linken Ufer errichtet. Zu jener Zeit gab es am rechten Seineufer zahlreiche Händler. Auf Veranlassung Philipp II. entstand am westlichen Stadtrand derLouvre. 1181 wurde die erste überdachte Markthalle eröffnet und 1301 auf derîle de la Citéein Königspalast gebaut. DieSorbonneim Süden von Paris hat sich aus mehreren kleinen Schulen entwickelt.Karl V.(1338–1380) ließ am linken Seineufer die Mauer zum Schutz der Stadt vor denEngländernerneuern. 1370 wurde auf seine Veranlassung am rechten Ufer, wo heute diegrands boulevardsverlaufen, ebenfalls eine Mauer errichtet. Während desHundertjährigen Kriegeswar Paris von 1420 bis 1436 von englischen Streitkräften besetzt. Während derHugenottenkriegezwischen 1562 und 1598 blieb die Stadt inkatholischemBesitz. In derBartholomäusnachtam 24. August 1572 wurden in Paris tausendeHugenottenermordet. Auf VeranlassungLudwigs XIV.(1638–1715) sind Straßenbeleuchtungen angebracht, die Wasserversorgung modernisiert und die KrankenhäuserInvalidesundSalpêtrièreerbaut worden. Er ließ diePariser Stadtmauernabtragen und an deren Stelle den „Nouveau Cours“ errichten, eineRingstraßeaus der später dieGrands Boulevardswurden. DieResidenzdes Königs wurde nachVersaillesverlegt. Dennoch blieb Paris das politische Zentrum Frankreichs, was auf seine hohe Bevölkerungszahl und seine führende wirtschaftliche Rolle im Land zurückzuführen war. Als im Jahre 1789 dieFranzösische Revolutionausbrach, war es die Bevölkerung von Paris, die den Weg zur Abschaffung derMonarchieund zur Einführung der ersten französischen Republik ebnete. 1844 wurde unter KönigLouis-Philippean der Stelle der heutigen StadtautobahnBoulevard périphériqueeine neue Befestigungsanlage errichtet, dieThierssche Stadtbefestigung. Sie hatte eine Länge von 39 Kilometern und war mit ihren 94Bastionenund 16Fortsdie größte Befestigungsanlage der Welt. Paris war in den Jahren 1855, 1867, 1878, 1889, 1900 und 1937 Veranstaltungsort von sechsWeltausstellungen, welche die kulturelle und politische Bedeutung der Stadt unterstrichen. ImZweiten Kaiserreichunter dem Präfekten von ParisHaussmannkam es zu großen Umgestaltungen der Stadt, die noch bis heute das Stadtbild prägen (weitgehender Abriss alter Viertel und Schaffung großer Straßenzüge (Boulevards)). Der katastrophale Verlauf desKrieges von 1870/71brachte das Ende des Zweiten Kaiserreichs; nach derBelagerungdurch deutsche Truppen kapitulierte die Hauptstadt, worauf sich im Frühjahr 1871 die sogenanntePariser Kommunebildete. Sie bestand aus Arbeitern, Handwerkern und Kleinbürgern und revoltierte gegen die konservative provisorische Regierung der Republik. Paris erlebte zur Zeit derDritten Republikvor 1914 eine wirtschaftliche und kulturelle Blütezeit in derBelle Époque. An einem Bahnhof, demGare de Lyon, an einer Brücke, derPont Alexandre IIIund den U-Bahn-Stationen ist der Baustil dieser Zeit beispielhaft zu erkennen. 1900 war Paris Austragungsort derII.und 1924 derVIII.Olympischen Spiele der Neuzeit. ImErsten Weltkriegwurde Paris am 30. August 1914 zum ersten Mal von einem deutschen Flugzeug aus der Luft angegriffen, und am 31. Januar 1918 wurde es von deutschen Zeppelinen und Gotha G-Bombern bombardiert, wobei 63 Menschen ums Leben kamen.[11]Der letzte deutsche Luftangriff des Ersten Weltkrieges auf Paris erfolgte im September 1918. Bei der Siegesfeier am 14. Juli 1919 paradierte auch der japanische Generalstab auf den Champs Élysées.[12] 1921 erreichte Paris mit rund 2,9 Millionen die bis heute höchste Einwohnerzahl seiner Geschichte.[13]Der städtische Wohnungsbau konnte mit der Nachfrage nicht mehr Schritt halten. Ab etwa 1925 begann in Frankreich eine innenpolitisch instabile Phase (sieheDritte Französische Republik). Es gab schnell wechselnde Regierungen. Dazu trug auch dieWeltwirtschaftskrisebei. Sie begann in vielen Ländern im viertenQuartal1929 und in Frankreich verzögert 1931. Am 6. Februar 1934 kam es in Paris zu einer großen antiparlamentarischen Straßenschlacht, an der diefaschistischeBewegungCroix de Feumaßgeblich beteiligt war. Nach dem Rücktritt vonÉdouard Daladier(1934) bildeteGaston Doumergueeine Regierung der nationalen Einheit(Union nationale)ohneKommunistenundSozialisten. Am 26. April und 3. Mai 1936 konnten die Parlamentswahlen von der neu gebildetenVolksfrontaus Sozialisten, Kommunisten undRadikalsozialistenmit derParole«Brot, Frieden, Freiheit» gewonnen werden. Der SozialistLéon Blumwurde 1936/37 und 1938 Ministerpräsident. Sein Nachfolger wurde zweimal der Radikalsozialist Édouard Daladier. Namen wieHeinrich HeineundKarl Marxverweisen auf die Bedeutung, die Paris bereits im 19. Jahrhundert als Zufluchtsort politisch Verfolgter aus Deutschland hatte. Die Stadt „galt als Ort revolutionärer Ideen und wurde ein Sammelbecken für politische Aktivisten aus ganz Europa“.[14] An diese Tradition knüpften viele Menschen in Deutschland nach derMachtübergabean dieNationalsozialistenan. „Die größte Zahl an Exilantinnen und Exilanten aus dem Deutschen Reich ging nach Frankreich, das auch kulturell und politisch das wichtigste Aufnahmeland wurde.“[15]Innerhalb Frankreichs wiederum entwickelte sich Paris zum Zentrum des deutschsprachigen Exils. „Bis 1939 lebten etwa 10.000 deutsche Flüchtlinge in der Hauptstadt und damit etwa 80% aller Exilanten, die nach Frankreich geflohen waren. Die überwiegende Mehrheit der Flüchtlinge waren als Juden Verfolgte.“ Der Blick auf die deutschen Exilanten in Paris ist stark geprägt von den vielen bekannten Künstlerinnen und Künstlern, Wissenschaftlerinnen und Wissenschaftler, Autorinnen und Autoren, die dort lebten und eine deutschsprachige Kulturlandschaft etablieren konnten. Zugleich formierte sich hier aber auch der politische Widerstand gegen das Deutsche Reich.[16]Das darf aber nicht darüber hinwegtäuschen, dass viele Emigranten auch in Paris unter prekären wirtschaftlichen Bedingungen leben mussten. „Sie wurden 1934 und 1935 nach und nach von der Ausübung bestimmter Berufe, wie z. B. Rechtsanwalt und Arzt, ausgeschlossen, und mussten einen offiziellen französischen Arbeitsvertrag vorweisen, um eine Arbeitserlaubnis zu erhalten.“[17]Am Schicksal des früheren Frankfurter RechtsanwaltsAdolf Moritz Steinschneider, der seit 1935 im Pariser Exil lebte, lässt sich diese wirtschaftliche Ausgrenzung exemplarisch nachvollziehen. Nach dem Sturz der Volksfrontregierung und der Ernennung Daladiers zum Ministerpräsidenten verschlechterte sich das politische Klima gegenüber den in Frankreich lebenden Ausländern. Deutsche Exilanten und die nach Frankreich flüchtenden Anhänger derZweiten Spanischen Republikwurden zunehmend zuunerwünschten Ausländern, aus denen nach dem Ausbruch des Zweiten Weltkriegsfeindliche Ausländerwurden. Frankreich hatte bereits 1935 rechtliche Voraussetzungen dafür geschaffen, im Falle einer tatsächlichen oder vermuteten äußeren Bedrohung Maßnahmen gegen im Lande lebende Ausländer zu ergreifen. Die ersten Opfer dieser Maßnahmen waren die Flüchtlinge aus demSpanischen Bürgerkrieg, die Anfang des Jahres 1939 in großen Lagern im Süden Frankreichs interniert wurden. Mit dem Ausbruch des Zweiten Weltkriegs wurden dann auch die in Frankreich lebenden Emigranten aus dem nationalsozialistischen Machtbereich zu Opfern landesweiter Internierungen. Eine Hochburg dieser Verfolgungsmaßnahmen war die Stadt Paris, wo wie andernorts auch „am 5. September 1939 alle männlichen deutschen Staatsangehörigen zwischen 17 und 50 Jahren durch Plakate und die Presse aufgefordert [wurden], sich unverzüglich mit leichtem Gepäck, Besteck und Lebensmitteln für zwei Tage an einem angegebenen Sammelpunkt einzufinden. Am 14. September wird diese Maßnahme auf die 50- bis 65jährigen ausgedehnt.“[18]:S. 48Als unmittelbare Folge wurden in Paris sofort und in den Folgemonaten großeCentres de rassemblement(CRE; Sammelzentren) eingerichtet, unter anderem im: Wegen der Überfüllung dieser Sammelzentren erfolgte die Verlegung der Internierten in Lager in denDépartementsund schließlich in dasCamp des Milles, das zum zentralen Sammellager im Südosten Frankreichs wurde.[18]:S. 50Die Folgen dieser dasAsylrechtaußer Kraft setzenden Maßnahmen für die Internierten lassen sich am Beispiel des SchriftstellersKurt Sternexemplarisch nachvollziehen. Bei den Razzien nach dem Kriegsausbruch gerieten auch Frauen ins Visier der Behörden, und zwar Frauen, „deren Lebensweise die Behörden aus unterschiedlichen Gründen als zweifelhaft ansahen, […] die bei einer Vorladung selbstbewusst aufgetreten waren oder von denen man nicht wusste, wie sie ihr Geld verdienten“.[19]Das richtete sich zunächst gegen Ausländerinnen, doch nach einem Gesetz vom 18. November 1939 richteten sich die Maßnahmen auch gegen Französinnen, die aus nationaler Sicht politisch verdächtig waren.[20]Der Internierungsort für diese Frauen war in Paris das GefängnisPetite Roquette, ihr weiterer Bestimmungsort dann ab Oktober 1939 das zum reinen Fraueninternierungslager umfunktionierteCamp de Rieucros.[19] Während desWestfeldzugsder deutschenWehrmachtkam es im Juni 1940 zur Schlacht um Frankreich, nachdem die Briten während derSchlacht von Dünkirchendas Festlandgeräumt hatten (26. Mai bis 4. Juni). Vor den auf Paris anrückenden deutschen Truppen wich diefranzösische RegierungüberToursnachBordeauxaus. Auch Tausende Einwohner flüchteten aus Paris. Auf GeneralWeygandsAntrag hin erklärte die Regierung, um unnötige Kämpfe und Zerstörungen abzuwenden, Paris am 11. Juni zuroffenen Stadt.[21] Nachdem dem Armeeoberkommando 18 unter GeneraloberstGeorg von Küchlerdurch einen Unterhändler die Räumung der Stadt durch die7. Französische Armeezugesichert worden war, zogen Wehrmachtsverbände am 14. Juni kampflos in das menschenleer wirkende Paris ein. Mit der Einnahme von Paris waren keine strategischen Ziele verbunden. AmArc de Triomphenahmen Küchler und der Oberbefehlshaber derHeeresgruppe B, GeneraloberstFedor von Bock, den Vorbeimarsch der18. Armeeab. 1943/44 unterhielt dieKriegsmarineeinMarinelazarettin der Stadt. Von größeren Zerstörungen blieb die Stadt verschont. Bis zurBefreiung am 25. August 1944war Paris von der deutschenWehrmachtbesetzt. Der deutsche Stadtkommandant von Paris, GeneralDietrich von Choltitz(1894–1966), kapitulierte an diesem Tag undverweigertedamit einen BefehlHitlers, Paris zu verteidigen oder „nur als Trümmerfeld in die Hand des Feindes fallen“ zu lassen.[22][23] Unter der deutschen Besatzung wandelte sich ab Oktober 1940 in Zusammenarbeit zwischen dem Vichy-Regime und den Besatzern die bis dahin eher alsCentre de Rassemblement des Etrangers(Sammelstelle für Ausländer) beziehungsweiseCentre de séjour surveillé(Zentrum für überwachten Aufenthalt) genutzteCaserne des Tourellesin „das einzige […] während der gesamten Besatzungszeit in Paris eingerichtete“ Internierungslager.[24] Eine weitere Folge der Besatzungszeit war die Verfolgung Zehntausender in Paris lebender Juden und deren Verhaftung.[25]Sie wurden vorwiegend über die drei Sammel- und DurchgangslagerDrancy,PithiviersundBeaune-la-Rolandein die von den Deutschen errichtetenVernichtungslagerdeportiert. Auftakt hierfür war dieRafle du Billet Vert(Grüne Briefe Razzia, benannt nach der Farbe der Polizeivorladung)[26]am 14. Mai 1941. Am Tag zuvor hatten mehrere tausend ausländische Juden eine vom Polizeikommissar unterzeichnete Vorladung erhalten, mit der sie aufgefordert wurden, am nächsten Tag in einem von fünf vorgegebenen Zentren zur „Prüfung ihrer Situation“ zu erscheinen. Innerhalb weniger Stunden wurden 3.700 Männer, überwiegend polnischer und tschechischer Nationalität oder Staatenlose, festgenommen. Sie wurden zumBahnhof Austerlitzgebracht und bestiegen dort die Züge zu den Lagern Pithiviers und Beaune-la-Rolande. Dort blieben sie ein Jahr, bevor sie nach Auschwitz deportiert wurden.[25][26] Auf die Razzia im Mai 1941 folgten zwei weitere: am 20. August und Ende 1941. Letztere erfolgte als Reaktion auf die von derRésistanceverübten Attentate, wodurch aber der Widerstand in der Stadt nicht zum Erliegen kam.[25] Ab Mai 1942 waren die Pariser Juden verpflichtet, denGelben Sternzu tragen.[25]Dieser Maßnahme folgte im Juli 1942 die nächste große Repressionswelle. Sie ging, benannt nach der als Sammellager für die Verhafteten auserkorenen RadsporthalleVélodrome d’Hiver, unter dem NamenRafle du Vélodrome d’Hiver(Razzia des Wintervelodroms) in die Geschichte ein. Über 13.000 Juden wurden im Vélodrome zusammengepfercht, bevor ab dem 19. Juli 1942 zunächst die Erwachsenen mit Viehtransportwagen in das VernichtungslagerKZ Auschwitz-Birkenaudeportiert wurden. Die Verfolgungen und Deportationen gingen von da an während der gesamten Besatzungszeit weiter.[25] An die französischen Opfer der Deportationen erinnert in Paris dasMémorial des Martyrs de la Déportation. Diese Gedenkstätte befindet sich „an der östlichen Spitze der Seine-Insel Île de la Cité, hinter den Gärten an der Rückfront von Notre Dame“.[27](Lage) Gewaltsame Auseinandersetzungen um denAlgerienkriegerschütterten Anfang der 1960er-Jahre auch Paris. Sowohl die rechtsextremeOAS[28]als auch die UnabhängigkeitsbewegungFLN[29]terrorisierten die Stadt mit Bombenanschlägen und Angriffen auf Polizisten und öffentliche Einrichtungen. Am 17. Oktober 1961 wollten rund 30.000 Menschen friedlich für die Unabhängigkeit Algeriens demonstrieren. ImMassaker von Parisschlug die Polizei diese Demonstration gewaltsam nieder; mindestens 150 Demonstranten wurden getötet.[30]Bei der gewaltsamen Auflösung einer Kundgebung desParti communiste françaisam 8. Februar 1962 durch die Polizei, kam es in derMétro-StationCharonneerneut zu einemZwischenfall, bei dem neun Menschen getötet wurden.[31] Während derMai-Unruhen 1968erlebte die Stadt Studentenrevolten und Massenstreiks. Die Vororte(Banlieues)von Paris waren Ausgangspunkt und Zentrum derUnruhen in Frankreich 2005, während denen es zu zahlreichen gewalttätigen Ausschreitungen von zumeist jugendlichen Einwanderern kam. Bei denislamistischenTerroranschlägen im Januar 2015, unter anderem auch auf die Redaktionsräume der SatirezeitschriftCharlie Hebdo, wurden von den drei Attentätern insgesamt 17 Menschen getötet. Bei einer ebenfalls islamistischenAnschlagserie am 13. November 2015an sechs Orten in Paris undSaint-Denismit Geiselnahmen in der KonzerthalleBataclan, Sprengstoffanschlägen um das FußballstadionStade de France, in dem ein Freundschaftsspiel gegen Deutschland vor 80.000 Besuchern stattfand und der StaatspräsidentHollandeanwesend war, und mehreren Schießereien starben weit über hundert Menschen.[32] Die Stadt Paris führt ein großes und ein kleines Wappen sowie eine blau-rote Flagge. Wappen undWahlspruchsind an vielen Bauwerken angebracht. An dem Kranz aus Eichen- und Wacholderlaub hängen die drei der Stadt verliehenenOrden(von rechts nach links in der Draufsicht):Ordre de la Libération(24. März 1945);Croix de Guerre(1914–1918, 28. Juli 1919),Ehrenlegion(9. Oktober 1900)[34] DieDeviselautet auf Latein „Fluctuat nec mergitur“ (etwa: „Sie verändert sich, geht aber nicht unter“ oder „Sie schwankt, aber sie geht nicht unter“). Der Wahlspruch ist seit mindestens 1581 in Verbindung mit der Stadt nachgewiesen;Georges-Eugène Haussmannmachte die Devise als Präfekt desDépartements Seine1853 zum offiziellen Leitspruch der Stadt.[35] Die beidenFarbenwerden meist den Farben der französischen Monarchie vor der Revolution zugeordnet. Dabei steht das Rot heraldisch seit den Römern für den Herrscheranspruch und das Blau war den Bourbonen-Lilien unterlegt. Eine andere Erklärung ist, dass Rot die Farbe einesFeldzeichensder Könige von Frankreich war, nämlich des Banners vonDionysius von Paris(französischSaint Denis), des ersten Bischofs der Stadt und Märtyrers der katholischen Kirche; die Farbe symbolisierte dabei das Blut des Heiligen. Das Blau habePhilippe Auguste(1165–1223) in seine Fahne genommen, weil es als Farbe für dieMutter Gottes(Vierge Marie) steht.[36] In der Antike und im Mittelalter ging die Bevölkerung durch die zahlreichen Kriege, Epidemien und Hungersnöte immer wieder zurück. So starben noch 1832 bei einerCholeraepidemierund 20.000 Menschen. Erst dieIndustrialisierungim 19. Jahrhundert führte zu einem starken Anstieg der Bevölkerung. 1846 lebten in Paris rund eine Million Menschen, bis 1876 verdoppelte sich diese Zahl auf zwei Millionen. 1921 hatte die Einwohnerzahl von Paris mit knapp drei Millionen ihren historischen Höhepunkt erreicht. Gegenwärtig leben etwas über zwei Millionen Menschen in der Hauptstadt. Im Großraum hingegen hat die Einwohnerzahl stark zugenommen. Lebten 1921 noch 4,85 Millionen Menschen in derMetropolregion, so waren es 94 Jahre später, im Jahre 2015 bereits 12,53 Millionen. Damit zählt Paris zu denMegastädten. Paris ist stark vomsozioökonomischenStrukturwandel (Gentrifizierung) betroffen: Der durchschnittliche Kaufpreis für Wohnungen lag 2011 bei 8010 Euro pro Quadratmeter, dem Vierfachen des damaligen Preises in Berlin. In beliebten Vierteln wieSaint-Germain-des-Préskonnte er damals bereits 15.000 Euro erreichen.[37]So wurde etwa das15. Arrondissement, das früher ein Wohnort der Arbeiterschicht gewesen war, zu einem Wohngebiet der wohlhabenden Mittelschicht. Die Preise für Altbauwohnungen sind im Sommer 2019 durchschnittlich auf über 10.000 Euro pro Quadratmeter gestiegen.[38] Paris zieht seit Jahrhunderten Menschen aus verschiedenen Ländern und Kulturen an, sei es wegen politischer Verfolgung, aus wirtschaftlichen Gründen oder wegen der kulturellen Anziehungskraft der Stadt. Gegen Ende des 19. Jahrhunderts zogen vor allemItalienerundosteuropäische Judenin die Stadt. Nach demErsten WeltkriegfolgtenArmenier(nach demVölkermord 1915),Polen,RussenundUkrainer(„weiße Russen“ nach derOktoberrevolution1917). Schon in der Zwischenkriegszeit, v. a. aber nach demZweiten Weltkrieg, kamen zahlreicheGastarbeiteraus Süd- und Osteuropa nach Frankreich und viele von ihnen ließen sich dort nieder, vor allem im Umland von Paris; so führten oftSpanierundPortugiesenden Haushalt der reichen Pariser Familien. Die jüngste und größte Einwanderungswelle stammte aus den ehemaligen französischen Kolonien, etwa von denAntillen, demMaghreb,Subsahara-AfrikaundIndochina. Vor allem die traditionellen Arbeiterviertel im Osten der Stadt waren Anziehungspunkte von Einwanderern, etwaBelleville(19.und20. Arrondissement), außerdem das10., das über eintamilisch-indisch geprägtes Viertel verfügt, das11.und das13. Arrondissement, das heute mit der größtenChinatownEuropas ostasiatisch geprägt ist. Teile des18. Arrondissementssind afrikanisch oder arabisch geprägt, vor allem dasQuartier de la Goutte-d’Or. Zwischen den überwiegend wohlhabenden und weißen Vierteln im Stadtzentrum und im Westen und den multikulturellen Randgebieten im Osten besteht dabei ein deutlicher Unterschied. Durch die erwähnte Gentrifizierung innerhalb der Stadtgrenzen werden zunehmend ärmere Haushalte und Mieter, oft Einwanderer, aus der Stadt heraus gedrängt. In den Vororten von Paris ist der Anteil der nicht-europäischen Einwanderer weit höher, vor allem in den nördlichen und östlichen, wo Armut, Arbeitslosigkeit und soziale Probleme verbreitet sind; es besteht ein Trend zurSegregationundGhettobildung(siehe dazu auch den ArtikelBanlieue). Da Frankreich die ethnische oder religiöse Zugehörigkeit seiner Bewohner nicht statistisch erfasst, gibt es wenig genaue Daten zur ethnischen Zusammensetzung der Pariser Bevölkerung. In Paris selbst sind 20,4 % der Bevölkerung Einwanderer, also nicht in Frankreich geboren, 14,4 % sind außerhalb Europas geboren. Der Anteil der Jugendlichen unter 18 Jahren mitMigrationshintergrund(mindestens ein Elternteil nicht in Frankreich geboren) beträgt 41 %. Mehr als die Hälfte dieser Jugendlichen haben ihre Wurzeln außerhalb Europas. In der Region Île-de-France liegt dieser Prozentsatz bei 37 %, in einigen Vororten bei über 50 %. Insgesamt sind nach einer Erhebung aus dem Jahr 2006 17 % der Bewohner der Region Île-de-France Einwanderer, 35 % haben einen Migrationshintergrund.[39] Etwa 65 % der Einwohner sindgetauft, rund 60 % bekennen sich zumrömisch-katholischenGlauben, die meisten praktizieren denlateinischen Ritus, einige wenige auch denarmenischenund ukrainischen Ritus. DerErzbischof von Parisist auch für die Katholiken der östlichen Riten zuständig. Insgesamt gibt es in Paris innerhalb der politischen Grenzen der Stadt 94 katholische Gemeinden, des Weiteren 73protestantischeKirchen der verschiedensten Konfessionen,[40]15griechisch- undrussisch-orthodoxeKirchen, sechsrumänisch-orthodoxeKirchen,[41]siebenSynagogenfür die etwa 220.000Judenund 19Moscheenfür die rund 80.000Muslime, überwiegendSunniten. Nur knapp 12 % der Christen und etwa 15 % der Juden sind praktizierende Gläubige. Am 1. Januar 2019 gingen die Gemeinde und das Département Paris unter dem NamenVille de Parisin einerGebietskörperschaftmit Sonderstatus auf.[42]Die Stadtregierung wird seit 1977 durch einen Bürgermeister geführt, der vom Stadtrat gewählt wird und gleichzeitig dessen Präsident ist.[42] Bürgermeisterin ist seit dem 5. April 2014Anne Hidalgo, nominiert von derParti socialiste. Ihr VorgängerBertrand Delanoë(PS) war 2001 der erste linke Politiker, der in das Rathaus der Hauptstadt einzog. Zuvor stellte mitJacques Chirac(1977 bis 1995) undJean Tiberi(1995 bis 2001) die gaullistischeRPRden Bürgermeister. Der erste Bürgermeister der HauptstadtJean-Sylvain Baillywurde am 15. Juli 1789 von der während derFranzösischen Revolutiongebildeten Pariser Selbstverwaltung eingesetzt. Da die Kommune an der diktatorisch organisiertenSchreckensherrschaft(La Terreur)beteiligt war, wurde sie 1794 von zwölf getrennten und dezentralisierten Gemeindeverwaltungen ersetzt. Der Staat übernahm die Kontrolle über die Stadt und schuf das Amt desPräfekten der Seine(Préfet de la Seine). Während derBürgerlichen Revolutionvon 1848 und derPariser Kommunevon 1870/1871 stand für wenige Monate ebenfalls ein Bürgermeister der Stadt vor. Am 20. März 1977 wurde Jacques Chirac der erste frei gewählte Bürgermeister von Paris. Die bis dahin einem von der Regierung ernanntenPräfektenunterstehende Hauptstadt hatte damit den gleichen Status wie alle übrigen Gemeinden in Frankreich. Eine Ausnahme bildet die Polizei, die weiterhin demPolizeipräfektenuntersteht. Ein Gesetz von 1982 etablierte dann zusätzlich die Ratsversammlungen derArrondissements. Diese sind beratende Organe, die über begrenzte Befugnisse verfügen. Der Stadtrat(Conseil de Paris)und der Bürgermeister(Maire de Paris)werden jeweils für sechs Jahre gewählt. Die letzte Wahl fand in einem ersten Gang am 15. März 2020 und in einem zweiten am 28. Juni 2020 statt. Die nächste Wahl findet turnusgemäß im Jahr 2026 statt. Der Pariser Stadtrat(Conseil de Paris)besteht aus 163 Mitgliedern. Die Wahlen zum Stadtrat finden alle sechs Jahre im Rahmen der französischen Kommunalwahlen statt. Gewählt wird dabei getrennt nachArrondissements, wobei jedes Arrondissement eine festgelegte Zahl an Stadträten wählt. Seit der Wahl des Pariser Stadtrates 2020 setzt sich dieser aus 163 Mitgliedern zusammen. Davon zählen 94 Sitze zur Regierung und 69 zurOpposition. Die aus 94 Mitgliedern bestehende Regierung setzt sich wie folgt zusammen: 41 Mitglieder derParti socialiste(PS), 21 Mitglieder derEurope Écologie-Les Verts(GEP), 13 Mitglieder derDivers gauche(DVG), 11 Mitglieder derParti communiste français(PCF) und 6 Mitglieder derGénération.s. Die der Regierung mit 69 Sitzen gegenüberstehende Opposition setzt sich folgendermaßen zusammen: 44 Mitglieder derRépublicains(LR), 5 Mitglieder derCentristes(LC), 5 Mitglieder desMouvement démocrate(MoDem), 3 Mitglieder derHorizons, 2 Mitglieder vonAgir, 2 Mitglieder vonRenaissance(LREM), 1 Mitglied vonObjectif France(OF), 1 Mitglied vonSoyons libres, 1 Mitglied vonLa France insoumise(LFI) sowie 2 Mitglieder derDivers gauche(DVG). Die nächste Wahl des Pariser Stadtrates findet turnusgemäß 2026 statt. Paris unterhält eine einzigeStädtepartnerschaftweltweit, und zwar mitRomseit 1956.[43] Darüber hinaus unterhält Paris mit folgenden Städten sogenannteFreundschafts- und Kooperationsabkommen: Frankreich erscheint inTourismus-Statistikenals das meistbesuchte Land der Erde. Die französische Hauptstadt hat eine Vielzahl sehenswerter kirchlicher und weltlicher Bauwerke, Straßen, Plätze und Parks, etwa 160 Museen, rund 200 Kunstgalerien, circa 100 Theater, über 650 Kinos und mehr als 10.000 Restaurants aufzuweisen. Das Angebot an kulturellen Veranstaltungen ist mit zahlreichen Konzerten, Ausstellungen, Musik- und Filmfestivals, Modenschauen sowie der Austragung sportlicher Wettbewerbe reichhaltig. Die Uferpromenade derSeinein Paris wurde 1991 in dieUNESCO-Liste desWeltkulturerbesaufgenommen. Im ersten Halbjahr 2016 sanken die Besucherzahlen wichtiger Museen in Paris aus verschiedenen Gründen im niedrigen zweistelligen Prozentbereich. 2015 waren die 15 meistbesuchten Museen und museale Monumente von mehr als einer Million Menschen besucht worden, derLouvrehatte über 8 Millionen Besucher.[44] Bedingt durch die Tradition desZentralismusin Frankreich haben die wichtigsten Theater- und Ballettensembles des Landes ihren Sitz in Paris. Das Programm ist mannigfaltig; es ist einem der Veranstaltungskalender,PariscopeoderOfficiel des Spectacles, zu entnehmen, die an jedem Zeitungskiosk erhältlich sind. Stark ermäßigte Theaterkarten sind jeden Tag ab 13:00 Uhr für Vorstellungen am Abend desselben Tages an einem der beiden Theaterkioske(Kiosque Théâtre)(vor demMontparnasse-Bahnhofund neben derMadeleine-Kirche) erhältlich. DiePariser Oper(heuteOpéra national de Paris) und ihre Vorgängerinstitute spielen in der Geschichte der Oper durch stilprägende Uraufführungen eine bedeutende Rolle. Heute betreibt sie zweiOpernhäuser. Die 1875 eröffnete, nach ihrem ArchitektenOpéra GarnieroderPalais Garniergenannte alte Oper ist mit einer Fläche von 11.237 Quadratmetern das größte Theater der Welt, während die 1989 eingeweihte neueOpéra Bastillesich durch ihre herausragendeBühnentechnikauszeichnet. Seit der Eröffnung der neuen Oper wird dasPalais Garnierhauptsächlich, aber nicht ausschließlich fürBallettaufführungenund klassische Opern genutzt. Die Pariser Oper unterhält ein hauseigenes Ballett, dasBallet de l’Opéra de Paris, mit einer angeschlossenen Ballettschule. Auch dieComédie-FrançaiseoderThéâtre français, deren Schauspielensemble sich rühmen darf, 1680 aus der Zusammenlegung vonMolièresehemaligem „Illustre Théâtre“ mit anderen Schauspieltruppen hervorgegangen zu sein, hat eine lange Tradition. Berühmte Schauspieler waren unter anderemSarah BernhardtundJean-Louis Barrault. Das heute staatliche Theater spielt ein vorwiegend klassischesRepertoire. DasThéâtre des Champs-Élysées, von 1911 bis 1913 nach Plänen vonHenry van de VeldevonAuguste Perretausgeführt, erregte Anfang des 20. Jahrhunderts durch seine Architektur und skandalumwitterte Aufführungen Aufsehen. Als Musiktheater und Konzerthaus ist es Heimstätte desOrchestre national de Franceund desOrchestre Lamoureuxsowie Stützpunkt derWiener Philharmonikerin Frankreich. Aufmerksamkeit gebührt auch den Programmen desThéâtre du ChâteletamPlace du Châteletund dem gegenüberliegendenThéâtre de la Ville(dt. Stadttheater). Zeitgenössische Komödien,Boulevard- undVaudeville-Stücke werden in unzähligen kleinen Theatern aufgeführt, wie beispielsweise imThéâtre des Bouffes-Parisiens, dasJacques Offenbacham 5. Juli 1855 gründete. Der Name des Theaters leitet sich ab von „Opéra bouffe“ – „Komische Oper“, wie Offenbach zahlreiche seiner Werke betitelte. Freunden desRevuetheaterssind die Shows desMoulin rouge, desLidound desParadis Latinzu empfehlen. DasMoulin rouge, am 6. Oktober 1889 vonJoseph Ollereröffnet, der bereits dieMusic HallL’Olympiabesaß, leitet seinen Namen ab von der markanten Nachbildung einer roten Mühle auf seinem Dach. Berühmt wurde es durch seineCancan- undChahut-Cancan-Tänzerinnen. Nicht ganz so aufwändig, aber unverhohlen erotischer sind die Darbietungen in denFolies Bergère. Rockkonzerte finden imZénithimParc de la Villetteund imPalais Omnisports de Paris-Bercystatt. DasZénithwurde 1983 auf Initiative des damaligen KulturministersJack Langnach Plänen derArchitekten Philippe Chaix und Jean-Paul Morelerbaut und am 12. Januar 1984 mit einem Konzert des französischen SängersRenaudeingeweiht. DieArènes de Lutèce(Arenen von Lutetia) gelten als ältestes noch erhaltenes Bauwerk der Hauptstadt. Das römischeAmphitheaterbefindet sich in derRue Monge, im5. Arrondissement. Die Arena stammt aus dem 1. Jahrhundert n. Chr. und wurde bis zum Ende des 3. Jahrhunderts genutzt. Circa 17.000 Personen konnten den Theatervorstellungen, aber auch Kämpfen auf Leben und Tod, beiwohnen. Mit dem Aufkommen des Christentums verloren die römischen Zirkusse allgemein an Bedeutung und als im 3. und 4. Jahrhundert die germanischen Stämme in das römischeGallieneinfielen, wurden dieArènes de Lutècestillgelegt und ihre Steine für den Bau von Stadtmauern und anderen Befestigungsanlagen verwendet. Das 1793 in der früheren Residenz der französischen Könige eröffneteMusée du Louvreenthält eine der weltweit bedeutendsten Sammlungen mit über 380.000 Werken, von denen etwa 35.000 ausgestellt werden. Die Exponate decken einen Zeitraum, der von der Antike bis zum Ende des 19. Jahrhunderts reicht. Das Gebäude liegt im Zentrum von Paris zwischen dem rechten Seineufer und derRue de Rivoli. Sein Innenhof liegt in einer Linie mit derAvenue des Champs-Élyséesund bildet damit den Ursprung der sogenanntenAxe historique, der historischen Achse. DasMusée d’Orsayentstand in dem ehemaligen gleichnamigen Bahnhof, demGare d’Orsay, am südlichen Ufer der Seine gegenüber demTuileriengarten. Das Bahnhofsgebäude wurde 1900 vonVictor Lalouxfür die Verbindung Paris–Orléansgebaut, 1939 wegen Kapazitätsproblemen geschlossen und 1978 als historisches Bauwerk eingestuft. Unter Leitung der ArchitektinGae Aulentiwurde es von 1980 bis 1986 unter behutsamer Wahrung der alten Bausubstanz zum heutigen Museum umgebaut. Weltweit einzigartig ist die Sammlung französischerImpressionisten. Daneben werden Gemälde, Skulpturen, Fotos und Möbel von herausragender Qualität aus der Zeit von 1848 bis 1914 gezeigt. Vertreten sind fast alle Stilrichtungen dieses Zeitraums sowie Werke vieler Einzelkünstler. Das 1977 nach Plänen der ArchitektenRenzo Piano,Richard RogersundGianfranco Franchinieröffnete Kunst- und KulturzentrumCentre Georges-Pompidou(Centre National d’Art et de Culture Georges Pompidou)sorgte durch seine Architektur aus Stahl und Glas für Aufsehen: alle Versorgungsleitungen sind an der Fassade angebracht. Es wurde als interaktives Informationszentrum gestaltet, das freien Zugang zu Wissen garantieren soll. Es beherbergt dieBibliothèque publique d’information(Bpi) und dasMusée National d’Art Modernemit einer hervorragenden Sammlung von Kunstwerken des 20. Jahrhunderts, vor allem Werken desSurrealismus,Fauvismus,Kubismusund desAbstrakten Expressionismus. Das MusikforschungsinstitutIRCAM(Institut de Recherche et Coordination Acoustique/Musique)ist ihm organisatorisch angeschlossen. DasMusée Picassobesitzt etwa 250 Werke aus allen SchaffensperiodenPicassos, insbesondere Gemälde und Skulpturen sowie Gemälde aus der persönlichen Sammlung des Künstlers, unter anderem vonGeorges Braque,Paul Cézanne,Henri Matisse,Joan MiróundAmedeo Modigliani. Das Museum befindet sich im ehemaligenHôtel Salé, einem in den Jahren 1656 bis 1659 imMaraisviertelerbautenHôtel particulier, dessen Bezeichnung sich von seinem damaligen Bauherrn, dem für die Eintreibung von Salzsteuer zuständigen königlichen StaatsbeamtenPierre Aubert, SpitznameSalé(„Gesalzener“), ableitet. DasMusée national du Moyen Âge(vor 1980:Musée de Cluny) in dem spätgotischen ehemaligen AbtspalastHôtel de Cluny(1485–1490) zeigt eine bedeutende Sammlung mittelalterlicher Kunstgegenstände. Es gestattet den Zutritt zu den benachbarten früheren Thermen aus gallo-römischer Zeit. Im September 2000 wurde neben demHôtel de Clunyder mittelalterliche Garten(Jardin médiéval)mit einer Fläche von zirka 5.000 Quadratmetern angelegt. DasGrand Palaisentstand nach Plänen der Preisträger desPrix de Rome, den Architekten Henri Deglane (1851–1932) und Albert Louvet (1860–1936), als Ausstellungshalle zurPariser Weltausstellung von 1900. Es besitzt eine 240 Meter lange und 20 Meter hohe Fassade mitionischen Säulen. Im Gebäude finden bedeutende Kunst- und Gemäldeausstellungen statt. Im Westflügel ist derPalais de la découverte(Palast der Entdeckung) untergebracht, ein naturwissenschaftliches Museum, das zu praktischen Erkundungen einlädt und einPlanetariumbetreibt. DemGrand Palaisgegenüber steht der zur gleichen Zeit und zu gleichem Zweck von dem ArchitektenCharles Girault(1880Prix de Rome) imneobarockenStil derBelle ÉpoqueerrichtetePetit Palais. Der mit einem prunkvoll vergoldeten schmiedeeisernen Eingangstor und reichen Deckenmalereien ausgestattete halbrunde Bau, dessen Fassaden fast nur aus Fenstern bestehen, beherbergt seit 1902 das städtische Museum der schönen KünsteMusée d’art moderne de la Ville de Paris. Nahe dem Eiffelturm befindet sich seit 2006 dasMusée du quai Branlyfür Völkerkunde. Mehrere naturkundliche Museen sind imMuséum national d’histoire naturellezusammengefasst und befinden sich an verschiedenen Standorten, etwa im Bereich desJardin des Plantes. Am 27. Oktober 2014 eröffnete dieStiftung Louis Vuittonein Privatmuseum, das die Kunstsammlung vonBernard Arnaultbeherbergt. 2021 eröffnete in derBourse de commercedieCollection Pinault, ein Privatmuseum, das die Kunstsammlung vonFrançois Pinaultbeherbergt. Die Seine fließt im Großraum Paris ab der Einmündung derMarnebeiVincennesim Pariser Becken in einem weiten Linksbogen von Südosten durch das Zentrum, um dann in einer engen Rechtskurve beiBoulogne-Billancourtsich wieder bisSt. Denisnach Norden zu biegen und dabei noch einmal die City von Norden zu umfassen. Danach biegt sie in einem Bogen umColombes/Villeneuve-la-Garenneerneut nach Nordwesten ab, um sich dann weiter Richtung Ärmelkanal zu schlängeln. Etwa 40 Brücken(ponts)und einige Stege überspannen die Seine und verbinden die zentralen Arrondissements miteinander. Die InselÎle de la Citéist über insgesamt 9 Brücken sowohl mit der benachbarten Île Saint-Louis verbunden(Pont Saint-Louis)als auch mit den beiden Ufern (rechtes Ufer, in Fließrichtung:Pont d’Arcole,Pont Notre-Dame,Pont au Change; linkes Ufer:Pont de l’Archevêché,Pont au Double,Petit Pont,Pont Saint-Michel). DerPont Neufführt über die Westspitze der Insel und verbindet die Insel mit beiden Ufern. Er ist die älteste der heutigen Pariser Seinebrücken. Die jüngste ist diePasserelle Simone-de-Beauvoir, die seit 2006 ohne Strebepfeiler 194 Meter Spannweite überbrückt. Viele Brücken entstanden im 19. Jahrhundert und sind Eisenkonstruktionen. Abends werden die Brücken nach einem bestimmten, die Baustrukturen betonenden Konzept angeleuchtet. Zusammen mit den Uferbefestigungen bilden die Brücken ein städtebaulich prägendes Merkmal der Stadt. Außer den Seinebrücken gibt es noch ca. 300 andere Brückenbauwerke in der Stadt: über Kanäle und Straßen, über Gleise und in Parks. Erste urbanistisch relevante Maßnahmen ergriff in Paris Anfang des 17. JahrhundertsHeinrich IV.mit der Anlage der ersten zwei von insgesamt fünf sogenannten „königlichen Plätzen“. Die quadratischePlace des Vosges(1605–1611), früherPlace RoyaleimLe Marais(4. Arrdt.) bietet ein einzigartig geschlossenes Ensemble von Bauten aus Back- und Quaderstein im Stil des frühen 17. Jahrhunderts. Die Mitte des Platzes ziert das Reiterstandbild vonLudwig XIII. Zur gleichen Zeit entstand in demselben Stil die dreieckigePlace Dauphine(1607–1612) an der westlichen Spitze derÎle de la Cité(1. Arrdt.), nach Plänen vonLouis MétezeauundJacques II. Androuet du Cerceau. Die Achse des später zu einem Drittel zerstörten Platzes lässt durch eine Öffnung im Westen den Blick auf die BrückePont Neufund auf das Reiterstandbild von Heinrich IV. frei. DiePlace des Victoires(1675), mit rundem Grundriss, wurde auf Initiative des HöflingsFrançois d’Aubusson de La Feuilladenach Plänen vonJules Hardouin-Mansartzu Ehren des SonnenkönigsLudwigs XIV.entworfen, um seinem Standbild vonMartin Desjardinseinen würdigen Rahmen zu geben. Letzteres wurde in der Revolution zerschlagen und erst 1822 durch das heutige Reiterstandbild von Bosio ersetzt. Hier so wie auf den folgenden „Königlichen Plätzen“ ersetzt der schöne hellgelbe Quaderstein, der sich hervorragend für den Steinschnitt eignet, den bisher üblichen Backstein. Auch die überaus harmonische und in ihrem ursprünglichen Zustand erhaltenePlace Vendôme(1690–1720) wurde zu EhrenLudwigs XIV.angelegt. Die Pläne lieferte abermals Jules Hardouin-Mansart. Das früher hier befindliche Reiterstandbild fiel, wie nahezu alle Abbilder der Mitglieder des französischen Königshauses, der Revolution zum Opfer, wasNapoléon I.Gelegenheit gab, hier 1806 in Erinnerung an dieSchlacht bei Austerlitzeine 44 Meter hohe Triumphsäule errichten zu lassen. Die ab 1755 angelegtePlace Louis XV(heutigePlace de la Concorde) sollte der größte und letzte der „Königsplätze“ von Paris werden. Der Platz blieb unvollendet. Während der Revolution inPlace de la Révolutionumbenannt, empfing er – an Stelle der zerstörten ReiterstatueLudwigs XV.– dieGuillotine, unter der im Jahre 1793Ludwig XVI.und die KöniginMarie-Antoinetteenthauptet wurden. Seit 1836 wird der Platz von dem 23 Meter hohenObelisken von Luxordominiert. Daneben befinden sich zwei aufwändig gestaltete Brunnen vonJakob Ignaz Hittorff. An derPlace de la Concordebeginnt die Prunk-, Pracht- und ParadestraßeAvenue des Champs-Élysées, eine der großen und berühmten „Weltstraßen“. Die 1,5 Kilometer lange und 71 Meter breite Avenue bildet das Kernstück und Rückgrat der einzigartigen vom Osten zum Westen weisendenAxe historique, einerSichtachse, die im Innenhof desLouvrebeginnt, über denTuileriengarten, diePlace de la Concordeund den Triumphbogen bis zurGrande Archeund darüber hinaus reicht. Hier befinden wir uns schon jenseits der westlichen Ausfallstraße, in dem vier Kilometer außerhalb von Paris gelegenen GeschäftsviertelLa Défense. Als unter Ludwig XIV. von dem HofgärtnerAndré Le Nôtredie ersten Bäume (Ulmen) derChamps-Élyséesgepflanzt wurden (1670), führte sie noch durch freie Felder. Die beliebte Promenade der Pariser war damals die Straßenkette der aneinandergereihtenBoulevards, die selten mit ihren verschiedenen Namen, sondern schlichtLes Grands Boulevardsgenannt werden. Die ältesten Bauwerke der Stadt stehen imQuartier Latinan den Hängen desMontagne Sainte-Geneviève, auf dem sich ab 52 v. Chr. die Römer in dominanter Lage ansiedelten. Die stark restaurierten Überreste der im 1. Jahrhundert n. Chr. erbautenArena von Lutetiaund die Ruinen der sogenanntenThermen von Cluny(in dasMusée national du Moyen Âge) aus der Zeit um 200 n. Chr. sind die einzigen sichtbaren Spuren aus dergallo-römischenEpoche. Nach demUntergang des Römischen Reichesentstanden zunächst vor allemSakralbauten, während die in Paris weilendenfränkischen Teilkönigesich den ehemaligen Palast der römischen Statthalter auf derÎle de la Citézu eigen machten, der im Laufe der Jahrhunderte mehrmals vergrößert und umgebaut wurde und heute alsPalais de la Citébekannt ist. Die ältesten erhaltenen Teile desPalais de la Citésind die in der ersten Hälfte des 13. Jahrhunderts unterLudwig IX. dem HeiligenvonPierre de Montreuilerrichtete PalastkapelleSainte-Chapelleund die unteren Partien des sogenanntenBonbec-Turmes an der Nordfassade. Die danebenliegenden beiden TortürmeTour d’Argent(Silberturm) undTour de César(auchTour de Montgomerygenannte) sowie der nach seiner UhrTour de l’Horlogegenannte, im 19. Jahrhundert stark veränderte Eckturm entstanden etwas später unterPhilippe IV. dem Schönen. Hinter der massiven Doppelturmanlage verbirgt sich die nach dem früheren Palastverwalter(Concierge)benannteConciergerie, die bereits um 1400 als Gefängnis genutzt wurde und während der Revolution als „Wartesaal für die Guillotine“ diente. Bereits bald nach 1358 war derPalais de la Citéals Königsresidenz aufgegeben worden, und zwar zu Gunsten des heute verschwundenenHôtel Saint-Pol, der im Osten von Paris entstandenenBurg von Vincennesund der schon 1190 unterPhilippe-Augusteentstandenen Wehranlage des früheren Louvre, deren mächtiger runderBergfriedseinerzeit das rechte Ufer beherrschte. Das StadtschlossLouvre, wie wir es heute kennen, ist das Ergebnis von zahlreichen Baukampagnen unter vielenKönigenund umfasst Teile aus dem Mittelalter, derRenaissance, derBarockzeit, dem Zweiten Kaiserreich sowie das bedeutende, seit 1981 auf Wunsch des StaatspräsidentenFrançois Mitterrandvon dem ArchitektenIeoh Ming Peigeschaffene „unterirdische Reich“ des Louvre, das in erster Linie der Schaffung fehlender Infrastrukturen für das hier angesiedelte Museum dient. Aus der zweiten Hälfte des 15. Jahrhunderts und dem 16. Jahrhundert stammen mehrere interessante, hierzulandehôtels particuliersgenannte Stadtpaläste des Marais-Viertels, wie beispielsweise dasHôtel de Sens, das zwischen 1475 und 1507 im Auftrag von Tristan von Salazar, Erzbischof von Sens, entstand, das ab 1548 für den Gerichtspräsidenten Jacques de Ligneris errichteteHôtel Carnavalet, das um 1585 fürDiane de Franceentworfene und jetzt Louis Métezeau zugeschriebeneHôtel d’Angoulême(heutigeBibliothèque historique de la ville de Paris) sowie derHôtel de Sullygenannte Stadtpalast des Finanzinspektors Mesme Gallet, den Roland de Neufbourg 1630 nach den Plänen vonJean I. Androuet du Cerceauvollendete. Er ist heute Sitz des Denkmalpflegevereins(Centre des monuments nationaux). Auf dem linken Ufer ließ unterdessen Jacques d’Amboise,Abt von Clunyzwischen 1485 und 1510, neben den Ruinen der römischen Thermen dasHôtel de Clunyvollkommen neu erbauen, das den Äbten von Cluny seit 1330 als Stadtresidenz diente. Das dort untergebrachteMusée national du Moyen Âge(Museum des Mittelalters) besitzt den einzigartigenMillefleursWandbehang mit Szenen zum Thema derLa Dame à la licorne(„Die Dame mit dem Einhorn“). Mit dem BrunnenFontaine des InnocentsschufenPierre LescotundJean Goujon1547 bis 1549 ein Werk, das heute zu den wichtigsten verbleibenden Zeugnissen der frühen Renaissance in Paris gezählt wird. Allerdings wurde die Anordnung der drei originalen Brunnenseiten, die ursprünglich eine Tribüne bildeten, in der zweiten Hälfte des 19. Jahrhunderts vollkommen verändert und eine vierte Seite von Pajou und Houdon hinzugefügt. Das ursprüngliche PariserHôtel de Ville(Rathaus) war zwischen 1551 und 1628 auf Anregung von KönigFranz I.nach Plänen des italienischen ArchitektenDomenico da Cortona, genanntIl Boccador(o), im Stil derRenaissanceschlösser des Loiretalsentstanden. Es brannte 1871 während des Aufstandes derKommuneab. Das heutige Rathaus ist eine Kopie des Vorgängerbaus. Das Gebäude im Stil desKlassizismusmit 146 Statuen auf der Fassade wurde in den Jahren 1874 bis 1882 nach Plänen der ArchitektenThéodore Ballu(1817–1885) undÉdouard Deperthes(1833–1898) errichtet. Es befindet sich im4. Arrondissementan der ehemaligenPlace de Grève, der heutigenPlace de l’Hôtel-de-Ville. DemPalais du Luxembourg, im Jahre 1615 vonMaria von Medicials Landschloss weit außerhalb der damaligen Stadtgrenzen bei dem ArchitektenSalomon de Brossein Auftrag gegeben, liegen wenigstens teilweise Pläne desPalazzo Pittiin Florenz zugrunde, in dem die Königinmutter und Regentin ihre Kindheit verlebt hatte. Die Gartenseite erfuhr im 19. Jahrhundert erhebliche Veränderungen. Hier tagt seit 1852 derfranzösische Senat, der den zu dem Palais gehörenden, früher königlichen, heute staatlichen SchlossparkJardin du Luxembourgder Öffentlichkeit zur Verfügung stellt. DerPalais Royal, nördlich vom Louvre, wurde in den Jahren 1627 bis 1629 vonJacques Lemercierfür den ersten MinisterLudwigs XIII.,Kardinal Richelieu, gebaut, kam nach dessen Tod an die Krone und nahm seinen heutigen Namen an. Dort wuchsLudwig XIV.auf. Heute beherbergt der Palais denStaatsrat(Conseil d’État), denVerfassungsrat(Conseil constitutionnel), dasKultusministerium, aber auch dieComédie-Française. An den Hof, in demDaniel Burenein interessantes begehbares Kunstwerk schuf, schließt sich ein schöner Garten an. Weitere wichtige Bauten des 17. Jahrhunderts sind die Barockkirche desVal-de-Grâce-Klosters, dasCollège des Quatre-Nations, heute Sitz desInstitut de France, dasHôtel des Invalidesund dasObservatoire. DerÉlysée-Palastursprünglich nach seinem AuftraggeberHôtel d’Évreuxund später nach der nahegelegenenAvenue des Champs-Élyséesbenannt, ist der Amtssitz des französischenStaatspräsidenten. Erbaut wurde er in den Jahren von 1718 bis 1722 nach den Plänen des Architekten Armand-Claude Mollet, der das umliegende Grundstück kurz zuvor an den Grafen von Évreux,Henri-Louis de la Tour d’Auvergne, verkauft hatte und von diesem nun mit dem Bau einer Residenz beauftragt wurde. Nach dem Tod des Grafen im Jahre 1753 erwarb Jeanne-Antoinette Poisson, besser bekannt alsMarquise de Pompadour, den Palast und ließ ihn durch ihren Architekten im Inneren stilvoll herrichten. Der Garten wurde auf ihre Vorstellungen hin vergrößert und um Säulengänge und Lauben sowie ein Labyrinth erweitert. Der Palast liegt nördlich der Seine in einer der weltweit wichtigsten EinkaufsstraßenRue du Faubourg Saint-Honoré, nur einige Schritte von denChamps-Élyséesund wenige Gehminuten von dem Concordenplatz entfernt. DerPalais Bourbonentstand ebenfalls im 18. Jahrhundert, wurde aber später mit einer klassizistischen Fassade versehen. Er liegt am südlichen Ufer der Seine und gab dem7. Arrondissementseinem Namen. In ihm tagt dieFranzösische Nationalversammlung. Die Kirche Sainte Marie Madeleine liegt dem Palast auf dem nördlichen Ufer in einer Sichtachse gegenüber. UnterLudwig XV.entstanden die grandiosen Bauten vonAnge-Jacques Gabriel, welche die Nordseite derPlace de la Concordebilden; dieLa MonnaieoderHôtel des MonnaiesgenannteMünzprägewerkstatt, zwischen 1771 und 1777 vonJacques Denis Antoinegeschaffen, und dieÉcole militaire(Militärschule), ebenfalls ein Werk von Ange-Jacques Gabriel. Der weitaus imposanteste, von weit her sichtbare Bau aus dieser Zeit ist jedoch dasPanthéon, ein Kuppelbau, der sowohl in die sakralen als auch in die profanen Bauten der Stadt eingereiht werden kann, da er mehrmals seine Bestimmung gewechselt hat. DasPanthéonwurde zwischen 1764 und 1790 vonJacques-Germain Soufflotund seinen Schülern als Klosterkirche für die damals hier befindlicheBenediktinerabteierrichtet, deren Refektorium sowie ein Turm in dem nahegelegenenLycée Henri IVerhalten sind, einer der ältesten und bekanntesten Schulen Frankreichs. Nach derFranzösischen Revolution 1789wurde die Kirche zur nationalen Ruhmeshalle erklärt. Nach mehreren Umwidmungen im 19. Jahrhundert ist sie seit 1885 erneut Ruhmeshalle Frankreichs. Entsprechend illuster ist die Liste der hier beigesetzten Personen:Voltaire,Victor Hugo,Émile Zola,Jean-Jacques Rousseau,PierreundMarie Curie. 1849 gelang dem PhysikerLéon Foucaultmit demnach ihm benannten Pendelhier der empirische Nachweis der Erdrotation. Das Pendel befindet sich heute in der Kapelle der ehemaligen AbteiSt-Martin-des-Champs,die Teil desMusée des arts et métiersgeworden ist. Das schönste, wenngleich nicht das repräsentativste Bauwerk des 1. Kaiserreiches schufen zwischen 1806 und 1808Charles Percierund Fontaine mit dem in der sogenanntenCour Napoléondes Louvre errichtetenArc de Triomphe du Carrousel. Noch während des Baus desArc de Triomphe du CarrouselgabNapoléon I.1806 den großenTriumphbogenan derPlace de l’Étoilein Auftrag, der erst 1836 unterLouis-Philippevollendet wurde. Als Inspiration diente der allerdings deutlich kleinereTitusbogeninRom. Der Triumphbogen steht im Zentrum des Platzes, der seit 1970Place Charles de Gaulle – Étoileheißt, am westlichen Ende derAvenue des Champs-Élyséesund ist Teil derAxe historique(historische Achse), einer Reihe von Monumenten und großen Straßen, die weiter westlich in das Défense-Viertel weisen. Im gleichen Jahr wurde der Bau eines Ruhmestempels zu Ehren der napoleonischenGrande Arméegeplant. Dieses erst 1842 fertiggestellte Gebäude kennen wir heute alsMadeleine-Kirche. Ebenfalls im 1. Kaiserreich wurde der Auftrag für die Errichtung der Börse vergeben. 1808 vonAlexandre-Théodore Brongniartbegonnen, wurde sie nach dessen Tod 1827 vonÉloi Labarrevollendet. Ab der Mitte des 19. Jahrhunderts verwandelte die bis dahin größtenteils noch vom Mittelalter geprägte Stadt sich in eine prestigevolle, beispielhafte und moderne Metropole, welche die Bewunderung von Tausenden von ausländischen Weltausstellungsbesuchern hervorrief. Der umwälzenden Stadtsanierung, die nach dem WillenNapoleons III.von dem ihm treu ergebenenBaron Haussmanndurchgeführt wurde, verdankt Paris seine breiten Straßen, mehrere Brücken, zahlreiche Plätze und Parks sowie die Anlage der beiden Stadtwälder und nicht zuletzt die Säumung der neuen Straßen mit den für Paris so typischen Häusern im sogenannten „Haussmann-Stil“. DurchCharles Marvillesind Fotografien aus der damaligen Umbruchszeit erhalten geblieben, die die alten Straßenzüge und Gebäude kurz vor der Neugestaltung dokumentieren. Krönung dieser schaffensfrohen Epoche wurde das alsPalais Garnierbezeichnete Opernhaus derPariser Oper, das 1875 vonCharles Garnierfertiggestellt wurde. Für den Neubau des Universitätsgebäudes derSorbonnewurde 1885 die größte Pariser Baustelle des 19. Jahrhunderts eröffnet, wenn man von der Konstruktion des Eiffelturmes, dem Werk eines Ingenieurs, absieht. Erst 1901 wurden die Arbeiten abgeschlossen. Die Sorbonne, eine der ältesten Universitäten nördlich derAlpen, war schon im 13. Jahrhundert imQuartier Latingegründet worden. Hier studierten und lehrten einige der bedeutendstenPhilosophen des Mittelalters. Das Wahrzeichen der Stadt ist der 300,51 Meter hoheEiffelturm(Tour Eiffel, 324,8 Meter mit Antenne), eine Konstruktion aus dem Jahre 1889, die für dieWeltausstellungnur temporär errichtet werden sollte. DerStahlfachwerkturmist nach seinem ErbauerAlexandre Gustave Eiffelbenannt. Er ist eine der größten Touristenattraktionen mit mehr als sechs Millionen Besuchern jährlich. Im Jahr 2002 wurde der 200-millionste Besucher gezählt. Über das ganze Stadtgebiet von Paris verteilt, hauptsächlich an den meistbenutzten Fußgängerwegen, befinden sich dieWallace-Brunnen. Die öffentlichen Trinkwasserspender in Form kleinergusseisernerSkulpturen sind nach dem EngländerRichard Wallacebenannt, der ihre Errichtung finanzierte. Ihrer herausragenden Ästhetik wegen gelten sie weltweit als ein Wahrzeichen der Stadt. Nicht unumstritten war der Bau derTour Montparnasseim Süden der Stadt. Der 210 m hohe Büroturm ist das höchste Gebäude in Paris und wurde nach vierjähriger Bauzeit 1973 eröffnet. In der zweiten Hälfte des 20. Jahrhunderts entwickelte Paris unter anderem dank der sogenanntengrands projets(Große Projekte) der französischen Staatspräsidenten eine rege Bautätigkeit. Georges Pompidou(Staatspräsident von 1969 bis 1974) war 1970 Initiator des neuen Kunst- und KulturzentrumsCentre Georges-Pompidou. Als Preisträger eines internationalen Wettbewerbes wurdenRenzo PianoundRichard Rogersmit der Errichtung der spektakulären Metallkonstruktion beauftragt, die zwischen 1972 und 1977 entstand. Der konservativereValéry Giscard d’Estaing(Staatspräsident von 1974 bis 1981) begnügte sich mit der Rehabilitation bereits bestehender Bauten, wie dem Umbau des stillgelegtenOrsay-Bahnhofeszu einem Museum und der Einrichtung derCité des sciences et de l’industriein der Rohbauruine der Schlachthöfe inLa Villette. Allerdings veranlasste Giscard d’Estaing 1980 auch die Gründung desInstitut du monde arabe(Institut der arabischen Welt), ebenfalls ein Kunst- und Kulturzentrum mit angeschlossenem Museum, Bibliothek und Theater. Der Bau wurde jedoch erst zwischen 1983 und 1987 unter seinem Nachfolger François Mitterrand von der französischen ArchitektengruppeJean Nouvel, Pierre Soria und Architecture Studio verwirklicht. François Mitterrand(Staatspräsident von 1981 bis 1995) kündigte seinerseits schon in seiner ersten Pressekonferenz nach seinem Amtsantritt den Umbau desLouvrezu einem „würdigen Museum Frankreichs“ an. Der Auftrag zu diesem Großprojekt ging ohne Ausschreibung an den renommierten amerikanischen Architekten chinesischer HerkunftIeoh Ming Pei. Die Notwendigkeit, ein neues Finanzministerium zu bauen, ergab sich unter anderem aus der Tatsache, dass die Kabinette der beiden Minister aufgrund des geplanten Umbaus des Louvre aus dem dortigen Nordflügel weichen mussten. Das neueMinistère des Finances(1984–1989), ein Gemeinschaftswerk vonPaul ChemetovundBorja Huidobro, entstand auf einem Gelände im Osten der Stadt, wo zur gleichen Zeit der neueParc de Bercyangelegt wurde und die Stadt Paris vonPierre Paratund Michel Andrault die Mehrzweck-SporthallePalais Omnisports de Paris-Bercyerrichten ließ. Persönliches Prestigeobjekt Mitterrands während seiner ersten Amtszeit wurde die neueOpéra Bastille(1983–1989) am gleichnamigen Platz, auf dem am 14. Juli 1789 mit demSturm auf die BastilledieFranzösische Revolutionausgebrochen war und Mitterrand 1981 seinen Wahlsieg gefeiert hatte. Symbolträchtig war auch die Wahl des Einweihungstages dieser nach Plänen des ArchitektenCarlos Ottin einer eigenwilligen Form aus Glas und Aluminium entstandenen neuen Oper: die erste Aufführung fand am 13. Juli 1989, dem Vorabend des 200. Jahrestags des Sturms auf dieBastille, statt. DieGrande ArchevonJohan Otto von Spreckelsen, ein torförmig durchbrochener Kubus von gewaltigen Ausmaßen, steht imDéfense-Viertelaußerhalb von Paris. Er wurde 1989 eingeweiht. Bereits einige Monate zuvor hatte Mitterrand ein weiteres Projekt ins Leben gerufen, um die alte Nationalbibliothek zu entlasten. Die neueBibliothèque nationale de France(Nationalbibliothek, 1990–1996) wurde vom ArchitektenDominique Perraultentworfen. Die vier Ecken des Gebäudes weisen je einen 79 Meter hohen Turm mit einer durchgehenden Glasfront auf. Die Türme sind L-förmig und symbolisieren ein aufgeschlagenes Buch.Jacques Chiracführte die Tradition der „Bauten der Präsidenten“ fort. Am 20. Juni 2006 weihte er das neueMusée du quai BranlyvonJean Nouvelein. Daneben entstanden in der letzten Hälfte des 20. Jahrhunderts zahlreiche sehenswerte kleinere Bauten, wie beispielsweise dieFondation Cartier(1994, Jean Nouvel) und dasAmerican Center(1994,Frank Gehry), jetzt Kinomuseum. Paris ist auch bekannt für seine vornehmen und eleganten Hotels, die unter anderem an derRue de Rivoligegenüber demTuilerien-Garten, in derrue Castiglioneund an derPlace Vendômeangesiedelt sind. Hier findet man dasHôtel Le Meurice, das „Westin“ (früher „Intercontinental“ mit seinem repräsentativenPatio), das Hôtel „Lotti“ und das berühmte„Ritz“. 2006 eröffnete dasMusée du quai Branly. 2014 wurden das MuseumFondation Louis VuittonimBois de Boulogneund das geschichtsträchtige GrandhotelHotel The Peninsula Parisnahe dem Triumphbogen eröffnet. 2015 wurde dasHexagone Balard, ein Gebäudeensemble, in dem dasfranzösische Verteidigungsministeriumseinen neuen Sitz hat, eröffnet. Es bietet 9300 Arbeitsplätze. Ebenfalls 2015 eröffnete die neuePariser PhilharmonieimParc de la Villette. Der Neubau desForum des Halleseröffnete im Jahr 2016. 2017 eröffnete derNeue Justizpalastim Nordwesten der Stadt. Der Wolkenkratzer mit 160 Meter Höhe stellt eine bedeutende neue Landmarke dar. Im Jahr 2021 wurden die 180 m und 122 m hohenTours Duoim 13. Bezirk im Südwesten der Stadt nach vier Jahren Bauzeit eröffnet. Laufende Großprojekte sind der Umbau und die Aufstockung derTour Montparnasse, die Erweiterung derGare du Nord, die Erweiterung und Renovierung desLouvreund die Errichtung der 180 m hohenTour Triangleim 15. Bezirk (Baubeginn: 2021; geplante Fertigstellung: 2026). Die frühereAbteikircheSaint-Germain-des-PrésamBoulevard Saint-Germain(6. Arrdt.) erinnert daran, dass derfränkischeKönigChildebert I.aus dem Geschlecht derMerowinger, ein Sohn vonChlodwig I., hier im Jahr 557 eine später sehr bedeutendeAbteigründete. DerPortalturmder heutigenKircheund die unteren Bereiche derKirchenschiffestammen aus dem 11. Jahrhundert, denChorweihte im Jahr 1163PapstAlexander II.DasBauwerkerfuhr bis zum 17. Jahrhundert verschiedene Änderungen. DieWandmalereienim Kirchenschiff schuf im 19. JahrhundertHippolyte Flandrin. DieKathedraleNotre Dame de Parisauf derÎle de la Cité(4. Arrdt.) ist eine der frühestengotischenKathedralen Frankreichs. Sie istMaria, der MutterJesu, geweiht (notre dame=Unsere Liebe Frau). Der Bau wurde im Jahr 1163 unter BischofMaurice de Sullybegonnen und erst 1345 fertiggestellt. Die Ausmaße des Kirchenschiffes betragen 130 mal 48 Meter bei einer Höhe von 35 Metern. Es bietet,Emporeeingeschlossen, Raum für 9000 Personen. Die beiden Türme sind 69 Meter hoch, derDachreitererreicht 90 Meter. DiePfarrkircheSaint-Germain-l’Auxerrois, die dem Ostportal desLouvre(1. Arrdt.) gegenüberliegt, stammt in ihren Grundzügen noch aus der Zeit derRomanik. Sie besitzt allerdings sowohl ein gotisches Strebwerk als auch ein hochgotisches Portal. Die Anbauten an dieser Kirche stammen aus demBarock. Diese Kirche ist dem heiligenGermanus von Auxerregeweiht(Saint Germain l’Auxerrois). Die PfarrkircheSaint-Sulpicesüdlich vomBoulevard Saint-Germain(6. Arrdt.) ist dem heiligenSulpicius II. von Bourgesgeweiht. Sie ersetzte einen romanischen Vorgängerbau aus dem Anfang des 13. Jahrhunderts. Die Arbeiten an der heute existierenden Kirche begannen im Jahre 1649, wurden aufgrund politischer und finanzieller Schwierigkeiten aber erst im 18. Jahrhundert abgeschlossen. DieklassizistischeFassade entwarfGiovanni Servandoniim Jahr 1732. Die Kirche ist berühmt für ihreCavaillé-Coll-Orgel, eine der größtenOrgelnFrankreichs. Die PalastkapelleSainte-ChapelleimPalais de la Cité(1. Arrdt.) unweit der Kathedrale ließLudwig der Heiligein den 1240er-Jahren erbauen, um sehr kostbareReliquienaufzunehmen: dieDornenkroneChristi und Teile des „Wahren Kreuzes“. Diese für den gotischenstyle rayonnantdes 13. Jahrhunderts beispielhafte Kapelle gehört zu den schönsten Baudenkmälern derGotik. Der größte Teil ihrer Wände wird von kostbarenBuntglasfensterneingenommen, wodurch der hohe Raum von unirdisch wirkendem Licht durchflutet wird. Mit dem Bau derPfarrkircheSaint-Eustachewurde im 16. Jahrhundert begonnen. Die Kirche wurde um 1640 fertiggestellt. Sie befindet sich im 1. Arrondissement und war die Kirche der Händler desbenachbarten Marktes(heute mit demForum des Hallesbebaut). Der spätgotische Sakralbau weist bereits Züge der aufkommenden Renaissance auf. DerDôme des Invalides(Invalidendom, eigentlich Invalidenkuppel) wurde zwischen 1670 und 1691 vonJules Hardouin-Mansartauf dem linken Seineufer erbaut (7. Arrdt.). Diese prächtige Kuppelkirche ist, so wie die benachbarte SoldatenkircheSaint-Louis des InvalidesTeil desHôtel des Invalidesund zählt zu den schönsten Bauten desklassizistischen Barocksin Frankreich. Ihr Inneres wurde im 19. Jahrhundert zu einem Grabmal für den französischenKaiserNapoléon I.umgestaltet. DessenLeichnamruht hier seit 1861 nach seiner Überführung ausSankt Helena1840, so wie verschiedene andere bedeutende Persönlichkeiten. Der Bau der KircheLa Madeleinenördlich derPlace de la Concorde(8. Arrdt.) begann 1764 nach dem Entwurf des ArchitektenPierre Contant d’Ivryund wurde im Dezember 1791 aufgrund derFranzösischen Revolutioneingestellt. Die Arbeiten wurden von dem ArchitektenJean-Jacques-Marie Huvé(1783–1852) wieder aufgenommen und im Jahre 1842 abgeschlossen, die Weihe zur Pfarrkirche erfolgte am 9. Oktober 1845. Die Innenausstattung entstammt vorwiegend den Jahren 1830–1840. Als besonders sehenswert gilt die Statue der Maria Magdalena vonCarlo Marochetti. Die Orgel des bedeutenden französischen OrgelbauersAristide Cavaillé-Coll(1811–1899) gilt als eine der klangvollsten der Stadt. DieBasilique du Sacré-Cœur(Basilika vom Heiligen Herzen) ist einerömisch-katholischeWallfahrtskircheauf dem Hügel vonMontmartreund bildet den höchstgelegenen Punkt der Stadt nach dem Eiffelturm. Der Bau der Kirche im „Zuckerbäckerstil“ wurde 1875 von demArchitektenPaul Abadiebegonnen, der sich in einem Wettbewerb gegen 78 Mitbewerber durchgesetzt hatte und dessen Entwurf deutlich durch denrömisch-byzantinischen Stilalter Kirchen wie derHagia Sophiaund desMarkusdomsinVenediginspiriert wurde. Abadie verstarb bereits 1884. Ihm folgten bis zur Fertigstellung 1914 sechs Architekten in der Bauleitung nach. Die Pariser Straßen sind mit rund 89.000 Bäumen gesäumt. Das städtischeGartenbauamtDirection des Parcs, Jardins et Espaces Verts de Parisunterhält innerhalb der Stadtgrenzen 2.437 Hektar Grünflächen, zu denen außer den beiden großenStadtwäldernBois de Vincennes(995 Hektar) undBois de Boulogne(846 Hektar) auch die 14 innerstädtischenFriedhöfe(92 Hektar) zählen, die GartenbauschuleÉcole Du Breuil(22 Hektar), das GartenbauzentrumJardin des Serres d’Auteuil(8,5 Hektar), in demBlumenundSträuchergezüchtet werden, und der neueCentre horticole de la Ville de Paris(Blumenproduktion) inRungis,FresnesundAchères(insgesamt 477 Hektar). Als Erholungsgebiet abzuziehen sind die bepflanzten Böschungen der RingautobahnBoulevard périphérique(51 Hektar). Auf die Grünanlagen von städtischen Sportanlagen, Schulen, Kindergärten und Krippen entfallen 36 Hektar. Die restliche Fläche (386 Hektar) wird von öffentlichenPromenaden,Parks,Gärten, densquaresgenannten begrüntenPlätzenund vonBlumenrabatteneingenommen. Die Stadt Paris besitzt darüber hinaus jenseits ihrer Grenzen sechs weitere Friedhöfe, denWaldBois de BeauregardbeiLa Celle-Saint-Cloud. Außer den städtischen Anlagen stehen den Bewohnern und Besuchern von Paris sieben vom Staat unterhaltene Gärten und Parks mit insgesamt 118 Hektar Fläche zur Verfügung. Der mit auffällig vielen Statuen geschmückteTuileriengartenerstreckt sich am rechten Seineufer vom Louvre bis zurPlace de la Concorde. Er erinnert an das frühere Schloss derKatharina von Medici, das nach ihr noch viele Herrscher bewohnen sollten, bis es 1871 während derPariser Kommunezerstört wurde. In dem westlichen Bereich des Gartens befinden sich das ehemalige BallhausJeu de Paume, in dem heute dieGalerie nationale du Jeu de Paumeuntergebracht ist, und die zum Museum umfunktionierte frühereOrangerie. Einer der beliebtesten städtischen Parks ist der im Jahre 1612 angelegteJardin du Luxembourgimquartier Latin, der zumPalais du Luxembourggehört. Der Garten umfasst streng geometrisch angelegte Partien, aber auch freier gestaltete Zonen. ImJardin du Luxembourgbefindet sich außerdem eine zwei Meter hoheKopiederNew YorkerFreiheitsstatue. An den Gittern des Parks sind regelmäßig Foto-Ausstellungen zu sehen. Der StadtwaldBois de Boulogne, an der westlichen Stadtgrenze beiBoulogne-Billancourtgelegen, ist mit einer Fläche von rund 8,5 Quadratkilometern das größte innerstädtische Erholungsgebiet. Dort befand sich von jeher eine große Waldfläche, derBois de Rouvray. Bereits der FrankenkönigDagobert I.kam im 7. Jahrhundert hierher, um zu jagen. 1848 übernahm der Staat den Wald und übertrug ihn 1852 der Stadt Paris. Im Zuge der Umgestaltung von Paris unterNapoleon III.durchHaussmannwurde der Wald unter der Leitung des GartenarchitektenJakob Ignaz Hittorffzu einem bewaldeten Park umgebaut. Es entstanden Wege und künstliche Wasserflächen. Fehlplanungen bewirkten, dass die künstlichen Seen nicht gefüllt werden konnten. Einige der Seen lagen am Hang. Hittorff wurde von Haussmann entlassen und durch den IngenieurJean-Charles Alphandund den LandschaftsgärtnerJean-Pierre Barillet-Deschampsersetzt. Die beiden lösten das Wasserproblem durch die Schaffung künstlicher Wasserfälle (Kaskaden). DerBois de Vincennesist der zweite, im Stil englischer Landschaftsgärten angelegte Pariser Stadtwald. Er war von jeher königliches Jagdrevier. In früheren Zeiten stand ein Jagdschloss in dem Wald, das später durch eine Festung ersetzt wurde, die wir heute alsSchloss Vincenneskennen. 1860 überließNapoleon III.den Wald der Stadt Paris mit dem Auftrag, ihn ähnlich wie denBois de Boulogneneu zu gestalten. Der Landschaftsarchitekt Jean-Charles Alphand ließ das Gelände aufforsten und mit künstlichen Hügeln und drei Seen versehen. Für dieOlympischen Sommerspielevon 1900 wurden Sportanlagen gebaut und die Wege für diesen Zweck ausgebaut. Der 1986 von dem ArchitektenBernard Tschumientworfene neue StadtparkParc de la Villettezählt mit 25 Hektar zu den größten Pariser Grünflächen. Er entstand auf dem Gelände des 1974 geschlossenen Schlachthofes vonLa Villetteund wird von demCanal de l’Ourcqdurchquert. Bereits 1984 wurde dasZénitheröffnet, an dessen Gestaltung sich die später errichteten Gebäude orientierten. Sämtliche Elemente des Parks sind in futuristischem Stil gebaut. Der Park beherbergt, neben anderem, dieCité des sciences et de l’industrie(ein Technikmuseum, ähnlich dem schweizerischenTechnorama), das kugelförmigeIMAX-KinoGéode, dieCité de la musique, dasZénithund das Unterseebootl’Argonaute. Die bereits bestehendenRauchverbotesollen 2019 auf 52 Parks ausgeweitet werden. Auf den 500Spielplätzengilt das Verbot bereits seit 2015.[45] Zu den Grünanlagen zählen in Paris auch die Friedhöfe. Zu Beginn des 19. Jahrhunderts wurden außerhalb derdamaligenGrenzen der Hauptstadt drei und in Paris ein neuer Friedhof angelegt: derCimetière de Montmartreim Norden, derCimetière du Père-Lachaiseim Osten, derCimetière du Montparnasseim Süden sowie derCimetière de Passy. Diese Friedhöfe sind aufgrund ihrer Stille und der Gräber vieler berühmter Persönlichkeiten beliebtes Ziel der Spaziergänger und Touristen. DerPère-Lachaiseist der größte Friedhof von Paris und einer der berühmtesten Friedhöfe der Welt. Er ist nachFrançois d’Aix de Lachaisebenannt, auf dessen Gärten der Friedhof errichtet wurde. Das Konzept des Père-Lachaise wurde 1808 dem neoklassischen ArchitektenAlexandre-Théodore Brongniartanvertraut, der zu dieser Zeit Generaloberinspekteur der zweiten Sektion für Öffentliche Arbeiten imDépartement Seineund der Stadt Paris war. Brongniart entwarf die großen Achsen sowie Grabmonumente, von denen aber nur das für die Familie Greffulhe im neogotischen Stil verwirklicht wurde. Durch das starke Bevölkerungswachstum im 19. Jahrhundert wurde der Platz auf den Friedhöfen in Paris(intra muros)knapp und es wurden mehrere Großfriedhöfe für die Pariser Bevölkerung in den Vorstädten(extra muros)angelegt, welche auch heute noch in Benutzung sind. Die wichtigsten von ihnen sind:Cimetière parisien de Bagneux,Cimetière parisien de Pantin,Cimetière parisien de Saint-Ouen,Cimetière parisien de ThiaisundCimetière parisien d'Ivry. Der größte Friedhof ist derCimetière parisien de Pantinmit über 200.000 Gräbern, in denen bis heute weit über eine Million Menschen beigesetzt wurden. Paris kann auf eine lange und erfolgreiche Filmgeschichte zurückblicken. Pariser Unternehmer und Gesellschaften wie dieGebrüder Lumière,Pathé FrèresoderGaumontwaren es, die den Film hinaus in die Welt trugen. So erfanden die Gebrüder Lumière im Jahre 1895 denCinématographen, ein Gerät, das sowohl Filme aufnehmen als auch abspielen konnte. Sie führten ihn am 22. März jenes Jahres erstmals vor. Die Aufführung in der PariserSociété d’encouragement pour l’industrie nationalegilt als eine der ersten Filmvorführungen der Welt. In der Folge bereisten die Lumières die größten Städte Europas, um ihre Erfindung zu verbreiten – mit Erfolg. In den folgenden Jahren machte sich rasch Konkurrenz in Paris breit. Die Pathé Frères stiegen bald zu einem der größten Filmproduzenten Europas auf und exportierten ihre Stummfilme weltweit. In den großen Städten Europas wurden Außenstellen und Kinos gegründet. Aber auch Paris selbst war in vielen Filmen Drehort und Filmkulisse. Abgesehen von den zahlreichen Aufnahmen der Stummfilmzeit, oft dokumentarischer Natur, war die Stadt sowohl in inländischen als auch in ausländischen Spielfilmproduktionen zu sehen. Paris ist regelmäßiger Austragungsort bedeutender Großveranstaltungen. Hierzu zählen unter anderem die Zieletappe derTour de FranceimStraßenradsport, derMarathon de Paris, das Grand-Slam-TurnierFrench Open(offiziellTournoi de Roland Garros) imTennis, dasMeeting Areva(vormalsMeeting Gaz de France) in derLeichtathletik, dieTrophée Eric Bompard(früherTrophée Lalique) imEiskunstlaufund dasSechs-Nationen-Turnier(Tournoi des Six Nations)imRugby. ImPferdesportist derPrix de l’Arc de Triomphe, einGalopprennenüber 2400 Meter für über dreijährige Rennpferde, neben demEpsom Derbyund demKentucky Derbyeines der prestigeträchtigsten internationalen Pferderennen seiner Kategorie. Das Rennen wird seit dem 3. Oktober 1920 alljährlich am ersten Sonntag im Oktober ausgetragen. Eingeführt wurde es während einer Feier zum Ende desErsten Weltkrieges. DerSpringsport in Parisist mit mehreren großen internationalen Turnieren vertreten. Paris war Gastgeber derOlympischen Sommerspielevon1900und1924. Darüber hinaus bewarb sich Paris für die Olympischen Sommerspiele von1956,1992,2008und2012. Am 13. September 2017 wurden auf der Vollversammlung desInternationalen Olympischen KomiteesinLimadieOlympischen Sommerspiele 2024an Paris vergeben. In der Hauptstadtregion gibt es zahlreiche Sportstätten von nationalem und internationalem Rang, darunter allein fünf moderne Stadien für durchschnittlich 42.000 Zuschauer. DasStade de France(„Frankreich-Stadion“) liegt inSaint-Denis, einem Vorort nördlich von Paris. Das multifunktionale und bis zu 80.000 Zuschauer fassende Nationalstadion von Frankreich wurde für dieFußball-Weltmeisterschaft 1998erbaut und ging als Endspielort des ersten französischen Weltmeistertitels in die Geschichte ein. Sowohl diefranzösische Fußballnationalmannschaftals auch diefranzösische Rugby-Union-Nationalmannschafttragen ihre Heimspiele imStade de Franceaus, das zudem Austragungsort der jährlichen Finalpartien der RugbyligaTop 14ist. Im Stade de France fanden unter anderem die jeweiligen Finalspiele der Fußball-Weltmeisterschaft 1998, derRugby-Union-Weltmeisterschaft 2007, derFußball-Europameisterschaft 2016, derRugby-Union-Weltmeisterschaft 2023sowie dieLeichtathletik-Weltmeisterschaften 2003statt. Es war auch Olympiastadion für dieOlympischen Sommerspiele 2024. DasPrinzenparkstadion(Parc des Princes)ist eine traditionelle Wettkampfstätte im Pariser Stadtkern, die überwiegend vom FußballvereinParis Saint-Germaingenutzt wird und für rund 49.000 Zuschauer konzipiert wurde. Es war das Endspielstadion der erstenFußball-Europameisterschaft1960und der ersten Austragung desEuropapokals der Landesmeister1956. Seit dem Bau des neuen Nationalstadions hat das Prinzenparkstadion an Bedeutung verloren, gehört aber weiterhin zu den modernsten Stadien Europas. DieUEFA(Union des Associations Européennes de Football)verlieh der Sportstättevier Sterne. Unmittelbar neben dem Prinzenparkstadion wurde 2013 das moderneJean-Bouin-Stadion(Stade Jean-Bouin)errichtet. Es bietet mehr als 20.000 Zuschauern Platz und dient dem renommierten RugbyvereinStade Français Parisals Heimspielstätte. Darüber hinaus war es das Endspielstadion derFrauen-Rugby-Union-Weltmeisterschaft2014. InNanterre, einem Vorort westlich von Paris, steht seit 2017 zudem die teilweise überdachteU Arena. Das unmittelbar hinter demGrande Archeerbaute Multifunktionsgebäude nimmt rund 40.000 Zuschauer auf und dient vor allem dem traditionsreichen RugbyvereinRacing 92als Heimspielstätte. Beide Bauwerke sind regelmäßig Austragungsorte verschiedener anderer Mannschaftssportarten. Weitere nennenswerte Einrichtungen sind das 20.000 Zuschauer aufnehmendeSébastien-Charléty-Stadion(Stade Sébastien Charléty)im Pariser Stadtkern oder dasPariser Olympiastadion(Stade Olympique Yves-du-Manoir)inColombes, einem Vorort nordwestlich von Paris, für etwa 10.000 Zuschauer. Es war unter anderem Austragungsort der Olympischen Sommerspiele 1924. Beide Stadien sind insbesondere Austragungsorte von Leichtathletikveranstaltungen und Partien kleinerer Fußball- oder Rugbyvereine. DieLongchamp-Pferderennbahn(Hippodrome de Longchamp)ist die wichtigste Pferderennsportanlage in Paris. Das heutige Hippodrom wurde 1857 auf den Mauern der bei derFranzösischen RevolutionzerstörtenAbtei Longchamperrichtet. Neben Pferderennen wie demPrix de l’Arc de Triomphefinden hier auchSpringturniereund andere Sportveranstaltungen statt. Im Januar findet in Paris die internationale ModenschauPrêt-à-porterinPorte de Versaillesund dasFestival Présences(Festival zeitgenössischer Musik) mit zahlreichen Gratiskonzerten in derMaison de Radio Francestatt. Der Februar, Monat desValentinstages, steht dank einer Initiative des PariserFremdenverkehrsamtes, an der sich geschulte Fremdenführer, Museen wie dasMusée de la Vie Romantique(9. Arrondissement) sowie dasHôtel Scheffer-Renanund Gaststättengewerbe beteiligen, unter dem Motto „Paris Romantique“. Im März startet imParc floral de ParisbeimSchloss VincennesderPariser Halbmarathon. Auch diePariser Buchmesseist im März. In Saint-Denis im Norden von Paris wird das Blues- und JazzfestivalBanlieues Bleuesveranstaltet und im Juli dasFestival Paris Cinéma. Im April gehen über 30.000 Teilnehmer desMarathon de Parisauf derAvenue des Champs-Élyséesan den Start. Gegen Ende April und Anfang Mai bietet Paris ein Schauspiel ganz besonderer Art: die vonElla Fitzgeraldin dem Lied „April in Paris“ besungeneKastanienblüte. Im Mai wird das renommierteste Pferderennen in Frankreich, dasGrand Steeple-Chase de ParisimHippodrome d’Auteuilund Ende Mai/Anfang Juni dieFrench Open, das zweite Tennisturnier derGrand-Slam-Serie, im Roland-Garros-Stadion, ausgetragen. Von Anfang Mai bis in den Monat Juli werden seit einhundert Jahren alljährlich anlässlich eines Rosenzüchterwettbewerbes imParc de Bagatelledie erlesensten Kreationen prämiert. Am Sommeranfang, dem 21. Juni, wird dieFête de la Musiqueveranstaltet, die vonJack Langinitiiert wurde und nun in ganz Frankreich gefeiert wird: es gibt überall kostenlose Konzerte bekannter und weniger bekannter Bands. Ende Juni findet dieGay-Pride-Paradeauf demPlace de la Républiqueund derBastillesowie weiteren Veranstaltungsorten statt. Die Festivitäten am 14. Juli, demNationalfeiertag, finden mit der Militärparade, die auf derAvenue des Champs-ÉlyséesvomArc de Triomphebeginnt und amPlace de la Concordeendet, ihren Höhepunkt. Während der französischen Sommerferien, in der ein großer Teil der Pariser Bevölkerung die Stadt verlässt, um in die Ferien zu fahren, findet seit dem Jahr 2002 die VeranstaltungParis-Plages(deutsch: Strände in Paris) vomQuai du Louvrebis zurPont de Sully, amPort de la Gareund amBassin de la Villettestatt. Damit soll den Daheimgebliebenen auf einigen Kilometern des für den Verkehr gesperrten Seineufers ein Stück Strandleben geboten werden. Diese Veranstaltung dauert meistens vier bis fünf Wochen von Mitte Juli bis Mitte August. Im September öffnen an einem Wochenende zu den sogenannten„Journées du Patrimoine“(Tage des Kulturerbes) sonst schwer zugängliche Pariser Paläste undHôtels particuliers/private Stadtpalästeihre Tore. Eine einmalige Gelegenheit, den Residenzen hoher Würdenträger einen Besuch abzustatten, wie beispielsweise demÉlysée-Palastoder demHôtel Matignon. In diesem Monat veranstaltet die Stadt Paris im Rahmen derFête des Jardins de Parisin den Pariser Parks und Gärten kostenlose Konzerte, Ausstellungen sowie Theater- und Kinovorführungen. Die Theatersaison wird mit demFestival d’Automne à Paris(Herbstfestival) eröffnet. Im Oktober finden im ersten Herbstmonat auf dem Weinberg desMontmartrehügelzum Auftakt der Weinlese eine farbenfrohe Parade, zahlreiche Partys und Weinproben statt. Es gibt an einem Wochenende seit 2002 dieNuit Blanche(„Lange Nacht der Kunst“) und alle zwei Jahre findet derPariser Autosalonstatt. Anfang November empfiehlt sich der Besuch auf einem der nachAllerheiligenblumenüberladenenFriedhöfe. Von Mitte Dezember bis Mitte Januar sind dieChamps-Élyséesbesonders sehenswert, da zu dieser Zeit die Kronen aller Bäume mit Lichterketten geschmückt sind. Das ganze Jahr hindurch steigt, vorbehaltlich günstigen Wetters, alle 15 Minuten derEutelsat-FesselballonvomParc André-Citroënauf. Aus 150 Metern Höhe bietet seine Gondel jeweils 30 Passagieren einen umfassenden Rundblick über den Westen der Stadt. Die zeitlich ersten Restaurants weltweit im heutigen Sinn entstanden mit derFranzösischen Revolutionin Paris, in der auch das alteZunftrechtaufgehoben wurde, nach dem beispielsweise Suppenküchen und Pastetenbäcker streng getrennt waren. Namensgeber des Restaurants war der Wirt einer Suppenküche in Paris, Boulanger, der laut Eigenwerbung „göttliche Restaurants“, besonders stärkendebouillons, anbot. 1765 erstritt er sich die Genehmigung, trotz der Zunftregeln neben Suppen auch Hammelfüße mit Sauce zu servieren. Von da an nannte er sich „Restaurateur“ und seinebouillonwurde zum Namensgeber der Restaurants, die verschiedene Speisen anboten. „Restaurateure sind diejenigen, welche die echten Brühen, genannt Restaurants, herstellen und außerdem alle möglichen Crèmes, Suppen mit Reis und Nudeln, Eier, Makkaroni, Hähnchen, Konfitüren, Kompott und andere gesunde und appetitliche Gerichte anbieten … Der Preis jedes Gerichts ist fest, und sie werden zu jeder Zeit am Tag serviert. Damen dürfen dort verkehren und sich Speisen zubereiten lassen.“ Vor der Revolution gab es in Paris noch weniger als hundert Restaurants, aber schon um 1800 waren es etwa 500 bis 600. Es wurde Sitte, dass sich zugezogene Abgeordnete, die oft wenig repräsentativ wohnten, und wohlhabend gewordene Bürger zu geschäftlichen Besprechungen und privaten Verabredungen im Restaurant trafen. Die Pariser Restaurants wurden mehrheitlich von Köchen und deren Brigaden betrieben, denen nach der Flucht ihreradligenArbeitgeber ins Ausland nichts anderes übrig blieb, als sich selbständig zu machen. Dabei brachten sie einen aufwändigen Kochstil mit, der Bürgerlichen bis dahin nicht zugänglich war. So verband sich dieHaute Cuisineim Restaurant mit den informellen, die adlige Etikette geringschätzenden, bürgerlichen Umgangsformen. Heute gibt es in Paris Tausende von Restaurants, die dem Gast Speisen derfranzösischen Küchewie auch internationale Gerichte anbieten. Paris hat viele Kaufhäuser, Einkaufszentren und Märkte. Einige davon sind wegen ihres Prestiges, ihrer Tradition und ihrer Architektur weltbekannt. So gilt das LuxuskaufhausLe Bon Marchéauf derRive gaucheals das erste moderne Warenhaus der Welt. Ebenfalls weltbekannt sind dieGaleries Lafayette, deren Pariser Stammhaus sich durch seine Jugendstilarchitektur auszeichnet. Die große Zentralhalle mit ihrer Glaskuppel ist ein Baumonument und Denkmal. Nur wenige Meter entfernt steht amBoulevard Haussmannim9. Arrondissementdas KaufhausPrintemps, dessen zentrale Halle gleichfalls über eine Jugendstil-Glaskuppel verfügt. In der Nähe derOpéra Bastilleliegt der FlohmarktMarché d’Aligre. Das Angebot reicht von Kleidung, Obst, Keramik und Bildern bis zu Lebensmitteln und Blumen. Der Markt ist morgens, täglich außer montags geöffnet. Überwiegend Kleidung aus allen Bereichen, aber auch moderne Kunstgegenstände hat derPuces de la Porte de Montreuilnahe der MetrostationPorte de Montreuilim Angebot. Kleidung und Haushaltswaren kann man auf demMarché aux puces de la Porte de Vanvesnahe der MetrostationPorte de Vanveserwerben. DerPuces de Saint-Ouen-Clignancourtbesteht aus einer Anzahl mehrerer Märkte, die miteinander verbunden sind. Einige der dortigen Händler haben sich auf hochwertige Kunstgegenstände spezialisiert, aber es werden auch preiswerte Artikel angeboten. DasLe Louvre des antiquairesnahe demPalais Royalund demLouvregehört zu den größten und bekanntesten Antiquitätengeschäften in Paris. In rund 250 Räumen und auf drei Etagen werden zahlreiche Waren aus der ganzen Welt angeboten. Neben Möbeln, Gemälden und Teppichen werden Kristall, Waffen, Spielzeug, Uhren und Schmuck zum Kauf angeboten. Antiquarische und gebrauchte Bücher werden an den vielen Buchhändlerständen(bouquinistes)an der Seine verkauft. In Paris gibt es zahlreiche Mode-Boutiquen, die auchPrêt-à-porterbekannter Modehäuser verkaufen.Haute Coutureist beiChanelin derRue Cambon, beiDiorin derAvenue Montaigneund beiChristian Lacroixin derRue du Faubourg Saint-Honorésowie in derAvenue Montaigneerhältlich. Laufstegmoden bekommt man beiGianni Versacein derRue des Saints-Pères, beiJean Paul Gaultierin der Nähe der MetrostationBourseund beiCerruti 1881nahe der MetrostationMadeleine. Elegante Kleidung einkaufen kann man auch inSaint-Germain, imLe Maraisoder in derGalerie Vivienne(naheLes Halles). InLa Défense, einem seit Ende der 1950er-Jahre in den westlichen VorortenCourbevoie,NanterreundPuteauxentstandenen Büro- und Geschäftsviertel, in dem Wolkenkratzer dominieren, befindet sich als westliche Fortführung der berühmtenPariser Achsedie sogenannteGrande Arche. Der 110 Meter hoheKubusist ein Entwurf des ArchitektenJohan Otto von Spreckelsen, der vonPaul Andreuausgeführt wurde. Er bildet den westlichen Ausgangspunkt deraxe historique, die zusammen mit demArc de Triompheund demArc de Triomphe du Carrouselbeim Louvre eine Gerade bildet. Die Einweihung erfolgte mit dem Gipfeltreffen der Staatschefs derG7am 14. Juli 1989 zur 200-Jahr-Feier derFranzösischen Revolution. Das Gebäude dient dem französischen Handels- und Verkehrsministerium als Sitz. DasSchloss Fontainebleauin dem gleichnamigen Ort 65 Kilometer südlich von Paris wurde im 16. Jahrhundert unterFranz I.undHeinrich II.an der Stelle eines Jagdschlosses gebaut. Der Architekt warPhilibert de l’Orme(1510–1570). Es ist vor allem für seineRenaissanceausstattungberühmt. DasSchloss Versailles, welches zu den größten Schlossanlagen Europas zählt, liegt in der westlich von Paris gelegenen StadtVersaillesund war Vorbild vieler europäischer Königs- und Fürstenschlösser. Für die Vergrößerung des JagdschlossesLudwigs XIII.zogLudwig XIV.im Jahre 1661 den ArchitektenLe Vau, den HofmalerLe Brunund den GartenarchitektenLe Nôtreheran. Den mittleren Flügel der insgesamt 750 m langenbarock-klassizistischenGartenfront nehmen die vielbewunderteSpiegelgalerie„Galerie des Glaces“ sowie die Ecksalons des Krieges und des Friedens ein. An diese schließen sich im Norden das Staatsgemach des Königs, im Süden das Gemach der Königin an. Beachtung verdienen weiter das zweite Schlafzimmer des Königs im Mittelpunkt des Schlosses, die Kapelle, die Oper, und die erst im 19. Jahrhundert ausgestattete Schlachtengalerie. DieBasilika Saint-Denisist eine ehemaligeAbteikirchein der StadtSaint-Denisnördlich von Paris und die Grabstätte der französischen Monarchen, welche seit dem Ende des 10. Jahrhunderts nahezu alle hier begraben liegen. Schon im 5. Jahrhundert stand hier über dem Grab desDionysius von Parisein Kloster, das im 7. Jahrhundert unterDagobert I.zurAbteierweitert wurde. In dem ab 1136 erneuertenChorwurde 1142 dasKreuzrippengewölbeerfunden. Damit wurde die Basilika das erste gotische Gebäude der Welt. Die Kirche hat seit 1966 den Status einerKathedrale. DasDisneyland Resort Parisin der PlanstadtMarne-la-Vallée, etwa 30 Kilometer östlich von Paris, ist ein 19,43 Quadratkilometer großer Freizeitkomplex mit zwei Themenparks – demDisneyland Parkund demWalt Disney Studios Park– einem Golfplatz, Vergnügungs- und Einkaufszonen, zehn Hotels und einem Stellplatz für Wohnmobile. Laut einer Studie aus dem Jahr 2014 erwirtschafte der Großraum Paris einBruttoinlandsproduktvon 715 Milliarden US-Dollar (KKB). In der Rangliste der wirtschaftsstärksten Metropolregionen weltweit belegte er damit den 6. Platz.[46] In einer Rangliste der Städte nach ihrer Lebensqualität belegte Paris im Jahre 2024 den 34. Platz unter 241 untersuchten Städten weltweit.[47] Paris ist das bedeutendste Wirtschaftszentrum Frankreichs. In derMetropolregionParis hat sich etwa ein Viertel der Produktionsbetriebe des Landes niedergelassen. Durch den riesigen Absatzmarkt, den die Stadt bietet, übt sie von jeher große Anziehungskraft auf Hersteller von Konsumgütern aus. Paris ist bekannt für die Produktion von Luxusgütern (Haute Coutureund Schmuck). Zu den wichtigsten Erzeugnissen der Stadt zählen chemische Produkte, Elektrogeräte, Kraftfahrzeuge und Maschinen. Fast alle großen Dienstleistungsunternehmen Frankreichs, insbesondere Banken und weitere Unternehmen des Finanzwesens, haben ihren Sitz in Paris. Unter anderem befindet sich hier auch diePariser Börse. Seit den 1990er-Jahren werden vermehrt Anstrengungen unternommen, multinationale Konzerne anzusiedeln. Die Stadt ist heute eine der wichtigsten Handelsmetropolen in Europa. Ein nicht zu unterschätzender Vorteil ist die Lage der Stadt inmitten einer der fruchtbarsten Agrarlandschaften in Europa. Die Landwirtschaft war deshalb schon in den früheren Jahrhunderten die bedeutendste Wirtschaftsgrundlage der Region und sicherte die Nahrungsmittelversorgung der Bevölkerung in der Stadt. Heute hat Paris den bedeutendsten Großmarkt der Welt für Lebensmittel, denGroßmarkt Rungis. Die Hauptstadtregion hat dank der starken Konzentration nationaler und internationaler Unternehmen einen Anteil von etwa einem Drittel amBruttoinlandsprodukt(BIP) des Landes. Sie gehört zu den wohlhabendsten Regionen Europas. Ein Problem ist die Arbeitslosigkeit, die in etwa dem nationalen Durchschnitt entspricht. Seit Anfang der 1990er-Jahre verlor Paris rund eine viertel Million Arbeitsplätze. Ein Grund ist der Abbau von Arbeitsplätzen in der Industrie und die Verlagerung wirtschaftlicher Aktivitäten in benachbarte Gemeinden wie das GeschäftszentrumLa Défense. Die meisten französischenFernseh- undRadiosendersowie die größten Medienkonzerne des Landes (Vivendi,Groupe Lagardère,TF1) haben ihren Sitz in Paris. Die Stadt ist Erscheinungsort international bedeutender Tageszeitungen (Le Figaro,Le Monde,Libération) und bedeutendstes internationales Zentrum des Verlagswesens. DerTourismusspielt eine besondere Rolle. Die Region Paris ist mit 42 Millionen Besuchern im Jahr das zahlenmäßig bedeutendste Ziel weltweit,[48]davon besuchen 35 Millionen die Stadt Paris.[49]Luxushotels berechneten 2011 durchschnittlich etwa den dreifachen Preis, der in Berlin gezahlt wird.[50]Ausländische Touristen brachten 2016 Einnahmen in Höhe von 12,9 Milliarden US-Dollar.[51] In einer Rangliste der wichtigsten Finanzzentren weltweit belegte Paris im Jahr 2018 den 24. Platz.[52] Paris ist Knotenpunkt desEisenbahn- und Straßenverkehrsnetzes in Frankreich. Am Stadtrand befinden sich vier internationaleFlughäfen. Paris wird von denPariser Kanälendurchzogen und hat den zweitgrößtenBinnenhafenin Europa (nach demDuisport). Paris ist über ein Netz von Autobahnen und Schnellstraßen mit dem ganzen Land verbunden. Eine bedeutende Rolle spielt dabei derBoulevard périphérique(Le Périph’). Diese achtspurige Stadtautobahn leitet den Verkehr rund um Paris und in die Stadt hinein. Fast alle wichtigen französischen Autobahnen führen auf Paris zu und münden aus allen Richtungen in denBoulevard périphérique: DieA 1ausLille, dieA 4ausReims, dieA 5ausDijon, dieA 6ausLyon, dieA 77ausNevers, dieA 10ausOrléans, dieA 13ausRouenund dieA 16ausAmiens. 2017 wurden 69,5 Millionen Passagiere auf demRoissy-Charles de Gaulleabgefertigt – dies war diezweithöchsteZahl aller Flughäfen in Europa. Mit 32,0 Millionen Passagieren nimmtOrlyden dreizehnten Platz ein. Der dritte FlughafenParis-Beauvaisbefindet sich außerhalb des eigentlichen Großraums und wird überwiegend vonBilligfluggesellschaftenangeflogen. Der vierte FlughafenParis-Le Bourgetwird nur für denGeschäftsflugverkehrgenutzt. Er ist der größte seiner Art in Europa. Insgesamt fertigten die vier Pariser Flughäfen im Jahr 2017 etwa 106 Millionen Passagiere ab. Damit zählt Paris neben London und New York zu den großen Luftdrehkreuzen weltweit. Darüber hinaus befindet sich in einiger Entfernung zu Paris derFlughafen Paris-Vatry, der hauptsächlich von Billigfluggesellschaften angeflogen wird. Die bedeutendenEisenbahnstreckenin Frankreich beginnen in Paris. In die Richtungen Lille im Norden,RennesundBordeauxim Westen, Lyon undMarseilleim Süden sowieStraßburgim Osten gibt esHochgeschwindigkeitsstrecken, die vomTGVbedient werden. Die Strecke desEurostarnachLondon,KölnundAmsterdamüberBrüsselgilt als bedeutende europäische Verbindung.ICEund TGV verkehren seit 2007 überSaarbrückennachFrankfurt am Mainsowie nachStuttgartundMünchen. Die wichtigsten Personenbahnhöfe sindGare d’Austerlitz,Gare de l’Est,Gare de Lyon,Gare Montparnasse,Gare du NordundGare Saint-Lazare. Dem Eisenbahngüterverkehr dienen unter anderem dieRangierbahnhöfeLe Bourget im gleichnamigen politisch selbständigen Vorort und Vaires, die durch die GroßeRingbahn(Grande Ceinture)mit den von beziehungsweise nach Paris führendenEisenbahnstreckenverbunden sind. Der Verkehr in Paris wird überwiegend über dieU-Bahnabgewickelt. DieMétro Parisist nachLondon(1863),GlasgowundBudapest(beide 1896) die viertälteste U-Bahn Europas. Die erste Métrolinie wurde am 19. Juli 1900 eröffnet. Das Pariser U-Bahn-Netz besteht aus 16 Linien (14 vollwertige und zwei Ergänzungslinien) und ist mit 245,6 Kilometern Gesamtlänge und 405 Haltepunkten in 321 Stationen eines der größten Netze der Welt. Die Métro wird täglich von rund 5 Millionen Menschen genutzt. Mit demGrand Paris Express, der seit 2015 in Bau ist und vier neue Linien sowie die Verlängerung von zwei bestehenden Métrolinien vorsieht (mit 200 neuen Streckenkilometern und 71 Stationen), soll sich die Länge allein des Métronetzes bis 2030 verdoppeln. Ergänzend zum Métro-Netz gibt es dasRéseau Express Régional(RER), dessen Züge Paris mit den Vororten(Banlieues)verbinden. Zum RER-Netz gehören die Linien A bis E, die auf den zentralen Streckenabschnitten Zugfolgen von bis zu zwei Minuten erreichen. Das jetzige RER hat seine Ursprünge in den von der staatlichen französischen EisenbahngesellschaftSNCFoder ihren Vorgängern stillgelegten Vorortbahnen, von denen eine Linie (der heutige südliche Abschnitt desRER B) schon 1937 von der Pariser Métro übernommen wurde. Von 1862 an bestand auch ein Personenverkehrsangebot auf einer Ringbahn entlang derThiersschen Stadtbefestigung, demChemin de Fer de Petite Ceinture(deutsch „kleine Gürtelbahn“), die auch für den Güterverkehr genutzt wurde. Der Personenverkehr auf derPetite centurewurde 1934 zugunsten von Omnibuslinien eingestellt. Mit RER-Zügen werden täglich 2,7 Millionen Fahrgäste vor allem in die Vororte transportiert – auf einem Streckennetz von 608 Kilometern mit 252 Haltepunkten (Stand 2024). Seit September 2020 können Einwohner unter 18 Jahren den ÖPNV gratis nutzen.[53] Der weitere Großraum Paris wird von dem NahverkehrssystemTransilienbedient. Dieses unterscheidet sich von den RER-Zügen unter anderem darin, dass die Transilien-Linien nicht die Stadt unterqueren, sondern in den großen Zentralbahnhöfen enden. Neben den fünf RER-Linien ergänzen weitere neun Transilien-Linien mit einem Streckennetz von 1.110 Kilometern mit 257 Bahnhöfen das Schienennetz der 12,4 Millionen Einwohner zählenden Hauptstadtregion Île-de-France. Das gesamte Nahverkehrsnetz erschließt sich dem Touristen durch das TicketParis Visiteoder die günstigeren TageskartenMobilis. Der Öffentliche Nahverkehr in der Hauptstadtregion Île-de-France zählte im Jahr 2023 4,2 Milliarden Fahrgäste, was rein rechnerisch über 11,4 Millionen Fahrgäste pro Tag entspricht, wobei die Fahrgastzahlen werktags noch höher als am Wochenende sind.[54]Im Jahr 2024 stieg das Fahrgastaufkommen im Öffentlichen Personennahverkehr in der Île-de-France auf 4,417 Milliarden Fahrgäste, was rein rechnerisch über 12,1 Millionen Fahrgäste pro Tag entspricht.[55]Das Netzwerk des öffentlichen Nahverkehrs in der Region Île-de-France steht weltweit auf Platz 2 hinsichtlich des Fahrgastaufkommens (nach Tokio), auf Platz 4 hinsichtlich der Netzlänge (2.186 Kilometer Schienennetz bei Métro, RER und Transilien zusammen) und auf Platz 2 hinsichtlich der Netzdichte (nach London).[56] Die Straßenbahn Île-de-France vervollständigt das Netz mit 14 Linien, die auf 186,6 Kilometern 278 Stationen bedienen. Am 21. November 1853 fuhren in Paris die erstenPferdestraßenbahnen, es waren die ersten in Europa. Mit der Elektrifizierung desStraßenbahnnetzesbegann man am 6. November 1881. Der Betrieb wurde am 14. August 1938 eingestellt. Nach 54 Jahren Unterbrechung verkehrt seit dem 6. Juli 1992 wieder eine Straßenbahn durch die Vororte, seit dem 16. Dezember 2006 verkehrt mit der neu gebauten Linie T3 dieStraßenbahnauch wieder in Paris selbst. In den letzten Jahren wurden mehrere Neubaustrecken eröffnet und bestehende Strecken erweitert. Heute (Dezember 2014) befahren die insgesamt neun Linien ein 105 Kilometer langes Streckennetz mit 183 Stationen. Die neueLinie T3führt entlang derBoulevards des Maréchauxin zwei Abschnitten von derSeine-BrückePont du Gariglianoim Südwesten bis zurPorte de Vincennesim Osten von Paris und von dort zurPorte de la Chapelleim Norden der Stadt. Die seit der Verlängerung im Dezember 2012 gut 22 Kilometer lange Strecke ist überwiegend alsRasengleisausgeführt und für 270.000 Fahrgäste pro Tag ausgelegt. Zugleich mit dem Streckenbau wurden die Straßen entlang der Strecke architektonisch neu gestaltet, eine Auflage der Pariser Behörden. Dazu gehören auch zahlreiche neu gepflanzte Bäume, Freiluftkunstwerke und neu gestaltete Fahrrad- und Fußwege. Paris ist auch von einem dichten Netz aus Buslinien durchzogen. Das Busnetz der Hauptstadtregion Île-de-France besteht aus 1500 Buslinien mit einem Streckennetz von etwa 25.000 Kilometern. Bis 2025 stieg die Anzahl der Buslinien in der Hauptstadtregion Île-de-France auf etwa 1900 an.[57]Die Busse mit den dreistelligen Nummern fahren in die Vororte, die Busse mit zweistelligen Nummern verkehren nur innerhalb der Stadt. Die meisten Omnibusse fahren zwischen 6:30 Uhr und 20:30 Uhr, die wichtigsten Linien länger bis etwa 1 Uhr nachts. Die NachtbusseNoctilienverkehren täglich die ganze Nacht.Trolleybussefuhren zum ersten Mal während derWeltausstellungin Paris zwischen April 1900 und November 1900, ein weiteres Mal zwischen 1912 und 1914 sowie nach einer Unterbrechung durch den Ersten Weltkrieg von April 1925 bis Juli 1935. Nach einer siebeneinhalbjährigen Pause wurde der Betrieb noch während des Zweiten Weltkrieges im Januar 1943 wieder aufgenommen und im April 1966 endgültig eingestellt. Seit 2007 gibt es ein flächendeckendes Netz von Fahrradmietstationen mitVelib'. 2010 umfasste das System über 20.000 Fahrräder an 1202 Stationen in Paris und einigen Gemeinden im Umland der französischen Hauptstadt und galt als das größte seiner Art weltweit.[58]Seit der Einführung von Vélib spielt Radverkehr erstmals seit vielen Jahrzehnten eine signifikante Rolle im Pariser Stadtverkehr; diese wurde seither von einer Vielzahl von privaten, stationslosenFahrradverleihsystemenergänzt. Seit 2016 gibt es einen Sharing-Service mitElektromotorrollern, seit 2018 eine Vielzahl von konkurrierendenE-Tretroller-Verleihsystemen. Bei einerBürgerbefragungam 2. April 2023 haben sich 89 Prozent der Stimmenden (bei einer Stimmbeteiligung von nur 7,46 %) gegen einen Weiterbetrieb der E-Tretroller-Verleihsysteme ausgesprochen. Die Lizenzen laufen im August 2023 aus.[59] Paris weist eine hoheLuftverschmutzungauf, die neben der Industrie und Haushalten vom Verkehr stammt. Die durchschnittliche Konzentration anFeinstaub(PM10) beträgt 38 Mikrogramm pro Kubikmeter.[60]Der Grenzwert von 80 Mikrogramm pro Kubikmeter wurde 2015 in manchen Stadtteilen häufig überschritten.[61]Die Stadtverwaltungerließmehrere Maßnahmen, darunter sowohl zeitlich beschränkte als auch dauerhafte, um die Luftverschmutzung zu verringern und den Kraftverkehr zu reduzieren: Bereits im Jahr 2013 wurde die südliche Seineuferstraße im Bereich der Innenstadt für den Autoverkehr gesperrt und in eine Fußgängerzone umgewandelt, im September 2016 folgte die nördlichen Uferstraße.[62]Im Oktober 2015ordnetedie Bürgermeisterin,Anne Hidalgo, einenautofreien Tagfür einen kleinen Teil der Innenstadt an.[63]Seit Mai 2016 werden dieChamps-Elyséesam jeweils ersten Sonntag des Monats für den Kraftverkehr gesperrt.[64]2016 wurden am Wochenende nach dem weltweiten autofreien Tag, dem 22. September, über 640 Kilometer für motorisierten Verkehr gesperrt.[65] Anfang Dezember 2016 bewegten wochenlange hohe PM10-Werte über 80 Mikrogramm pro Kubikmeter, die zu Einschränkungen in der Nutzung von privaten Personenkraftwagen in Paris und den Nachbargemeinden führten: über mehrere Tage wurde u. a. wechselweise das Fahren von Autos mit geraden bzw. ungeradenKennzeichenzahlenverboten und diekostenfreie Nutzung öffentlicher Verkehrsmitteleingeführt.[66]Seit Sonntag, 15. Januar 2017, wurde eineUmweltzonein der Innenstadt, dieZone à circulation restreinte, eingerichtet, die auch für Fahrzeuge aus dem Ausland gilt. Ausgenommen ist die StadtautobahnBoulevard périphérique. Die erforderliche Plakette ist nach Schadstoffklassen gestaffelt und erlaubt differenziertere Fahrverbote je nach Belastung.[67]Hidalgo beabsichtigt die Zahl der Personenkraftwagen langfristig zu halbieren[68]um damit vor allem dieLuftqualitätbezüglichStickstoffdioxidund der Feinstaubwerte zu verbessern.[69]Die Halbierung der Verkehrs soll durch die Verbannung desDurchgangsverkehrserreicht werden, welcher rund die Hälfte des aktuellenVerkehrsaufkommensausmacht.[70]Per 30. August 2021 wurde auf den meisten StraßenTempo 30eingeführt.[71]Seit dem 1. September 2022 müssen Zweiräder mit Verbrennungsmotor Parkgebühren zahlen, was bis dahin nicht der Fall war.[72] Die Gegensätze zwischen Paris und dem Rest des Landes werden besonders im Bereich Bildung deutlich, da die angesehensten Bildungsstätten Frankreichs sich in Paris befinden. Die bestenGrandes écolesFrankreichs haben ihren Sitz in Paris, darunter dieÉcole polytechnique(eröffnet 1794),École des hautes études commerciales de Paris(HEC),Sciences Po Paris,dieÉcole normale supérieure(ENS) sowie dieÉcole des hautes études en sciences sociales(EHESS). Die EliteverwaltungsschuleÉcole nationale d’administration(ENA) ist jedoch nach Straßburg ausgelagert worden.Édith Cressonsetzte als Premierministerin 1992 gegen erhebliche Widerstände die Verlegung durch. Über zehn Jahre hinweg lief der Betrieb der ENA zugleich in Paris und in Straßburg ab, bevor 2005 der Umzug der gesamten Schule dorthin abgeschlossen wurde, das ehemalige ENA-Gebäude in Paris wird nun vonSciences Po Parisgenutzt. Weitere höhere Bildungseinrichtungen sind das im Jahre 1530 eröffneteCollège de France, dasInstitut catholique(1875) und dieÉcole du Louvre(1882). Die 1257 gegründeteSorbonneist die älteste Universität in Frankreich und geht aufGründungen um 1200zurück. Die Gründung als Theologenschule wird aufRobert von Sorbon(1201–1274), den HofkaplanLudwigs des Heiligen, zurückgeführt; die BestätigungsbulleClemens’ IV.datiert von 1268. Ursprünglich ein Alumnat für arme Studenten derTheologie, gelangte die Sorbonne (welchen Namen die Anstalt erst seit dem 14. Jahrhundert erhielt) durch berühmte Lehrer, welche an ihr wirkten, sowie durch reiche Ausstattung gegenüber anderen ähnlichen Kollegien zu immer größerem Ansehen. Im Jahre 1968 wurde die Universität von Paris durch eine umfassende Reform in 13 unabhängige Teile aufgegliedert. Fünf von ihnen liegen außerhalb der Stadt. (Siehe:Liste der Universitäten in Frankreich) DieAcadémie françaiseist eine der ältesten Institutionen Frankreichs im Bereich des geistigen Lebens und zugleich die prestigereichste. Sie residiert seit 1801 imCollège des Quatre-Nationsgegenüber demLouvre; dort hat auch der auf Lebenszeit gewählte und wohlbeamteteSecrétaire perpétuelseine Dienstwohnung. DieAcadémie françaiseist hervorgegangen aus einem Pariser Literatenzirkel, der sich seit 1629 bei dem heute praktisch unbekannten AutorValentin Conrarttraf und 1634 durch den regierenden MinisterKardinal de Richelieuauf 34 Mitglieder aufgestockt und am 2. Januar 1635 durchLudwig XIII.zu einer staatlichen Institution erhoben wurde. Die von Richelieu vorgesehenen Statuten und Regelungen wurden 1637 vom Obersten Pariser Gerichtshof, demParlement de Paris, registriert und damit rechtskräftig. Seit dem Jahre 1803 gehört die Akademie dem Institut de France an. Von den zahlreichenBibliotheken in Parisist dieFranzösische Nationalbibliothek(Bibliothèque nationale de France)die größte. Sie wurde 1368 von KönigKarl V.auf Basis seiner persönlichen Bibliothek im Louvre gegründet und umfasste zu Beginn 911Manuskripte. Damals war es allerdings üblich, die Dokumente des Königs nach seinem Tod zu vernichten, so dass die eigentliche Bibliothekssammlung erst mit KönigLudwig XI.aufgebaut wurde, der mit diesem Brauch brach. Am 14. Juli 1988 kündigte der französische StaatspräsidentFrançois Mitterrandden Neubau des Bibliotheksgebäudes an, der im Dezember 1990 begann. Die neue Bibliothek wurde nach Plänen des ArchitektenDominique Perraultentworfen und am 20. Dezember 1996 der Öffentlichkeit übergeben. Die moderne Bibliothek enthält allePublikationen, die in Frankreich verlegt werden, und umfasst mehr als zehn Millionen Bände. Nach der Ernennung des Malers, Grafikers und BildhauersPablo Picassozum Ehrenbürger der Stadt Paris im Jahr 1971 wurden bis zum Jahr 2003 keine derartigen Ehrungen mehr vorgenommen. Seither wurden zu Ehrenbürgern ernannt: der US-amerikanische Journalist und schwarze PolitaktivistMumia Abu-Jamal(2003), die französisch-kolumbianische Kämpferin gegen Korruption und kolumbianische PräsidentschaftskandidatinÍngrid Betancourt(2003), die birmanische Politikerin und FriedensnobelpreisträgerinAung San Suu Kyi(2004), die nigerianische Rechtsanwältin und BürgerrechtlerinHauwa Ibrahim(2006). Darüber hinaus ernannte der Stadtrat im Jahr 2008 den chinesischen BürgerrechtlerHu Jia, denDalai Lama, die bangladeschische FrauenrechtlerinTaslima NasrinundGilad Shalitzu Ehrenbürgern, im Jahr 2010 die iranische MenschenrechtsaktivistinSchirin Ebadi, im Jahr 2011 den iranischen FilmregisseurJafar Panahiund den brasilianischen UmweltschutzaktivistenRaoni Metuktire.[73] →In Paris geborene Persönlichkeiten Paris war Geburtsort zahlreicher bekannter Persönlichkeiten. Dazu gehören unter anderen der französische Premierminister undStaatspräsidentJacques Chirac, der französische StaatspräsidentNicolas Sarkozy, der KomponistGeorges Bizet, die Schriftstellerin, Philosophin und FeministinSimone de Beauvoir, die FilmregisseureClaude Chabrol,Jean-Luc Godard,Roman PolańskiundFrançois Truffaut, der Pädagoge, Historiker und SportfunktionärPierre de Coubertin, der Chansonnier, Komponist und SchriftstellerSerge Gainsbourg, der Präfekt und StadtplanerGeorges-Eugène Haussmann, die Chemikerin und NobelpreisträgerinIrène Joliot-Curie, die MalerinAdélaïde Labille-Guiard, der MalerÉdouard Manet, die SchauspielerinSophie Marceau, der MalerClaude Monet, die ChansonsängerinÉdith Piaf, die SchriftstellerinGeorge Sandsowie die Sängerin und SchauspielerinCaterina Valente. →Bekannte Einwohner von Paris Zu den Persönlichkeiten, die in Paris gewirkt haben, gehören unter anderem die US-amerikanisch-französische Tänzerin, Sängerin und SchauspielerinJosephine Baker, der SchriftstellerHonoré de Balzac, der polnische KomponistFrédéric Chopin, die SchauspielerinMarlene Dietrich, der MetallbauingenieurGustave Eiffel, der deutsche SchriftstellerHeinrich Heine, der US-amerikanische Sänger und LyrikerJim Morrisonvon der RockgruppeThe Doors, der deutschstämmige französische KomponistJacques Offenbachund die irisch-englischen LiteratenOscar WildeundJames Joyce. Der spätere NobelpreisträgerErnest Hemingwaylebte von 1921 bis 1928 in Paris.[74]Seine Erlebnisse sind vor allem in dem BuchParis – Ein Fest fürs Lebenveröffentlicht. Auch viele bedeutende bildende Künstler wirkten in Paris, etwaMarc ChagallundPablo Picasso, der über 40 Jahre lang dort lebte. Seit den 1950er-Jahren war Paris ein Anziehungspunkt für afroamerikanische Jazzmusiker, die sich dort wesentlich freier bewegen konnten als in den damals noch von derRassensegregationbeherrschten Vereinigten Staaten:Sidney Bechetzog es nach Frankreich, „weil es näher an Afrika liegt“. Bei den Jazzfestivals 1948 in Nizza und Paris triumphierte der jungeMiles Davis, der an der SeineJuliette Grécokennen und lieben lernte. Paris beflügelte nicht nur ihn, sondern auchBud Powell,Idrees SuliemanoderBenny Waters. Dieser damaligen Szenerie US-amerikanischer Musiker in Paris ist der SpielfilmRound MidnightvonBertrand Taverniergewidmet. Regisseure wieLouis Malle(„Fahrstuhl zum Schafott“) undRoger Vadimexperimentierten in den 1950er-Jahren mit spontan zur Leinwand improvisierten Jazz-Soundtracks. Ende der 1960er emigrierten Musiker wieAnthony Braxton, dasArt Ensemble of ChicagooderFrank Wrightan die Seine. Brüssel(Belgien) |Sofia(Bulgarien) |Kopenhagen(Dänemark) |Berlin(Deutschland) |Tallinn(Estland) |Helsinki(Finnland) |Paris(Frankreich) |Athen(Griechenland) |Dublin(Irland) |Rom(Italien) |Zagreb(Kroatien) |Riga(Lettland) |Vilnius(Litauen) |Luxemburg(Luxemburg) |Valletta(Malta) |Amsterdam(Niederlande) |Wien(Österreich) |Warschau(Polen) |Lissabon(Portugal) |Bukarest(Rumänien) |Stockholm(Schweden) |Bratislava(Slowakei) |Ljubljana(Slowenien) |Madrid(Spanien) |Prag(Tschechien) |Budapest(Ungarn) |Nikosia(Zypern) 01Ain|02Aisne|03Allier|04Alpes-de-Haute-Provence|05Hautes-Alpes|06Alpes-Maritimes|07Ardèche|08Ardennes|09Ariège|10Aube|11Aude|12Aveyron|13Bouches-du-Rhône|14Calvados|15Cantal|16Charente|17Charente-Maritime|18Cher|19Corrèze|21Côte-d’Or|22Côtes-d’Armor|23Creuse|24Dordogne|25Doubs|26Drôme|27Eure|28Eure-et-Loir|29Finistère|30Gard|31Haute-Garonne|32Gers|33Gironde|34Hérault|35Ille-et-Vilaine|36Indre|37Indre-et-Loire|38Isère|39Jura|40Landes|41Loir-et-Cher|42Loire|43Haute-Loire|44Loire-Atlantique|45Loiret|46Lot|47Lot-et-Garonne|48Lozère|49Maine-et-Loire|50Manche|51Marne|52Haute-Marne|53Mayenne|54Meurthe-et-Moselle|55Meuse|56Morbihan|57Moselle|58Nièvre|59Nord|60Oise|61Orne|62Pas-de-Calais|63Puy-de-Dôme|64Pyrénées-Atlantiques|65Hautes-Pyrénées|66Pyrénées-Orientales|67Bas-Rhin|68Haut-Rhin|69DRhône|69MMétropole de Lyon(ab 2015) |70Haute-Saône|71Saône-et-Loire|72Sarthe|73Savoie|74Haute-Savoie|75Paris(ab 1968) |76Seine-Maritime|77Seine-et-Marne|78Yvelines(ab 1968) |79Deux-Sèvres|80Somme|81Tarn|82Tarn-et-Garonne|83Var|84Vaucluse|85Vendée|86Vienne|87Haute-Vienne|88Vosges|89Yonne|90Territoire de Belfort|91Essonne(ab 1968) |92Hauts-de-Seine(ab 1968) |93Seine-Saint-Denis(ab 1968) |94Val-de-Marne(ab 1968) |95Val-d’Oise(ab 1968) 971Guadeloupe|972Martinique|973Französisch-Guayana|974Réunion|976Mayotte 20Corse(bis 1975) |2ACorse-du-Sud(1976–2017) |2BHaute-Corse(1976–2017) |75Seine(bis 1967) |78Seine-et-Oise(bis 1967) 75Paris| 77Seine-et-Marne| 78Yvelines| 91Essonne| 92Hauts-de-Seine| 93Seine-Saint-Denis| 94Val-de-Marne| 95Val-d’Oise Kulturstädte:1985:Athen| 1986:Florenz| 1987:Amsterdam| 1988:West-Berlin| 1989:Paris| 1990:Glasgow| 1991:Dublin| 1992:Madrid| 1993:Antwerpen| 1994:Lissabon| 1995:Luxemburg| 1996:Kopenhagen| 1997:Thessaloniki| 1998:Stockholm Kulturhauptstädte:1999:Weimar| 2000:Avignon,Bergen,Bologna,Brüssel,Helsinki,Krakau,Prag,Reykjavík,Santiago de Compostela| 2001:Porto,Rotterdam| 2002:Brügge,Salamanca| 2003:Graz| 2004:Genua,Lille| 2005:Cork| 2006:Patras| 2007:Hermannstadt,LuxemburgundGroßregion| 2008:Liverpool,Stavanger| 2009:Linz,Vilnius| 2010:Istanbul,Pécs,Ruhrgebiet| 2011:Tallinn,Turku| 2012:Guimarães,Maribor| 2013:Košice,Marseille| 2014:Riga,Umeå| 2015:Mons,Pilsen| 2016:Breslau,Donostia / San Sebastián| 2017:Aarhus,Paphos| 2018:Leeuwarden,Valletta| 2019:Matera,Plowdiw| 2020–2021*:Galway,Rijeka| 2022:Esch an der Alzette,Kaunas,Novi Sad| 2023:Eleusis,Timișoara,Veszprém| 2024:Bad Ischl,Bodø,Tartu| 2025:Chemnitz,Nova Gorica/Gorizia| 2026:Oulu,Trenčín| 2027:Liepāja *Von derEuropäischen Kommissionwegen derCOVID-19-Pandemievorgeschlagene Verlängerung Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geografie 1.1Lage 1.2Klima 1.3Geologie 1.4Seine 1.5Inseln 1.6Hügel 1.7Stadtgliederung 2Geschichte 2.1Antike 2.2Mittelalter 2.3Neuzeit 2.3.1Paris – Stadt des Exils 2.3.2Paris im Zweiten Weltkrieg 2.3.3Paris nach dem Ende des Zweiten Weltkriegs 3Hoheitssymbole 4Gesellschaft 4.1Demografie 4.2Einwanderung 4.3Religionen 5Politik 5.1Stadtregierung 5.2Stadtrat(Conseil de Paris) 5.3Städtepartnerschaften 6Kultur und Sehenswürdigkeiten 6.1Theater 6.2Museen 6.3Bauwerke 6.3.1Brücken 6.3.2Plätze und Straßen 6.3.3Weltliche Bauwerke"
  },
  {
    "label": 0,
    "text": "Philosophie – Wikipedia Philosophie Inhaltsverzeichnis Einführung Begriffsgeschichte Wissenschaftsgeschichte Disziplinen Philosophiegeschichte aus westlicher Perspektive Lehr- und Forschungsbetrieb Siehe auch Literatur Weblinks Einzelnachweise Begriffsdefinition Sinn und Arten des Philosophierens Methoden Allgemein Abgrenzung theoretische und praktische Philosophie Theoretische Philosophie Praktische Philosophie Neuere Disziplinen Antike Mittelalter Frühe Neuzeit 19. Jahrhundert 20. Jahrhundert Gegenwart Einführungen Hilfsmittel/Nachschlagewerke Periodika Logik Erkenntnistheorie Wissenschaftstheorie Metaphysik und Ontologie Sprachphilosophie Ethik und Metaethik Rechtsphilosophie Politische Philosophie Philosophie des Geistes und des Bewusstseins Moderne philosophische Anthropologie Rationalitäts-, Handlungs- und Spieltheorie Philosophische Mystik In derPhilosophie(altgriechischφιλοσοφίαphilosophía,latinisiertphilosophia, wörtlich „Liebe zur Weisheit“) wird versucht, dieWeltund die menschlicheExistenzzu ergründen, zu deuten und zu verstehen. Von anderen Wissenschaftsdisziplinen unterscheidet sich die Philosophie dadurch, dass sie sich oft nicht auf ein spezielles Gebiet oder eine bestimmteMethodologiebegrenzt, sondern durch die Art ihrer Fragestellungen und ihre besondere Herangehensweise an ihre vielfältigen Gegenstandsbereiche charakterisiert ist. In diesem Artikel geht es um die westliche (auch: abendländische) Philosophie, die im 6. Jahrhundert v. Chr. imantiken Griechenlandentstand. Nicht behandelt werden hier die mit der abendländischen Philosophie in einem mannigfaltigen Zusammenhang stehenden Traditionen derjüdischenund derislamischen Philosophiesowie die ursprünglich von ihr unabhängigen Traditionen derafrikanischenund deröstlichen Philosophie. In derantiken Philosophieentfaltete sich das systematische und wissenschaftlich orientierteDenken. Im Laufe der Jahrhunderte differenzierten sich die unterschiedlichen Methoden und Disziplinen der Welterschließung und der Wissenschaften direkt oder mittelbar aus der Philosophie, zum Teil auch in Abgrenzung zu irrationalen oder religiösen Weltbildern oderMythen. Kerngebiete der Philosophie sind dieLogik(als die Wissenschaft des folgerichtigen Denkens), dieEthik(als die Wissenschaft des rechten Handelns) und dieMetaphysik(als die Wissenschaft der ersten Gründe des Seins und derWirklichkeit). Weitere Grunddisziplinen sind dieErkenntnistheorieundWissenschaftstheorie, die sich mit den Möglichkeiten des Erkenntnisgewinns im Allgemeinen bzw. speziell mit den Erkenntnisweisen der unterschiedlichen Einzelwissenschaften beschäftigen. Es gibt Probleme, die sich nicht oder nur unzureichend mit Hilfe der exakten Wissenschaften bearbeiten lassen: die Fragen etwa nach dem, was „gut“ und „böse“ ist, was „Gerechtigkeit“ bedeutet, ob es einen Gott gibt, ob der Mensch eine unsterbliche Seele besitzt oder was der „Sinn des Lebens“ ist. Eine weitere Klasse von Fragen kann ebenfalls nicht eigentlicher Gegenstand z. B. von Naturwissenschaften sein: In allen solchen Fällen versagen die Erklärungsmodelle der Einzelwissenschaften, es sindphilosophischeFragen. Der griechische PhilosophPlaton(428/27–348/47 v. u. Z.) hegte deshalb Zweifel an dem Bild, das der Mensch von sich selbst und von der Welt entwickelte. In seinem berühmtenHöhlengleichnis[1]reflektierte er unter anderem die begrenzte Wahrnehmungs- und Erkenntnisfähigkeit des gewöhnlichen Menschen. Dieser sitzt mit seinesgleichen nebeneinander aufgereiht in einer Höhle, alle in einer Weise gefesselt, dass sie nur starr geradeaus die Höhlenwand vor sich betrachten können. Licht gibt ein Feuer, das weit im Rücken der Menschen im entfernten Teil der Höhle brennt. Zwischen den Menschen und dem Feuer befindet sich – ebenfalls in ihrem Rücken – eine Mauer, hinter der verschiedene Gegenstände getragen und bewegt werden, welche die Mauer überragen und den auf ihre Höhlenwand fixierten Menschen als mobile Schatten erscheinen. Stimmen und Geräusche von dem Treiben hinter der Mauer würden den fixierten Beobachtern demzufolge ebenfalls als Hervorbringungen der Schatten vor ihren Augen gelten müssen. Mit diesem Szenario kontrastiert Platon die uns geläufige „wirkliche“ Welt im Sonnenlicht außerhalb der Höhle und macht durch diesen Kunstgriff begreiflich, warum Philosophen die Wahrheit, d. h. die Nähe zur Wirklichkeit menschlicher Wahrnehmung, infrage stellen. Die Philosophie behandelt zumeist Sachverhalte, die im Alltag zunächst einmal völlig selbstverständlich erscheinen: „Du sollst nicht töten“, „Demokratie ist die beste aller Staatsformen“, „Wahrheit ist, was nachprüfbar stimmt“, „Die Welt ist, was sich im Universum vorfindet“ oder „Die Gedanken sind frei“. Für manche Philosophen ist erst der Augenblick, in dem solche Überzeugungen, in dem das bisher fraglosHingenommene fragwürdigwird, der Geburtsmoment der Philosophie. Menschen, denen nichts fragwürdig erscheint, werden demnach nie Philosophie betreiben. Auch das kindlicheStaunenwird oft als Beginn philosophischen Denkens angeführt: „Das Staunen ist die Einstellung eines Mannes, der die Weisheit wahrhaft liebt, ja es gibt keinen anderen Anfang der Philosophie als diesen.“ „Staunen veranlasste zuerst – wie noch heute – die Menschen zum Philosophieren.“ Anders alsReligionen, religiöse Gemeinschaften und Weltanschauungen stützt sich die Philosophie bei der Bearbeitung der oben genannten „philosophischen“ Fragen allein auf die Vernunft, d. h. auf rationale Argumentation, die keine weiteren Voraussetzungen (wie z. B. den Glauben an eine bestimmte zugrundeliegende Lehre) erfordert. „Philosophie“ lässt sich nichtallgemeingültigdefinieren, weil jeder, der philosophiert, eine eigene Sicht der Dinge entwickelt. Daher gibt es annähernd so viele mögliche Antworten auf die oben gestellte Frage wie Philosophen.Carl Friedrich von Weizsäckerhat einmal formuliert: „Philosophie ist die Wissenschaft, über die man nicht reden kann, ohne sie selbst zu betreiben.“[2]Daneben hat der Begriff in einem weiteren, übertragenen Sinn auch Bedeutungen wieWeltanschauung,Unternehmenskulturetc.[3] Umso erstaunlicher ist die materialistische Fassung des Begriffes: „In ihrer wissenschaftlich begründeten Gestalt als dialektischer und historischer Materialismus ist Philosophie dieWissenschaftvon den allgemeinen Bewegungs- und Strukturgesetzen der Natur, der Gesellschaft und des Denkens (Erkennens) sowie der Stellung des Menschen in der Welt.“[4] Zu den philosophischen Arbeitsfeldern gehört zunächst die Untersuchung von Methoden, Prinzipien und der Gültigkeit jeglicherErkenntnisgewinnung wie auch derArgumenteundTheorienauf wissenschaftlicher Ebene. Philosophie kann in diesem Zusammenhang alsGrundlagenwissenschaftverstanden werden. Denn philosophisches Nachdenken und Infragestellen haben die Einzelwissenschaften stets befruchtet und in ihrer Entwicklung gefördert. Die Philosophie stellt Fragen von einer Art, die Spezialwissenschaften (bisher) nicht beantworten können, die durch Versuche, Berechnungen oder andere Forschungen mit den bisherigen Instrumenten nicht zu beantworten sind. Derartige Problemstellungen können aber das Forschen in eine neue Richtung lenken. So werden mitunter neuartigeForschungsfragenin den einzelnen Wissenschaften auf den Weg gebracht; Philosophie leistet folglich über das ureigene Feld hinaus einen Beitrag zurHypothesenbildung. Weitergehende philosophische Bemühungen erstrecken sich auf eine systematische Ordnung menschlichenWissenszwecks Herstellung eines in sich schlüssigenWeltbildsunter Einbeziehung menschlicherWerte,RechteundPflichten. Viele Menschen betreiben Philosophie um ihrer selbst willen: um sich selbst und die Welt, in der sie leben, besser zu verstehen; um ihr Handeln, ihr Weltbild auf einegut begründeteBasis zu stellen. Wer ernsthaft philosophiert, stellt kritische Fragen an die ihn umgebende Welt sowie an sich selbst, lässt sich im Idealfall nicht so leicht täuschen oder von anderen seelisch-geistig manipulieren, übt sich inWahrhaftigkeitund begeht nicht so leichtFehlschlüsse. Ein kritisches Potenzial der Philosophie liegt im Hinterfragen der gesellschaftlichen Verhältnisse ebenso wie in einer Relativierung der Ansprüche von Wissenschaften und Religionen. Hierbei beschränkt sich die Philosophie nicht auf die kritische Analyse, sondern sie liefert auch konstruktive Beiträge, beispielsweise durch dierationale Rekonstruktionund Präzisierung vorhandener Wissenssysteme oder die Formulierung von Ethiken. Ein selbstbestimmtes und vernunftbasiertes Leben auf der Grundlage eigenen Nachdenkens (sapere aude!) ist das Ziel vieler Philosophierender. Bei dem aufindividuellenNutzen gerichteten Philosophieren sind vor allem zwei Arten oder Ausrichtungen zu unterscheiden: Sehr ausgeprägte Anwendungsformen einer philosophisch bestimmten Lebensweise hat es insbesondere in derAntikegegeben, vor allem in den Reihen derStoiker, derEpikureerund derKyniker. Für das Ideal der Übereinstimmung von Denken und Tun hat der KynikerDiogenes von Sinopedurch seine von radikaler Enthaltsamkeit gekennzeichnete Lebensweise Anhängern wie Gegnern dieser Art philosophischer Ausrichtung ein oft zitiertes Beispiel gegeben. Die Einheit von Theorie und Praxis wird jedoch auch in der östlichen Philosophie betont. Diogenes, der seinem philosophischen Denken Ausdruck verlieh, indem er dem weltlichen Treiben entsagte, zeugt auch davon, dass zum Philosophieren Ruhe undMußegehören. (Noch das WortSchulegeht auf das griechische Wort in der alten Bedeutung für „Muße“ [σχολή,scholḗ] zurück.) Ein großer Gewinn des Philosophierens besteht in der Schulung des Denkens und des Argumentierens, denn sowohl in methodischer Hinsicht als auch beim sprachlichen Ausdruck werden im fachlichenDiskursstrenge Anforderungen an die Philosophierenden gestellt. DasakademischePhilosophieren unterscheidet sich vomalltäglichenPhilosophieren nicht prinzipiell durch die Fragen, sondern eher durch den Rahmen – in der Regel dieUniversität– und durch bestimmte Formen der Aus- und Abgrenzung philosophischer Tätigkeit. Es gelten verschiedene Übereinkünfte über die Formen des Argumentierens und der wissenschaftlichenPublikationsowie die zugelasseneFachterminologie. Die Tätigkeiten des akademisch Philosophierenden umfassen dabei dieunten genannten Methoden. Philosophisch gebildete Menschen unterscheiden sich von den übrigen nicht unbedingt darin, dass ihnen mehr (nützliches) Wissen zur Verfügung stünde. Ihnen steht allerdings in der Regel ein besserer Überblick über die Argumente zur Verfügung, die in einer philosophischen Debatte hinsichtlich eines bestimmten Diskussionsgegenstands bereits vorgebracht wurden. So kann es etwa hilfreich sein, bei einem aktuell diskutierten Problem (z. B.Euthanasie) danach zu fragen, welche Antwortmöglichkeiten die Philosophie in den letzten 2500 Jahren dazu angeboten hat und wie die Auseinandersetzungen um diese Vorschläge bisher verlaufen sind. Neben dieser historischen Kenntnis sollte ein ausgebildeter Philosoph eher in der Lage sein, die prinzipiell vertretbaren Positionen zu unterscheiden, deren Folgen vorauszusehen sowie Probleme und Widersprüche zu erkennen. Weitere Anwendungen und Aufgaben der Philosophie bestehen darin, Die Methoden der Philosophie umfassen verschiedene geistige Bemühungen. „Geistige Bemühungen“ kann dabei das Nachspüren von Denkrichtungen, Denktraditionen und Denkschulen meinen. Um dasDenkengeht es beim Philosophieren immer. Denken kannNach-Denkensein,AnalysierenoderSystematisieren.IntuitiveErkenntnisse,Glaubenswahrheiten und rationale Argumente werden auf der Grundlage der Lebenswirklichkeit des philosophierenden Menschen mithilfe der Mittel desvernünftigen,rationalenundkritischenDenkens geprüft. Zudem vermag die philosophische Geisteshaltung in einemmethodischen Zweifelradikal alles infrage zu stellen – sogar die Philosophie selbst. Dabei beginnt die Philosophie mit jedem Philosophierenden gleichsam wieder bei null. Es gehört zur Haltung eines Philosophierenden, auch scheinbar grundlegende oder alltägliche Gewissheiten infrage stellen zu können. Menschen, denen sich die Lebenswirklichkeit nicht auch als Frage oder Problem aufdrängt, erscheint solch fundamentaler Zweifel nicht selten befremdlich. Über lange Zeiträume gesehen stellt die Philosophie in zentralen Bereichen immer wieder dieselben Grundfragen, deren Antwortmöglichkeiten sich prinzipiell ähneln (Philosophia perennis). Aufgrund der historischen und sozialen Veränderungen der Lebensumstände undWeltanschauungenwerden jeweils neue Formulierungen für die Antworten auf dieGrundfragendes Menschen notwendig. Anders als in den einzelnen Wissenschaften häufen weder die Philosophie noch die einzelnen Philosophierenden Wissen an oder verfügen über definitive und allgemein anerkannte Ergebnisse („Skandal der Philosophie“). Sie sammeln historische Antworten, reflektieren diese und können dadurch zeitgebundene Blickwinkelverengungen, wie sie in manchen Spezialwissenschaften anzutreffen sind, vermeiden. Insofern kann der philosophische Diskurs als ein in sich nicht abschließbarer Prozess betrachtet werden, als ein kontroverses Gespräch über die Jahrhunderte hinweg. Grundsätzlich lassen sich zwei Ansätze bzw. Bereiche des heutigen „professionellen“ Philosophierens unterscheiden: diehistorischeund diesystematischeVorgehensweise: Die historischen und die systematischen Herangehensweisen bzw. Bereiche sind dabei prinzipiell durch das jeweilige Ziel der philosophischen Untersuchungen voneinander abgrenzbar. Viele Philosophen arbeiten allerdings sowohl historisch als auch systematisch. Beide Ansätze ergänzen einander insofern, als einerseits die Schriften herausragenderphilosophischer Autorenauch für aktuelle systematische Fragen hilfreiche Überlegungen enthalten und andererseits systematische Ausarbeitungen oft Positionen der Klassiker präzisieren helfen. Außerdem können aktuelle Fragen in vielen Fällen nur dann präzise gestellt und beantwortet werden, wenn der historische Hintergrund für ihr Aufkommen und die seitdem für die Behandlung des Problems entwickelten Begrifflichkeiten und Lösungsvorschläge bekannt sind und verstanden werden. Der Begriff „Philosophie“ (bis ins 19. Jahrhundert im Deutschen auch gelegentlichFilosofiegeschrieben[5]), zusammengesetzt aus griechischφίλος(phílos) „Freund“ undσοφία(sophía) „Weisheit“, bedeutet wörtlich „LiebezurWeisheit“ bzw. einfach „zumWissen“ – dennsophíabezeichnete ursprünglich jede Fertigkeit oder Sachkunde, auch handwerkliche und technische. Das Verbphilosophierentaucht erstmals beim griechischen HistorikerHerodot(484–425 v. Chr.) auf (I,30,2), wo es zur Beschreibung des Wissensdurstes des Athener StaatsmannesSolon(ca. 640–559 v. Chr.) dient. DassHeraklitschon den Begriffphilósophosverwendete,[6]ist nicht anzunehmen. In der Antike pflegte man die Einführung des BegriffsPhilosophiePythagoras von Samoszuzuschreiben. Der PlatonikerHerakleides Pontikosüberlieferte eine Erzählung, wonach Pythagoras gesagt haben soll, nur ein Gott besitze wahresophía, der Mensch könne nur nach ihr streben. Hier ist mitsophiabereitsmetaphysischesWissen gemeint. Die Glaubwürdigkeit dieses – nur indirekt und fragmentarisch überlieferten – Berichts des Herakleides ist in der Forschung umstritten. Erst bei Platon tauchen die BegriffePhilosophundphilosophiereneindeutig in diesem von Herakleides gemeinten Sinne auf, insbesondere in PlatonsDialogPhaidros,[7]wo festgestellt wird, dass dasStrebennach Weisheit (das Philosophieren) undBesitzder Weisheit sich ausschließen und letzterer nurGottzukomme. Philosophie wurde im Laufe ihrer Geschichte als Streben nach dem Guten, Wahren und Schönen (Platon) oder nach Weisheit, Wahrheit und Erkenntnis (Hobbes,Locke,Berkeley) definiert. Sie forsche nach den obersten Prinzipien (Aristoteles) und ziele auf den Erwerb wahren Wissens (Platon). Sie ringe um die Erkenntnis aller Dinge, auch der unsichtbaren (Paracelsus), sei Wissenschaft aller Möglichkeit (Wolff) und vom Absoluten (Fichte,Schelling,Hegel). Sie ordne und verbinde alle Wissenschaft (Kant,Mach,Wundt), stelle die „Wissenschaft aller Wissenschaften“ dar (Fechner). Die Analyse, Bearbeitung und exakte Bestimmung von Begriffen stehe in ihrem Mittelpunkt (Sokrates, Kant,Herbart). Philosophie sei jedoch zugleich auch die Kunst, sterben zu lernen (Platon), seinormativeWertlehre (Windelband), das vernunftgemäße Streben nach Glückseligkeit (Epikur,Shaftesbury) bzw. das Streben nach Tugend und Tüchtigkeit (Aristoteles,Stoa). Aus europäischer Sicht verbindet sich der Begriff Philosophie mit den Ursprüngen im antiken Griechenland. Die gleichfalls jahrtausendealten asiatischen Denktraditionen (östliche Philosophie) werden oftmals übersehen oder unterschätzt. Auch religiöseWeltanschauungengehören zur Philosophie, insoweit ihre Vertreter nichttheologisch, sondern philosophisch argumentieren. Das Selbstverständnis der Philosophie als Wissenschaft hat sich im Laufe ihrer Geschichte immer wieder gewandelt. Die ersten griechischen Philosophen bis etwa zur Zeit von Sokrates und Platon verstanden ihre Tätigkeit als vernunftgelenktes Erkenntnisstreben im Unterschied zum bloßen Übernehmen einesmythischenWeltbilds und religiöser Traditionen. Einerseits emanzipierte sich so das Denken vom Mythos, andererseits wurden die Mythen in der Regel nicht grundsätzlich verworfen. Die Philosophen bedienten sich ihrer gern und nutzten dichterische Ausdrucksmittel, um ihre Lehren zu verbreiten. Während Sokrates und seine Schüler das Erkenntnisstreben als Selbstzweck betrachteten, boten dieSophistenihren Unterricht gegen Entgelt an. Für manche Sophisten ging es dabei vor allem um die Kunst, in einer Debatte mitrhetorischen Mittelnund logischen Kunstgriffen einen Gegner zu besiegen. Ihr Ziel war es, notfalls auch mit Tricks (Sophismen), „die schwächere Seite zur stärkeren zu machen“ (vgl.Eristik). Nachdem sich dasChristentumin derSpätantikedurchgesetzt hatte, war Philosophie für viele Jahrhunderte nur noch auf der Basis des damaligen religiösen Weltbilds möglich; sie durfte nicht mit den Grundannahmen derchristlichen Theologiein Konflikt geraten. Eine analoge Begrenzung bestand auch im Islam und im Judentum. In Westeuropa dominierte daher lange Zeit das Bild der Philosophie als einer „Magd derTheologie“ (ancilla theologiae), also einer Hilfswissenschaft, welche die göttlichen Offenbarungen mit rationalen Argumenten stützen sollte. An den im Mittelalter neu entstehendenUniversitätenwurde die Philosophie zu einem grundlegenden („propädeutischen“) Lehrfach. Der Kern des Studiums war durch die sogenanntenArtes liberalesbestimmt, zu denen „Grammatik“, „Dialektik“, „Rhetorik“ sowie „Geometrie“, „Arithmetik“, „Astronomie“ und „Musik“ gehörten. Ein erster Abschluss in diesemstudium generalean der so genannten Artistenfakultät war notwendig, um die „höheren“ Studien in Medizin, Recht und Theologie aufnehmen zu können. (Aus dieser Tradition stammen noch heute die Bezeichnungen der akademischen Grade desB.A.,M.A.,Ph.D.bzw.Dr. phil.) In Westeuropa führte im 13. Jahrhundert die verstärkte Auseinandersetzung mit der Philosophie des Aristoteles zu höherer Eigenständigkeit der Philosophie, welche die Grenzen derartes-Disziplinen überschritt. Zahlreiche Philosophen und Theologen wieAlbert der GroßeundThomas von Aquinversuchten, Anschluss an die Aristotelesrezeption des Ostens zu halten und die aristotelische Philosophie mit den Lehren der katholischen Kirche zu einer in sich geschlossenen Gesamtdeutung der Wirklichkeit zusammenzuführen. Eine solche Synthese legte etwa Thomas in derSumma theologicavor. Unabhängig davon kam es schon seit dem 12. Jahrhundert zu einer neuen Hochschätzung des Erfahrungswissens, die eine Voraussetzung für die Entstehung des neuzeitlichen naturwissenschaftlichen Denkens und der experimentellen Vorgehensweise bildete. Seit derRenaissanceüberschritt die Philosophie zunehmend die Grenzen, welche die Theologie ihr gesetzt hatte. Die Philosophen scheuten sich nicht mehr, Ansichten zu vertreten, die mit kirchlichen Lehren oder sogar mit dem Christentum unvereinbar waren. Seit den Zeiten desRenaissance-Humanismusund derAufklärungsetzte sich die Philosophie bis in die Gegenwart hinein kritisch mit der Religion auseinander, grenzte sich von ihr ab und betrachtete sich ihr oft als überlegen. Es gab aber auch stets zahlreiche Philosophen, die großen Wert darauf legten, dass ihre Positionen mit ihren religiösen Überzeugungen in vollem Einklang stehen. Vor allem in bestimmten Phasen der Neuzeit wurde die Philosophie als eine allenEinzelwissenschaftenübergeordneteUniversalwissenschaftbegriffen, die, um dieWirklichkeitals Ganzes zu erfassen und zu den letzten Ursachen und Prinzipien vorzudringen, ewig gültige, allgemeineWahrheitenaufdeckt und zugänglich macht (Philosophia perennis). Das heißt,die Chance, dass Philosophie untergeht, ist von allen Fächern wohl am geringsten. Wenn man nur Philosophie betreibt, braucht man sich auf nichts weiter spezialisieren, denn Philosophie ist dasjenige Fach, das alle Grundlagen benutzen kann(Heißler).[8] Noch bis ins 18. Jahrhundert hinein blieb die Philosophie eine der klassischen vierFakultäten. Weiterhin war eine grundlegende Ausbildung in Philosophie erforderlich, bevor sich die Studenten z. B. naturwissenschaftlichen Fragen und Forschungen zuwenden durften. An einigen traditionsbewussten Universitäten ist ein „Philosophicum“ im Grundstudium bis heute für alle Studenten Pflicht. Im 19. Jahrhundert begann eine zunehmende Verselbstständigung zunächst der Naturwissenschaften und später auch derphilologischenund dergesellschaftswissenschaftlichenFächer. Die philosophischenLehrstühlegerieten in der Folge in ihrer inhaltlichen Ausrichtung zunehmend unter den Spezialisierungsdruck der sich verselbständigenden Fachwissenschaften. In der Moderne verblieb der Philosophie zeitweise nur die Aufgabe der Reflexion der Fachwissenschaften und die Diskussion über deren Voraussetzungen. Die moderne Fachwissenschaft Philosophie zieht ihre Rechtfertigung aus dem Anspruch, philosophische Methoden könnten auch für andere Wissens- und Praxisgebiete hilfreich sein. Darüber hinaus betrachten die Philosophen die Erörterung ethischer Themen und Grundsatzfragen als ihr ureigenes Gebiet. Die Universitäten sind in ihrem Selbstverständnis gegenwärtig durch die Vermittlung der traditionellen philosophischen Disziplinen Logik, Ethik, Erkenntnistheorie, Wissenschaftstheorie und Philosophiegeschichte im Rahmen der Lehrerausbildung geprägt. So findet der Diskurs der Philosophie an den Universitäten häufig abgetrennt nicht nur von der Religion, sondern auch von den Sozialwissenschaften, von Literatur und Kunst weitgehend als theoretische Philosophie mit einer starken Betonung von Wissenschaftstheorie, Sprachanalyse und Logik statt. Dennoch gibt es auch in der „Fachwissenschaft Philosophie“ immer wieder Impulse, an öffentlichen Debatten der Gegenwart teilzunehmen und Stellung zu beziehen, z. B. zu ethischen Fragen der Verwendung von Technik, zur Ökologie, zur Genetik, (seit der Antike auch) zu medizinischen Problemen[9](Medizinphilosophie,Medizinethik) oder zu solchen der interkulturellen Philosophie. Neben der universitären Philosophie gab es jedoch auch immer eigenständige Denker außerhalb der Institutionen. Seitdem dieAufklärerVoltaire,RousseauundDiderot(als Impulsgeber derEnzyklopädiemit dem Ziel der Aufklärung durch Wissen) in Frankreichphilosophesgenannt wurden, verstand man darunter in der Tradition von Montaigne allgemein auch gelehrte Schriftsteller, die sich über populäre, also über Themen von allgemeinem öffentlichen Interesse äußerten; so auch Universalgelehrte wieGoetheundSchiller. Denkern des 18. und 19. Jahrhunderts wieAdam Smith,Abraham Lincoln,Jean Paul,Friedrich Nietzsche,Émile Zola,Lew Tolstoi,Karl Marx,Sigmund FreudoderSøren Kierkegaardwar gemeinsam, dass sie allesamt nicht an eine Universität angebunden waren und keine akademische Schulphilosophie betrieben. Dennoch gingen von ihnen in der Öffentlichkeit viel beachtete philosophische Impulse aus und sie reflektierten die Philosophiegeschichte eigenständig – vergleichbar mit in der Gegenwart viel gelesenen Denkern wiePaul Watzlawick,Umberto EcooderPeter Sloterdijk. Eine recht junge Entwicklung ist die Einrichtung vonPhilosophischen Praxen, die eine Alternative zu anderen gesellschaftlichen Beratungs- und Orientierungsmöglichkeiten anbieten wollen. Im Mai 1988 kam es im Zuge derPerestroikazu einer Wiederbelebung der philosophischen und wissenschaftlichen Tradition. Es wurde eine Bibliothek mit etwa vierzig Bänden, darunter Werke von Denkern des neunzehnten Jahrhunderts, die in der Sowjetunion nicht mehr publiziert worden waren, und Texte von Intellektuellen, die das Land auf demPhilosophenschiffhatten verlassen müssen, zusammengestellt.[10] Die heutige Philosophie gliedert sich in systematische Sachdisziplinen und diePhilosophiegeschichte. Erstere lassen sich im Wesentlichen der theoretischen oder praktischen Richtung zuordnen (s. u.). Berührungspunkte zwischen systematischem Philosophieren und Philosophiegeschichte finden sich etwa in derSystematologie.Systematische Philosophieim strengen Sinne erhebt den Anspruch, „die Totalität der in irgend einem Zeitpunkt erreichten Erkenntnisse als ein Ganzes darzustellen, dessen Teile durchgängig in logischen Verhältnissen verknüpft sind“.[11] Auch wenn sich der Bereich, den die Philosophie insgesamt umfasst, in gewissem Sinne nicht eingrenzen lässt (da sie „alles“ behandelt), gibt es doch bestimmteDomänen, in denen sie hauptsächlich tätig ist. Der PhilosophImmanuel Kanthat diese in den folgenden Fragen zusammengefasst:[12] Etwas weniger allgemein gestellt können diese Fragen ungefähr so lauten: Die Unterscheidung zwischenpraktischerundtheoretischer Philosophiegeht aufAristoteleszurück. Für ihn richtete sich die theoretische Philosophie auf zweckfreie Erkenntnis notwendiger Gründe, die praktische Philosophie dagegen auf das optionale, zweckgebundene praktische und politischeHandelndes Menschen. Ab dem 17. Jahrhundert wurde diese Unterscheidung wieder aufgegriffen und – vor allem in der Schulphilosophie desChristian Wolff– terminologisch fixiert. Vor dem Hintergrund der Forderung nach Wissenschaftlichkeit verkehrte sich jedoch der Sinn dieser Unterscheidung: Theoretische und praktische Philosophie sollten beide gleichermaßen wissenschaftlich werden. Nach einer vielfach aufgenommenen UnterscheidungImmanuel Kantshandelt die praktische Philosophie von dem, wassein soll, während die theoretische Philosophie sich mit dem beschäftigt, wasist. Einige interdisziplinäre Gebiete derPhilosophie der Gegenwartwidersetzen sich teilweise dieser Zweiteilung, siehe etwa die Kritik vonJürgen HabermasanEdmund Husserlund die Kontroverse derWerturteilsfreiheit.[14] Klassischerweise werden der theoretischen Philosophie Logik, Metaphysik und Ontologie, Erkenntnis- und Wissenschaftstheorie, aber auchmathematischeundNaturphilosophiezugerechnet. Vor allem die ersten drei beanspruchen Priorität als oberste philosophische Grundlagendisziplin. Zur praktischen Philosophie werden Ethik, Rechtsphilosophie, politische Philosophie,Handlungstheorie,WirtschaftsphilosophieundSozialphilosophiegezählt. Die Logik beschäftigt sich nicht mit konkreten Inhalten, sondern mit den Gesetzmäßigkeiten der Folgerichtigkeit. Sie fragt, auf Grundlage welcher Regeln aus bestimmtenVoraussetzungen(„Prämissen“) bestimmteSchlussfolgerungen(„Konklusionen“) gezogen oder nicht gezogen werden können (vgl.Fehlschlüsse). Insofern thematisiert sie die Grundlage aller aufArgumentenbasierenden Arten von Wissenschaft. In früheren Zeiten wurde der Ausdruck „Logik“ in weiterer Bedeutung verwendet als heute. Typisch ist das Beispiel der Logik derStoa. Diese umfasste auch den Bereich, der heute Erkenntnistheorie genannt wird, sprachphilosophische Probleme sowie dieRhetorik. Ganz ähnlich gilt dies noch für viele Logikbücher bis ins frühe 20. Jahrhundert. In der modernen Philosophie bezeichnet Logik als Wissenschaft des korrekten Folgerns nur noch dieformale Logik. Diese überschneidet sich mit Gebieten ausMathematikundInformatik. DieLogizistenmeinen sogar, die gesamte Mathematik sei, abgesehen von Axiomfindung, nurlogisches Ableitenbzw.Folgern. Inwieweit sich Logik auch auf andere Gebiete ausdehnt (z. B.Argumentationstheorie,Sprechakttheorie), ist hingegen umstritten. Zu den wichtigstenLogikernder Philosophiegeschichte zählenAristoteles,Chrysipp,Johannes Buridanus,Gottlob Frege,Charles Sanders Peirce,Bertrand RussellmitAlfred N. Whitehead,Kurt GödelundAlfred Tarski. Die Erkenntnistheorie fragt nach der Möglichkeit, Wissen zu erlangen und zu sichern.Umfang des Wissens,Natur des Wissens,Arten des Wissens,Quellen des WissensundStruktur des Wissenswerden untersucht, ebenso die Problematik derWahrheitoderFalschheitvonTheorien. DieWahrnehmungderWirklichkeitstellt sie genauso auf den Prüfstand wie den Einfluss von Sprache und Denken auf den Erkenntnisvorgang. Außerdem versucht sie, die Grenzen derErkenntnisabzustecken und zu definieren, was prinzipiell als „wissenschaftlich“ bezeichnet werden kann. DieseErkenntniskritikstellt seit Immanuel Kant für viele Philosophen den fundamentalen Kern der Erkenntnistheorie dar. WichtigeErkenntnistheoretikerwaren u. a.Platon,Aristoteles,René Descartes,John Locke,David Hume,Immanuel Kant,Auguste Comte,Edmund HusserlundLudwig Wittgenstein. Die Wissenschaftstheorie ist eng verbunden mit der Erkenntnistheorie und analysiert bzw. postuliert die Voraussetzungen, Methoden und Ziele vonWissenschaft. Sie legt vor allem die Kriterien für die Begriffe „Wissenschaft“ und „wissenschaftlich“ fest und versucht sie damit vonPara-undPseudowissenschaftenabzugrenzen. Dazu haben sich heute mehrere grundlegende, nicht durch die Einzelwissenschaften selbst zu rechtfertigende methodische Vorgaben herausgebildet. Beispielsweise sind so die Notwendigkeit der Wiederholbarkeit vonExperimenten, das Ökonomieprinzip („Ockhams Rasiermesser“) und das Prinzip derFalsifizierbarkeitals Voraussetzung für sinnvolle wissenschaftliche Aussagen Bestandteile dieser Wissenschaftsmodelle. Weiterhin beschäftigt sich die Wissenschaftstheorie mit dem Verhältnis zwischen wissenschaftlichen Erkenntnissen und den Konzepten von Wahrheit bzw. Wirklichkeit. Auch die mögliche Einteilung und Ordnung des menschlichen Wissens in Gebiete undihre Hierarchisierungsowie die Untersuchungen der Prinzipien des wissenschaftlichen Fortschreitens (vgl.Paradigmenwechsel) gehören zu ihrem Aufgabenbereich. WichtigeVertreter der Wissenschaftstheoriesind z. B. Aristoteles,Francis Bacon,Rudolf Carnap,Karl Popper,Thomas Kuhn,Paul FeyerabendundHilary Putnam. Die Metaphysik bildet fast seit jeher den Kern der Philosophie. Sie versucht, die gesamte Wirklichkeit, wie sie uns erscheint, in einen sinnvollen Zusammenhang – oft auch in ein universelles System – zu bringen. Sie untersucht die Fundamente und allgemeinen Strukturen der Welt. Des Weiteren stellt sie die „letzten Fragen“ nach dem Sinn undZweckallenSeins. Traditionell wird die Metaphysik in einen generellen und einen speziellen Zweig geteilt. Die generelle Metaphysik ist dieOntologie, welche in der Tradition des Aristoteles die Frage nach den Grundstrukturen alles Seienden und dem Sein stellt. Ihr Gegenstandsbereich ist uneingeschränkt. Philosophiegeschichtlich ist die Metaphysik vor allem durch drei Grundfragen geprägt: Die spezielle Metaphysik teilt sich in drei Disziplinen auf, die folgende Fragen stellen: Diese Fragen können und wollen dieNaturwissenschaftenmit ihrem Instrumentarium aus prinzipiellen Gründen nicht mehr behandeln, da die Gegenstände der Metaphysik prinzipiell jeder (sinnlichen) menschlichen Erfahrungsmöglichkeit entzogen sind. Wird die Existenzempirischnicht untersuchbarer Bereiche der Wirklichkeit bestritten oder für nicht relevant erklärt, so erübrigen sich die Fragen der Metaphysik. Die traditionelle Metaphysik wurde auf zwei verschiedene Weisen kritisiert. Während derPositivismusund Vertreter analytischer Philosophie in der ersten Hälfte des 20. Jahrhunderts tendenziell auf eine Abschaffung der Metaphysik durch logische Analyse der Sprache drängten, versuchte beispielsweiseMartin Heidegger, in einer Überwindung der Metaphysikgeschichte und in einer radikalen Wende der Fragestellung auf die Analyse des menschlichenDaseinseinen Neuansatz für eine alternative Metaphysik zu schaffen (Fundamentalontologie,Existenzphilosophie). Mittlerweile finden traditionelle metaphysische, insbesondere ontologische Fragen und Probleme wieder breitere Beachtung in der philosophischen Diskussion, auch in viel debattierten Disziplinen wie derPhilosophie des Geistes. Wichtige Metaphysiker waren u. a.Platon,Aristoteles,Thomas von Aquin,René Descartes,Gottfried Wilhelm Leibnizsowie die Vertreter desDeutschen Idealismusund derNeuscholastik. Die Sprachphilosophie untersucht die Beziehung zwischenSprache, Denken und Wirklichkeit.[17]Die Analyse von Sprache, z. B. mittels der genauen Zerlegung vonBegriffen, ist in der Philosophie von jeher betrieben worden. Von Anfang an war damit die überragende Bedeutung der Sprache für kommunikative Prozesse, Wahrheitsfindung, Erkenntnismöglichkeiten und die Beschreibung und Wahrnehmung der Welt ein zentrales Thema der Philosophie. So wurde beispielsweise bereits in der Antike die Frage erörtert, ob einem Ding eine bestimmte Bezeichnung „von Natur aus“ oder nur durch willkürliche Festlegung durch den Menschen zukomme. Auch das sich hieran anschließende wichtige Thema der mittelalterlichen Philosophie – derUniversalienstreit– kann teilweise als ein Problem dieses Bereichs begriffen werden. Die moderne Sprachphilosophie, welche im 20. Jahrhundert die so genannte „Linguistische Wende“ (linguistic turn) auslöste, befasst sich u. a. mit der Abhängigkeit der Wirklichkeitserfassung von den individuellen sprachlichen Möglichkeiten (vgl.Sapir-Whorf-Hypothese), mit der Herstellung von Wahrheit, Erkenntnis und Wissen durch Kommunikation (vgl.Sprachspiel), wie man mit Hilfe sprachlicher Äußerungen Handlungen vollzieht (John Langshaw Austin: „How to do things with words“, vgl.Pragmatik), dem verzerrenden Einfluss der Sprache auf die Realität (z. B. in derfeministischen Linguistik) sowie mit der Frage, was „Bedeutung“ ist. Zu den wichtigstenSprachphilosophenzählenGottlob Frege,Charles S. Peirce,George Edward Moore,Bertrand Russell,W.v.O. Quine,Saul KripkeundLudwig Wittgenstein. Wichtige Beiträge haben auch die SchülerFerdinand de Saussures(Strukturalismus), Martin Heidegger (EtymologieundNeologismen),Michel Foucault(Diskursanalyse) undJacques Derrida(Poststrukturalismus) geliefert. Praktische Philosophie bezeichnet gemäß deraristotelischenTradition denjenigen Teilbereich der Philosophie, der sich aus den DisziplinenEthik,Rechtsphilosophie,Staatsphilosophie,Politische Philosophieund den Grundlagen derÖkonomie(siehe auchWirtschaftsphilosophie) zusammensetzt. Praktische Philosophie ist auf die philosophische Erforschung der menschlichenPraxisgerichtet. Aristoteleshatte dertheoretischen Philosophie, die sich auf zweckfreie Erkenntnis notwendiger Gründe richtet, die praktische Philosophie (Ethik, Ökonomie und Politik) gegenübergestellt, die sich auf das zweckgebundene praktische und politischeHandelndes Menschen im Bereich dessen bezieht, was sich auch anders verhalten kann. Vor dem Hintergrund der Forderung nach Wissenschaftlichkeit relativierte sich jedoch der Sinn dieser Unterscheidung: Theoretische und praktische Philosophie sollten beide gleichermaßen wissenschaftlich werden. Mitte des 19. Jahrhunderts begannen sich die einzelnen Teildisziplinen der praktischen Philosophie zu spezialisieren und allmählich als Einzeldisziplinen herauszubilden. Die philosophische Ethik befasst sich mit Antworten auf dieKantscheFrage „Was sollen wir tun?“. Sie erstellt Kriterien für die Beurteilung vonHandlungenund bewertet diese hinsichtlich ihrerMotiveundKonsequenzen. Dabei unterscheidet sie sich von derMoral, die bestimmte Handlungen traditionell oder konventionell vorschreibt, obgleich das Ziel dernormativen Ethikin der Begründung von allgemeingültigen Normen und Werten gesehen werden kann. Dieses Ziel gilt vielen Philosophen als gescheitert, da es gemäß derdeontischen Logikals auch aufgrund vonHumes Gesetzunmöglich ist, Normen aus nichtnormativen Sätzen zu deduzieren, d. h. bestimmte Werte, Normen oder Präferenzen müssen immer schon vorausgesetzt werden, damit weitere Normen abgeleitet werden können. Rationale Ethik bestünde daher nur in der Prüfung, ob bestimmte Normen mit übergeordneten Zielen logisch vereinbar sind oder nicht. Bei einer voraussetzungslosen Philosophie hingegen wären ethische Maßstäbe für grundsätzliche Zweckorientierungen logisch nicht zu gewinnen. Andere Philosophen versuchen trotzdem, in verschiedenen, einander widersprechenden Konzepten, eine absolute Begründung von Normen zu finden. Am bekanntesten in Deutschland ist dietranszendentalpragmatische, absolute Normenbegründung derDiskursethiknach Apel, der zufolge jeder Zweifler bereits Teilnehmer an einem Diskurs ist und daher ethische Diskursregeln anerkannt habe. Praktische Philosophen versuchen auch oft, eine oberste Regel oder ein allgemeines Kriterium für moralisches Handeln zu finden. Dabei ist dieGoldene Regelwenig populär, da sie gleiche Wünsche aller Beteiligten voraussetzt. DemUtilitarismuszufolge ist das oberste Moralprinzip, das größte Glück der größten Zahl anzustreben. Verbreitet ist auch Kantskategorischer Imperativ: „Handle nur nach derjenigen Maxime, durch die du zugleich wollen kannst, dass sie ein allgemeines Gesetz werde.“„Handle so, dass du die Menschheit sowohl in deiner Person, als in der Person eines jeden anderen jederzeit zugleich als Zweck, niemals bloß als Mittel brauchst.“ Diedeskriptive Ethikhingegen beschäftigt sich mit den verschiedenen vorhandenen Moralvorstellungen und versucht diese genau zu fassen und zu beschreiben, sie ist eher Teil der empirischen Humanwissenschaften als der Philosophie. Basis der allgemeinen Ethik ist dieMetaethik, die das Sprechen über Ethik und ethische Begriffe („gut“, „böse“, „Handlung“) sowie normenlogische Folgerungen analysiert. Die Ethik gehört zu den wenigen Disziplinen der Philosophie, die bisher nur in geringem Maße von (anderen) Wissenschaften infrage gestellt wurden. Dies ist nämlich logisch kaum möglich, da empirische Wissenschaften nur Fakten beschreiben und Mittel zur Erreichung von Zwecken entwickeln und verbessern, aber nicht sagen können, welche Zwecke jemand überhaupt verfolgen soll. Die Infragestellung aller ethischen Werte durchAmoralismusundRelativismussteht im Kontrast zur gesellschaftlichen Nachfrage nach Bereichsethiken wie derMedizin-,Tier-oderWissenschaftsethikbis hin zurHacker-undInformationsethik, aber auch der Schaffung von Institutionen wie demNationalen Ethikrat. EinflussreicheEthikersind unter anderem Aristoteles, dieStoikerundEpikureer, Thomas von Aquin, Immanuel Kant,Jeremy BenthamundJohn Stuart Mill,Max Scheler,Hans JonasundKarl-Otto Apel. Eine direkte Anwendung der Ethik findet sich in der Rechtsphilosophie, die zugleich eine der Grundlagendisziplinen derRechtswissenschaftendarstellt. Basierend auf der Beurteilung von Handlungen in „gut“ und „schlecht“ wird die Frage nachRechtundGerechtigkeitund der Folge der Verletzung von moralischen und ethischenNormengestellt. Natürlich fragt die Rechtsphilosophie auch nach der Entstehung, Einsetzung und Legitimation des Rechts, dem Verhältnis von „natürlichem Recht“ (vgl.Menschenrechte) und „gesetztem Recht“ („positives Recht“), nach der Reihenfolge der Wichtigkeit vonRechtsnormenund ihrer Außerkraftsetzung. Hier gibt es Überschneidungen mit der politischen Philosophie. BekannteRechtsphilosophensindHugo Grotius,Niccolò Machiavelli,Thomas Hobbes,Hans Kelsen,Gustav Radbruch,H.L.A. Hart,Niklas Luhmann,Jürgen Habermas,John Rawls,Ronald DworkinundRobert Alexy. Die politische Philosophie ist ähnlich wie die Rechtsphilosophie in großen Teilen von den benachbarten Wissenschaften vereinnahmt worden. So finden große Teile der philosophischen Diskussion in den Rechts- bzw.Politikwissenschaftenstatt. Die Entstehung, Rechtmäßigkeit und Verfasstheit einesStaateswird von derStaatstheorieuntersucht. Diepolitische Theoriefragt nach der bestenHerrschaftsform, dem Verhältnis zwischenBürgerund Staat, nachMachtverteilung,Gesetz,Eigentum,SicherheitundFreiheit. Wichtige Beiträge hierzu haben u. a. diepolitischen DenkerPlaton,Aristoteles,Augustinus,Marsilius von Padua,Niccolò Machiavelli,Thomas Hobbes,John Locke,Jean-Jacques Rousseau,Immanuel Kant,Karl Marx,Michail Bakunin,Carl Schmitt,Hannah Arendt,Karl Popper,Michel Foucault,Isaiah BerlinundRobert Nozickgeliefert. Obgleich sie sehr alte Fragestellungen behandeln, ist die Philosophie des Geistes bzw. die Philosophie des Bewusstseins noch eine junge,interdisziplinärangelegte Disziplin, die an dieKognitions-undNeurowissenschaftenangrenzt. Im Mittelpunkt stehen Fragen nach demWesenvonGeistbzw.Bewusstsein, nach dem Verhältnis von Leib undSeele, Materie und Geist. Aber auch die Möglichkeit einesfreien Willens, sowie das Wesenmentaler Zustände, von Bewusstseinsinhalten undEmotionen(Qualia) wird hier untersucht. Weiterhin befasst sich dieses Gebiet mit der Beurteilung verschiedenerBewusstseinszustände, Überlegungen zukünstlicher Intelligenz, mit derIdentitätdesSelbstund mit dem Problem eines möglichenWeiterlebens nach dem physischen Tod. Untersuchungsebenen sind die ontologische, die epistemologische, die semantische und die methodologische.[18] BekannteVertreter dieser ProblemfeldersindGottfried Wilhelm Leibniz,Baruch de Spinoza,Alan Turing,Hilary Putnam,John Searle,Jaegwon KimundDonald Davidson. Von großer philosophischer Bedeutung sind hier auch im Kontext desBuddhismusausgearbeitete Theorien. Die moderne philosophische Anthropologie befasst sich mit dem Wesen des Menschen, und zwar vornehmlich nicht als Individuum, sondern als Gattungswesen. Da sie von Menschen selbst betrieben wird, ist sie eine (dialektische) Selbstreflexion, die gleichzeitig eine Innen- und eine Außenperspektive aufweist. Die Daseinssituation des Menschen wird unter Einbeziehung aller wichtigen einzelwissenschaftlichen Erkenntnisse untersucht. Das Wesen des Menschen gibt viele Rätsel auf. Seine Stellung imKosmos, das Verhältnis vonKulturzuNatur,VereinzelungundVergemeinschaftung, die Probleme derGeschlechtlichkeit, die Rolle vonLiebeundTodsind einige der Grundfragen der philosophischen Anthropologie. Ob der Mensch von Natur aus gut oder böse sei, ob Gewalt und Leid zwingend zur menschlichen Existenz gehören, ob das Leben überhaupteinen Sinn hat: all dies sind weitere Probleme dieser Disziplin. Sie untersucht aber auch grundsätzliche menschlicheBedürfnisseund Fähigkeiten wieSelbstverwirklichung,Kreativität,Neugierund Wissensdurst, Machtstreben undAltruismus, das Phänomen der Freiheit und die Wahrnehmung des Anderen. Wichtige Philosophen, die zu anthropologischen Problemen gearbeitet haben, sindThomas von Aquin,Immanuel Kant,Arthur Schopenhauer,Friedrich Nietzsche,Søren Kierkegaard,Max Scheler,Arnold Gehlen,Ernst Cassirer,Helmuth Plessnerund dieVertreter der Existenzphilosophie. Besonders in der zweiten Hälfte des 20. Jahrhunderts haben einige Philosophen Theorien über allgemeine Wesenszüge des Menschen kritisiert, darunter beispielsweise (mit unterschiedlicher Akzentuierung)Michel FoucaultoderJürgen Habermas. Zu den aktuellen Problemen der philosophischen Forschung gehört die Analyse des menschlichen Handelns unter dem Gesichtspunkt der Vernünftigkeit. Dabei werden weniger die ethischen Motive berücksichtigt, sondern vielmehr rein mathematische Kosten-Nutzen-Abwägungen oder das logische Kalkül unter der Voraussetzung, dass der Mensch gewöhnlich rational handelt. Einige Philosophen verwenden dieSpieltheorie, um Modelle für ethische Probleme zu entwickeln. Sowohl individuelle (z. B. dasGefangenendilemma), als auch gesellschaftliche Paradoxa (z. B. dieTragik der Allmende) lassen sich in diesem Rahmen, wenn schon nicht lösen, so doch verstehen. DieHandlungstheorieversucht, motivierte Handlungen zu erklären, so etwa, ob und wie es möglich ist, bei zwei alternativen Handlungen, frei und absichtlich die selbst für schlechter gehaltene zu wählen (Akrasia). Die Klärung des Begriffs „Rationalität“ ist, gerade wenn die Rationalität von Handlungen untersucht wird, ein in jüngerer Zeit umfänglich debattiertes Gebiet. In der Geschichte der Philosophie waren die Begriffe „Verstand“ und „Vernunft“, „ratio“ und „Intellekt“ oft strittig. An ihrer Bestimmung entschied sich oft, welche Konzeption von Philosophie vertreten wurde. In der Moderne ist „Rationalität“ in verschiedener Hinsicht zunehmend fragwürdig geworden, sodass die gegenwärtige Philosophie vor der Aufgabe steht, ihre eigene Minimalbestimmung kritisch zu hinterfragen. Obwohl mystische Elemente in westlichen und östlichen philosophischen Traditionen oft präsent waren, ist der Begriff der „Philosophischen Mystik“ noch jung. Sie hält zum einen – ähnlich derPhilosophia perennis– daran fest, dass es ewige, unveränderliche und universal gültige Wahrheiten bezüglich der Wirklichkeit und des Menschen zu erkennen gibt. Zum anderen betont sie, wie alle mystische Strömungen, den Vorrang des gegenwärtigen Hier-und-jetzt-Daseins, die Wichtigkeit der zweckfreienKontemplation, dieWürdeder Schöpfung und die zentrale Bedeutung des Eingebettetseins der individuellen Existenz in das Ganze des Weltgefüges. In ihrer Arbeitsweise überschreitet sie die Grenzen von Vernunft und Verstand und betont auch erfahrbare, aber dennoch intersubjektiv mitteilbare und philosophisch behandelbare Gewissheiten. Zentrale Themen der philosophischen Mystik sind u. a. die Erfahrung der Aufhebung derSubjekt-Objekt-Spaltung, der Zusammenfall aller Gegensätze in Gott (coincidentia oppositorum), die mögliche Einheit des Menschen mit dem All-Ganzen (unio mystica) und die Spur des Göttlichen im menschlichen Wesen (scintilla animae). Einige westliche Philosophen, in deren Lehren sich mystische Elemente finden, sindPlotin,Meister Eckhart,Nikolaus von Kues,Jakob Böhme, Gottfried Wilhelm Leibniz,Blaise Pascal, Baruch de Spinoza, Martin Heidegger,Simone WeilundKen Wilber. In der außereuropäischen, besonders der östlichen Philosophie, spielt dieMystiktraditionell eine große Rolle. Typischerweise überwindet sie nicht nur die Grenzen der Philosophie, sondern auch die derReligion, so etwa imZen, imYoga, imSufismus, in derKabbalaund in derchristlichen Mystik. Die Geschichte der westlichen Philosophie beginnt im6. Jahrhundert v. Chr.imantiken Griechenland. Zu ihren wesentlichen Merkmalen gehört, dass immer wieder neue Antworten auf die philosophischen Grundfragen gefunden, begründet und diskutiert wurden. Dies lässt sich teils auf veränderte Bedürfnisse des jeweils herrschendenZeitgeists, teils auf die fortdauernde Weiterentwicklung der übrigen Wissenschaften zurückführen. „Fortschritte“ im Sinne eines endgültigen Widerlegens oder Beweisens von Lehren macht die Philosophie aus Sicht mancher Philosophen allerdings kaum. Der Philosoph Alfred North Whitehead charakterisierte die Geschichte der europäischen Philosophie seit Aristoteles einmal als bloße „Fußnotenzu Platon“.[19]Da philosophische Ideen und Begriffe nicht veralten, hat für die Philosophie die Untersuchung ihrer eigenen Geschichte eine weitaus größere Bedeutung als für die meisten anderen Wissenschaften. In den Städten desantiken Griechenlandkam es infolge kultureller Fortschritte und verstärkten Kontakts zu benachbarten Kulturen zu wachsender Kritik am traditionellen, vomMythosgeprägtenWeltbild. In diesem geistigen Klima begann mit denVorsokratikern– wie man die griechischen Philosophen vor oder zu Lebzeiten des Sokrates nennt – die Geschichte der westlichen Philosophie. Ihr nur bruchstückhaft überliefertes Denken ist vonnaturphilosophischenFragen nach den Grundlagen der Welt bestimmt. Mittels einer Mischung aus Spekulation und empirischer Beobachtung versuchten sie, die Natur und die Vorgänge in ihr zu begreifen. Sie wollten alle Dinge auf ein ursprüngliches Prinzip (griechisch ἀρχήarché), und zwar einen „Urstoff“ zurückführen. So hielt der erste bekannte PhilosophThalesvonMiletdas Wasser für diesen „Urstoff“.Empedoklesbegründete die bis zum 18. Jahrhundert in der Naturphilosophie herrschende Lehre von denvier ElementenWasser, Feuer, Erde und Luft, aus denen alle Dinge zusammengesetzt seien. Neben diesen Ansätzen gab es noch andere Modelle der Welterklärung.Pythagorasund seine Schule hielten dieZahlfür das alles bestimmende Prinzip und nahmen damit einen wichtigen Grundsatz der modernen Naturwissenschaften vorweg.Heraklitbetonte dasWerdenundVergehenund sah als Grundlage der Wirklichkeit denLogos, ein einheitsstiftendes Prinzip derGegensätze. Die Philosophie vonParmenides, der im Gegensatz dazu die Einheit und Unvergänglichkeit des Seins annahm, wird als Beginn derOntologieaufgefasst. Mit dem Auftreten derSophistenMitte des 5. Jahrhunderts trat der Mensch in den Mittelpunkt philosophischer Betrachtung (Protagoras: „Der Mensch ist das Maß aller Dinge“).[20]Sie beschäftigten sich besonders mit ethischen und politischen Problemen, etwa mit der Frage, ob Normen und Werte naturgegeben oder von Menschen festgelegt sind. Zu einem Leitbild der europäischen Philosophie wurde der AthenerSokrates(469–399 v. Chr.). Seine Methode derMäeutik(„Hebammenkunst“) bestand darin, dass Sokrates in scheinbarer Naivität seine Gesprächspartner durch eine tiefgründige und zielgerichtete Fragetechnik auf Widersprüche in ihrem Denken hinwies und zu Einsichten führte („beim Gebären unterstützte“), die ihnen zu einem philosophisch veränderten Blick auf die Welt verhalfen. Seine demonstrative geistige Unabhängigkeit und sein unangepasstes Verhalten trugen ihm ein Todesurteil wegenGottlosigkeitund Verderbnis der Jugend ein (vgl.Apologie). Da Sokrates selbst nichts schriftlich festhielt, ist sein Bild maßgeblich von seinem SchülerPlaton(ca. 428–347 v. Chr.) bestimmt worden, in dessen Werk Sokrates zentrale Bedeutung hat. Dieses weitestgehend inDialogformabgefasste Werk bildet einen zentralen Ausgangspunkt der abendländischen Philosophie. Ausgehend von der sokratischen Was-ist-Frage („Was ist Tugend? Gerechtigkeit?Das Gute?“) schuf Platon die Ansätze einer Definitionslehre. Außerdem war er Urheber einerIdeenlehre, der die Vorstellung einerzweigeteilten Wirklichkeitzugrunde liegt: Dem mit den Sinnesorganen wahrnehmbaren dinglichen Objekt steht auf der Ebene der Ideen eine nur dem dafür empfänglichen Intellekt zugängliche abstrahierte, allgemeine Entsprechung gegenüber. Nach Platons Überzeugung führt das Wissen von diesen Ideen zu einem tiefergehenden Verständnis der gesamten Wirklichkeit. Platons SchülerAristoteles(384–322 v. Chr.) verwarf die Ideenlehre als eine unnötige „Verdopplung der Welt“. Für ihn bestand dasWeseneines Dinges nicht in einer zusätzlich existierenden Idee, sondern in derForm, die dem Ding innewohnt. Seine Schule begann die gesamte erfahrbare Wirklichkeit – Natur und Gesellschaft – in verschiedene Wissensgebiete zu gliedern, zu analysieren und wissenschaftlich zu ordnen. Außerdem begründete Aristoteles die klassische Logik (Syllogistik), Wissenschaftssystematik undWissenschaftstheorie. Dabei führte er philosophische Grundbegriffe ein, die bis in die Neuzeit maßgeblich blieben. Am Übergang vom 4. zum 3. Jahrhundert v. Chr. entstanden in Athen imHellenismuszwei weitere philosophische Schulen, die in deutlicher Akzentverschiebung gegenüber derplatonischen Akademieund dem aristotelischenPeripatosdas individuelle Seelenheil in das Zentrum ihres Bemühens stellten: FürEpikur(ca. 341–270 v. Chr.) und seine Anhänger einerseits sowie für dieStoikerumZenon von Kitionandererseits diente Philosophie hauptsächlich dazu, mitethischenMitteln psychisches Wohlbefinden bzw. Gelassenheit zu erlangen. Epikur sah dafür ein maßvoll gestaltetes, wohldosiertes Genussleben vor, das sich von aller politischen Betätigung fernhielt. Die Stoiker erstrebten dieSeelenruhe, indem gegenüber allen inneren und äußeren Herausforderungen Gleichmut bewahrt werden sollte. Dies sollte vor allem durch Kontrolle der Emotionen in Verbindung mit einer schicksalsbejahenden Grundhaltung im Einklang mit der Ordnung des Universums erreicht werden; zugleich wusste man um die Verpflichtungen gegenüber den Mitmenschen und der Gemeinschaft. Diese Lehre fand später Eingang in führende Kreise derRömischen Republik. Während die Anhänger derpyrrhonischenSkepsisgrundsätzlich die Möglichkeit sicherer Urteile und unzweifelhaften Wissens bestritten, formtePlotinim 3. Jahrhundert Platons Ideenlehre um (Neuplatonismus). Seine Konzeption von der Abstufung des Seins (vom „Einen“ bis hinab zur Materie) bot demChristentummannigfaltige Anknüpfungsmöglichkeiten und war die vorherrschende Philosophie derSpätantike. Die Philosophie des Mittelalters sonderte sich nur allmählich von derTheologieab und blieb auch dann wesentlich durch religiöse Institutionen, Lebensformen und Lehren geprägt. Sie orientierte sich methodisch und inhaltlich stark an Traditionen und Autoritäten. Fundament und Bezugsgröße bildeten im christlichen Kontext wesentlich die Lehren, welche dieKirchenväterderPatristikgeschaffen hatten. Als maßgeblich erwiesen sich bis zum Beginn desSpätmittelaltersvor allem die Ansichten desAugustinus von Hippo. Er fasste dieWeltgeschichteals unablässigen Kampf des Reichs des Bösen gegen das Reich des Guten auf. Gesellschaft und Kirche, Theologie und Philosophie bilden demnach eine Einheit, die keine Zweifel an Entscheidungen derKirchezulässt. Der „letzte Römer“ und „erste Scholastiker“Boethiusstand am Anfang der mittelalterlichen Versuche, eine Synthese zwischen dem platonischen und dem aristotelischen Denken zu bilden, begründete die mittelalterliche Logik, bildete Begriffe wie „Person“ oder „Natur“, löste den Universalienstreit aus und entwarf eine folgenreiche Wissenschaftskonzeption, an die etwa dieSchule von Chartresanschloss. Während im Osten das griechischsprachigebyzantinische Reichwichtige Teile des antiken Wissens bewahrte, beschränkte sich die bruchstückhafte Erhaltung des antiken Erbes im „lateinischen Westen“ bis zum Beginn des Spätmittelalters weitgehend auf dieKloster-undDomschulen. Bis 1100 traten nur wenige Philosophen hervor, darunterAnselm von Canterbury, der einen rein philosophischenGottesbeweisformulierte, dem eine anhaltende Nachwirkung beschieden war. Seit dem späten11. Jahrhunderterlebte die westliche Philosophie einen Aufschwung. Dabei spielte die Verbreitung von übersetzten Werken arabischsprachiger Philosophen, die ihrerseits an antike Traditionen anknüpften, eine wesentliche Rolle. Eines der Hauptthemen der mittelalterlichen Philosophie wurde schon früh derUniversalienstreit. Dabei ging es um die Frage, ob Allgemeinbegriffe bloße gedanklicheAbstraktionenundKonventionenzum Zweck der Verständigung sind oder ob sie eine eigenständige objektive Realität bezeichnen, wie dieplatonische Traditionmit ihrerIdeenlehrebehauptet. Im Zusammenhang mit diesem Problemfeld setzten sich viele Denker intensiv mit derSprachlogikauseinander; es entstand die „spekulative Grammatik“, die nach der Verbindung zwischen einer Theorie derGrammatikund einer Theorie derWirklichkeitfragt. Viele Philosophen nahmen im Universalienstreit vermittelnde Positionen ein, darunterPetrus Abaelardus. Dieser trug viel zur Herausbildung derscholastischen Methodeder Gegenüberstellung und Abwägung gegensätzlicher Lehrmeinungen bei. Im 13. Jahrhundert wurden zahlreiche bisher im Westen unbekannte Werke des Aristoteles in neuen Übersetzungen zugänglich; hinzu kamen die Schriften der arabischsprachigen Aristoteleskommentatoren. Sie wurden zur Grundlage desuniversitärenUnterrichts. BesondersAlbertus Magnusund sein SchülerThomas von Aquinsorgten für die Verbreitung des Aristotelismus, der sich schließlich gegenüber dem bisher vorherrschenden Platonismus bzw.Augustinismusweitgehend durchsetzte und bis tief in dieFrühe Neuzeithinein die maßgebliche philosophische Richtung in der akademischen Welt blieb. Thomas begründete denThomismus, einen großangelegten Versuch der Zusammenführung aristotelischer Philosophie mit den Lehren der katholischen Kirche. Während der Orden derDominikanerschon früh diese zunächst noch verurteilte Konzeption durchsetzte, entwarfen besonders Denker derFranziskanerwieJohannes Duns ScotusAlternativen. Dieser erkannte u. a. die Eigenständigkeit der Philosophie gegenüber der Theologie an. Gegenstand der Metaphysik war für ihn nicht Gott (Averroes), sondern das Seiende als Seiendes (Avicenna). Darüber hinaus bestand er auf der Differenz zwischengeglaubtemund im Rahmen der PhilosophiegedachtemGott, was zahlreiche rein philosophische Beweisverfahren – etwa für die Unsterblichkeit der Seele – unmöglich machte. Konzepte, in denen geistige Erkenntnis nicht auf das Allgemeine, sondern auf das Einzelne abzielte, ermöglichten die Begründung einer erfahrungsorientierten Wissenschaft, wie sie auch ein anderer Vorläufer naturwissenschaftlichen Denkens,Roger Bacon, forderte: durch eine Abkehr von Spekulation und Autoritätsgläubigkeit. Ein weiterer Vorbereiter der Moderne war der prominenteste Vorkämpfer desNominalismus,Wilhelm von Ockham, der im frühen 14. Jahrhundert einen neuen Weg in der Philosophie einschlug (via moderna).Marsilius von Paduabegründete eine neue Staatstheorie, in der sich wichtige Ideen der Neuzeit (Gesellschaftsvertrag,Trennung von Kirche und Staat) ankündigten. Wichtigster Vertreter derchristlichen Mystikdes Mittelalters warMeister Eckhart, der sich als „Lebensmeister“ sah und die Bedeutung der praktischen Umsetzung philosophischer Erkenntnis im eigenen Lebensvollzug betonte. Ebenfalls in dieser Tradition standNikolaus von Kues, der an der Schwelle zur Neuzeit viele Entwicklungen der folgenden Jahrhunderte vorwegnahm. Seine Ideen, die von derUnerkennbarkeit Gottesbis zu den Gesetzen und Grenzen derPhysikoder der Erkenntnis reichen, weisen auf spätere Denker wie Immanuel Kant,Isaac NewtonundAlbert Einsteinvoraus. Der Übergang vom Mittelalter zurNeuzeitwird von derRenaissanceund demHumanismusmarkiert. In dieser Epoche konnte sich neben der breiten Strömung der traditionellen Scholastik allmählich die neuzeitliche Philosophie etablieren. Besonders diepolitische Philosophiegeriet in der Renaissance in Bewegung:Niccolò MachiavellisThese, die Ausübung politischerHerrschaftsei nicht unter moralischem, sondern allein unter dem Nützlichkeitsaspekt zu beurteilen, erregt noch heute Anstoß. Eine ganz andere Richtung schlugThomas Morusein, der in seinerUtopie(Utopia, 1516) einen Staat mit Bildung für alle, mitReligionsfreiheitund ohnePrivateigentumentwarf, womit er einige Ideen der Moderne vorwegnahm. Während der HumanistPico della Mirandolaversuchte, eine grundsätzliche Übereinstimmung aller philosophischen Traditionen zu erweisen, wurde das Denken von Männern wieJohannes Kepler,Nikolaus KopernikusoderGiordano Brunovon dem Versuch bestimmt, Philosophie und Naturwissenschaften miteinander zu verbinden. Vorstellungen wie dasheliozentrische Weltbild, die des unendlichen Kosmos oder desAllgottglaubensstießen dabei auf heftigen Widerstand der Kirche. Das naturwissenschaftliche Weltbild, die Methoden derMathematikund der Glaube an die Vernunft bestimmten diePhilosophie der Neuzeitim 17. und 18. Jahrhundert. In der Theorie nahm sie die politischen Umbrüche vorweg, die dann in derFranzösischen Revolutiongipfelten. Der Welterklärung desRationalismusliegen „vernünftige Schlussfolgerungen“ zugrunde, somit auch dem vonRené Descartes(1596–1650) begründetenCartesianismus. Sein Satz „Ich denke, also bin ich“,[21]mit dem er den unbezweifelbaren Ursprung aller Gewissheiten gefunden zu haben glaubte, gehört zu den bekanntesten philosophischen Thesen. Denker wieSpinozaundLeibnizentwickelten seinen Ansatz in großen metaphysischen Systementwürfen (vgl.Monade) weiter. Diese erkenntnistheoretische Vorgehensweise wurde auf alle Teilgebiete der Philosophie angewendet; man versuchte, selbst die elementaren Grundsätze menschlicher Moral aus „vernünftigen“ Überlegungen abzuleiten, die so zwingend seien wie geometrische Beweise (Ethica, ordine geometrico demonstrata, 1677). Bei dem Theorietyp desEmpirismuswerden nur solcheHypothesenanerkannt, die sich auf „sinnliche Wahrnehmung“ zurückführen lassen. Ihm verpflichtet waren u. a.Thomas Hobbes,John LockeundDavid Hume. Das Prinzip der Ableitung aller Erkenntnis ausSinneserfahrungen hat als Grundlage des naturwissenschaftlichen Arbeitens eine überragende Bedeutung bis in die Gegenwart. So ist auch dieanalytische Philosophiein dieser Denktradition verwurzelt. Die emanzipatorisch-bürgerliche Bewegung derAufklärungerhob dieVernunftzur Grundlage aller Erkenntnis und zum Maßstab allen menschlichen Handelns. Sie forderte dieMenschenrechteein und dachte über die Wiederherstellung einer „unverfälschten natürlichen Lebensweise“ nach. Sie trat für staatlicheGewaltenteilung(Montesquieu) und Mitspracherechte insbesondere desBürgertumsein. Eine theoretische Basis dafür war die Idee einesGesellschaftsvertrags(z. B. beiJean-Jacques Rousseau);Verfassungensollten die neuen Rechte absichern. Die französischen AufklärerVoltaireundDiderotkritisierten die Macht der Kirche und der absolutistischen Monarchen. DieEnzyklopädisten(d’Alembert) versuchten erstmals, das gesamte Wissen ihrer Zeit in einem Lexikon zusammenzufassen. Radikalere Vertreter der französischen Aufklärung warenHolbach, der erstmals einenaturalistischeSicht des Menschen im Sinne der Naturwissenschaft ohne Gott und Metaphysik entwarf,La Mettrie, der den Menschen als Maschine und Lust als Lebensziel ansah, undSade, der aus beiden die Konsequenz zog, jegliche allgemein verbindliche Ethik zu verneinen. Schließlich erarbeitete einer der zentralen Philosophen der Neuzeit,Immanuel Kant, seine von vielen Zeitgenossen als revolutionär empfundeneErkenntniskritik. Sie besagt, dass wir nicht dieDinge selbsterkennen können, sondern immer nur derenErscheinungen, die von den Möglichkeiten, die der Verstand und die Sinne bieten, vorgeformt werden. Danach ist jede Erkenntnis immer vom erkennenden Subjekt abhängig. Auch Kants weitere Arbeiten u. a. zur Ethik („kategorischer Imperativ“),Ästhetikund zumVölkerrecht(Zum ewigen Frieden, 1795/96) hatten erhebliche Bedeutung für die nachfolgenden Jahrhunderte. Ein Teil der Philosophie war in der ersten Hälfte des 19. Jahrhunderts von dem Streben geprägt, die Erkenntnisse Kants zu „vollenden“, zu „verbessern“ oder zu übertreffen. Kennzeichnend für denDeutschen Idealismus(Fichte,Schelling,Hegel) sind die allumfassenden spekulativen metaphysischen Systeme, in denen das „Ich“, das „Absolute“ bzw. der „Geist“ die Grundlagen der Welt bestimmen. Eine andere Richtung schlugen empiristisch geprägte Strömungen wie derPositivismusein, der die Welt allein mit Hilfe der empirischen Wissenschaften, d. h. ohne Metaphysik erklären wollte. In England erarbeitetenBenthamundMilldenUtilitarismus, der derÖkonomieund der Ethik durch ein konsequentes Kosten-Nutzen-Konzept und mit der Idee einer Art „Wohlstand für alle“ (das Prinzip desgrößten Glücks der größten Zahl) wichtige Impulse gab. Die Ökonomie steht neben derGeschichtsphilosophieauch im Mittelpunkt der Philosophie vonMarx, der im Anschluss an Hegel und dieMaterialistendenKommunismusbegründete. Marx forderte, theoretische Reflexionen an der Umgestaltung der konkreten sozialen Verhältnisse zu messen: „Die Philosophen haben die Welt nur verschieden interpretiert; es kommt aber darauf an, sie zu verändern.“ Prominente Denker, die neue Wege einschlugen, warenArthur Schopenhauer,Sören KierkegaardundFriedrich Nietzsche. Schopenhauer betonte im Anschluss an dieindische Philosophiedie Priorität und Übermacht des Willens gegenüber der Vernunft. Seine pessimistische Weltsicht, die von der Erfahrung des Leidens bestimmt ist, geht auch von buddhistischen Vorstellungen aus. Friedrich Nietzsche, der wie Schopenhauer großen Einfluss auf die Künste hatte, bezeichnete sich selbst alsImmoralisten. Für ihn waren die Werte der überkommenen christlichen Moral Ausdruck von Schwäche undDekadenz. Er thematisierte Ideen desNihilismus, desÜbermenschenund der „ewigen Wiederkunft“, der endlosen Wiederholung der Geschichte. Der religiöse Denker Sören Kierkegaard war in mancher Hinsicht ein Vorläufer desExistenzialismus. Er vertrat einen radikalenIndividualismus, der nicht danach fragt, wie mangrundsätzlichrichtig handeln könne, sondern wie man sich als Individuum in der jeweils konkreten Situation zu verhalten habe. Die Philosophie des 20. Jahrhunderts zeichnete sich durch ein großes Spektrum von Positionen und Strömungen aus. In seinen Anfängen war dieses Jahrhundert von einer starken Fortschritts- und Wissenschaftsgläubigkeit geprägt. Erst in der zweiten Hälfte des 20. Jahrhunderts – das auf gesellschaftlicher Ebene die Erfahrung der beidenWeltkriege, derShoaund der Bedrohung des Planeten durchKernwaffengebracht hatte und das die Gefährdung derÖkosystemedurch den Menschen selbst hat hervortreten lassen – kamen die nach Rousseau weitgehend an den Rand gedrängtenFortschrittsskeptikerauch in der Philosophie wieder stärker zur Geltung. Die enormen Erfolge der Technik im 19. Jahrhundert führten zu einem ErstarkenneopositivistischerPositionen. Derlogische EmpiristRudolf Carnapplädierte dafür, die Philosophie gänzlich durch eine „Wissenschaftslogik“ – d. h. durch die logische Analyse derWissenschaftssprache– zu ersetzen. Derkritische RationalistKarl Popperargumentierte, dass wissenschaftlicher Fortschritt vor allem durchWiderlegungeinzelner Theorien durchExperimente(„Falsifizierung“) geschehe. Seiner Ansicht nach setzen sich in einemevolutionsartigenSelektionsprozessdiejenigen wissenschaftlichen Theorien durch, die der Wahrheit am nächsten kommen.Thomas S. Kuhnhielt dagegen verschiedene Theorien zur selben Frage prinzipiell für unvergleichbar, eine Überlegenheit der einen über die andere daher für nichtsachlichbegründbar, wodurch die Dominanz einer Theorie eine Sache derRhetorikwürde. In eine ähnliche Richtung ging auch das PlädoyerPaul Feyerabendsfür methodische Freiheit. Für denPragmatismusschließlich müssen Theorien unter dem Gesichtspunkt ihrer Brauchbarkeit und Anwendbarkeit in der Praxis beurteilt werden. Als Reaktion auf die zunehmende Verwissenschaftlichung aller Lebensbereiche können jene Denkströmungen verstanden werden, die sich dem Einzelnen und dem Leben zuwenden. So war das Grundverständnis derLebensphilosophie, dass sich dieGanzheitlichkeitdes Lebens nicht allein durch Wissenschaft, Begriffe und Logik beschreiben lässt.Henri Bergsonetwa sah einen fundamentalen Unterschied zwischen der individuell erlebten Zeit und der analytischen Zeit der Naturwissenschaft. Ähnlich kritisch forderte auchEdmund Husserl, der Begründer derPhänomenologie, dazu auf, sich bei der analytischen Betrachtung der Dinge zunächst an das zu halten, was demBewusstseinunmittelbar erscheint, um eine vorschnelle Weltdeutung zu vermeiden. Von großem Einfluss war dieExistenzphilosophieseines SchülersMartin Heidegger. Dessen Ausgangspunkt war die Analyse der allgemeinen menschlichen Befindlichkeit und führte ihn zu der Frage nach demSinnvon Sein überhaupt. Im Anschluss an Heidegger vertrat derExistenzialismus, insbesondere repräsentiert durchJean-Paul Sartre, die These, dass der Mensch „zur Freiheit verurteilt“ sei. Er müsse mit jeder seiner Handlungen eine Wahl treffen, für die er selbst verantwortlich sei. „Es gibt nur ein wirklich ernstes philosophisches Problem: denSelbstmord. Sich entscheiden, ob das Leben es wert ist, gelebt zu werden oder nicht, heißt, auf die Grundfrage der Philosophie antworten. Alles andere – ob die Welt drei Dimensionen und der Geist neun oder zwölf Kategorien hat – kommt später. Das sind Spielereien; erst muss man antworten.“ Das20. Jahrhundertwar von sozialen Umwälzungen und dem Konflikt zwischenSowjetkommunismusund westlich-kapitalistischenGesellschaftsformen geprägt. Im Zuge dieser Auseinandersetzung, die imKalten Kriegkulminierte und mit derGlobalisierungweltweite Dimensionen annahm, wurden geschichts- und sozialphilosophische Fragestellungen in der philosophischen Debatte stark akzentuiert. Das vonKarl Marxam Ende allerKlassenkämpfein Aussicht gestellte „Reich der Freiheit“ suchteErnst BlochinPrinzip Hoffnungalskonkrete Utopiezu erweisen, die gegenüber allen vorherigen Utopien den Vorzug habe, auf dem Fundament desDialektischen Materialismuszu gründen. AuchHerbert Marcuseund die Begründer derKritischen Theorie,Theodor W. AdornoundMax Horkheimer, entwickelten ihre philosophischen Ansätze zurEntfremdungsproblematikvor dem Hintergrund der Gesellschaftsanalysen vonMarxundEngels. MitJürgen Habermashat die auch alsFrankfurter SchulebezeichneteKritische Theorieeinen Philosophen hervorgebracht, der mit seinerTheorie des kommunikativen Handelnsund dem Ideal des „herrschaftsfreienDiskurses“ ebenfalls dem Leitbild einer aus Abhängigkeitsverhältnissen befreiten Gesellschaft verpflichtet ist, dabei aber die chancenreichen Potentiale der westlichen Demokratien schätzt. Vor den Gefahren eines „atomistischen Individualismus“ in modernen Gesellschaften warnt der Vordenker desKommunitarismusCharles Taylor, der den Weg zur Erhaltung bzw. Schaffung humaner gesellschaftlicher und gesamtökologischer Lebensbedingungen in einer noch zu findenden Balance zwischen Individualrechten und Gemeinschaftspflichten der Menschen sieht. Die Philosophie der Gegenwart steht vor dem Problem, ihren Gegenstand überhaupt zu erfassen, da eine rückblickende Bewertung der verschiedenen Ansätze noch nicht vorzunehmen ist. DieWissenschaftstheorieist jedoch weiterentwickelt worden, indem sie klarere Begriffe von „Bestätigung“ und „Theorienreduktion“ prägte. Seit Ende des 19. Jahrhunderts wird der Sprache eine zunehmend zentrale Stellung in der Philosophie eingeräumt.Ludwig Wittgensteinentwarf ein völlig neues Verständnis von Sprache, die er als ein unüberschaubares Konglomerat einzelner „Sprachspiele“ begriff. Dabei behandle die Philosophie nur „Scheinprobleme“, d. h. sie heile lediglich ihre eigenen „Sprachverwirrungen“. Philosophieren sei also keine „erklärende“, sondern eine „therapeutische“ Tätigkeit: „Die Philosophie ist ein Kampf gegen die Verhexung unseres Verstandes durch die Mittel unserer Sprache.“ Die anfangs vorwiegend sprachphilosophisch orientierteanalytische Philosophiedominiert in angelsächsischen Kontexten und zunehmend auch im deutschen Sprachraum die Methode akademischer Philosophie. An den meisten Universitäten herrscht jedoch ein ausgeprägterPluralismusbezüglich der gelehrten philosophischen Themen und Strömungen. In den deutschsprachigen Ländern eher wenig beachtet, stellt auch dieNeuscholastik, vor allem derNeuthomismus, weltweit eine einflussreiche Strömung der Gegenwartsphilosophie dar, seitdem diekatholische Kirchediese Ende des 19. Jahrhunderts zum offiziellen Lehrinhalt u. a. der Priesterausbildung erhoben hatte. DiePostmoderne(z. B.Gilles Deleuze,Jean-François Lyotard,Jean Baudrillard,Jacques Derrida) ist eine Gegenbewegung zu den Ideen der Moderne und betont die Differenzen von Denk- und Lebenswelten. Auch die menschliche Identität schätzt sie als instabil ein. Die der Postmoderne nahestehendefeministische Philosophiezielt auf die Abhängigkeit der Weltinterpretation vom Geschlecht. Der philosophische Lehr- und Forschungsbetrieb umfasst die wissenschaftlichen Einrichtungen des Faches Philosophie. In Europa handelt es sich dabei meist um vom Staat finanzierte philosophischeInstitute, die Teil einerUniversitätsind. Ihre wissenschaftlichen Aufgaben sind erstens die Organisation eines Lehrbetriebs, der von Interessenten im Rahmen eines gesetzlich geregeltenStudiumsdurchlaufen werden kann, und zweitens dieForschung. Dazu haben die Institute bezahlte Stellen zur Verfügung, sowohl für wissenschaftliche Angestellte als auch für Verwaltungsbeamte. Neben den Universitäts-Instituten existieren eigene philosophische Einrichtungen wie beispielsweise dieHochschule für Philosophie München. Im Jahr 2011 waren inDeutschland1.191 Philosophen in Vollzeit angestellt, 2002 waren es noch 869. 2008 gab es an über 150 Lehrstühlen etwa 330 Professoren. In demselben Jahr studierten ungefähr 15.000 Personen Philosophie. Diese Zahl ging gegenüber 1996, als 24.000 Personen studierten, deutlich zurück, wodurch sich das Betreuungsverhältnis erheblich verbesserte.[22] In Österreich kann an den UniversitätenWien,Graz,Innsbruck,SalzburgundKlagenfurtPhilosophie studiert werden. 2010 gab es insgesamt 3.651 Eingeschriebene. Das größte Institut befindet sich an der Universität Wien. Philosophiebibliographie: Einführungen in die Philosophie– Zusätzliche Literaturhinweise zum Thema Philosophiebibliographie: Hilfsmittel zur Philosophie– Zusätzliche Literaturhinweise zum Thema Dieser Artikel ist als Audioversion verfügbar: Mehr Informationen zur gesprochenen Wikipedia Philosophie der Antike|Philosophie des Mittelalters|Philosophie der Renaissance und des Humanismus|Philosophie der Neuzeit|Philosophie des 19. Jahrhunderts|Philosophie des 20. Jahrhunderts|Philosophie der Gegenwart Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Einführung 1.1Begriffsdefinition 1.2Sinn und Arten des Philosophierens 1.3Methoden 2Begriffsgeschichte 3Wissenschaftsgeschichte 4Disziplinen 4.1Allgemein 4.2Abgrenzung theoretische und praktische Philosophie 4.3Theoretische Philosophie 4.3.1Logik 4.3.2Erkenntnistheorie 4.3.3Wissenschaftstheorie 4.3.4Metaphysik und Ontologie 4.3.5Sprachphilosophie 4.4Praktische Philosophie 4.4.1Ethik und Metaethik 4.4.2Rechtsphilosophie 4.4.3Politische Philosophie 4.5Neuere Disziplinen 4.5.1Philosophie des Geistes und des Bewusstseins 4.5.2Moderne philosophische Anthropologie 4.5.3Rationalitäts-, Handlungs- und Spieltheorie 4.5.4Philosophische Mystik 5Philosophiegeschichte aus westlicher Perspektive 5.1Antike 5.2Mittelalter 5.3Frühe Neuzeit 5.419. Jahrhundert 5.520. Jahrhundert 5.6Gegenwart"
  },
  {
    "label": 0,
    "text": "Physik – Wikipedia Physik Inhaltsverzeichnis Geschichte von Begriff und Disziplin der Physik Methodik Theoriengebäude Themenbereiche der modernen Physik Grenzen der physikalischen Erkenntnis Beziehung zu anderen Wissenschaften Physik in der Gesellschaft Siehe auch Literatur Weblinks Einzelnachweise Experimentalphysik Theoretische Physik Weitere Aspekte Klassische Mechanik Elektrodynamik und Optik Thermodynamik Relativitätstheorie Quantenphysik Teilchenphysik Hadronen- und Atomkernphysik Atom- und Molekülphysik Kondensierte Materie und Fluiddynamik Astrophysik und Kosmologie Interdisziplinäre Themenbereiche Einsteiger Fachbücher Referenzwerke und Monographien Lexika und Enzyklopädien Sachbücher und Andere Historische Werke und Klassiker Mathematische Physik Angewandte Physik Simulation und Computerphysik DiePhysik(bundesdeutsches Hochdeutsch: [fyˈziːk],[1]österreichisches Hochdeutsch: [fʏˈsɪk],[2]Schweizer Hochdeutsch: auch [fɪˈziːk][3]) ist eineNaturwissenschaft, die grundlegendePhänomenederNaturuntersucht. Um derenEigenschaftenund Verhalten anhand vonquantitativenModellenund Gesetzmäßigkeiten zu erklären, befasst sie sich insbesondere mitMaterieundEnergieund derenWechselwirkungeninRaumundZeit. Erklären bedeutet hier einordnen, vergleichen, allgemeineren Erscheinungen zuordnen oder aus allgemein gültigenNaturgesetzenfolgern.[4]Dazu ist häufig die Bildung neuer geeigneterBegriffenötig, teilweise auch solcher, die der unmittelbaren Anschauung nicht mehr zugänglich sind. Erklärungen in dem philosophischen Sinn, „warum“ die Natur sich so verhält, kann die Physik nicht leisten. Stattdessen setzt sie sich mit dem „wie“ auseinander. Zum Beispiel kann sie nicht erklären, warum Massen einander anziehen. Dieses Verhalten kann lediglich mit verschiedenen Modellen beschrieben werden. Newton tat dies, indem er annahm, dass zwischen Körpern eine Anziehungskraft herrscht. Eine ganz andere Vorstellung hatte Einstein, der die Gravitation damit erklärte, dass Materie die Raumzeit krümmt. Die Arbeitsweise der Physik besteht in einem ZusammenwirkenexperimentellerMethoden und theoretischerModellbildung. PhysikalischeTheorienbewähren sich in der Anwendung auf Systeme der Natur, indem sie Vorhersagen über spätere Zustände erlauben, wenn ein früherer Zustand bekannt ist. Erkenntnisfortschritte ergeben sich durch das Wechselspiel von Beobachtung oder Experiment mit der Theorie. Eine neue oder weiterentwickelte Theorie kann bekannte Ergebnisse besser oder überhaupt erstmals erklären und darüber hinaus neue Experimente und Beobachtungen anregen, deren Ergebnisse dann die Theorie bestätigen oder ihr widersprechen. Unerwartete Beobachtungs- oder Versuchsergebnisse geben Anlass zur Theorieentwicklung in verschiedener Gestalt, von schrittweiser Verbesserung bis hin zur völligen Aufgabe einer lange Zeit akzeptierten Theorie. Erkenntnisse und Modelle der Physik werden intensiv in derChemie,Geologie,Biologie,Medizinund denIngenieurwissenschaftengenutzt. Die Disziplin der Physik in ihrer heutigen Gestalt hat ihre Ursprünge in derPhilosophie, die sich seit derAntikeim weitesten Sinne mit den Gründen und Ursachen aller Dinge befasst. VonAristotelesbis ins beginnende 19. Jahrhundert wurde die Physik als das Teilgebiet der Philosophie verstanden, das sich alsNaturlehre, Naturgeschichte, Chemieoderangewandte Mathematikmit den Gegebenheiten der Natur beschäftigt.[5]Gegenüber den rein philosophischen Erklärungsversuchen der Naturvorgänge spielte die Art von Erkenntnis, die durch systematische und genaue Beobachtung, alsoempirischzu gewinnen ist, lange Zeit keine Rolle. Ab Mitte des 13. und im Laufe des 14. Jahrhunderts plädierten dann einige von den die Natur erforschenden Philosophen – wie etwaRoger Bacon– für ein größeres Gewicht der durch Beobachtung zu erlangenden Naturerkenntnis. Diese Tendenzen mündeten ab dem frühen 17. Jahrhundert, namentlich mitGalileo GalileiundIsaac Newton, in die Entwicklung einer Methodologie der physikalischen Erkenntnis, die vorrangig an empirischen und sogar experimentellen Standards orientiert ist und diesen vor überkommenen philosophischen Grundsätzen im Zweifelsfall sogar den Vorrang einräumt. Dieser Ansatz wurde zunächst als „experimentelle Philosophie“ bezeichnet und führte beim Verständnis vieler unterschiedlicher Naturvorgänge rasch zu bedeutenden Erfolgen. Dennoch dauerte es noch bis ins 19. Jahrhundert, dass er sich endgültig in der Physik durchsetzen konnte und sie damit als eigenständige Disziplin in ihrem heutigen Sinn etablierte. Hinsichtlich ihrer Methode, ihres Gegenstandsbereichs, ihrer wissenschaftssystematischen und institutionellen Verortung teilt sich die Physik im Wesentlichen in zwei große Gebiete auf. Dietheoretische Physikbeschäftigt sich vorwiegend mit formalen mathematischen Beschreibungen und denNaturgesetzen. Sie abstrahiert Vorgänge und Erscheinungen in der wirklichen Natur in Form eines Systems vonModellen, allgemeingültigen Theorien und Naturgesetzen sowieinduktivgewähltenHypothesen. Bei der Formulierung von Theorien und Gesetzen bedient sie sich vielfach der Methoden derMathematikund derLogik. Ziel ist, das Verhalten einesSystemstheoretisch vorherzusagen, damit dies durch Vergleich mit den Vorgängen und Erscheinungen in der wirklichen Natur überprüft werden kann. Diese Überprüfung in Form reproduzierbarerMessungenan gezielt gestalteten physikalischen Experimenten oder durchBeobachtungnatürlicher Phänomene ist das Gebiet derExperimentalphysik. Das Ergebnis der Überprüfung bestimmt über die Gültigkeit und Vorhersagekraft des Modells und der darin gewählten Begriffe, Hypothesen und Methoden. Die Physik steht in enger Verbindung zu denIngenieurwissenschaftenund den anderenNaturwissenschaftenvon derAstronomieundChemiebis zurBiologieund denGeowissenschaften. Die Physik wird dabei häufig als grundlegende oder fundamentale Naturwissenschaft aufgefasst, die sich am stärksten mit den Grundprinzipien befasst, die die natürlichen Vorgänge bestimmen. Die Grenzziehung zu den anderen Naturwissenschaften hat sich historisch ergeben, wird jedoch insbesondere mit dem Aufkommen neuer Wissenschaftsdisziplinen immer schwieriger. In der heutigen Physik ist vor allem die durchAtom-undMolekülphysikundQuantenchemiemarkierte Grenze zur Chemie fließend. Zur Abgrenzung gegenüber der Biologie wurde die Physik oftmals als die Wissenschaft von der unbelebten im Gegensatz zur belebten Natur bezeichnet, womit jedoch eine Beschränkung impliziert wird, die so in der Physik nicht existiert. Die Ingenieurwissenschaften sind durch ihren engen Bezug zur praktischen technischen Anwendung von der Physik abgegrenzt, da in der Physik das Verständnis der grundlegenden Mechanismen im Vordergrund steht. Die Astronomie hat keine Möglichkeit, Laborexperimente durchzuführen, und ist daher allein auf Naturbeobachtung angewiesen, was hier zur Abgrenzung gegen die Physik herangezogen wird. Der Erkenntnisgewinn in der Physik verläuft in enger Verzahnung von Experiment und Theorie, besteht also ausempirischerDatengewinnung und -auswertungundgleichzeitig dem Erstellen theoretischer Modelle zu ihrerErklärung. Dennoch haben sich im Verlauf des 20. Jahrhunderts Spezialisierungen herausgebildet, die insbesondere die professionell betriebene Physik heute prägen. Demnach lassen sich grob Experimentalphysik und theoretische Physik voneinander unterscheiden. Während manche Naturwissenschaften wie etwa die Astronomie und dieMeteorologiesich methodisch weitgehend aufBeobachtungenihres Untersuchungsgegenstandes beschränken müssen, steht in der Physik das Experiment im Vordergrund. Die Experimentalphysik versucht durch Entwurf, Aufbau, Durchführung und Auswertung von Experimenten Gesetzmäßigkeiten aufzuspüren und mittels empirischer Modelle zu beschreiben. Sie versucht einerseits physikalisches Neuland zu betreten, andererseits überprüft sie von der theoretischen Physik gemachte Vorhersagen. Grundlage eines physikalischen Experimentes ist es, die Eigenschaften eines zuvor präparierten physikalischen Systems, zum Beispiel eines geworfenen Steins, eines eingeschlossenen Gasvolumens oder eines Teilchens bei einem Stoßprozess durchMessungin Zahlenform auszudrücken, etwa als Aufprallgeschwindigkeit, als Druck oder als Länge der beobachtbaren Teilchenspuren im Detektor. Konkret werden entweder nur die zeitunabhängigen (statischen) Eigenschaften eines Objektes gemessen oder es wird die zeitliche Entwicklung (Dynamik) des Systems untersucht, etwa indem Anfangs- und Endwerte einerMessgrößevor und nach dem Ablauf eines Vorgangs bestimmt werden oder indem kontinuierliche Zwischenwerte festgestellt werden. Dietheoretische Physiksucht dieempirischenModelleder Experimentalphysik mathematisch auf bekannte Grundlagentheorien zurückzuführen oder, falls dies nicht möglich ist,Hypothesenfür eine neue Theorie zu entwickeln, die dann experimentell überprüft werden können. Sie leitet weiterhin aus bereits bekannten Theorien empirisch überprüfbare Voraussagen ab. Bei der Entwicklung eines Modells wird grundsätzlich die Wirklichkeit idealisiert; man konzentriert sich zunächst nur auf ein vereinfachtes Bild, um dessen Aspekte zu überblicken und zu erforschen. Nachdem das Modell für diese Bedingungen ausgereift ist, wird es weiter verfeinert. Zur theoretischen Beschreibung eines physikalischen Systems benutzt man die Sprache der Mathematik. Seine Bestandteile werden dazu durch mathematische Objekte wie zum BeispielSkalareoderVektorenrepräsentiert, die in durchGleichungenfestgelegten Beziehungen zueinander stehen. Aus bekannten Größen werden unbekannte errechnet und damit zum Beispiel das Ergebnis einer experimentellen Messung vorhergesagt. Diese aufQuantitätenkonzentrierte Sichtweise unterscheidet die Physik maßgeblich von der Philosophie und hat zur Folge, dass nicht quantifizierbare Modelle, wie dasBewusstsein, nicht als Teil der Physik betrachtet werden. Das fundamentale Maß für den Erfolg einer naturwissenschaftlichen Theorie ist die Übereinstimmung mit Beobachtungen und Experimenten. Durch den Vergleich mit dem Experiment lassen sich der Gültigkeitsbereich und die Genauigkeit einer Theorie ermitteln; allerdings lässt sie sich niemals „beweisen“, bestenfalls in immer mehr Fällen bestätigen. Um eine Theorie zu widerlegen oder die Grenzen ihres Gültigkeitsbereiches zu zeigen, genügt im Prinzip ein einziges Experiment mit unerklärbarem Ergebnis, sofern es sich alsreproduzierbarerweist. Experimentalphysik und theoretische Physik stehen also in steter Wechselbeziehung zueinander. Es kann allerdings vorkommen, dass Ergebnisse der einen Disziplin der anderen vorauseilen: So sind derzeit viele Voraussagen derStringtheorienicht experimentell überprüfbar; andererseits sind viele teilweise sehr genau gemessene Werte aus dem Gebiet derTeilchenphysikzum heutigen Zeitpunkt (2022) durch die zugehörige Theorie, dieQuantenchromodynamik, nicht berechenbar. Zusätzlich zu dieser grundlegenden Teilung der Physik unterscheidet man manchmal noch weitere methodische Unterdisziplinen, vor allem diemathematische Physikund dieangewandte Physik. Auch die Arbeit mit Computersimulationen hat Züge eines eigenen Bereiches der Physik. Die mathematische Physik wird gelegentlich als Teilgebiet der theoretischen Physik betrachtet, unterscheidet sich von dieser jedoch darin, dass ihr Studienobjekt nicht konkrete physikalische Phänomene sind, sondern die Ergebnisse der theoretischen Physik selbst. Sie abstrahiert damit von jedweder Anwendung und interessiert sich stattdessen für diemathematischenEigenschaften eines Modells, insbesondere seine tiefer liegendenSymmetrien. Auf diese Weise entwickelt sie Verallgemeinerungen und neue mathematische Formulierungen bereits bekannter Theorien, die dann wiederum als Arbeitsmaterial der theoretischen Physiker in der Modellierung empirischer Vorgänge Einsatz finden können. Die angewandte Physik steht in (unscharfer) Abgrenzung zur Experimentalphysik, teilweise auch zur theoretischen Physik. Ihr wesentliches Kennzeichen ist, dass sie ein gegebenes physikalisches Phänomen nicht um seiner selbst willen erforscht, sondern um die aus der Untersuchung hervorgegangenen Erkenntnisse zur Lösung eines (in der Regel) nicht-physikalischen Problems einzusetzen. Ihre Anwendungen liegen auf dem Gebiet derTechnik, aber auch zum Beispiel in denWirtschaftswissenschaften, wo imRisikomanagementMethoden der theoretischen Festkörperphysik zum Einsatz kommen. Auch gibt es die interdisziplinären Bereiche derMedizinphysik,physikalischen Chemie,AstrophysikundBiophysik. Mit der fortschreitenden Entwicklung der Rechensysteme hat sich in den letzten Jahrzehnten des 20. Jahrhunderts, beschleunigt seit etwa 1990, dieComputersimulationals neue Methodik innerhalb der Physik entwickelt. Computersimulationen werden häufig als Bindeglied zwischen Theorie und Experiment verwendet, um Vorhersagen aus einer Theorie zu gewinnen, andererseits können Simulationen auch in Form einereffektiven Theorie, die ein experimentelles Ergebnis nachmodelliert, einen Impuls an die theoretische Physik zurückgeben. Naturgemäß hat dieser Bereich der Physik zahlreiche Anknüpfungspunkte an dieInformatik. Das Theoriengebäude der Physik beruht in seinem Ursprung auf derklassischen Mechanik. Diese wurde im 19. Jahrhundert um weitere Theorien ergänzt, insbesondere denElektromagnetismusund dieThermodynamik. Die moderne Physik beruht auf zwei Erweiterungen aus dem 20. Jahrhundert, derRelativitätstheorieund derQuantenphysik, die bestimmte Grundprinzipien der klassischen Mechanik verändert und verallgemeinert haben. Beide Theorien enthalten die klassische Mechanik über das sogenannteKorrespondenzprinzipals Grenzfall und haben daher einen größeren Gültigkeitsbereich als diese. Während die Relativitätstheorie teilweise auf denselben konzeptionellen Grundlagen beruht wie die klassische Mechanik, löst sich die Quantenphysik deutlich davon. Die klassische Mechanik wurde im 16. und 17. Jahrhundert maßgeblich vonGalileo Galileiund Isaac Newton begründet. Aufgrund der zu dieser Zeit noch recht begrenzten technischen Möglichkeiten sind die Vorgänge, die die klassische Mechanik beschreibt, weitgehend ohne komplizierte Hilfsmittel beobachtbar, was sie anschaulich erscheinen lässt. Die klassische Mechanik behandelt Systeme mit wenigen massiven Körpern, was sie von derElektrodynamikund der Thermodynamik unterscheidet. Raum und Zeit sind dabei nicht Teil der Dynamik, sondern ein unbewegter Hintergrund, vor dem physikalische Prozesse ablaufen und Körper sich bewegen. Wesentliche Grundbegriffe der Physik (wieGeschwindigkeit,Beschleunigung,Masse,Kraft,Energie) sind zuerst in der Klassischen Mechanik gebildet worden. Für sehr kleine Objekte tritt die Quantenphysik an die Stelle der klassischen Mechanik, während die Relativitätstheorie zur Beschreibung von Körpern mit sehr großen Geschwindigkeiten und Massen geeignet ist. Die mathematische Behandlung der klassischen Mechanik wurde im späten 18. und frühen 19. Jahrhundert in Form desLagrange-Formalismusund desHamilton-Formalismusentscheidend vereinheitlicht. Diese Formalismen sind auch mit der Relativitätstheorie anwendbar und sind daher ein bedeutender Teil der klassischen Mechanik. Obwohl die klassische Mechanik nur für mittelgroße, anschauliche Systeme gültig ist, ist die mathematische Behandlung komplexer Systeme bereits im Rahmen dieser Theorie mathematisch sehr anspruchsvoll. DieChaostheoriebefasst sich in großen Teilen mit solchen komplexen Systemen der klassischen Mechanik und ist derzeit (2009) ein aktives Forschungsgebiet. In der Elektrodynamik werden Phänomene mit bewegtenelektrischen Ladungenin Wechselwirkung mit zeitlich veränderlichen elektrischen und magnetischenFeldernbeschrieben. Um die Entwicklung der Theorien derElektrizitätund desMagnetismusim 18. und 19. Jahrhundert zusammenzuführen, wurde eine Erweiterung des Theoriengebäudes der klassischen Mechanik notwendig. Ausgangspunkt war das vonMichael FaradayentdeckteInduktionsgesetzund die nachHendrik Antoon LorentzbenannteLorentzkraftauf eine bewegte elektrische Ladung in einem Magnetfeld. Die Gesetze der Elektrodynamik wurden im 19. Jahrhundert vonJames Clerk Maxwellzusammengefasst und in Form derMaxwell-Gleichungenerstmals vollständig formuliert. Grundsätzlich wurden elektrodynamische Systeme mit den Methoden der klassischen Mechanik behandelt, allerdings ermöglichen die Maxwell-Gleichungen auch eine Wellenlösung, dieelektromagnetische Wellenwie das Licht beschreiben. Diese Theorie brachte unter anderem in Form derWellenoptikauch einen eigenen Formalismus hervor, der sich grundlegend von dem der klassischen Mechanik unterscheidet. Besonders dieSymmetriender Elektrodynamik sind mit denen der klassischen Mechanik unvereinbar. Dieser Widerspruch zwischen den beiden Theoriegebäuden wurde durch die spezielle Relativitätstheorie gelöst. Die Wellenoptik ist in Form dernichtlinearen Optiknoch heute (2011) ein aktives Forschungsgebiet. Etwa gleichzeitig mit der Elektrodynamik entwickelte sich mit der Thermodynamik ein weiterer Theorienkomplex, der sich grundlegend von der klassischen Mechanik unterscheidet. Im Gegensatz zur klassischen Mechanik stehen in der Thermodynamik nicht einzelne Körper im Vordergrund, sondern einEnsembleaus vielen kleinsten Bausteinen, was zu einem radikal anderen Formalismus führt. Die Thermodynamik eignet sich damit zur Behandlung von Medien allerAggregatzustände. Die Quantentheorie und die Relativitätstheorie lassen sich in den Formalismus der Thermodynamik einbetten, da sie nur die Dynamik der Bausteine des Ensembles betreffen, aber den Formalismus zur Beschreibung thermodynamischer Systeme nicht prinzipiell ändern. Die Thermodynamik eignet sich beispielsweise zur Beschreibung vonWärmekraftmaschinenaber auch zur Erklärung vieler moderner Forschungsgegenstände wieSupraleitungoderSuprafluidität. Besonders im Bereich derFestkörperphysikwird daher auch heute (2009) noch viel mit den Methoden der Thermodynamik gearbeitet. Die vonAlbert Einsteinbegründete Relativitätstheorie führt ein völlig neues Verständnis derPhänomeneRaum und Zeit ein. Danach handelt es sich bei diesen nicht um universell gültige Ordnungsstrukturen, sondern räumliche und zeitliche Abstände werden von verschiedenen Beobachtern unterschiedlich beurteilt. Raum und Zeit verschmelzen zu einer vierdimensionalenRaumzeit. DieGravitationwird auf eineKrümmungdieser Raumzeit zurückgeführt, die durch die Anwesenheit vonMassebzw.Energiehervorgerufen wird. In der Relativitätstheorie wird erstmals dieKosmologiezu einem naturwissenschaftlichen Thema. Die Formulierung der Relativitätstheorie stellt gleichzeitig den Beginn dermodernen Physikund die Vollendung derklassischen Physikdar. Die Quantenphysik beschreibt dieNaturgesetzeimatomarenund subatomaren Bereich und bricht noch radikaler mit klassischen Vorstellungen als die Relativitätstheorie. In der Quantenphysik sind auchphysikalische Größenselbst Teil des Formalismus und keine bloßen Kenngrößen mehr, die ein System beschreiben. Der Formalismus unterscheidet also zwischen zwei Typen von Objekten, denObservablen, die die Größen beschreiben und denZuständen, die das System beschreiben. Ebenso wird der Messprozess aktiv in die Theorie miteinbezogen. Dies führt in bestimmten Situationen zurQuantisierungder Größenwerte. Das heißt, die Größen nehmen stets nur bestimmtediskrete Wertean. In derQuantenfeldtheorie, der am weitesten entwickelten relativistischen Quantentheorie, tritt auch Materie nur in Portionen, denElementarteilchenoderQuanten, in Erscheinung. Die Gesetze der Quantenphysik entziehen sich weitgehend der menschlichenAnschauung, und über ihreInterpretationherrscht auch heute noch kein Konsens. Dennoch zählt sie hinsichtlich ihresempirischenErfolges zu dem am besten gesicherten Wissen der Menschheit überhaupt. Die Theorien der Physik kommen in verschiedenen Themenbereichen zum Einsatz. Die Einteilung der Physik in Unterthemen ist nicht eindeutig und die Abgrenzung der Unterthemen gegeneinander ist dabei ähnlich schwierig wie die Abgrenzung der Physik zu anderen Wissenschaften. Es gibt dementsprechend viele Überschneidungen und gegenseitige Beziehungen der verschiedenen Bereiche zueinander. Hier wird eine Sammlung von Themengebieten nach betrachteter Größenordnung der Objekte dargestellt und im Zuge dessen auf Themengebiete verwiesen, die damit verwandt sind. Die aufgeführten Themen lassen sich nicht eindeutig einer Theorie zuordnen, sondern bedienen sich je nach dem untersuchten Gegenstand verschiedener theoretischer Konzepte. Die Teilchenphysik befasst sich mit Elementarteilchen und ihren Wechselwirkungen untereinander. Von denvier Grundkräftender Physik wird die Gravitation derzeit ausgespart, weil es noch keine Theorie derQuantengravitationgibt, die die gravitativen Wechselwirkungen von Elementarteilchen beschreiben kann. Die anderen drei Wechselwirkungen werden durch den Austausch von Elementarteilchen, sogenanntenEichbosonen, beschrieben. In der Teilchenphysik werden relativistische Quantentheorien zur Beschreibung der Phänomene verwendet. Eines der Ziele der Teilchenphysik ist es, alle Grundkräfte in einem vereinheitlichten Gesamtkonzept zu beschreiben (Weltformel). Bisher ist es jedoch lediglich gelungen, dieelektrischeund diemagnetischeWechselwirkung zur elektromagnetischen Wechselwirkung zu vereinigen sowie diese und die schwache Wechselwirkung als Auswirkungen einer sogenanntenelektroschwachen Wechselwirkungdarzustellen. Zur Vereinigung der elektroschwachen und der starken Wechselwirkung wurde unter anderem die Theorie derSupersymmetrieerdacht, deren weitere Voraussagen bislang jedoch nicht experimentell bestätigt werden konnten. Die größten Schwierigkeiten treten wie bereits erwähnt im Bereich der Gravitationskraft auf, da einerseits Elementarteilchen nur im Rahmen der Quantentheorie beschrieben werden können, andererseits noch keine Theorie der Quantengravitation vorliegt. Typische Experimente zur Überprüfung der Theorien der Teilchenphysik werden anTeilchenbeschleunigernbei Kollisionen von Teilchen hoher Energie durchgeführt. Daher wird der Begriff derHochenergiephysikoft nahezu deckungsgleich mit dem Begriff der Teilchenphysik verwendet. Um hohe Kollisionsenergien zu erreichen, werden vor allemCollider-Experimente eingesetzt, bei denen Teilchen nicht auf ein festes Ziel, sondern gegeneinander geschossen werden. Der Teilchenbeschleuniger mit der derzeit höchsten Kollisionsenergie ist der 2011 in Betrieb gegangeneLarge Hadron Collider. Eine andere bedeutende Experimentklasse dient der Erforschung derNeutrinos, wofür spezielleNeutrinodetektorenkonzipiert werden wie beispielsweise derSuper-Kamiokande. Die Elementarteilchen, die der starken Wechselwirkung unterliegen, die sogenanntenQuarks, kommen nicht einzeln, sondern immer nur in gebundenen Zuständen, denHadronen, vor, zu denen unter anderem dasProtonund dasNeutrongehören. Die Hadronenphysik hat viele Überschneidungen mit der Elementarteilchenphysik, da viele Phänomene nur erklärt werden können, indem berücksichtigt wird, dass die Hadronen aus Quarks aufgebaut sind. Die Beschreibung der starken Wechselwirkung durch die Quantenchromodynamik, eine relativistische Quantenfeldtheorie, kann jedoch die Eigenschaften der Hadronen nicht vorhersagen, weshalb die Untersuchung dieser Eigenschaften als eigenständiges Forschungsgebiet aufgefasst wird. Es wird also eine Erweiterung der Theorie der starken Wechselwirkung für kleine Energien angestrebt, bei denen sich die Hadronen bilden. Atomkerne stellen gegenüber Elementarteilchen die nächste Komplexitätsstufe dar. Sie bestehen aus mehrerenNukleonen, also Protonen und Neutronen, deren Wechselwirkungen untersucht werden. In Atomkernen herrschen die starke und die elektromagnetische Wechselwirkung vor. Forschungsgebiete der Atomkernphysik umfassenradioaktive Zerfälleund Stabilität von Atomkernen. Ziel ist dabei die Entwicklung vonKernmodellen, die diese Phänomene erklären können. Dabei wird aber auf eine detaillierte Ausarbeitung der starken Wechselwirkung wie in der Hadronenphysik verzichtet. Zur Erforschung der Eigenschaften von Hadronen werden Teilchenbeschleuniger eingesetzt, wobei hier der Schwerpunkt nicht so sehr wie in der Teilchenphysik auf hohen Kollisionsenergien liegt. Stattdessen werdenTarget-Experimente durchgeführt, die zwar geringereSchwerpunktsenergien, aber sehr viel höhere Ereigniszahlen liefern. Allerdings werden auch Collider-Experimente mitSchwerionenvor allem eingesetzt, um Erkenntnisse über Hadronen zu gewinnen. In der Kernphysik werden zur Erzeugung vonTransuranenschwere Atome zur Kollision gebracht undRadioaktivitätmit einer Vielzahl experimenteller Aufbauten untersucht. Atome bestehen aus dem Atomkern und meist mehreren Elektronen und stellen die nächste Komplexitätsstufe der Materie dar. Ziel der Atomphysik ist es unter anderem, dieLinienspektrender Atome zu erklären, wozu eine genaue quantenmechanische Beschreibung der Wechselwirkungen der Elektronen der Atome notwendig ist. Da Moleküle aus mehreren Atomen aufgebaut sind, arbeitet die Molekülphysik mit ähnlichen Methoden, allerdings stellen insbesondere große Moleküle meist deutlich komplexere Systeme dar, was die Rechnungen sehr viel komplizierter und häufig den Einsatz von Computersimulationen erforderlich macht. Die Atom- und Molekülphysik stehen über die Untersuchung der optischen Spektren von Atomen und Molekülen mit der Optik in enger Beziehung. So baut beispielsweise das Funktionsprinzip desLasers, einer bedeutenden technischen Entwicklung, maßgeblich auf den Ergebnissen der Atomphysik auf. Da die Molekülphysik sich auch intensiv mit der Theorie derchemischen Bindungenbefasst, sind in diesem Themengebiet Überschneidungen mit der Chemie vorhanden. Ein wichtiger experimenteller Zugang besteht in der Einwirkung von Licht. So werden beispielsweise optische Spektren von Atomen und Molekülen mit ihren quantenmechanischen Eigenschaften in Verbindung gesetzt. Umgekehrt kann dann mit spektroskopischen Methoden die Zusammensetzung eines Stoffgemisches untersucht werden und anhand von Sternenlicht Aussagen über die Elemente in der Sternenatmosphäre getroffen werden. Andere Untersuchungsmethoden betrachten das Verhalten unter dem Einfluss von elektrischen und magnetischen Feldern. Beispiele sind dieMassenspektroskopieoder diePaulfalle. Die Physik der kondensierten Materie und die Fluiddynamik sind in dieser Auflistung das Gebiet mit der größten thematischen Bandbreite, von derFestkörperphysikbis zurPlasmaphysik. All diesen Bereichen ist gemeinsam, dass sie sich mit makroskopischen Systemen aus sehr vielen Atomen, Molekülen oderIonenbefassen. Dementsprechend ist in allen Bereichen dieses Themengebiets die Thermodynamik ein wichtiger Teil des theoretischen Fundamentes. Je nach Problem kommen aber auch Quantentheorie und Relativitätstheorie zum Einsatz, um die Systeme zu beschreiben. Auch Computersimulationen sind ein fester Bestand der Forschung an solchen Vielteilchensystemen. Aufgrund der thematischen Bandbreite existieren Überschneidungen mit nahezu allen anderen Gebieten der Physik, zum Beispiel mit der Optik in Form laseraktiver Medien oder nichtlinearer Optik, aber auch mit der Akustik, Atom-, Kern- und Teilchenphysik. Auch in der Astrophysik spielt die Fluiddynamik eine große Rolle bei der Erstellung von Modellen zur Entstehung und zum Aufbau von Sternen sowie bei der Modellierung vieler anderer Effekte. Viele Forschungsbereiche sind dabei sehr anwendungsorientiert, wie dieMaterialforschung, die Plasmaphysik oder die Erforschung derHochtemperatursupraleiter. Die Bandbreite der experimentellen Methoden in diesem Bereich der Physik ist sehr groß, sodass sich keine typischen Methoden für das ganze Gebiet angeben lassen. Die quantenmechanischen Effekte wieSupraleitungundSuprafluidität, die eine gewisse Bekanntheit erlangt haben, werden derTieftemperaturphysikzugerechnet, die mit typischen Kühlungsmethoden einhergeht. Astrophysik und Kosmologie sind interdisziplinäre Forschungsgebiete, die sich stark mit der Astronomie überschneiden. Nahezu alle anderen Themenbereiche der Physik gehen in die astrophysikalischen Modelle ein, um Prozesse auf verschiedenen Größenskalen zu modellieren. Ziel dieser Modelle ist es, astronomische Beobachtungen auf der Grundlage der bisher bekannten Physik zu erklären. Die Kosmologie baut insbesondere auf den Grundlagen der allgemeinen Relativitätstheorie auf, allerdings sind im Rahmen derQuantenkosmologieauch die Quantentheorien sehr bedeutsam um die Entwicklung des Universums in sehr viel früheren Phasen zu erklären. Das derzeit (2009) am meisten vertretenekosmologische Standardmodellbaut dabei maßgeblich auf den Theorien derDunklen Materieund derDunklen Energieauf. Weder Dunkle Materie noch Dunkle Energie konnte bisher direkt experimentell nachgewiesen werden, es existieren aber eine Vielzahl von Theorien, was genau diese Objekte sind. Da in der Astrophysik nur in sehr beschränktem Ausmaß Experimente möglich sind, ist dieses Teilgebiet der Physik sehr stark auf die Beobachtung unbeeinflussbarer Phänomene angewiesen. Dabei kommen auch Erkenntnisse der Atomphysik und der Teilchenphysik und typische Messmethoden dieser Fachgebiete zur Anwendung, um Rückschlüsse auf astrophysikalische oder kosmologische Zusammenhänge zu ziehen. Beispielsweise geben die Spektren von Sternenlicht Auskunft über die Elementverteilung der Sternenatmosphäre, die Untersuchung derHöhenstrahlungerlaubt Rückschlüsse auf diekosmische Strahlungund Neutrinodetektoren messen nach einerSupernovaeinen erhöhten Neutrinostrom, der gleichzeitig mit dem Licht der Supernova beobachtet wird. Methoden der Physik finden in vielen Themengebieten Anwendung, die nicht zum Kernthemenbereich der Physik gehören. Einige dieser Anwendungen sind in den vorigen Kapiteln bereits angesprochen worden. Die folgende Aufzählung gibt einen kurzen Überblick über die wichtigsten interdisziplinären Themenbereiche. Der derzeitige Stand der Physik ist nach wie vor mit noch ungelösten Problemen konfrontiert. Zum einen handelt es sich dabei um den weniger grundsätzlichen Fall von Problemen, deren Lösung prinzipiell möglich, aber mit den derzeitigen mathematischen Möglichkeiten bestenfalls annäherbar ist. Zum anderen gibt es eine Reihe von Problemen, für die noch unklar ist, ob eine Lösung im Begriffsrahmen der heutigen Theorien überhaupt möglich sein wird. So ist es bislang nicht gelungen, eine vereinheitlichte Theorie zu formulieren, welche sowohl Phänomene beschreibt, die der elektroschwachen wie der starken Wechselwirkung unterliegen, wie auch solche, welche der Gravitation unterliegen. Erst bei einer solchen Vereinigung von Quantentheorie und Gravitationstheorie (allgemeiner Relativitätstheorie) könnten alle vier Grundkräfte einheitlich behandelt werden, sodass eine vereinheitlichte Theorie der Elementarteilchen resultierte. Die bisherigen Kandidaten vonQuantengravitations­theorien,SupersymmetrieundSupergravitations-,String-undM-Theorienversuchen, eine solche Vereinheitlichung zu erreichen. Überhaupt ist es ein praktisch leitendes Ziel heutiger Physiker, sämtliche Vorgänge der Natur durch eine möglichst geringe Anzahl von möglichst einfachenNaturgesetzenzu beschreiben. Diese sollen das Verhalten möglichst grundlegender Eigenschaften und Objekte (etwaElementarteilchen) beschreiben, sodass höherstufige (emergente) Prozesse und Objekte auf diese Beschreibungsebene reduzierbar sind. Ob dieses Ziel prinzipiell oder praktisch erreichbar ist, ist eigentlich nicht mehr Gegenstand der einzelwissenschaftlichen physikalischen Erkenntnisbemühung, ebenso wenig, wie es allgemeine Fragen darüber sind, welchen Gewissheitsgrad physikalische Erkenntnisse grundsätzlich erreichen können oder faktisch erreicht haben. Derartige Fragen sind Gegenstand derEpistemologieundWissenschaftstheorie. Dabei werden ganz unterschiedliche Positionen verteidigt. Relativ unbestritten ist, dass naturwissenschaftliche Theoriebildungen in dem Sinne nur Hypothesen sind, dass man nicht mit Gewissheit wissen kann, ob es sich dabei um wahre und gerechtfertigte Auffassungen handelt. Man kann hier noch in spezifischerer Weise vorsichtig sein, indem man sich auf die Theorie- und Begriffsvermitteltheit aller empirischen Erkenntnisse beruft oder auf die Tatsache, dass der Mensch als erkennendes Subjekt ja unter den Gegenstandsbereich physikalischer Theorien fällt, aber nur als wirklich Außenstehender sicheres Wissen haben könnte. Denn für Beobachter, die mit ihremErkenntnisobjektinteragieren, bestehen prinzipielle Grenzen der Prognostizierbarkeit im Sinne einer Ununterscheidbarkeit des vorliegenden Zustandes – eine Grenze, die auch dann gelten würde,[6]wenn der Mensch alle Naturgesetze kennen würde und die Welt deterministisch wäre. Diese Grenze hat praktische Bedeutung bei deterministischen Prozessen, für welche geringe Änderungen des Anfangszustands zu großen Abweichungen in Folgezuständen führen – Prozesse, wie sie durch dieChaostheoriebeschrieben werden. Aber nicht nur eine praktische Voraussagbarkeit ist in vielen Fällen nur begrenzt möglich, auch wird von einigen Wissenschaftstheoretikern eineAussage­fähigkeit physikalischer Modelle über die Realität überhaupt bestritten. Dies gilt in verschiedenen Ausarbeitungen eines sogenanntenwissenschaftstheoretischen Antirealismusin unterschiedlichem Ausmaß: für unterschiedliche Typen physikalischer Begriffe wird eine reale Referenz bestritten oder für unwissbar gehalten.[7]Auch eine prinzipielle oder wahrscheinliche Zusammenführbarkeit einzelner Theorien wird von einigen Wissenschaftstheoretikern bestritten.[8] Die Beziehungen zurPhilosophiesind traditionell eng, hat sich doch die Physik aus der klassischen Philosophie entwickelt, ohne ihr jemals grundsätzlich zu widersprechen, und waren nach heutigen Kategorien zahlreiche bedeutende Physiker zugleich wichtige Philosophen und umgekehrt. Gemäß der heutigen philosophischen Disziplinenunterscheidung ist die Physik insbesondere auf dieOntologiebezogen, welche die Grundstrukturen der Realität in möglichst allgemeinen Begriffen zu beschreiben versucht, darüber hinaus auf dieErkenntnistheorie, welche die Gütekriterien von Wissen überhaupt zu erfassen versucht, spezieller noch auf dieWissenschaftstheorie, welche die allgemeinen Methoden wissenschaftlicher Erkenntnis zu bestimmen versucht und natürlich auf dieNaturphilosophiebzw.Philosophie der Physik, die oftmals als Unterdisziplin der Ontologie oder Wissenschaftstheorie behandelt wird, jedenfalls aber spezieller gerade auf die Einzelerkenntnisse der Physik bezogen arbeitet, deren Begriffssystem analysiert und ontologische Interpretationen physikalischer Theorien diskutiert. Auch die Beziehungen zurMathematiksind eng. Die gesamte Physik verwendet die mathematische Sprache. Zahlreiche bedeutende Physiker waren nach heutigen Kategorien zugleich wichtige Mathematiker und umgekehrt. Gemäß der heutigen mathematischen Disziplinenunterscheidung ist die Physik insbesondere auf dieGeometriebezogen, die die Grundstrukturen des Raumes in möglichst allgemeinen Begriffen zu beschreiben versucht, darüber hinaus auf dieAlgebra, spezieller noch auf dieAlgebraische Geometrie, auf dieDifferentialgeometrieund dieMathematische Physik. Da die Physik als die grundlegende Naturwissenschaft gilt, werden physikalisches Wissen und Denken bereits in der Schule meist im Rahmen eines eigenen Schulfaches unterrichtet. Im Rahmen des Schulsystems wird Physik in der Regel als Nebenfach ab Klassenstufe 5–7 unterrichtet und wird in der Oberstufe oft auch als Leistungskurs geführt. Verschiedene Serien existieren. Einige Beispiele sind: Es existieren verschiedene Werke. Einige der Bekannten sind: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte von Begriff und Disziplin der Physik 2Methodik 2.1Experimentalphysik 2.2Theoretische Physik 2.3Weitere Aspekte 2.3.1Mathematische Physik 2.3.2Angewandte Physik 2.3.3Simulation und Computerphysik 3Theoriengebäude 3.1Klassische Mechanik 3.2Elektrodynamik und Optik 3.3Thermodynamik 3.4Relativitätstheorie 3.5Quantenphysik 4Themenbereiche der modernen Physik 4.1Teilchenphysik 4.2Hadronen- und Atomkernphysik 4.3Atom- und Molekülphysik 4.4Kondensierte Materie und Fluiddynamik 4.5Astrophysik und Kosmologie 4.6Interdisziplinäre Themenbereiche 5Grenzen der physikalischen Erkenntnis 6Beziehung zu anderen Wissenschaften 7Physik in der Gesellschaft 8Siehe auch 9Literatur 9.1Einsteiger 9.2Fachbücher 9.3Referenzwerke und Monographien 9.4Lexika und Enzyklopädien 9.5Sachbücher und Andere"
  },
  {
    "label": 0,
    "text": "Politik – Wikipedia Politik Inhaltsverzeichnis Wortherkunft Politikbegriffe Mehrdimensionaler Politikbegriff der jüngeren politikwissenschaftlichen Diskussion Zivilitätstheoretische Diskussion des mehrdimensionalen Politikbegriffs Abgrenzung von Politisch und Sozial – Politik im engeren und weiteren Sinn Kurze Entwicklungsgeschichte wichtiger politischer Konzeptionen Zentrale politische Begriffe Politische Systeme und Ideologien Klassische politische Denker Siehe auch Literatur Weblinks Einzelnachweise Regierungszentriert versus emanzipatorisch Normativ versus deskriptiv Konfliktorientiert versus konsensbezogen Policy: normative, inhaltliche Dimension Politics: prozessuale Dimension Polity: formale, institutionelle Dimension Altertum Mittelalter Neuzeit Politikbezeichnet die Strukturen(Polity), Prozesse(Politics)und Inhalte(Policy)zur Regelung der Angelegenheiten einesGemeinwesensdurch allgemein verbindliche und somit in der Regel auf politischerMachtberuhendeEntscheidungen.[1] Politik regelt dabei insbesondere das öffentliche, aber teilweise auch das private (Zusammen-)Leben derBürger, die Handlungen und Bestrebungen zur Führung des Gemeinwesens nach innen und außen sowie die Willensbildung und Entscheidungsfindung über Angelegenheiten des Gemeinwesens.[2][3][4]Abstrakt formuliert wird in derPolitikwissenschaftauch von der „Verteilung von Werten (materiellen wieGeldoder nicht-materiellen wieDemokratie)“ gesprochen.[5] Der Ausdruck Politik wurde, mit Umwegen über dasLateinische(politica,politicus, woraus auchPolitikumabgeleitet ist), nachaltgriechischπολιτικάpolitikágebildet. Dieses Wort bezeichnete in denStadtstaatendesantiken Griechenlandsalle diejenigen Tätigkeiten, Gegenstände und Fragestellungen, die das Gemeinwesen – und das hieß zu dieser Zeit: diePolis– betrafen. Entsprechend ist die wörtliche Übersetzung vonpolitikáanzugeben als „Dinge, die dieStadtbetreffen“ bzw. die „politischen Dinge“. In dieser Bedeutung ist „Politik“ vergleichbar mit dem römischen Begriff derres publica, aus dem der moderne Terminus der „Republik“ hervorgegangen ist. Eine begriffsgeschichtlich besonders prominente Verwendung fand das Wort als Titel eines Hauptwerks des antiken PhilosophenAristoteles, derPolitik(Πολιτικά). „Politik ist die Summe der Mittel, die nötig sind, um zur Macht zu kommen und sich an der Macht zu halten und um von der Macht den nützlichsten Gebrauch zu machen“ „Die politische Wissenschaft … lässt sich als derjenige Spezialzweig der Sozialwissenschaften definieren, der sachlich-kritisch den Staat unter seinem Machtaspekt sowie alle sonstigen Machtphänomene unter Einbeziehung sonstiger Zielsetzungen insoweit untersucht, wie diese Machtphänomene mehr oder weniger unmittelbar mit dem Staat zusammenhängen.“ „Politik ist das Streben nach Machtanteil oder nach Beeinflussung der Machtverteilung…“ „Politik ist die Lehre von den Staatszwecken und den besten Mitteln (Einrichtungen, Formen, Thätigkeiten) zu ihrer Verwirklichung.“ „Politik ist der Komplex sozialer Prozesse, die speziell dazu dienen, das Akzept administrativer (Sach-) Entscheidungen zu gewährleisten. Politik soll verantworten, legitimieren und die erforderliche Machtbasis für die Durchsetzung der sachlichen Verwaltungsentscheidungen liefern.“ „Unter Politik verstehen wir den Begriff der Kunst, die Führung menschlicher Gruppen zu ordnen und zu vollziehen.“ „Politik ist die Führung von Gemeinwesen auf der Basis von Machtbesitz.“ „Beziehungen der Überordnung und Unterordnung und ihre Auswirkungen auf das Verhalten der Menschen zu untersuchen (ist das Ziel der Politikwissenschaft).“ „Politik ist Kampf um die rechte Ordnung.“ „Der Gegenstand und das Ziel der Politik ist der Friede … der Friede ist die politische Kategorie schlechthin.“ „Politische Wissenschaft ist die Wissenschaft von der Freiheit.“ „Praktisch-kritische politische Wissenschaft zielt auf eine politische Theorie, die die Befunde der Gesellschaftskritik integriert. Im Begriff der Demokratie gewinnt sie einen Leitbegriff für die Analyse der politisch relevanten Herrschaftsstrukturen der Gesellschaft.“ „Politik ist die Gesamtheit aller Aktivitäten zur Vorbereitung und Herstellung gesamtgesellschaftlich verbindlicher und/oder am Gemeinwohl orientierter und der ganzen Gesellschaft zugute kommender Entscheidungen.“ „Politik (ist) gesellschaftliches Handeln, … welches darauf gerichtet ist, gesellschaftliche Konflikte über Werte verbindlich zu regeln.“ „Politik ist die autoritativ (von Regierenden, von Herrschenden) verfügte Verteilung von materiellen und immateriellen Werten in der Gesellschaft.“ „Politik ist der Kampf um die Veränderung oder Bewahrung bestehender Verhältnisse.“ „Politik (ist) der alle Bereiche des gesellschaftlichen Lebens durchdringende Kampf der Klassen und ihrer Parteien, der Staaten und der Weltsysteme um die Verwirklichung ihrer sozialökonomisch bedingten Interessen und Ziele.“ DiekontroversenPolitikbegriffe und -definitionen können in drei Dimensionen sortiert werden, ohne dass diese sich untereinander ausschlössen. Zu den regierungszentrierten oder gouvernementalen Politikbegriffen gehören die KonzepteMacht, HerrschaftundFührung.Im 19. Jahrhundert galt derStaatund seineMacht(Gewaltmonopol) als das Hauptwesen der Politik. Alle Machtphänomene wurden versucht dem Staat zuzuordnen. In den internationalen Beziehungen ist Macht bis heute einer der Grundpfeiler der Theoriebildung (vgl. zum BeispielPolitischer Neorealismus).Kurt Sontheimer(1962) weist auf die Gefahr hin, dassPolitikwissenschaftbei diesem Politikverständnis leicht zum Handlanger der Macht und der Mächtigen werden kann. EmanzipatorischePolitikauffassungen konzentrieren sich dagegen auf Machtbeschränkungen durchPartizipation,GleichheitundDemokratisierungals Gegengewicht zu einer ordnenden Macht. Dazu gehört auch die kritische Analyse der vorherrschenden Herrschaftsstrukturen undGesellschaftskritik. Zu den normativen Politikbegriffen lassen sich die Konzepterechte Ordnung,Frieden,FreiheitundDemokratiezählen und insbesondere auch alle emanzipatorischen Politikdefinitionen. Dabei geht es nicht um die reine Beschreibung politischer Phänomene, sondern es wird ein wertender Soll- oder Zielwert als Hauptkategorie eingesetzt. Das Konzept Freiheit kann zum Beispiel als ein Gegenbegriff zum KonzeptMachtoderHerrschaftverstanden werden. Meist werden harmonischeGemeinwohlvorstellungenangeboten, die sich nur schwer mit den heutigenpluralistischenGesellschaftsbedingungen vereinbaren lassen. Ein spezielles Problem der KategorieFriedenist, dass sie nicht bloß die Abwesenheit von Gewalt, sondern auch den Abbau von Ungleichheiten meinen kann. Die rein deskriptiven, also beschreibenden, Politikvorstellungen lehnen Sollwerte als Wesen der Politik ab. Zu ihnen zählen die in der Einleitung gegebene Politikdefinition, diejenige von Lehmbruch und die vonDavid Easton(authoritative allocation of values;Systemtheorie). Ebenso wie die regierungszentrierten, Macht betonenden Politikbegriffe stehen diese in Gefahr, denStatus quozu stabilisieren und den gerade Herrschenden zu nutzen. Konfliktorientierte Politikbegriffe gehen von der Existenz vonKonfliktenals unabänderlichen und notwendigen Erscheinungen des politisch-sozialen Lebens aus. Diese Konflikte müssten durch die politischen Prozesse geregelt werden. Die Voraussetzung für die Verwendung der Kategorie Konflikt ist, dass eine hinreichend flexible und stabile Gesellschaftsstruktur vorhanden ist, die die friedliche Konfliktaustragung zwischen den verschiedenensozialen Gruppenmit ihren divergierendenInteressenermöglicht. Zu den konfliktorientierten Politikbegriffen gehören neben dem deskriptiven systemtheoretischen Politikverständnis auch die Konflikttheorien vonRalf DahrendorfundLewis Coser, die Konflikte als die Triebkräfte jedes sozialen Wandels begreifen. Auch dermarxistischePolitikbegriff fußt auf Konflikt als Grundkategorie, nämlich dem Kampf derKlassenund ihrerParteienum die Durchsetzung ihrer primär sozialökonomisch bedingten Interessen. Im Gegensatz dazu ist bei konsensbezogenen Politikbegriffen das gesellschaftlicheGemeinwohlnur durchKonsensherstellbar. Zu diesen Politikbegriffen zählt neben dem klassischen emanzipatorischen PolitikverständnisJean-Jacques Rousseausauch der Politikbegriff von Thomas Meyer. Auch ohne Entscheidung überdieHauptkategorie von Politik kann man drei Dimensionen unterscheiden, die uns eine begriffliche Klärung und Unterscheidung der komplexen Wirklichkeit der in verschiedener Gestalt auftretenden Politik ermöglichen. Dafür haben sich im deutschsprachigen Raum die englischen BezeichnungenPolicy,PoliticsundPolityeingebürgert.[6] UnterschiedlichenormativeVorstellungen (wie etwas sein sollte) über den Inhalt, also Aufgaben und Ziele, von Politik, führen aufgrund begrenzter Mittel (Ressourcenknappheit) dazu, dass nicht alle Wünsche befriedigt werden können. Es kommt zu Interessenkonflikten innerhalb der unterschiedlichsten Politikbereiche, wieSicherheitspolitik,Wirtschaftspolitik,Sozialpolitikund viele weiteren. Diese Konflikte müssen im Sinne der Stabilität des politischen Systems durchKompromisseund folgende allgemeinverbindliche Entscheidungen vermittelt werden. Policysteht also für die inhaltliche Dimension der Politik. Bezüglich der Politik einerParteioderRegierungumfasst der Begriff, was diese zu tun beabsichtigt bzw. auch tut. Dazu gehören neben den von einer Regierung vergebenen und bewilligten materiellenGüternauch immaterielle Aspekte. Da aber die allermeisten Maßnahmen der Politik eine materiell-ökonomische Seite besitzen, können die öffentlichenHaushalteoder die eingebrachten Haushaltsentwürfe einen Eindruck geben welchepolicyein Land bzw. eine Regierung umsetzt. Wenn im Alltag von „guter“ und „schlechter Politik“ gesprochen wird, dann ist damit in der Regel diepolicyder Regierung gemeint. Insofern als die Bevölkerung damit beurteilt, was bei einer bestimmten Politik für wen dabei herauskommt, ist dies die Sicht der von politischen Entscheidungen Betroffenen. Die Beurteilungskriterien sind dabei in denpluralistischenGesellschaftenallerdings in der Regel sehr verschieden, abhängig von den jeweiligenWert- undGerechtigkeitsvorstellungen, abhängig davon, mit welchen gesellschaftlichen Gebilden (einer bestimmten gesellschaftlichen Gruppe oderKlasse, derNationoder einem über die Landesgrenzen hinausreichenden gesellschaftlichenKollektiv) sich identifiziert wird. Da es in derpolicystets um gesellschaftliche Inhalte, Werte und Interessen geht, geht es nie nur um die Antwort auf die Frage nach der besten Politik. Vielmehr stehen auch die an politischenEntscheidungsprozessenBeteiligten und die Konsequenzen der Entscheidung für den Einzelnen im Fokus der Analyse. Ebenfalls relevant ist die Frage nach den Begünstigten und den Belasteten. Kategorien:Politisches Problem; Programme, Ziele, Lösungen; Ergebnisse der Politik; Bewertung der Politik nach der räumlichen Abgrenzung: Die ablaufenden politischen Willensbildungs- und Interessenvermittlungsprozesse prägen die möglichen Ergebnisse derpolicymaßgeblich. BesondersMachtund ihre Durchsetzung im Rahmen der formellen und informellen Regeln bestimmen diesepolitics-Prozesse (Regierungskunst im weitesten Sinne) zusätzlich. In liberal-demokratischen Systemen (moderneDemokratie, mitRechtsstaatundMarktwirtschaft) wird die Akzeptanz der Kompromissbildung dadurch erhöht, dass frühzeitig neben denParteienauch gesellschaftlicheInteressengruppen(Lobbyverbände wieGewerkschaftenundUnternehmensverbände) und Einzelpersonen in den Prozess der Entscheidungsfindung eingebunden werden. Bei der Entwicklung und Beeinflussung derpolicyzeigt sich die Politik von ihrer konflikthaften Seite, dem Kampf um Macht und Einfluss der verschiedenen Gruppen und Personen. Damit inhaltliche Handlungsprogramme umgesetzt werden können, bedarf es neben der Erringung, dem Erhalt und dem Ausbau von Machtpositionen, auch der geschickten Auswahl des politischen Führungspersonals, der Formulierung der Wünsche und Interessen der gesellschaftlichen Gruppen, der Abstimmung mit anderen Forderungen und Interessen, um so ein umfassendes Handlungsprogramm anbieten zu können und wählbar zu sein. Dies erfordert die ständige Berücksichtigung anderer Menschen (Wähler, Parteikollegen usw.), deren mögliche Reaktionen bei der Erstellung und Durchführung derpolicyvon vornherein mit einkalkuliert, antizipiert, werden müssen. Gerade in demokratischen Systemen geht es also auch immer um das Sammeln von Zustimmung und Einwilligung zu den Handlungsprogrammen. Für diePolitikerselbst ist aber daher auch der Aspekt des Kampfes um Entscheidungsbefugnis, welches mehr umfasst als die Erlangung der staatlichen Machtpositionen, entscheidend. Denn im Gegensatz zu typischen Verwaltungsbeamten, derenKompetenzbereichklar über dasAmtgeregelt ist, muss sich der Politiker diesen Bereich erst erarbeiten und dann behaupten. Daher ist es für ihn zu wenig, nur die rein sachlichen Gesichtspunkte bei seiner Entscheidungsfindung zu berücksichtigen. Die Aspekte des Machterwerbs und des Machterhalts sind gerade in demokratischen, eben responsiven, Systemen besonders wichtig; insofern ist gerade die Demokratie eine hochpolitischeRegierungsform. Politicsspielt aber auch inautoritären Systemeneine Rolle, in denen die Führer weniger Rücksicht auf die Bevölkerung nehmen müssen. Solange die Handelnden unter einem gewissen Zwang zur Rücksichtnahme auf andere Akteure stehen und versuchen müssen, Zustimmungsbereitschaft zu erzeugen, mit welchen Mitteln auch immer, kann vonpoliticsgesprochen werden. Auf welche Art die Zustimmung geschaffen wird (Interessenberücksichtigung, Kompromiss, Überzeugung, Zwang usw.) kann dann durchaus für eine Beurteilung von Politik als „gut“ oder „schlecht“ dienen. „Unter einem ‚klugen und geschickten Politiker‘ verstehen wir offensichtlich nicht einfach einen ‚guten Fachmann‘, der viel von der Sache versteht – wenn er auch das tut, umso besser –, sondern eine Person, die die Fähigkeit hat, Menschen dazu zu bringen, bestimmten Handlungsprogrammen zuzustimmen und Folge zu leisten.“[63] Dabei kann zwischenpolicyundpoliticsnicht immer streng getrennt werden. Es gibt nicht erst ein inhaltliches Programm und dann das Bemühen um Zustimmung zu diesem. Die politische Gruppenbildung (Interessenkoalitionen) findet in Wechselwirkung mit der Programmentwicklung statt. So wird eine die Regierungsmacht anstrebende politische Partei, die gewisse gesellschaftlicheReformenbeabsichtigt (oder verhindern möchte), in der Regel auch weitere Programmpunkte vertreten, die ihr zwar weniger wichtig sind, aber für die Chance auf Gewinn der Regierungsmehrheit als notwendig erachtet werden. Dies ist von der „Regierungskunst“ nicht zu trennen. Die gedankliche Unterscheidung vonpolicyundpoliticsrechtfertigt sich dadurch, dass es uns erlaubt, „Ordnung in unser Nachdenken über das Politische zu bringen.“[64] Kategorien:politische Akteure, Beteiligte und Betroffene; Partizipation; Konflikte; Kampf um Machtanteile und um Entscheidungsbefugnis; Interessenvermittlung, -artikulation, -auswahl, -bündelung, -durchsetzung; Legitimationsbeschaffung durch Verhandlungen, Kompromisssuche, Konsensfindung DieVerfassung, die geltendeRechtsordnungundTraditionenbestimmen die in einem politischen System vorhandenenInstitutionenwie zum BeispielParlamenteund Schulen. Dadurch wird die Art und Weise der politischen Willensbildung geprägt und der Handlungsspielraum der anderen Dimensionen beeinflusst. Politik im Sinne vonpolicyundpoliticsvollzieht sich stets innerhalb dieses Handlungsrahmens. Dieser ist nicht unveränderbar, aber doch so stabil, dass er nicht beliebig und jederzeit zur Disposition steht. In (modernen)Staatendrückt sich dieser zunächst einmal durch die Verfassung aus, welche hier allgemein als grundlegendeOrganisationsform, die das Verhältnis der Staatsorgane untereinander regelt, verstanden wird, und nicht die schon inhaltlich bestimmte Vorstellung des „Verfassungsstaats“ meint, welcher schon mit konkreten Ordnungsvorstellungen wieRechtsstaatlichkeit,Gewaltenteilungund Garantie von Freiheits- undBürgerrechtenverbunden ist. Ferner geht diepolityals Organisationsform auch über den Inhalt der geschriebenen Verfassung im engeren Sinn hinaus und umfasst auch weitere grundlegende Gesetze wie beispielsweise in derBundesrepublik DeutschlanddasBundeswahlgesetzoder die Bestimmungen, die das Verhältnis vonParlamentundRegierung, Regierung undVerwaltung, Bund und Ländern regeln. Zurpolitygehören auch die Grenzen, die dem politischen Handeln gesetzt sind (beispielsweise durch die Bürgerrechte, die Bürgerdefinition oder die Staatsgrenzen). Eine solche staatliche „Verfassung“ beruht also auch auf einer Einheit (Volkoder Bürgerbevölkerung), die durch diese „verfasst“ wird. Somit gehört zurpolityauch der Aspekt der Abgrenzung. Neben die offiziellen, geschriebenen Regelwerke (Verfassung, Gesetze) tritt auch die jeweiligePolitische Kultureines Landes; man sprach auch schon von einer „doppelten politischen Verfassung“. So kann die geschriebene Verfassung eineparlamentarische Demokratievorsehen, aber das Desinteresse der Bevölkerung oder der Missbrauch durch die Regierenden die tatsächliche Verfasstheit des Staates alsautoritärbegründen. Gerade die nach 1945 versuchte, allzu einfache Übertragung von westlichen Verfassungsvorstellungen auf Länder der Dritten Welt hat dies durch ihr teilweise grandioses Scheitern gezeigt. Rechtliche Regelungen und politischeInstitutionenallein, egal wie ausgeklügelt das politische Institutionensystem auch sein mag, genügen nicht zur Stabilisierung einespolitischen Systemsund zur Erklärung der tatsächlichen Funktionsweise. GesellschaftlicheNormenundSitten, zum Beispiel die Anerkennung faktenbasierter Tatsachen im politischen Diskurs oder den politischen Gegner nicht unter die Gürtellinie zu schlagen, sind meist wichtiger für das Fortbestehen guter politischer Umgangsformen und damit für die Stabilität des politischen Systems als die Möglichkeiten, gegen politischeVerleumdungengerichtlich, also im Rahmen der geschriebenen Verfassung, vorgehen zu können. Zur politischen Kultur einer Gesellschaft gehören die typischen politischen Orientierungs- und Verhaltensmuster der Menschen. Kategorien:Internationale Abkommen und Regelungen; Grundgesetz; Zentrale Verfassungsprinzipien; politische Institutionen; Gesetze und Rechtsnormen; Politische Kultur Nach der Theorie der zivilen Moderne vonVolker von Prittwitzzeichnen sich zivile Ordnungen durch mehrdimensionale Koordination aus. Dabei werden freund/feind-, macht- und interessenlogische Interaktionsformen durch gemeinsam anerkannte Regeln aller Beteiligten gebunden(Bound Governance). Nur in solchen Ordnungen besteht eine unabhängige Polity-Dimension, und nur in deren Schutz, so im Schutz der Menschenrechte, können Sachpolitiken(Policies)frei entwickelt und diskutiert werden. Damit korrespondiert diePolicy/Politics/Polity-Trias, die sich seit den 1980er Jahren in der Politikwissenschaft hochentwickelter Industrieländer ausbreitet, mit der Entwicklung der zivilen Moderne.[65] In vormodernen Gesellschaften und in Ländern unziviler (lediglich technischer) Moderne dominieren dagegen eindimensionale Politikformen, in denen die Herrschenden auch in geltende Verfahren, geltendes Verfassungs- und Gesetzesrecht sowie in Verläufe sachpolitischer Diskussion durchgreifen können.PolityundPoliciesbilden hier also keine unabhängigen Politikdimensionen, sondern reflektieren lediglich aktuelle Macht- und Interessenkonstellationen. Vollständig verloren geht mehrdimensionale Politik, wenn die Freund/Feind-Logik zwingend herrscht; denn die Logik des Kriegs widerspricht prinzipiell einem Politikmodell, in dem alle Beteiligte bei gemeinsam anerkannten Regeln zu gemeinsam anerkannten Beschlüssen kommen.[65] Da auch ein entfaltetes mehrdimensionales System politischer Willensbildung und Entscheidungen wieder untergehen kann, ist politische Zivilität nie völlig sichergestellt. Vielmehr findet – häufig latent – ein ständiger Kampf um die zivile Moderne statt. Dies gilt für innenpolitische Konflikte, etwa zwischen demokratischer Öffentlichkeit, Populismus, Fundamentalismus und Extremismus; es gilt für Konflikte um Staatsgründung und staatliche Separation (Separationskrieg versus einvernehmliche Differenzierung), und es gilt für das Spannungsfeld zwischen dem UN-KonzeptHerrschaft des Rechts(auf Grundlage der Menschenrechte) und dem Streben nach absoluter Macht, unilateralen Interessenstrategien und Freund/Feind-Mustern zwischen Kulturen wie Staaten.[66] Politische Fragentauchen zwar meist im Zusammenhang mit Sachfragen auf, aber sie können nicht von Fachleuten reinwissenschaftlich,technokratischentschieden werden. Zur Beantwortung sind immernormativeGrundentscheidungen und Abwägungen von prinzipiell gleichberechtigten Ansprüchen nötig, bei denen es kein Richtig oder Falsch im Sinne absoluterWahrheitgibt. Bei politischen Fragen geht es immer auch um Fragen des menschlichen Zusammenlebens. Daher spielen bei der Beantwortung nebensubjektivenMeinungenundÜberzeugungenüber unsereInteressenundRechteauch der Wille, diese durchzusetzen, eine Rolle. Als der beste Agent unserer eigenen Interessen sieht die liberaleDemokratietheoriedabei uns selbst an, daher die Notwendigkeit vonGrundrechtender politischen Mitwirkung. Politische Fragen sind also normative Fragen, die nicht wissenschaftlich entscheidbar sind (siehePolitische TheorieundWissenschaftstheorie). Doch nicht alle zwischenmenschlichen Probleme sind auch politische Probleme. Als menschliches Handeln definiert man allgemein ein Verhalten, mit dem der Handelnde einen subjektiven Sinn verbindet, undsoziales Handelnals Handeln, dessen gemeinter Sinn auf das Verhalten anderer bezogen ist (Max Weber). Dazu benötigen MenschenEmpathie, die Fähigkeit, sich in denInteraktionspartnerhineinzuversetzen und die Situation „mit seinen Augen“ zu sehen. Dieses Soziale wird nun politisch, sobald das Zusammenleben der Menschen als solches zum Problem wird (konfliktorientierter Politikbegriff). In allen sozialen Beziehungen (Freundeskreis, Kollegen usw.)kannein spezifisches Vorgehen nötig werden, um Konflikte zu regeln. Alle Anstrengungen, die zu einer Vermittlung und Regelung führen (sollen), kann man alsPolitik im weiteren Sinnebezeichnen. Diese Art Politik ist aber nicht der eigentliche Zweck dieser informellen Gruppen und sozialenOrganisationen(zum Beispiel Sportverein). Erst auf der Ebene der nicht mehr auf persönlicher Bekanntschaft aufbauenden,anonymenGesellschaft wird Politik auch zum eigentlichen Zweck, weil das Zusammenleben der vielen sozialen Gruppen, Interessen und Weltanschauungenstetskonfliktanfällig ist und der Regelung bedarf. Alles soziale Handeln, das gesamtgesellschaftlich verbindliche Regelungen bezweckt, wird alsPolitik im engeren Sinnebezeichnet. Früh befassten sich Gelehrte damit, wie Politik auszusehen hat; dabei standen die Fragen „Was ist eine gute und gerechte Staatsordnung?“ und „Wie erlangt man wirklich Macht im Staat?“ im Mittelpunkt der Diskussion. Schon imAltertumverglich beispielsweiseAristoteles(384bis322 v. Chr.) alle ihm bekanntenVerfassungen(Politische Systeme) und entwickelte eine auch heute viel zitierteTypologiein seinem WerkPolitik. Er ist der erste Philosoph im Abendland, der daspolitische Vermögen und des Menschenin fachspezialiserter Abgrenzung zu den anderen Arten der Tiere herauszuarbeiten beginnt. Neben der Anzahl der an der Macht Beteiligten (einer, wenige, alle) unterschied er zwischen einer guten gemeinnützigen Ordnung (Monarchie,Aristokratie,Politie) und einer schlechten eigennützigenStaatsordnung(Tyrannis,Oligarchie,Demokratie). Erste geschriebeneGesetzebelegen, dass Politik sich nicht nur mit den Herrschenden, sondern auch früh schon mit sozialen Regeln befasste, die bis heute überliefert wurden. DerCodex Hammurapi(Babylon, etwa1700 v. Chr.) oder dasZwölftafelgesetz(Rom, etwa450 v. Chr.) sind Beispiele verbindlicher Regeln, die sicher als Ergebnis von Politik gewertet werden können. Befasst man sich mit den Politikern derRömischen Republikund demRömischen Kaiserreich, erkennt man viele Elemente damaliger Politik auch heute noch. Es wurde mit Kreide Wahlwerbung an die Hauswände geschrieben (etwa inPompeji). Es gab einen komplexen Regierungsapparat und hitzige Rivalität zwischen den Amtsträgern.Korruptionwar ein Thema derGesetzgebungund römischer Gerichtsverhandlungen. BriefeCicerosan einen Verwandten belegen, wie gezielt die Wahl in ein Staatsamt auch taktisch vorbereitet wurde. Mit dem Verfall des Römischen Reiches verlor Politik in Europa wieder an Komplexität, die Gemeinwesen wurden wieder überschaubarer und Konflikte kleinräumiger. In der Zeit derVölkerwanderungund des frühenMittelalterswar Politik mehr kriegerische Machtpolitik und weniger durchInstitutionenund allgemein akzeptierte Regeln geprägt. Je stärker derFernhandel,GeldundStädtewieder an Bedeutung gewannen, desto mehr wurden wieder feste Machtzentren gebraucht und desto wichtiger wurden Institutionen. Beispielsweise bildete sich dieHanseals Interessen- und Machtverbund einflussreicher sich selbst regierender Städte. Wichtiges relativ konstantes Machtzentrum war diekatholische Kirche. Aus sozialen Gemeinschaften, die bestimmten Führern die Treue schworen (Personenverband), wurden langsamErbmonarchienmit festenGrenzen. InFrankreichentwickelte sich der Urtypus desabsolutistischen Herrschers, inEnglandentstand die an Recht und Gesetz gebundenekonstitutionelle Monarchie. Dort waren bald auch die wohlhabendenBürgeroffiziell an der Politik beteiligt. Mit der Zeit wurde dann dasZensuswahlrechtauf größere Teile der Bevölkerung ausgeweitet. In der Zeit derAufklärungerdachten Gelehrte neue Modelle der Staatskunst. StattNiccolò MachiavellisModell der absoluten Macht, das er in seinem BuchDer Fürst(Il Principe)darstellte, definierteJohn Lockedas Modell derGewaltenteilung. DieBürgerlichen Freiheitenwurden von verschiedenen politischen Philosophen gefordert. MitThomas JeffersonsMenschenrechtserklärung und derUS-amerikanischen Verfassungbegann die Zeit der modernen Verfassungsstaaten. DieFranzösische Revolutionund die FeldzügeNapoleonswälzten Europa um. Mit demCode civilin Frankreich wurde das erste Gesetzbuch auf Basis derMenschenrechteeingeführt. Überall fielen allmählich dieStandesschranken. Politik wurde zu einer Angelegenheit des ganzen Volkes. Es entstanden Parteien, die zuerst von außen eineOppositionorganisierten, um später selbst dieRegierungzu stellen. EinigeParteienwie dieSPDoder später dieGrünenentstanden aussozialen Bewegungenwie derArbeiterbewegungoder der Anti-Atom- undFriedensbewegung, andere formierten sich vor einem religiösen Hintergrund (Zentrum). Im20. Jahrhundertkam es schließlich zur Herausbildunginternationaler Organisationenmit zunehmendem Einfluss auf die Politik. Der erste Versuch, im sogenanntenVölkerbundeine Völkergemeinschaft zu bilden, scheiterte mit demZweiten Weltkrieg. Heute existiert neben denVereinten Nationeneine Vielzahl weiterer internationaler Organisationen. Eine Besonderheit stellt dieEuropäische Uniondar, die ein höheres Integrationsniveau als eine klassische internationale Organisation aufweist, aber trotzdem keinföderaler Staatist. Anarchismus–Autoritarismus–Christdemokratie–Demokratie–Diktatur–Faschismus–Institutionalismus–Kapitalismus–Kommunismus–Kommunitarismus–Konservatismus–Kontextualismus–Politischer Liberalismus–Neoliberalismus–Marxismus–Nationalismus–Nationalsozialismus–Parlamentarismus–Sozialdemokratie–Sozialismus–Totalitarismus Platon–Aristoteles–Niccolò Machiavelli–Baruch de Spinoza–Jean Bodin–Hugo Grotius–Charles de Montesquieu–Jean-Jacques Rousseau–Thomas Hobbes–John Locke–John Stuart Mill–Karl Marx–Michail Bakunin–Max Weber–John Rawls–Hannah Arendt Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Wortherkunft 2Politikbegriffe 2.1Regierungszentriert versus emanzipatorisch 2.2Normativ versus deskriptiv 2.3Konfliktorientiert versus konsensbezogen 3Mehrdimensionaler Politikbegriff der jüngeren politikwissenschaftlichen Diskussion 3.1Policy: normative, inhaltliche Dimension 3.2Politics: prozessuale Dimension 3.3Polity: formale, institutionelle Dimension 4Zivilitätstheoretische Diskussion des mehrdimensionalen Politikbegriffs 5Abgrenzung von Politisch und Sozial – Politik im engeren und weiteren Sinn 6Kurze Entwicklungsgeschichte wichtiger politischer Konzeptionen 6.1Altertum 6.2Mittelalter 6.3Neuzeit 7Zentrale politische Begriffe 8Politische Systeme und Ideologien 9Klassische politische Denker 10Siehe auch 11Literatur 12Weblinks 13Einzelnachweise Afrikaans Alemannisch አማርኛ Aragonés Ænglisc العربية الدارجة مصرى অসমীয়া"
  },
  {
    "label": 0,
    "text": "Reisen – Wikipedia Reisen Reisenist der Familienname folgender Personen: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden Cebuano English עברית Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Links auf diese Seite Änderungen an verlinkten Seiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink QR-Code herunterladen Als PDF herunterladen Druckversion Commons Wikidata-Datenobjekt Reise Reisen (Eitting) Eitting Reisen (Birkenau) Rydzyna Abraham Reisen Helmut Reisen"
  },
  {
    "label": 0,
    "text": "Rugby – Wikipedia Rugby Inhaltsverzeichnis Grundlagen des Spiels Varianten Geschichte Popularität Rugbykultur Unterschiede der einzelnen Rugby- und Footballspielarten Gehälter der Top-Rugbyspieler im Jahr Sonstiges Siehe auch Literatur Weblinks Einzelnachweise Rugby(englischauchRugby Football) gehört zur Familie derMannschaftssportarten, die gemeinsam mit demFußballinEnglandentstanden sind.American FootballundCanadian Footballhaben sich später aus dem Rugby entwickelt. Die am weitesten verbreiteten Varianten sindRugby UnionundRugby League. Rugby Union hat sich seit seiner Professionalisierung in den1990erJahren als die global führende Rugby-Sportart etabliert, auch wenn sich imBreitensportzunehmend die kontaktarme VarianteTouch Rugbydurchsetzt und die VarianteSiebener-Rugbyseit der Aufnahme als olympische Sportart 2016 einen Entwicklungsschub erfährt. Allen Rugby-Varianten gemeinsam ist die Verwendung einesBallsin Form eines verlängertenRotationsellipsoids. Ziel ist es, den Ball am Gegner vorbei zu tragen oder zu kicken und dadurch Punkte zu erzielen. Dies kann auf verschiedene Arten geschehen: Der Ball darf mit der Hand nur nach hinten geworfen oder übergeben werden. Wenn der Ball jedoch nach vorne geworfen wird, muss einGedrängeausgeführt werden (engl.scrum). Das Gedränge beschreibt das gegenseitige Anbinden der mit 1 bis 8 nummerierten Spieler, die dann „um den Ball schieben“. Man könnte es als eine Art Kräftemessen bezeichnen: Die Spieler stehen in gebückter Haltung mit geradem Rücken, mit dem Gegner ineinander verschachtelt, voreinander und versuchen durch gemeinsames Drücken den Gegner wegzuschieben und somit den Ball für das eigene Team freizugeben. Treten des Balles ist in alle Richtungen erlaubt. Nur der balltragende Spieler darf angegriffen werden. Es ist erlaubt, diesen durch Umklammern und Tiefhalten (engl.tackle) unterhalb der Schulterlinie zu behindern und ihn nach Möglichkeit zu Fall zu bringen. Ein Spieler, welcher mit mehr Körperfläche als den Fußsohlen den Boden berührt, hat den Ball unmittelbar loszulassen und darf nicht mehr danach greifen; tut er es nicht, bekommt die gegnerische Mannschaft den Ball. Wenn der Spieler auf dem Boden liegt, dürfen von beiden Teams andere Spieler nach dem Ball schieben und drücken, dürfen dabei aber nicht die Hände benutzen. Schlagen und Beinstellen ist beim Tiefhalten streng verboten. Die Spielkleidung besteht aus einem festenTrikot, kurzen Hosen, Kniestrümpfen undStollenschuhen. Das Tragen einesZahnschutzesist Pflicht. Harte Schutzbekleidung ist nicht erlaubt. Manche Spieler tragen jedoch fakultativ eine Kappe aus weichem, dünn gepolstertem Material, die primär die Ohren schützen soll, oder ein dünnes, schaumstoffgefüttertes Schulterpolster unter dem Trikot. Frauen dürfen einen Brustschutz tragen. Das Tor besteht aus zwei senkrechtenMalstangen, mit einem Abstand von 5 bis 6 Meter und einer Querstange in 3 Meter Höhe. Sie stehen in der Mitte derMallinie. An den unteren Enden der Malstangen werden Schutzpolster befestigt, um Verletzungen vorzubeugen (Rugby Union,7er-RugbyundRugby League). Typische Spielelemente beim Rugby sind angeordnete und offene Gedränge (engl.ruck), das Paket (engl.maul), dieGassevon der Seitenlinie (engl.lineout) und das Lauf- und Passspiel der sogenannten Dreiviertelreihe (engl.three quarters). Aufgrund seiner Geschichte liegt Rugby heute in zwei grundsätzlich verschiedenen Varianten vor, dem am weitesten verbreitetenRugby Union(Fünfzehnerrugby) und dem weniger verbreitetenRugby League(Dreizehnerrugby). Während der ursprüngliche Rugby-Union-Sport mit 15 Spielern (pro Mannschaft) auf dem Platz gespielt wird, gibt es mittlerweile eine Variante, bei der sieben Spieler auf dem Platz stehen, das7er-Rugby. Da der Ball hauptsächlich mit der Hand geführt wird, kann man Rugby Union auch im Sand spielen, worausBeachrugbyentstanden ist. Varianten mit zwölf und zehn Rugbyspielern sind ebenfalls möglich, es gibt für sie jedoch kein eigenes Regelwerk, es gelten dieRegelndes 15er-Rugby-Union. Äußerst beliebt sind in jüngster Zeit die aus dem Rugby League entstandenen, fast kontaktlosen VersionenTouch RugbyundTag Rugby. →Siehe auch:Geschichte des Fußballs(Rugby- und Fußballgeschichte sind eng miteinander verknüpft) Der Legende nach soll Rugby während eines Fußballspiels in dergleichnamigen Stadtentstanden sein. Als der Mannschaft vonWilliam Webb Ellis1823 eine Niederlage bevorstand, packte dieser den Ball mit den Händen und legte ihn ins Tor des Gegners. Obwohl berechtigte Zweifel am Wahrheitsgehalt der Geschichte bestehen – der Ball wurde bereits zuvor in den meisten Spielvarianten mit der Hand getragen –, ist der Pokal derRugby-Union-Weltmeisterschaftnach William Webb Ellis benannt (derWebb Ellis Cup). 1863 wurde der englische FußballverbandFootball Association(FA) mit dem Ziel gegründet, die noch vielfältigenFußballregelnzu vereinheitlichen. Aufgrund von Streitigkeiten über Regeländerungen zogen sich einige Vereine aus dem Verband zurück und gründeten am 26. Januar 1871 mit derRugby Football Union(RFU) einen konkurrierenden Verband, der in der Folgezeit nach und nach die Regeln derRugby Schoolstandardisierte. Bereits am 27. März desselben Jahres fand inEdinburghzwischenSchottlandundEnglanddas erste Länderspiel statt. 1895 erfolgte aufgrund eines Streits über denAmateurgedankeneine weitere Trennung, diesmal innerhalb der RFU. 21 Vereine, vor allem aus Arbeitervierteln Nordenglands, spalteten sich als Northern Rugby Union (heuteRugby Football League) ab, legten ihre eigenen Regeln fest und erlaubten die Professionalisierung des Sports. Aus den veränderten Regeln entwickelte sich die VarianteRugby League. Bis heute existieren beide Varianten des Rugbys nebeneinander. Internationale Begegnungen von Nationalmannschaften werden sowohl nach den Regeln der Rugby Union wie auch nach denen der Rugby League abgehalten. Seit 1995 sind im Rugby Union ebenfalls Profisportler zugelassen. Rugby in seinen verschiedenen Varianten ist vor allem in Teilen des britischenCommonwealthund Ländern dersüdlichen Hemisphäreäußerst populär. Führende Nationen sindNeuseeland,Australien,SüdafrikaundArgentinienin der südlichen Hemisphäre sowieFrankreich,England,Wales,Irland,SchottlandundItalienin Europa. Auch in denozeanischenLändernFidschi,SamoaundTongaist Rugby der Nationalsport. Neben diesen Ländern nehmenJapan, dieUSA,Rumänien,Namibia,Kanada,GeorgienundSpanienregelmäßig an denRugby-Union-Weltmeisterschaftenteil und verfügen über eine ausgeprägte Rugbykultur. Als Hochburgen von Rugby League gelten England, Neuseeland und insbesondere Australien. In deutschsprachigen Ländern versteht man unter dem Begriff Rugby meistens Rugby Union. Es gibt aber auch inDeutschland,Österreichund derSchweizBestrebungen, eine Rugby-League-Kultur aufzubauen. Wachsender Popularität erfreuen sich Varianten wieTouch Rugbyan Schulen. Von 1900 bis 1924 war Rugby Union eineDisziplinderOlympischen Spiele. Am 9. September 2009 beschloss dasIOC, Rugby im Jahr 2016 wieder ins Programm aufzunehmen, wobei das Format7er-Rugbyzur Anwendung kommt. Ein Sprichwort in Großbritannien lautet:Football is a gentleman’s game played by ruffians and rugby is a ruffian’s game played by gentlemen(„Fußball ist eine von Raufbolden gespielte Gentleman-Sportart und Rugby ist eine von Gentlemen gespielte Raufbold-Sportart“).[1] In zahlreichen traditionellen Rugbynationen gilt Rugby Union allgemein als Sportart des „Establishment“, das hauptsächlich von der Oberschicht und der höheren Mittelschicht gespielt wird. Es ist auch die an Privatschulen und Gymnasien bevorzugte Variante.[2]Im Gegensatz dazu gilt Rugby League vor allem im NordenEnglandsund im OstenAustralienstraditionell als Sportart der „Arbeiterklasse“ und der unteren Mittelschicht. Eine Ausnahme von diesenStereotypenist insbesondereWales, wo Rugby Union mit Mannschaften aus kleinen Dörfern in Zusammenhang gebracht wird, in denen Bergleute und Industriearbeiter leben.[3]InNeuseeland,Südfrankreich, in Teilen Südenglands,Schottland,Irlandund auf den pazifischen Inseln ist Rugby Union ebenfalls im Arbeitermilieu verbreitet.[4][5]In Südfrankreich, insbesondere im Südwesten, ist Rugby populärer als Fußball. In Großbritannien nennen Rugby-Union-Fans ihre Sportart bisweilen auch „rugger“.[6]Neuseeländer nennen Rugby im Allgemeinen „footy“ oder „football“, die Hauptvarianten einfach „union“ oder „league“.[7] Die wichtigsten Unterschiede zwischen Rugby undGridiron Football: In den 2020er Jahren haben die Gehälter der Top-Rugbyspieler die Grenze von 1 Mio. USD überschritten, z. B.Handre Pollard(Südafrika): 1,8 Mio. USD,Finn Russell(Schottland): 1,25 Mio. USD, Charles Piutau (Neuseeland): 1,25 Mio. USD,Owen Farrell(England): 1 Mio. USD,Antoine Dupont(Frankreich): 600.000 USD.[8][9]Das Durchschnittsgehalt der Profispieler in Großbritannien betrug 137.000 USD.[10]In Frankreich betrug das Durchschnittsgehalt der 30 Top-Verdiener 320.000 EUR,[11]alle Zahlen von 2023. Zum Vergleich: 2010 verdienteJonny Wilkinson, damals der Top-Verdiener, 300.000 EUR.[11] Die Eindrücke eines Rugby-Matches wurden vonArthur Honeggerin einem seiner sinfonischen Sätze verarbeitet (Näheres dazu im ArtikelRugby (Honegger)). Rugby Union|Rugby League|Siebener-Rugby|Touch Rugby|Tag Rugby|Beachrugby|Hallenrugby|Rollstuhlrugby|Unterwasser-Rugby Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Grundlagen des Spiels 2Varianten 3Geschichte 4Popularität 5Rugbykultur 6Unterschiede der einzelnen Rugby- und Footballspielarten 7Gehälter der Top-Rugbyspieler im Jahr 8Sonstiges 9Siehe auch 10Literatur 11Weblinks 12Einzelnachweise Afrikaans Alemannisch Aragonés Ænglisc العربية الدارجة مصرى অসমীয়া Asturianu Azərbaycanca تۆرکجه Башҡортса Boarisch Žemaitėška Беларуская Беларуская (тарашкевіца) Български বাংলা Brezhoneg"
  },
  {
    "label": 0,
    "text": "San Diego – Wikipedia San Diego Inhaltsverzeichnis Geographie Geschichte Bevölkerung Wirtschaft und Infrastruktur Politik Kultur Söhne und Töchter der Stadt Siehe auch Weblinks Einzelnachweise Lage Klima Einwohnerentwicklung Demografie Religion Wirtschaft Bildung Polizei Militär Verkehr Regierung Städtepartnerschaften Filmstadt San Diego Sport Theater Sehenswürdigkeiten Straßenverkehr Öffentlicher Verkehr Luftverkehr San Diego(Ausspracheenglisch[ˌsæn diːˈeɪgoʊ],spanisch[sanˈdjeɣo]) ist die zweitgrößte Stadt imUS-BundesstaatKalifornienund die achtgrößte derVereinigten Staaten. Die Stadt hatte laut der letzten Volkszählung im Jahr 2020 1.386.932[2]Einwohner und liegt im Südwesten von Kalifornien nahe der Grenze zuMexiko. San Diego ist Sitz der Countyverwaltung desSan Diego Countyund stellt eineprincipal cityderMetropolitan Statistical Area(MSA)San Diego–Chula Vista–Carlsbaddar, die mit dem County deckungsgleich ist. Wegen des angenehmen Klimas wird San Diego von seinen Bewohnern als „America’s Finest City“ bezeichnet. Die Wellen desPazifikseignen sich insbesondere zumWellenreiten. Die Stadt hat sich in den letzten Jahren zu einem der bedeutendsten Zentren derTelekommunikations- undBiotechindustrieentwickelt. San Diego liegt an einem künstlichen Hafenbecken an der Südspitze Kaliforniens, etwa zweieinhalb Autostunden südlich vonLos Angelesund etwa eine halbe Stunde nördlich vonTijuana, Mexiko. Im Westen wird die Stadt vomPazifischen Ozeanbegrenzt. Im Osten bilden Berge sowie derAnza-Borrego-Wüstenpark eine natürliche Grenze. San Diego wird durch die StadtChula Vistain zwei Teile geteilt. Den höchsten Punkt im Stadtgebiet bildet derCowles Mountain(486m) imMission Trails Regional Park; weitere Erhebungen sind derBlack Mountainmit475mundMount Soledadmit251m. Das Stadtgebiet dehnt sich immer weiter ins Landesinnere aus. Im Süden reicht es bis zur mexikanischen Grenze. Durch die Stadt fließt derSan Diego River. Die Jahresdurchschnittstemperatur liegt bei 17,3 °C, die durchschnittliche monatliche Niederschlagsmenge bei 24 mm. Die Winter sind generell mild mit einer Durchschnittstemperatur von 14 °C. Das Gebiet des heutigen San Diego wurde lange Zeit von denKumeyaay-Indianern bewohnt. Am 28. September 1542 ging der in spanischen Diensten stehende PortugieseJuan Rodríguez Cabrilloals erster Europäer an Land. Er erklärte seine Entdeckung zum Besitz der Spanischen Krone und nannte sie San Miguel. Kalifornien gehörte administrativ zumVizekönigreich Neuspanien. Der nächste Spanier, der die Region besuchte, warSebastián Vizcaíno. Im Auftrag Spaniens segelte er die Westküste entlang, um diese zu kartografieren. Im November 1602, am Festtag zu Ehren vonSan Diego de Alcalá, erreichte er San Miguel und gab dem Ort seinen heutigen Namen, San Diego. Erst 1769 errichteteGaspar de Portoláeinen Militärposten und derFranziskanerJunípero Serradie ersteMission, die Mission San Diego de Alcalá. Wegen dauernder Probleme mit der Wasserversorgung und dem schlechten Ackerboden verlegte der Priester Luis Jayme 1774 die Mission um rund zehn Kilometer, bald darauf folgten die erstenSiedler. 1775 konnten sich die Spanier erfolgreich gegen eineindigeneRevolte durchsetzen. In der Folge dieser Ereignisse wurden Luis Jayme und zwei weitere Personen getötet. 1776 kehrte Junípero Serra zurück und organisierte den Wiederaufbau der Mission. 1797 war San Diego de Alcalá die größte Mission in Kalifornien mit über 1400 frisch Bekehrten (Neophyten). 1821 wurde Mexiko unabhängig von Spanien. Das Vizekönigreich Neuspanien, Alta California und somit auch San Diego gingen in den Nationalstaat Mexiko über. 1834 wurde San Diego zur Stadt erklärt und die ersten Wahlen fanden statt.Juan Maria Osunagewann die Bürgermeisterwahl gegenPio Picound wurde erster Bürgermeister. 1838 verlor San Diego nach einem drastischen Bevölkerungsrückgang seinen Stadtstatus wieder. Infolge desMexikanisch-Amerikanischen Kriegesfiel San Diego 1850 an die Vereinigten Staaten und wurde zur Stadt und zum Sitz des San Diego County erklärt. Der erste US-amerikanische Bürgermeister wurdeJoshua Bean. 1869 lösten lokale Goldfunde einen Boom aus. In dessen Folge wurde San Diego 1885 an das nationale Eisenbahnnetz angeschlossen. Ende der 1880er Jahre ebbte derGoldrauschab; die Bevölkerung San Diegos fiel von 40.000 auf 16.000 Einwohner. 1915/16 beherbergte San Diego diePanama-California Exposition. Im Jahr 1917 brachte derEintritt der USA in den Ersten Weltkriegzahlreiche Militäreinrichtungen nach San Diego. 1927 überführteCharles Lindberghdie in San Diego gebauteSpirit of St. Louisvom später nach ihm benanntenFlugfeldin Rekordzeit an die Ostküste zumRoosevelt FieldinNew York, dem Ausgangspunkt seines legendären Transatlantikflugs. In San Diego begann sich zur gleichen Zeit die Flugzeugindustrie zu etablieren. Nach demAngriff auf Pearl Harborverlegte dieUnited States Navy1941 dasHauptquartierihrerPazifikflottezurück nach San Diego. Seit demZweiten Weltkriegprägte das Militär das Stadtbild San Diegos und löste einen Wirtschaftsboom aus. Bis heute ist es der wichtigste Arbeitgeber in der Region. Am 25. September 1978 stieß eineBoeing 727derPacific Southwest Airlinesmit einem Sportflugzeug vom TypCessna 172zusammen. Beim Absturz desPacific-Southwest-Airlines-Fluges 182sterben insgesamt 144 Menschen. Am 29. Januar 1979 erschoss die 16-jährigeBrenda Ann Spencervor der Grover Cleveland Elementary School zwei Menschen und verletzte neun weitere. 1981 nahm mit derSan Diego Trolleydas erste moderneStadtbahnsystemder Vereinigten Staaten seinen Betrieb auf. Am 18. Juli 1984 fand im Stadtteil San Ysidro einer der schlimmsten Amokläufe der US-Geschichte statt: Der 41-jährigeJames Oliver Hubertybetrat eineMcDonald’s-Filiale, erschoss 21 Menschen und verletzte 19 weitere, ehe er von der Polizei erschossen wurde. Nach dem Ende desKalten Kriegesnahm die Präsenz des Militärs erheblich ab, seitdem lebte in San Diego die Biotech- und die Telekommunikationsindustrie auf. Der Asteroid des inneren Hauptgürtels(3043) San Diegoist nach der Stadt benannt.[3] San Diego wächst rasant. Die Bevölkerung der Stadt hat sich von 1950 bis 2010 fast vervierfacht, von 1980 bis 2010 stieg die Einwohnerzahl um fast 50 Prozent: ¹1950–2020: Volkszählungsergebnisse des US Census Bureau Die Bevölkerung bestand laut dem Zensus von 2010 zu 45,1 Prozent aus Weißen und zu 6,7 Prozent aus Afroamerikanern; 15,9 Prozent waren asiatischer Herkunft. 28,8 Prozent der Bevölkerung warenHispanics. DerMediandes Einkommens je Haushalt lag 2015 bei 66.116US-Dollar. 15,4 Prozent der Bevölkerung lebten unterhalb der Armutsgrenze.[5] In der Stadt hat das römisch-katholischeBistum San Diegoseinen Sitz. Als Bischofskirche fungiert dieSt. Joseph Cathedral. Neben denStreitkräftensind die Telekommunikations- und die Biotechnologieindustrie die wichtigsten Industriezweige; San Diego gehört zu den drei bedeutendsten Biotechnologiezentren der USA. Auch dieErneuerbaren Energienschaffen hier immer mehr Arbeitsplätze. Der Aufstieg San Diegos zu einem der wichtigsten Zentren der Telekommunikation ist eng mitQualcommverbunden. Die 1985 vonIrwin Jacobsgegründete Firma hat ihren Hauptsitz in San Diego und hat maßgeblich dazu beigetragen, dass San Diego zur weltweiten „CDMA-Hauptstadt“ wurde. Die Ryan-Flugzeugwerke, die Charles Lindberghs FlugzeugSpirit of St. Louisgebaut haben, gehören heute als Ryan Aeronautical Center zum KonzernNorthrop Grumman(NGC). Sie haben die typischen Hallen am Flughafen der Stadt verlassen und sind nach Rancho Bernardo, einem nördlichen Stadtteil, gezogen. Das Werk hat noch vor der Übernahme durch NGC das unbemannte FlugzeugGlobal Hawkentwickelt und gebaut. Das ChemieunternehmenWD-40 Companyund das PharmazieunternehmenAmylin Pharmaceuticalshaben ihren Sitz in San Diego. Ebenso ist dieQuidelCorporation, ein Hersteller vonDiagnostika, die im Mai 2020 durch dieFood and Drug Administration(FDA) die ersteNotfallgenehmigungfür einenCOVID-19-Antigentesterteilt bekam, dort ansässig. Ferner sind in San Diego mehrere Schiffswerften beheimatet. Die Zahl der Beschäftigten lag in der Region San Diego 2003 bei 1.481.400, die Arbeitslosenquote bei 4,3 % (Kalifornien 6,7 %). Die meisten Arbeitnehmer verzeichnet 2004 der Dienstleistungssektor mit 509.500, gefolgt vom Staat mit 206.600, dem Einzelhandel mit 137.000 und der verarbeitenden Industrie mit 103.900 Angestellten. Die Metropolregion von San Diego erbrachte 2016 eine Wirtschaftsleistung von 215,3 Milliarden US-Dollar und belegte damit Platz 17 unter den Großräumen der USA.[6] San Diego verfügt über mehrereHochschulen, wobei dieUniversity of California, San Diego(UCSD) als die bekannteste gilt. Daneben existieren dieSan Diego State University(SDSU) sowie diekatholischeUniversity of San Diego(USD). Das San Diego Police Department (SDPD) ist die Polizeibehörde von San Diego, diese verfügt über 2781 Mitarbeiter und 840 freiwillige Helfer.[7]Das jährliche Budget beträgt 277 Mio. $[7]Das SDPD verwaltet einen Bereich, in dem etwa 1,4 Millionen Menschen leben. DieNaval Base Coronadoin der Bucht vor San Diego ist nebenNorfolk,Virginia, die größteMarinebasisder USA. Sie istHeimathafendesFlugzeugträgersRonald Reagan, mehrerer amphibischer Angriffsschiffe und andererKriegsschiffederPazifik-FlottederUS Navy. Zur Marinebasis gehört auch dieNaval Air Station North Island, die Heimat einer großen Anzahl von Flugzeugen der Navy und sie ist Sitz desUnited States Naval Special Warfare Command, dem Führungszentrum derSpezialeinheitNavy Seals. Von 1925 bis 1997 befand sich im Norden der Bucht dasNaval Training Center San Diego. Auf derNaval Submarine Basesind mehrereAtom-U-Bootestationiert. DasMarine Corpsunterhält unter anderem denStützpunkt Miramar, der bis Mitte der 1990er Jahre als Naval Air Station Miramar das berühmte AusbildungsprogrammTop Gunbeheimatete. Am 11. Mai 1962 wurde im Rahmen einer Testserie eine Atomwaffe mit dem CodenamenSwordfishin nur 740 km Entfernung von San Diego unter der Wasseroberfläche gezündet. Der Marinekomplex derNaval Base San Diegobefindet sich ebenfalls in San Diego. Für mehr als 80 % der Einwohner San Diegos ist das Auto das wichtigste Verkehrsmittel. Dadurch gibt es ein sehr umfangreiches Schnellstraßen- undAutobahn-Netz. Die wichtigste Autobahn ist dieInterstate 5. Sie beginnt an dermexikanischenGrenze, verläuft dann durch San Diego,Los Angeles,Sacramento,Seattleund endet schließlich an derkanadischenGrenze. Weitere bedeutende Autobahnen und Schnellstraßen: Die 1969 eröffneteSan Diego Coronado Bay Bridgeüberspannt dieSan Diego Bayund verbindet die Innenstadt mit dem KüstenortCoronado. Nur etwa 3 % der Einwohner nutzen den öffentlichen Verkehr. Er besteht aus der StraßenbahnSan Diego Trolley, denCoaster-Vorortzügen nach Oceanside,Amtrak-Fernzügen undBusverkehren. Die beiden wichtigsten Bahnhöfe sind dasSanta Fe Depotund dasOld Town Transit Center. In San Diego gibt es mehrere Flughäfen. Der größte und somit wichtigste ist derSan Diego International Airport. Er ist rund 4,5 km vom Stadtzentrum entfernt. Jährlich werden hier über 18 Millionen Passagiere abgefertigt, somit ist er der 30. größte Flughafen derVereinigten Staaten. Bürgermeister der Stadt ist der DemokratTodd Gloria. Die Stadt San Diego unterhält mit folgenden 16 StädtenStädtepartnerschaften(Sister cities):[8] Bis zu seinem Umzug nach Los Angeles war dasFootballteamder Stadt dieSan Diego Chargers, dasBaseballteamdieSan Diego Padres. Die Spiele beider Sportarten wurden bis 2003 imSan Diego Stadiumausgetragen. Seit 2004 verfügen die Padres über ihr eigenes Stadion, denPETCO Parkin der Innenstadt von San Diego. Eishockey wird in San Diego seit den 60er Jahren gespielt. Das Profiteam der Stadt heißtSan Diego Gullsund spielt momentan in derAmerican Hockey League. Lange Zeit spielteWillie O’Reefür die originalen Gulls in der Western Hockey League. Im Fußball spieltSan Diego Wave FCin derNational Women’s Soccer LeagueimSnapdragon Stadium, das 2022 am ehemaligen Standort des San Diego Stadium eröffnet wurde. Das Snapdragon Stadium, das für die Football-Mannschaft derSan Diego State University, die San Diego State Aztecs, gebaut wurde, wird auch die Heimat desSan Diego FCaus derMajor League Soccersein, wenn diese Mannschaft in der Saison 2025 ihren Spielbetrieb aufnimmt. San Diego gilt als die Wiege des modernenTriathlon-Sportes. Am 25. September 1974 fand in derMission Bay, einerLagunemit Strand inmitten der Stadt, der erste als solcher bezeichnete Triathlon statt. Wenige Jahre später entstand daraus derIronman Hawaii. Zu den kulturellen Einrichtungen zählen ein Musical Theater, einOpernhausund einePhilharmonie, die sich seit 2003 dank einer Spende in Höhe von einer Million Dollar von Qualcomm-Gründer Jacobs zu einem der besten Orchester des Landes entwickelt hat. Schließlich besitzt San Diego auch eine Reihe von Museen und Galerien. Am bekanntesten ist das Timken Museum mit seiner auch überregional bedeutendenAltmeistersammlung. DasSan Diego Museum of Artist ein bedeutendes Kunstmuseum der Stadt. Seit 1970 findet in San Diego jährlich dieComic-Con Internationalstatt, die weltweit größte Comic-Convention. Jährliche Musikfestivals sind dasSun God Festivalder Universität oder dasCRSSD Festivalim Waterfront-Park. 82 Bauwerke, Stätten und Bezirke in der Stadt und ihrer näheren Umgebung sind insgesamt imNational Register of Historic Places(NRHP) eingetragen (Stand 19. Oktober 2020), neun davon haben den Status einesNational Historic Landmarkshaben.[13] Allied Gardens|Bankers Hill|Black Mountain Ranch|Carmel Mountain Ranch|Carmel Valley|City Heights|Clairemont|College Area|Columbia|Core|Cortez Hill|Del Cerro|Del Mar Mesa|Downtown San Diego|Dryden Historic District|East Village|El Cerrito|Encanto|Gaslamp Quarter|Gateway|Golden Hill|Grantville|Hillcrest|Kearny Mesa|Kensington|La Jolla|Linda Vista|Little Italy|Logan Heights|Marina|Middletown|Mira Mesa|Miramar|Mission Beach|Mission Hills|Mission Valley|Mount Hope|Navajo|Normal Heights|North Park|Oak Park|Ocean Beach|Otay Mesa|Pacific Beach|Paradise Highlands Ranch|Paradise Hills|Point Loma|Rancho Bernardo|Rancho Peñasquitos|Redwood Village|Rolando|Sabre Springs|San Carlos|San Pasqual Valley|San Ysidro|Scripps Ranch|Serra Mesa|Sorrento Valley|South Park|Talmadge|Tierrasanta|Torrey Highlands|Torrey Hills|Torrey Pines|University City|University Heights|University Square|Webster Carlsbad•Chula Vista•Coronado•Del Mar•El Cajon•Encinitas•Escondido•Imperial Beach•La Mesa•Lemon Grove•National City•Oceanside•Poway•San Diego•San Marcos•Santee•Solana Beach•Vista Alpine•Bonita•Bonsall•Borrego Springs•Bostonia•Boulevard•Campo•Camp Pendleton Mainside•Camp Pendleton South•Casa de Oro-Mount Helix•Crest•Del Dios•Descanso•Elfin Forest•Eucalyptus Hills•Fairbanks Ranch•Fallbrook•Granite Hills•Harbison Canyon•Harmony Grove•Hidden Meadows•Hillcrest•Jacumba Hot Springs•Jamul•Julian•Lake San Marcos•Lakeside•La Presa•Mount Laguna•Oak Grove•Pala•Pine Valley•Potrero•Rainbow•Ramona•Rancho San Diego•Rancho Santa Fe•San Diego Country Estates•Spring Valley•Valley Center•Winter Gardens Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geographie 1.1Lage 1.2Klima 2Geschichte 3Bevölkerung 3.1Einwohnerentwicklung 3.2Demografie 3.3Religion 4Wirtschaft und Infrastruktur 4.1Wirtschaft 4.2Bildung 4.3Polizei 4.4Militär 4.5Verkehr 4.5.1Straßenverkehr 4.5.2Öffentlicher Verkehr 4.5.3Luftverkehr 5Politik 5.1Regierung 5.2Städtepartnerschaften 6Kultur 6.1Filmstadt San Diego 6.2Sport 6.3Theater 6.4Sehenswürdigkeiten 7Söhne und Töchter der Stadt 8Siehe auch 9Weblinks 10Einzelnachweise Afrikaans አማርኛ"
  },
  {
    "label": 0,
    "text": "San Francisco – Wikipedia San Francisco Inhaltsverzeichnis Geographie Geschichte Einwohnerentwicklung Politik Kultur und Sehenswürdigkeiten Wirtschaft und Infrastruktur Persönlichkeiten Literatur Weblinks Einzelnachweise Geografische Lage Geologie Stadtgliederung Klima Indianische Besiedlung und erste Europäer 19. Jahrhundert und Neuzeit Demografie Obdachlosigkeit / Opioidkrise Bürgermeister Stadtflagge von San Francisco Städtepartnerschaften Architektur Kirchen Opern und Theater Museen Sehenswürdigkeiten Parks und Plätze Natursehenswürdigkeiten Sport Feste und Feiertage Essen und Trinken Literarisches Leben Coffee Houses Film, Fernsehen und Musik Wirtschaft Verkehr Medien Bildung Golden Gate Bridge Lombard Street Transamerica Pyramid Mission Dolores Presidio Painted Ladies Haight-Ashbury Coit Tower Weitere bekannte Bauwerke ÖPNV Eisenbahn Straßen Flugverkehr Seehafen Zeitungen Fernsehen Hörfunk Internet Vorwahl 415 San Francisco(englische Aussprache [ˌsæn fɹənˈsɪskoʊ], deutsch auchSan Franzisko[4]), offiziellCity and County of San Francisco(Stadt und Kreis San Francisco), ist eine Stadt in der MetropolregionSan Francisco Bay AreaimUS-BundesstaatKalifornienan der Westküste derVereinigten StaatenamPazifischen Ozean. Mit 873.965 Einwohnern (Volkszählung 2020)[5]ist sie die viertgrößte Stadt Kaliforniens. Im globalen Vergleich gilt sie neben ähnlich großen Städten wie etwaFrankfurt am MainoderAmsterdamals mittelgroßeWeltstadt. Der Name der Stadt istspanischenUrsprungs. San Francisco ist nach dem Heiligen Franziskus, alsoFranz von Assisi, benannt. Die Großstadt liegt an der nördlichen Spitze der San-Francisco-Halbinsel, welche dieBucht von San Franciscosüdwestlich abschließt. Das Stadtgebiet, welches deckungsgleich mit dem gleichnamigen County ist (offizielle Eigenbezeichnung der Stadt:City and County of San Francisco), besitzt einen näherungsweise quadratischen Umriss mit ca. 11 km Kantenlänge. Die Stadt wird im Westen vomPazifik, im Norden vomGolden Gateund im Osten von der Bucht begrenzt. San Francisco ist auch berühmt für seine Hügel, die ab dreißig Metern HöheHillgenannt werden; im gesamten Stadtgebiet gibt es derer 42. Im Zentrum des Stadtgebiets liegen die rund 275 m hohenTwin Peaks, die von den spanischenMissionarenauf Grund ihres Aussehens „Los Pechos de la Chola“, auf Deutsch „Die Brüste des Indianermädchens“, genannt wurden. Auf einem Rücken, der die Twin Peaks mit dem benachbarten Mount Sutro verbindet, befindet sich der fast 300 m hohe SendeturmSutro Tower, der das Stadtbild weithin sichtbar dominiert. San Francisco besitzt wie viele US-amerikanische Großstädte ein großräumig strikt rechtwinkliges Straßennetz, welches in der Regel ungeachtet der geografischen Verhältnisse angelegt wurde. Dies führt speziell in den älteren, nordöstlichen Stadtteilen zu teilweise sehr steilen Straßenabschnitten, zu deren komfortabler ÜberwindungAndrew Smith Hallidieum 1870 dieCable Carsentwickelte. Lediglich im Bereich um die höchsten der Hügel (Twin Peaks, Mount Davidson) wurden die Straßenverläufe der Geografie angepasst. San Francisco ist eine der bedeutendsten Hafenstädte an der WestküsteNordamerikas. Diese Bedeutung erlangte die Stadt durch den vom Meer geschütztenHafen(Naturhafen). In der Bucht von San Francisco liegen die bekannte, heute nur noch als Museum dienende GefängnisinselAlcatraz,Angel Island,Treasure Island,Yerba Buena Islandund weitere kleine Inseln. Im Pazifik vor San Francisco liegen dieFarallon-Inseln. Die Nähe der Stadt zurSan-Andreas-Verwerfungbirgt ein erhöhtes Risiko fürErdbeben. Am 18. April 1906 ereignete sich dasbislang schwerste Erdbeben. Es erstreckte sich vonSan Juan BautistabisEurekaund hatte eine Stärke von 7,8 auf derRichterskala. Als Folge von Bränden und Sprengungen wurden dabei rund 3000 Menschen getötet und drei Viertel von San Francisco zerstört beziehungsweise erheblich beschädigt. DasLoma-Prieta-Erdbebenvon 1989 war bis heute das letzte große Beben in der Region (7,1 auf der Richterskala). Es hatte erhebliche Auswirkungen auf Teile der Stadt. Viele Straßen undFreewayswurden beschädigt. DerEmbarcadero Freewayan der nördlichen Seite der Stadt ist dem Beben vollständig zum Opfer gefallen und wurde abgerissen. Teile der oberen Fahrbahn der zweistöckigenBay Bridgefielen auf die darunterliegende Ebene. Experten befürchten für die Zukunft ein noch stärkeres Erdbeben als das von 1906. Im Jahr 2008 veröffentlichte derGeologische Dienst der USAund dasSüdkalifornische Erdbebenzentrumeine Studie. Darin sagen die Forscher im Rahmen einer 30-Jahre-Prognose ein schweres Erdbeben in Kalifornien voraus. Die Wahrscheinlichkeit für ein Beben der Stärke 6,7 liegt demnach bei 99,7 Prozent, für die Stärke 7,5 oder mehr beträgt sie 46 Prozent.[6] Wie in vielen amerikanischen Städten gibt es eineJapantownund eineChinatown. DieChinatown von San Franciscobildet zusammen mit den Chinesen in den Sunset und Richmond Districts eines der größten Chinesenviertel außerhalb derVolksrepublik China. Außerdem gibt es einevietnamesischeGemeinde im Stadtteil Tenderloin, eine derFilipinosin Crocker Amazon, eineitalienische Gemeindein North Beach, einFrench Quartersowie eineirischeundrussischeGemeinde im Richmond District. Der heutehispanischgeprägteMission Districtist einer der ältesten Stadtteile. Er geht auf eine der 21 Missionen zurück, die von spanischen Missionaren gegründet wurden.Russian Hillerhielt den Namen aufgrund der während desGoldrauschesdort entdeckten Gräber russischer Trapper.Haight-Ashburyerlangte in den 1960er-Jahren seine Berühmtheit als eine der prominentesten Ansammlungen vonHippies. DasCastroist das größteLesben- und Schwulenviertelder Stadt. Die größteafro-amerikanischeGemeinde befindet sich südöstlich von Bayview und Hunters Point. Richmond, an der Westseite der Stadt nördlich desGolden Gate Parksgelegen, ist im Wesentlichen vonasiatischenEinwanderern geprägt. Südlich derMarket Street, die als eine der wenigen Straßen quer verläuft, liegt der Stadtteil Soma (South of Market), bekannt für seine Galerien und Kunstaktivitäten. Das vorherrschendemediterraneKlima wird stark von der Lage San Franciscos an der Küste desPazifikbeeinflusst, insbesondere durch den aus nördlicher Richtung kommenden, kaltenKalifornienstrom. Die Sommer sind deshalb kühler als in anderen Regionen vergleichbarer geografischer Breite und nahezu regenfrei. Die wärmsten Monate sind August und September, ebenfalls ungewöhnlich für diese geografische Lage. Die Winter sind vergleichsweise mild und niederschlagsreich, Frost gibt es nur sehr selten. Die Tagestemperatur im Sommer reicht von 15 bis 25 Grad Celsius. Bekannt sind aber auch die morgendlichen Nebelschwaden, die vom Meer her über die Hügel ziehen. DieserAdvektionsnebelentsteht dadurch, dass sich die durch Westwinde transportierte, relativ warme, feuchte Luft vom offenen Meer kommend über demKalifornienstromabkühlt und kondensiert. Ebenso bekannt sind die sehr unterschiedlichenMikroklimate. So kommt es vor, dass es am Golden Gate empfindlich kühl und im Stadtzentrum gleichzeitig hochsommerlich warm ist. Ursprünglich war die Bucht von San Francisco durch den Indianerstamm derMuwekma Ohlonebesiedelt, die bis ins 19. Jahrhundert fast ausgerottet waren. Die spanischen Eroberer schickten im 16. Jahrhundert zwei Expeditionen in den Norden Amerikas, um die Westküste zu erkunden.Hernán Cortéshatte eine „Halbinsel zwischen Golf und Ozean“ entdeckt und nannte sie California. Aber die offizielle Entdeckung erfolgte mitJuan Rodríguez Cabrilloerst zehn Jahre später. Die schwer zu findende Einfahrt zur Bucht wurde erst 1775 entdeckt, obwohl viele Entdecker, unter anderemFrancis Drake, die Region schon im 16. Jahrhundert erkundeten. Jedoch verhinderte der Nebel oftmals die Sicht auf die Meerenge und die Bucht. Die ersten Europäer siedelten ab 1776 in der heutigen Stadt. Spanische Soldaten undMissionaregründeten die heutige KircheMission Doloresam 29. Juni an einer Lagune, die sieNuestra Señora de los Doloresnannten, sowie einPresidioamGolden Gatezur Sicherung der Mission. Die Stadt wurde später von den Missionaren in Gedenken an den HeiligenFranz von AssisiSan Francisco de Asísgenannt, auf Englisch wurde daraus dannSaint Francis. Einer nahe gelegenen Siedlung wurde der NameYerba Buenagegeben. (Hierba Buena bedeutet wörtlich gutes Kraut, und ist die spanische Bezeichnung für eine lokal vorkommende Minzeart.) Der ausPetra/MallorcastammendeFranziskanerpaterJunípero Serraleitete damals die Missionsgründungen. Er wird noch heute sehr verehrt. Im Jahr 1792, 300 Jahre nach der Entdeckung Amerikas durchChristoph Kolumbus, gründete der britische ForscherGeorge Vancouvereine kleine Niederlassung nahe Yerba Buena (die spätere Downtown von San Francisco). Sie wurde zu einer Ausgangsbasis für europäische und russische Siedler, Pelzhändler und Pioniere. 1846 organisierte der amerikanische Entdecker und PolitikerJohn Charles Frémonteinen Aufstand unter den anglo-amerikanischen Siedlern der Region. Auch die hispanischen Kalifornier erhoben sich gegen Mexiko (Junta von Monterey). Am 14. Juni 1846 riefen 33 US-amerikanische Siedler dieRepublik Kalifornienaus; sie wussten nicht, dass derMexikanisch-Amerikanische Kriegbereits einen Monat zuvor begonnen hatte. Anfang Julieroberten eine Fregatte und zwei Sloops der US NavyMonterey; danach gab Frémont seine Idee einer eigenen Republik auf. San Francisco erlebte einen ersten großen Aufschwung durch den 1848 beginnendenGoldrausch in Kalifornien. Der NameGolden Gateleitet sich davon ab. Die Bevölkerungszahl stieg dabei von etwa 900 auf über 20.000 in einem einzigen Jahr an. Im Umfeld der Mission wurden bald mehr irische als spanische Grabsteine aufgestellt. Viele Schiffe wurden von ihren Besitzern aufgegeben und havarierten im Hafen. Um neuen Platz zu schaffen, wurde beschlossen, die Schiffswracks zu nutzen, und man füllte den Hafen mit Erde und Schutt auf. Große Teile der heutigen Innenstadt sind auf diesenLandfillsgebaut. In dieser Zeit entwickelte sich die Stadt auch zum wirtschaftlichen Zentrum Kaliforniens. Es wurden Banken – wie etwa dieWells Fargo Bank– und auch viele andere namhafte Unternehmen in San Francisco gegründet, beispielsweiseLevi Strauss & Co.und dieGhirardelli Chocolate Company. Um Platz in der aufstrebenden Stadt zu schaffen, beschäftigte das Rathaus mitWilliam Mathewson EddyeinenLandvermesser, der im Dezember 1849 den einzigen heute erhaltenen Stadtplan jener Zeit veröffentlichte. Er schuf innerhalb von drei Monaten etwa 600 neue Parzellen für interessierteSiedler. Der spätere selbsternannteKaiser der Vereinigten StaatenundSchutzherr von MexikoJoshua Nortonwanderte 1849 mit einem Startkapital von 40.000 $ aus Südafrika nach San Francisco ein. Den durch Grundstücksgeschäfte erlangten Reichtum verspielte er bis 1859 wieder. Durch Spekulationen gescheitert und bankrott, wollte Norton die Dinge nun selbst in die Hand nehmen und ernannte sich am 17. September 1859 in Briefen an die örtlichen Zeitungen und Politiker selbst zum Kaiser von Amerika und Schutzherren von Mexiko. Einundzwanzig Jahre lang erließ er nun kaiserlicheEdikte. Am Morgen des 18. April 1906 verwüstetenein Erdbeben und ein FeuerSan Francisco. Im Allgemeinen wird die Zahl der Todesopfer auf 700 geschätzt, einige Quellen geben aber eine drei- bis viermal höhere Zahl an. Das Missionsgebäude überstand das Erdbeben ohne Schaden, so dass es heute das älteste Gebäude der Region ist. In den 1930er Jahren wurden dieGolden Gate Bridgenach Norden und dieOakland Bay Bridgenach Osten fertiggestellt. Dadurch war die Stadt wesentlich einfacher zu erreichen, und die Bevölkerungszahl stieg nochmals erheblich an. 1939 fand auf derTreasure IslandeineWeltausstellung(„Golden Gate International Exposition“) statt. Die Insel wurde eigens für die Ausstellung neben der Yerba Buena Island aufgeschüttet. Nach der Ausstellung war sie bis 1996 ein Stützpunkt der US-Marine. Das Gelände wird seitdem wieder als Wohnort genutzt. 1945 fand die Nachkriegskonferenz statt, aus der als Ergebnis dieCharta der Vereinten Nationenund dieUNhervorgingen. San Francisco gilt somit als Gründungsstätte derVereinten Nationen. In den 1960er Jahren wurde die Stadt zu einem Zentrum der US-amerikanischenGegenkultur, der Auflehnung gegen das politische Establishment und derGegenöffentlichkeit. DieHippie-Bewegung feierte 1967 in der Stadt imSummer of Loveeinen Höhepunkt.Janis Joplinund Bands wieGrateful DeadoderJefferson Airplanebeeinflussten die Rockmusik weltweit. Begünstigt wurde das durch die Nähe desBerkeley-CampusderUniversity of California. Autoren wieTimothy Leary,Phil K. DickoderRobert Anton Wilsonlebten damals dort.Scott McKenziesAufnahme des vonJohn Phillipsgeschriebenen SongsSan Francisco (Be Sure to Wear Flowers in Your Hair)wurde zu einem Welthit. Seit den 1970ern zogen zunehmendHomosexuellein die Stadt, insbesondere in denCastro District. Die Stadt gilt bis heute als „die“ Stadt der Homosexuellen in den USA, undQueerPoliticshat großen Einfluss auf die Stadtpolitik. Diese Zeit wird auch in den „Stadtgeschichten“ vonArmistead Maupinausführlich beschrieben. Ende des 20. Jahrhunderts waren die Stadt und das nahegelegeneSilicon ValleyZentrum des wirtschaftlichen Aufschwungs derInformationstechnik. Während desDotcom-Boomsin den 1990ern zogen immer mehr Softwarefirmen, Unternehmer und Marketingexperten nach San Francisco und beeinflussten die soziale Landschaft gravierend. Ehemals arme Arbeiterviertel wandelten sich zu „IN“-Gegenden; die Immobilienpreise stiegen stark.[7] In San Francisco liegen 17National Historic Landmarks.[8]Insgesamt sind 185 Bauwerke und Stätten der Stadt imNational Register of Historic Placeseingetragen.[9] Die Einwohnerzahl der Stadt San Francisco sank von 1950 bis 1980 kontinuierlich, steigt aber seitdem wieder. Die Zunahme der Bevölkerung von 1980 bis 2010 betrug knapp 20 %. Zu beachten ist, dass sich diese Zahlen nur auf das Gebiet der politischen Stadt beziehen, die Metropolregion ist jedoch viel größer. Nach demCensus 2010leben in San Francisco 805.195 Menschen. Von diesen sind 48,1 % Weiße, 33,3 % Asiaten, 6,1 % Schwarze oder Afroamerikaner, 0,5 % Indianer und 0,4 % Pazifische Insulaner. 6,6 % gehören anderenRassenan und 4,7 % zwei oder mehr Rassen. Unabhängig von der Rasse sind 15,1 % Hispanics oder Latinos. Nicht-Hispanische Weiße machen 41,9 % der Bevölkerung aus, womit es sich bei San Francisco um eineMajority-Minority-Stadthandelt. Im Verhältnis zur übrigen Bevölkerung leben in San Francisco die meisten Asiaten in ganz Nordamerika, dieChinatownist nach derjenigen New Yorks die größte der USA. Im Gegensatz zur üblichen Verteilung in den meisten Gegenden der Welt leben in San Francisco mehr Männer als Frauen. Das Verhältnis beträgt 103,1: 100. Die Nähe zumSilicon Valleymit High-Tech-Unternehmen wieAppleundFacebookhat die Preise für Mietwohnungen und Häuser in der Stadt in den vergangenen Jahren explodieren lassen. Nach Schätzungen haben in San Francisco und der Bay-Area um die 40.000 Menschen keine Wohnung. Wie in vielen US-amerikanischen Großstädten finden sich auf den Straßen Notunterkünfte aus Campingzelten, zugleich werden öffentliche Plätze durch das Fehlen von Parkbänken so gestaltet, dass der dauerhafte Aufenthalt von Obdachlosen erschwert wird. Die hohe Obdachlosigkeit und die gemeinsame Grenze von Kalifornien und Mexiko führen dazu, dass San Francisco in großen Ausmaß von derOpioidkrisebetroffen ist. Stadtviertel wie Tenderloin und Teile des Missions-Distrikts werden aufgrund der allgegenwärtigen Drogenkonsumenten von Anwohnern und Touristen zunehmend gemieden. Um die Polizei und die Gefängnisse zu entlasten werden Diebstähle bis zu einem Wert von 950 $ nicht mehr strafrechtlich verfolgt. Waren in den Supermärkten der Innenstadt sind aus diesem Grund zunehmend in Vitrinen eingeschlossen und können nur durch die Verkäufer entnommen werden.[12][13][14] Seit den 1960ern und demSummer of Lovehat sich die Stadt zu einem linken Zentrum der USA und zur Hochburg derDemokratischen Parteigewandelt. Der Trend zeigt sich ebenfalls in der Tatsache, dassDwight D. Eisenhower1956 als letzter republikanischer Präsidentschaftskandidat diesen Wahlbezirk gewonnen hatte. Diese eher links orientierte politische Einstellung wird auch sichtbar in einer nicht umgesetzten Verordnung vom Frühjahr 2006, in der das Tragen, Besitzen und Verkaufen von Handfeuerwaffen innerhalb des Stadtgebietes verboten werden sollte. San Francisco besitzt seit 1856 einen Doppelstatus als konsolidierte Stadt mit Landkreis. Das bedeutet Stadt undCounty(Landkreis) unterliegen derselbenJurisdiktion. Es ist die einzige Verwaltungskonstruktion dieser Art in Kalifornien. Der Bürgermeister der Stadt ist auch gleichzeitig Vorsitzender des Verwaltungsrates des Countys. Ihm beigeordnet sind elf weitere Mitglieder im Verwaltungsrat, das Board of Supervisors. Diese werden aus ihrem jeweiligen District (Bezirk), in dem sie leben, von den Bürgern direkt gewählt. Als bekanntestes Beispiel wäre hierHarvey Milkanzuführen. Das Wappentier stellt einen Phönix dar, der aus der Asche aufersteht. Dies wurde in Anlehnung an die verheerende Katastrophe von 1906 gewählt. Unter dem Phönix steht der Spruch: „Oro en Paz, Fierro en Guerra“, was so viel bedeutet wie: „Gold im Frieden, Eisen im Krieg“. San Francisco listet folgende neunzehnPartnerstädteauf:[15] San Francisco ist bekannt für die große AnzahlviktorianischerHäuser. Sie wurden während der Goldgräberzeit Mitte des 19. Jahrhunderts gebaut. Über die Hälfte derVictorianssind dem Erdbeben und dem darauf folgenden Feuer von 1906 zum Opfer gefallen. Grundlegend sind die vier viktorianischen Stilrichtungen:Stick Style,Italianate-Stil, einer der populärsten Stile in der damaligen Zeit,Gothic Revival StyleundQueen Anne Style. Neben weiteren basiert die Mehrzahl der ursprünglichen Häuser auf diesen vier architektonischen Stilrichtungen. Neben der Mission Dolores Kirche (gegründet am 9. Oktober 1776) gibt es noch weitere sehenswerte Sakralbauten in der Stadt. Die 1971 eingeweihteCathedral of Saint Mary of the Assumptionliegt an der Gough Street auf Cathedral Hill. Die 2.500 Plätze, die sich um den Altar in einem Halbkreis anordnen, bieten den Gläubigen in der eher spartanisch ausgestatteten Kirche Platz für Messen. Die zentrale Kuppel, die sich über 15 Etagen erstreckt, wird durch Strebepfeiler gestützt. Die Wände können somit aus Glas bestehen und verleihen dem Gotteshaus eine luftige Anmut. Die Kathedrale ersetzte die alte Saint-Mary-Kirche, die 1962 durch einen Brand zerstört wurde. Der Neubau kostete ca. sieben Millionen Dollar. 1987 zelebrierte PapstJohannes Paul II.hier während seines Besuches imErzbistum San Franciscoeine Messe. Die neugotischeSt. Patrick’s Catholic Churchbefindet sich an der Mission Street, dieSt. Paul’s Catholic Churchim Stadtteil Noe Valley. Erhalten ist als Pfarrkirche auch die erste katholische Kathedrale der Stadt, dieOld St. Mary’s Cathedralan der California Street. DieEpiskopalkircheGrace Cathedralbefindet sich aufNob Hill. Sie wurde im Jahr des Goldrausches 1849 als kleine Kapelle gebaut. Die dritte Kirche, die erstmals GraceCathedralgenannt wurde, ist beim großen Erdbeben von 1906 zerstört worden. Der Eisenbahnbaron und Bankier Crocker vermachte sein Grundstück nach der Katastrophe der benachbarten Gemeinde. Die Arbeiten für das heutige Gebäude begannen 1928. Lewis Hobart baute sie im französisch-gotischen Stil. Die 1964 vollendete Kathedrale ist die drittgrößte Episkopalkirche in den USA. Sie soll durch ihre Beton-Stahl-Konstruktionerdbebensichergebaut sein. DieSaints Peter and Paul Churchan der 666Filbert Streetist eine römisch-katholische Kirche im StadtteilNorth Beach. Sie war Drehort für den FilmSister Act 2. 1923 wurde dieSan Francisco OperavonGaetano Merola(1881–1953) gegründet. Zur Eröffnung am 26. September 1923 im City Civic Auditorium wurdeLa Bohèmeaufgeführt. Im Jahr 1932 eröffnete das Ensemble das neu errichtete War Memorial Opera House am 15. Oktober mit einer Aufführung vonTosca. Es wurden bis heute viele erfolgreicheInszenierungenaufgeführt. Die legendäreGreat American Music Hallan der O’Farrell Street wurde 1907 nach dem großen Beben errichtet. Bis zurGroßen Depression1933 in den Vereinigten Staaten wurde sie erfolgreich geführt. Erst 1936 brachte Sally Rand neues Leben in die Halle. 1948 wurde das Haus in einen Jazzclub umgewandelt. In den 1950ern verfiel das Gebäude zunehmend und wurde 1972 von Fans vor dem beabsichtigten Abriss bewahrt. In der Great American Music Hall traten viele bekannte Künstler auf, beispielsweise Duke Ellington, Sarah Vaughan, Count Basie,Van Morrison, the Grateful Dead und Bobby McFerrin. DasAmerican Conservatory Theateran der Geary Street ist ein nichtkommerzielles Theater, das sowohl klassische als auch zeitgenössische Werke aufführt. Das Bill Graham Civic Auditorium an der Ecke Grove und Larkin Street ging aus einem Ausstellungsgebäude der Panama-Pacific International Exposition von 1915 hervor. DasAuditoriumfasst circa siebentausend Besucher. Viele Konzerte und Veranstaltungen fanden hier im Verlaufe des Bestehens statt. Ähnlich wie amHollywood Walk of Famewerden hier Bronzemedaillen von ausgezeichneten Künstlern in den Boden des Eingangsbereichs eingelassen. Bill Grahameröffnete 1965 einen Rock-Tanzpalast an der Ecke Fillmore und Geary Street. 1968 wurde er in den ehemaligen Carousel Ballroom an der Market Street verlegt und inFillmore Westumbenannt. Zur gleichen Zeit nahm dasFillmore Eastin der New Yorker Second Avenue ebenfalls den Betrieb auf. Kuriositäten wie das Museum antikerVibratorenoder eineBarbra Streisandgewidmete Ausstellung kommen hinzu. Außer den Bauwerken und Museen gibt es noch viele weitere Sehenswürdigkeiten. Dass alte, nicht mehr gebrauchte Lagerhallen im Hafengebiet nicht verkommen oder abgerissen werden müssen, zeigt das ViertelFisherman’s Wharfmit seinen Cafés, Kneipen und Restaurants. DiePier 39, ein Teil von Fisherman’s Wharf, ist ein ganzjähriger Rummel mit Souvenir-Läden, Fahrgeschäften und Restaurants. Hier gibt es auch einAquarium. An Pier 39 haben sich Seelöwen auf Anlegestellen niedergelassen und nutzen diese als Ruheplätze. Haupteinkaufstraße ist der nördliche Teil der Market Street und die Gegend um den Union Square, die zum Bummeln und Shoppen einlädt. Eine Sehenswürdigkeit in der Innenstadt sind dieCable Cars, eines der Erkennungszeichen von San Francisco. Heute fahren nur noch drei Linien. Sie dienen hauptsächlich als Touristenattraktion, weniger als Verkehrsmittel. Für Interessierte wurde zusätzlich ein Cable Car Museum eingerichtet. Des Weiteren kann man eine der wenigenKurvenrolltreppenim Westfield-Kaufhaus an der Market Street bewundern. Weltberühmt ist die vonJoseph B. StrausskonstruierteGolden Gate Bridgeüber das Golden Gate, die Öffnung zur Bucht von San Francisco. Sie ist 2,8 km lang und 25 m breit, die beidenPylonesind jeweils 227 m hoch und stehen 1.280 m voneinander entfernt. Die Brücke wurde am 19. April 1937 fertiggestellt und am 28. Mai des Jahres offiziell für den Verkehr freigegeben. Der blumengeschmückte, gewundene Teil derLombard Streetwird als „kurvenreichste Straße der Welt“ bezeichnet. Bei einem Gefälle von 27 % war es notwendig, die Straße inSerpentinenzu führen. Mit den gepflegten Häusern und der Bepflanzung entwickelte sich dieser Straßenabschnitt zu einem Touristenmagneten. Der WolkenkratzerTransamerica Pyramidim Financial-District ist ein typisches Wahrzeichen der Stadt mit Verkaufs- und Büroräumen. Das Gebäude wurde Ende der 1960er vonWilliam Pereiraentworfen und 1972 fertiggestellt. Es ist 260 m hoch und hat 48 Stockwerke. Die Pyramide mit ihren prägnanten „Flügeln“ sticht in der Skyline besonders hervor. Sie ist für Touristen nicht zugänglich. Die spanischeMission Dolores, das älteste Bauwerk in der Stadt, wurde am 9. Oktober 1776 vom Franziskaner PadreJunípero Serragegründet. Sie hat als eines der wenigen Gebäude mehrere Erdbeben fast unbeschadet überstanden. Sie ist eine der 21 Missionen amEl Camino Realaus der Zeit der spanischen Eroberung. 1958 entstanden hier Filmszenen fürAlfred HitchcocksThrillerVertigo – Aus dem Reich der Toten. DasPresidiowurde im Zuge der Eroberung Neuspaniens zusammen mit der Mission Dolores errichtet. Direkt amGolden Gatewar es von 1776 bis 1994 der wichtigste Militärstützpunkt an der Westküste und enthält heute auf sechs Quadratkilometern Gewerbe- und Wohngebiete. Es ist außerdem Schauplatz von vielen Filmen und TV-Produktionen. 2005 wurde im Presidio dasLetterman Digital Arts Centereröffnet, das die Firmen vonGeorge Lucas(Lucasfilm, Industrial Light & Magic, LucasArts) beherbergt. Das Presidio selbst taucht imStar-Trek-Universum vonGene Roddenberryals Hauptsitz der Sternenflotte der Föderation auf. DiePainted Ladiessind ein Straßenzug mit historischen Häusern am Alamo Square, die mit ihren gepflegten Fassaden den Blick auf die Skyline untermalen. Haight-Ashburyist ein östlich desGolden Gate Parksgelegener Stadtteil von San Francisco, benannt nach der Kreuzung von Haight Street und Ashbury Street. Bekanntheit erlangte das Gebiet in den 60er Jahren durch dieBeatnik- undHippie-Bewegung. Berühmte Musiker, die in Haight-Ashbury ihren Wohnsitz hatten und die dortige Musikszene entscheidend mitprägten, sind sowohlJanis JoplinundJimi Hendrixwie auch die GruppenGrateful DeadundJefferson Airplane. Haight-Ashbury ist auch heute noch Anziehungspunkt für eine alternative Gegenkultur. DerCoit Tower, ein Aussichtsturm auf demTelegraph Hillin San Francisco, wurde von Arthur Brown Jr. und Henry Howard 1934 erbaut. Lillie Hitchcock Coit, ein Kind der High Society, war eine große Verehrerin der Feuerwehrleute von San Francisco. Der Legende nach half sie bei einem Brand in der Nähe von Telegraph Hill und wurde so das Maskottchen derEngine Company No. 5 of the Volunteer Fire Department(Freiwillige Feuerwehr). Bei ihrem Tod im Jahr 1929 hinterließ sie der Gemeinde 100.000 $ für die Verschönerung der Stadt. Mit dieser Spende wurde 1934 der 64 Meter hohe Aussichtsturm im Stil desArt décozu Ehren der Freiwilligen Feuerwehr errichtet. Die ZwillingshügelTwin Peaksim Süden der Stadt sind ein markantes Wahrzeichen. Die nähere Umgebung bietet viele weitere Natursehenswürdigkeiten: diePazifikküste, die WeinbaugebieteNapa ValleyundSonomaValley, die Redwood-Bäume imMuir Woods National Monumentim Norden, die Strände derHalf Moon Bayund derPoint Reyes National Seashoremit dem markanten Leuchtturm. San Francisco ist Heimat dreier Teams in den amerikanischen Profiligen: In San Francisco gibt es mit demOracle Parkein Baseballstadion, in welchem die San Francisco Giants ihre Heimspiele austragen. Zuvor waren die Giants von 1960 bis 1999 imCandlestick Parkbeheimatet. Von 1971 bis 2013 war der Park auch die Heimspielstätte der 49ers. Nach dem Umzug der 49ers in dasLevi’s StadiuminSanta Clarawurde das Stadion am 14. August 2014 geschlossen und 2015 abgerissen.[18]2019 zogen dieGolden State WarriorsvonOaklandnach San Francisco in das neueChase Centerum. Von 1962 bis 1971 war dasFranchiseunter dem NamenSan Francisco Warriorsschon mal in der Stadt ansässig. MitCalifornia Victoryhatte die Stadt auch einen Fußballverein, der 2007 in derUSL First Division, der damals zweithöchsten Spielklasse im nordamerikanischen Fußball spielte. Nach einer Saison wurde der Spielbetrieb wieder aufgegeben. In San Francisco sind durch das Zusammentreffen von Einwohnern mit vielfältigen kulturellen Hintergründen auch deren kulinarische Traditionen fest verankert. Aus dem asiatischen Raum sind das vor allem die japanische und verschiedene chinesische Küchen, des Weiteren auch die koreanische, vietnamesische und thailändische. Neben der leichten kalifornischen Küche gibt es auch italienische, dänische, russische, mexikanische und deutsche Restaurants. Durch die Lage am Pazifischen Ozean finden sich insbesondere Fisch und andere Meeresfrüchte auf den Speisekarten. So gibt es am Fisherman’s WharfKrabbenküchen, die beispielsweise „Krabbensuppe in Sourdough Bread“ anbieten. DiesesSauerteigbrot, das während der Goldgräberzeit aus Europa eingeführt wurde – und den Goldgräbern den Spitznamen Sourdough eintrug – wurde zu einer Spezialität der Stadt. Der Sourdough Sam, ein vollbärtiger Mann in Goldgräbermontur, ist das Maskottchen der American-Football-MannschaftSan Francisco 49ers. Die Küche in Kalifornien und besonders in San Francisco und derBay Areagilt als eine der besten der USA. So erscheint seit 2006 eine ausschließlich der Bay Area und San Francisco gewidmete Ausgabe des bekannten RestaurantführersGuide Michelin. Ein 1947 veranstaltetesFestival of Modern Poetrybegründete die literarische BewegungSan Francisco Renaissance. Hier ist die Avantgarde der amerikanischen Literatur vertreten. In den 1950ern wurden die sogenannten Coffee houses, ähnlich den europäischenKaffeehäusern, zu einem zentralen Treffpunkt von Künstlern, Poeten und politischen Aktivisten.Francis Ford Coppolaschrieb große Teile des Drehbuchs seiner TrilogieDer Pateim Caffè Trieste, das 1956 eröffnet wurde. In San Francisco wurden zahlreiche bekannte Filme gedreht, darunter: Mehrere Filme, wie zum BeispielFlucht von AlcatrazundThe Rock, spielen auf der nahe gelegenen GefängnisinselAlcatraz. Weitere Filme wurden auf dem Gelände desPresidiosgedreht. Zudem spielen die FilmeEin toller KäferundHerbie groß in Fahrtaus der FilmreiheHerbiein der Stadt. Zu den bekanntesten Serien, die in San Francisco spielen, zählenDie Straßen von San Francisco,Nash Bridges,Full House,Monk,Dharma & Greg,Kung Fu,Die Fälle des Harry Fox,Looking, dieMystery-SerieCharmed – Zauberhafte HexenundEli Stone. Durch diese Serien erlangte San Francisco ebenfalls einen guten Werbeeffekt. BeiCharmedz. B. wurde häufig die erste Minute nach den Opening Credits für Luftaufnahmen und Aufnahmen quer durch die Stadt genutzt. Das Hauptquartier derSternenflotteausStar Trekliegt ebenfalls in San Francisco, so dass die Stadt auch für das Franchise eine wichtige Rolle spielt und dementsprechend regelmäßig in den Serien und Kinofilmen vorkommt.[19] Das LiedSan Francisco (Be Sure to Wear Flowers in Your Hair)vonScott McKenziestammt aus den 1960er Jahren und ist weltweit ein Klassiker geworden. In den 1980er Jahren entwickelte sich San Francisco und die umliegendeBay Areazu einem bedeutenden Zentrum derMetalszeneund wurde zum Ausgangspunkt derThrash-Metal-Bewegung. Besondere Bedeutung haben hier insbesondere die BandsMetallica,TestamentundExodus. Weitere Bands aus San Francisco sind u. a.:Grateful Dead,Jefferson Airplane,Dead Kennedys,Primus,The Residents,Flipper,Tuxedomoon,ChromeundMoby Grape. Die Metropolregion San Francisco-Oakland erbrachte 2016 eine Wirtschaftsleistung von 470,5 Milliarden US-Dollar und belegte damit Platz 7 unter den Großräumen der USA und belegt auch weltweit einen der vordersten Ränge. San Francisco zählt zu den wohlhabendsten und dynamischsten Städten des Landes.[20]Die Arbeitslosenquote betrug 2,4 Prozent und lag damit deutlich unter dem nationalen Durchschnitt von 3,8 Prozent (Stand: Mai 2018).[21]In einer Rangliste der Städte mit der höchsten Lebensqualität weltweit belegte San Francisco im Jahre 2018 den 30. Platz von 231 untersuchten Städten und den ersten innerhalb der Vereinigten Staaten.[22] San Francisco ist Sitz einiger großer Unternehmen wie etwaLevi Strauss & Co.,GAP,Wells FargoundGeorge Lucas’ FirmaIndustrial Light & Magic. Die Stadt gilt als Finanzzentrum Kaliforniens und als einer der bedeutendstenFinanzplätze der Welt. In einer Rangliste der wichtigsten Finanzzentren weltweit belegte San Francisco den 8. Platz (Stand: 2018).[23]Ende des 20. Jahrhunderts siedelten sich Firmen derNew Economyhier an. Ein großer Arbeitgeber in der Region ist auch dieUS-Navy, die hier einige Stützpunkte unterhält. Regional wird im nahe gelegenenNapaundSonoma ValleyWein von internationalem Rang angebaut (siehe auchWeinbau in Kalifornien). Seit 1990 sitzt hier das PharmaunternehmenNektar Therapeutics. 2005 wurde San Francisco der Hauptsitz des kalifornischenStammzellenforschungsprogramms.[24][25] Der Fremdenverkehr spielt eine wichtige Rolle für die Wirtschaft von San Francisco. Mit 3,9 Millionen ausländischen Besuchern stand San Francisco 2016 auf Platz 33 der meistbesuchten Städte weltweit. Touristen brachten im selben Jahr Einnahmen von 5,9 Milliarden US-Dollar. Die meisten ausländischen Besucher kamen aus Asien und Europa.[26] San Francisco hat für amerikanische Verhältnisse ein ausgeprägtes öffentliches Verkehrsnetz, das durch dieSan Francisco Municipal Railway, abgekürztMUNI, betrieben wird. Neben – teilweise historischen –Straßenbahnenaus aller Welt, demOberleitungsbus San Francisco,Omnibussenund der U-BahnMuni Metroverkehren in manchen Straßen der Stadt dieCable Cars, seilgezogene Straßenbahnen aus dem 19. Jahrhundert. Es gibt heutzutage drei Cable-Car-Linien: Die California-Line, die Powell-Mason-Line und die Powell-Hyde-Line. Am ehesten dem verbreiteten Bild von San Francisco mit seinen steilen Hügeln kommt die Powell-Hyde-Line nahe. Vom Embarcadero werdenFährverbindungenzu den angrenzenden Orten an der Bucht angeboten. Die Region östlich derBucht von San Franciscolässt sich mit demS-Bahn-ähnlichen SystemBay Area Rapid Transit, abgekürztBART, erreichen. Die San Francisco-Halbinsel und dasSilicon Valleysind durch denCaltrainmit der Stadt verbunden, auch dieses System ist einer S-Bahn vergleichbar. In der Nähe des Fußes derBay Bridgelag der BusbahnhofTransbay Terminal, von dem aus zahlreiche Orte derBucht von San Franciscozu den Hauptverkehrszeiten mit Expressbussen erreicht werden konnten. Die Benutzung derMUNIist für Personen bis zu 18 Jahren generell kostenlos.[27] Die nächstgelegenen vonAmtrakbetriebenenFernverkehrsbahnhöfebefinden sich inEmeryville,OaklandundSan José. DieDiridon Stationist beispielsweise ein beliebter Umsteigebahnhof in der Downtown vonSan José. Es bestehen von hier Anschlüsse mit dem RegionalzugCaltrainins Zentrum von San Francisco (siehe beiÖPNV), von Emeryville und Oakland mitThruway-Bussen der Amtrak sowie denBART-Zügen (sieheÖPNV). Emeryville ist Ausgangspunkt des transkontinentalen FernverkehrszugesCalifornia ZephyrnachChicago. Der FernzugCoast StarlightRichtungSeattlebzw.Los Angeleshält an allen drei Fernverkehrsbahnhöfen. 2018 wurde dasSalesforce Transit Centerals Busbahnhof eröffnet, das langfristig auch zentraler Eisenbahnknotenpunkt werden soll. Die Straßen von San Francisco sind ungeachtet der geologischen Beschaffenheiten überwiegend schachbrettförmig angelegt. Das heißt, sie verlaufen direkt über die Hügel. Daher sind viele Straßen extrem steil. So müssen bergab parkende Autos aus Sicherheitsgründen ihre Räder zum Straßenrand hin und bergauf parkende Autos vom Straßenrand weg eingeschlagen haben. Nach Norden ist San Francisco über dieGolden Gate Bridgemit demMarin Countyverbunden. Richtung Osten führt die ursprünglich 1936 errichteteSan Francisco-Oakland Bay BridgeüberYerba Buena IslandnachOakland. Der östliche Teil der Brücke wurde mittlerweile durch einen 2013 eröffneten Neubau ersetzt. DerSan Francisco International Airport[SFO] ist nachLos Angelesder bedeutendste internationale Flughafen an der US-amerikanischen Westküste. Er liegt 12,9 km südlich etwas außerhalb der Stadt direkt an der Bucht. Der Standort wurde eigens für den Flughafen in der Bucht aufgeschüttet. Der Hafen von San Francisco (englischPort of San Francisco)[28]war lange Zeit der größte und geschäftigste an der ganzen amerikanischen Westküste. Die Piers wurden in den 1970er Jahren mit demAufkommen von Containernund dem Wachstum der durchschnittlichen Schiffsgröße zu klein und damit überflüssig. Der gesamte Container-Umschlag wurde nachOaklandauf der gegenüberliegenden Seite der Bucht ausgelagert. DiePiersverfielen lange Zeit. Teilweise wurden sie von der Stadt verkauft, bis man sich besann und einige (etwa Pier 39 im HafenviertelFisherman’s Wharf) zu neuem Leben erweckte. Heute sind vom alten Hafen nur noch wenige Piers und die alte Hafenmeisterei am Anfang derMarket Streetübrig. Dieses Gebäude wurde in den letzten Jahren aufwendig renoviert. Da die Piers vollständig aus Holz sind, besteht eine ständige Brandgefahr. Ende der 1990er brannte eines dieser alten Bauwerke ab. Am 23. Mai 2020 kam es zu einem Großbrand in einem Lagerhaus auf dem historischen Kai Pier 45.[29][30] DiePrintmedienlandschaftin San Francisco ist so vielfältig wie seine Einwohner. Hier seien nur einige wichtige Publikationen genannt. Zu den Publikationen für die unterschiedlichen ethnischen Gruppen zählt dieSing Tao Daily. San Francisco und die Bay Area sind der fünftgrößte Fernseh- und viertgrößte Radiomarkt in den USA. Alle großen Fernseh-Networkshaben entsprechende lokale Vertragssender, die das Programm der Networks in entsprechenden Zeitfenstern (meistPrime-time) übertragen. LediglichCNNundBBChaben direkte regionale Filialen. In San Francisco gibt es folgende lokaleFernsehsender: In San Francisco befindet sich der Hauptsitz desInternetarchivsmit 450 Milliarden Webseiten (Stand: März 2015). Die Vorwahl 415 von San Francisco war bis zum 20. Februar 2015 eine der letzten Vorwahlen einer US-Großstadt, die bei Ortsgesprächen nicht mitgewählt werden musste. Es wurden also nur die 7 Ziffern der Rufnummer gewählt ohne die Vorwahl 415. Seit der Einführung der neuen zusätzlichen Vorwahl 628 am 21. Februar 2015 ist auch in San Francisco die Anwahl einer Telefonnummer im Ortsbereich im zehnstelligen Format verpflichtend, man muss also bei Anrufen von einer Nummer mit Vorwahl 415 zu einer anderen Nummer mit der Vorwahl 415 diese jetzt mitwählen. San Francisco ist der Geburtsort zahlreicher prominenter Personen. SieheListe von Persönlichkeiten aus San Francisco. Liste der Countys in KalifornienAlameda|Alpine|Amador|Butte|Calaveras|Colusa|Contra Costa|Del Norte|El Dorado|Fresno|Glenn|Humboldt|Imperial|Inyo|Kern|Kings|Lake|Lassen|Los Angeles|Madera|Marin|Mariposa|Mendocino|Merced|Modoc|Mono|Monterey|Napa|Nevada|Orange|Placer|Plumas|Riverside|Sacramento|San Benito|San Bernardino|San Diego|San Francisco|San Joaquin|San Luis Obispo|San Mateo|Santa Barbara|Santa Clara|Santa Cruz|Shasta|Sierra|Siskiyou|Solano|Sonoma|Stanislaus|Sutter|Tehama|Trinity|Tulare|Tuolumne|Ventura|Yolo|Yuba Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geographie 1.1Geografische Lage 1.2Geologie 1.3Stadtgliederung 1.4Klima 2Geschichte 2.1Indianische Besiedlung und erste Europäer 2.219. Jahrhundert und Neuzeit 3Einwohnerentwicklung 3.1Demografie 3.2Obdachlosigkeit / Opioidkrise 4Politik 4.1Bürgermeister 4.2Stadtflagge von San Francisco 4.3Städtepartnerschaften 5Kultur und Sehenswürdigkeiten 5.1Architektur 5.2Kirchen 5.3Opern und Theater 5.4Museen 5.5Sehenswürdigkeiten 5.5.1Golden Gate Bridge 5.5.2Lombard Street 5.5.3Transamerica Pyramid 5.5.4Mission Dolores 5.5.5Presidio 5.5.6Painted Ladies 5.5.7Haight-Ashbury 5.5.8Coit Tower 5.5.9Weitere bekannte Bauwerke 5.6Parks und Plätze"
  },
  {
    "label": 0,
    "text": "Schokolade – Wikipedia Schokolade Inhaltsverzeichnis Etymologie Geschichte Herstellung Handel Marktsituation Schokoladensorten Produkte aus Schokolade und Kuvertüre Trinkschokolade Schokolade und Gesundheit Schokolade und Haustiere Schokoladenkonsum Hersteller Filme Bücher Schokoladenmuseen Siehe auch Literatur Weblinks Einzelnachweise Herstellung von Schokolade Temperieren (Vorkristallisieren) von Schokolade Eintafeln Herstellung von gefüllter Schokolade Rohstoffpreise Subventionen Inhaltsstoffe Ernährungsphysiologie Schadstoffe Positive Wirkungen Einfluss auf Blutdruck und Blutgefäße Annahmen und Irrtümer Ökologische und soziale Aspekte des Schokoladenkonsums Spielfilme Dokumentarfilme Traditionelle Temperierung Impfkristallisierung Temperiergrad One-Shot-Verfahren Wendeverfahren Kaltstempeln Tauchverfahren Schokoladeist einNahrungs-undGenussmittel, das infesteroderflüssigerForm vorkommt und sich überwiegend ausKakaoerzeugnissenundZuckerarten– im Falle von Milchschokolade auchMilcherzeugnissen– zusammensetzt. Schokolade wird in reiner Form genossen und alsHalbfertigfabrikatweiterverarbeitet. Diespanischen ErobererMexikosbrachten das Getränk und die Bezeichnung im 16. Jahrhundert nach Europa mit. Dabei handelte es sich um eine Mischung ausWasser, Kakao,VanilleundCayennepfeffer. Die Wortherkunft ist umstritten. Im Gegensatz zu einer weit verbreiteten Meinung kommt „das Wortchocolatlin keiner frühen Quelle über dasNahuatloder dieaztekische Kulturvor“, weder „im ersten Nahuatl-Spanischen Wörterbuch aus 1555 von Fray Alonso de Molina[1], [noch] inSahagúnsgroßer Enzyklopädie oder imHuehuetlatolli, den ‚Sprichwörtern der Alten‘“. „In diesen ursprünglichen Quellen lautet das Wort für Schokoladecacahuatl, ‚Kakaowasser‘.“[2]Das Getränk wurde von denAztekenkalt getrunken, von denMayaaber heiß genossen.[3] Die meisten Linguisten gehen von einer Nahuatl-Formchocolatlaus.[4][2][5]Einige sehen inchocol, dialektale Variante vonchacau, ein Wort aus einer derMaya-Sprachenin der Bedeutung „heiß“, wobei daran erinnert werden muss, dass auch die Spanier (im Gegensatz zu den Azteken) den Kakao heiß tranken. Chocolatl wäre dann, nach der zuerst von Ignacio Dávila Garibi[6]geäußerten Ansicht, eine Kombination aus einem Maya-Wort und einem Nahuatl-Wort.[2] Ein Ursprungswort in der Formxocol-atlbzw.xoco-atlließe sich am leichtesten mitxocolia(„etwas bitter oder sauer machen“[4]) bzw.xococ(„bitter“[7]) in Verbindung bringen. Die Sprache der Azteken istNahuatl. Alle Deutungen der Nahuatl-Wörter für das Kakaogetränk, zuerstcacahuatl, später dannchocoatlbzw.xocoatltrennen den zweiten Bestandteil desKompositumsalsatl(Nahuatl für „Wasser“) ab. Die Deutung des ersten Wortbestandteils ist aber schwierig[8]und bleibt umstritten. Da sich in der kurzen Zeit seit Ankunft der Spanier nicht die spätere Form durchLautwandelaus der früheren entwickelt haben kann, muss man für das Kompositum einenNeologismusannehmen. Einige Autoren[9]setzen die Nahuatl-Form alschicol-atlan und wollen darin ein Wort für „Schneebesen, Quirl“ (*ci-:englischsmall stick, twig) erkennen, wonach das ganze Wort dann „Quirl-Getränk“ (englischstirrer drink) bedeuten würde. Chocolatlist alsLehnwortinsSpanischeübernommen worden. Von dort (spanischchocolate) verbreitete es sich in alle Sprachen (englischchocolate,französischchocolat,niederländischchocolade). Über niederländische Vermittlung gelangte es ins Deutsche.[10] Das erste Mal wurde derKakaobaum(Theobroma cacao)vermutlich um 1500 v. Chr. von denOlmekengenutzt, die im Tiefland der mexikanischen Golfküste lebten. Um 600 n. Chr. wurde der Kakao dann von denMayaangebaut. Die ersten Kakaobohnen brachteChristoph Kolumbusaus Amerika mit, ohne dass man zu dieser Zeit etwas damit anfangen konnte. 1528 brachte dannHernán Cortésden Kakao nachEuropa. Die Schokolade war aber unverarbeitet ungenießbar. Erst nach der Zugabe vonHonigundRohrzuckerwurde daraus ein Getränk mit wachsender Beliebtheit. Die industrielle Herstellung von Schokolade ist technisch anspruchsvoll, sodass die Angabe eines Rezeptes zur Herstellung von qualitativ hochwertiger Schokolade im kleinen Maßstab schwierig ist. Zunächst werdenKakaobohnenzuKakaomasseverarbeitet. Soll aus der Kakaomasse Schokolade werden, wird sie mitZucker, gegebenenfalls auchKakaobutterundMilchprodukten(heute fast ausschließlich in trockener Form, zum Beispiel alsMilchpulver) und Gewürzen vermischt. Diese Schokoladenmasse wird nun in Walzwerken fein vermahlen, sodass insbesondere die Zuckerkristalle auf eine durchschnittliche Größe von 10–20 Mikrometer reduziert werden. Hauptziel ist die Eliminierung der durch große Partikel hervorgerufenen Sandigkeit der Schokoladenmasse im Mund. Heinrich Stollwerck, Sohn vonFranz Stollwerckund Maschinenbauer, erhielt 1873 das Reichspatent auf seinen Fünfwalzenstuhl. Diese Konstruktion lieferte ein feineres Mahlergebnis und verarbeitete die doppelte bis vierfache Menge in gleicher Zeit.[11]In den sogenanntenConchen(Concha,span.für „Muschel“, frühere Form des Gerätes) wird die Schokoladenmasse nun erwärmt und gerieben. Dies geschah ursprünglich in flachen, wannenförmigen Behältern mit rotierenden und oszillierenden Walzen. Das Conchieren dauerte bis zu 90 Stunden. Moderne Technik ermöglichte eine starke Verkürzung dieses Vorgangs, der die Feuchtigkeit reduziert, das Aroma erschließt und unerwünschte Aromabestandteile (vor allem die Essigsäure) entfernt. Fürweiße Schokoladewerden außer Kakaobutter keine Kakaobestandteile verwendet. Um dieViskositätder Masse zu beeinflussen, wird zumeistSojalecithinin einer Menge von maximal 0,2 % hinzugefügt. In derEUsind laut der Richtlinie 2000/36/EG[12]– die in Deutschland durch die Kakao- und Schokoladenverordnung[13]umgesetzt wurde – alsPflanzenfettneben Kakaobutter allerdings nurnicht-laurische Fettevon einigen tropischen Pflanzen erlaubt, und dies auch nur bis maximal 5 %. Erlaubt sind die Ölsorten: Mit dieser Richtlinie wird das europäische Recht harmonisiert, da einzelnen Mitgliedsstaaten auch zuvor bereits erlaubt war, einen Teil der Kakaobutter zu ersetzen. Ein Kilogramm Schokolade verursacht ca. 3,5 KilogrammCO2-Äquivalente.[14]DerWasserverbrauchliegt bei 10.000 Liter pro Kilogramm Schokolade.[15] Zur Herstellung existieren zweiTemperierungsverfahren. Bevor Schokolade aus dem flüssigen Zustand verarbeitet und zum Erstarren gebracht wird, muss sie temperiert werden, d. h., sie wird gekühlt, bis derFettanteilin der Schokolade erste Erstarrungskristallebildet. Man unterscheidet zwischen sechs verschiedenen Erstarrungskristallen von Schokolade, wobei diese sich in Aussehen, Geschmack und der Schmelztemperatur unterscheiden. Die Kristallform 5 ist die gewünschte Form für den Verzehr. Kristallform 6 ist gekennzeichnet durch weiß-faseriges Aussehen ähnlich angeschmolzener und wieder erstarrter Schokolade. Bei dieser unerwünschten Kristallform liegt der Schmelzpunkt über der Temperatur, die beim Verzehr zum angenehmen Effekt desSchmelzens auf der Zungeführt. Zur Herstellung dieser Erstarrungskristalle ist ein definiertes Abkühlen und daran anschließend ein Erwärmen der Schokoladenmasse notwendig. Beim Nachwärmen der Schokolade werden niedrig schmelzende Kristalle aufgeschmolzen, sodass nur hochschmelzende Kristalle sich in der flüssigen Schokolade befinden. Die Schokolade erstarrt dann beim anschließenden Kühlen in einem Kristallgefüge, das aus diesen hochschmelzenden Kristallen gebildet wird. Die Wärmebehandlung der flüssigen Schokolade wird in der Fachsprache alsTemperierenbezeichnet, die Schokolade wird vorkristallisiert genannt. Dunkle Kuvertüre wird auf 31 bis 32 °C, Vollmilchkuvertüre auf 30 bis 31 °C und weiße Kuvertüre auf etwa 28 bis 30 °C Endtemperatur temperiert. In den verschiedenen Temperierverfahren kommen qualitative Unterschiede vor. Für guten Glanz, hohe Lagerbeständigkeit und feinkörnigen Bruch des Endproduktes ist wichtig, dass die Temperiermaschine Fettkristalle in einer hochschmelzenden Kristallform bildet, dass diese Kristall-Agglomerate in kleinen Abmessungen vorliegen und dass sie homogen in der Masse verteilt sind. Alternativ zum oben beschriebenen Temperierverfahren kann Schokoladenmasse auch mittels Impfkristallisation auskristallisiert werden. Dabei werden in einem separaten Verfahrensschritt in reiner Kakaobutter hochschmelzende Kristallformen in einem Scherkristallisator durch Scherung in Verbindung mit einer definierten Erhitzung/Erkaltung produziert und anschließend der Schokoladenmasse beigemischt. Die in der Schokoladenmasse entstehenden Kristalle wachsen um die hochschmelzenden Impfkristalle. Für den Produktionsablauf ist der Temperier- und Vorkristallisationsgrad entscheidend, also der Anteil (die Menge) der erstarrten Fettkristalle. Ein zu geringer Anteil (Untertemperierung) ergibt zu lange Erstarrungszeiten bei der Endkühlung und kann schlechten Glanz sowie geringe Lagerbeständigkeit zur Folge haben. Ein zu hoher Erstarrungsanteil (Übertemperierung) ergibt eine erhöhte Viskosität der zu verarbeitenden Schokoladenmasse und kann geringereKontraktionbei der Endkühlung sowie schlechten Glanz zur Folge haben. Da Kakaobutter aus einemGlyceridgemischmit unterschiedlichen Schmelzpunkten besteht, ergibt die Schmelzkurve von Schokolade keinen Haltepunkt, sondern einen Schmelzbereich. Beim Abkühlen von Schokolade macht sich ein weiteres Charakteristikum der Kakaobutter stark bemerkbar. Kakaobutter ist sehr träge in der Bildung von Kristallisationskeimen, sie lässt sich sehr stark unterkühlen, bevor die Erstarrung einsetzt. Obwohl der Schmelzbereich der Kristallform bei ca. 34 °C liegt, lässt sich restlos aufgeschmolzene (also nicht vorkristallisierte) Masse, wenn sie bewegungsfrei abgekühlt wird, auf Temperaturen von unter 20 °C bringen, bevor die Erstarrung merklich einsetzt. Die Erstarrung dieser Schokolade erfolgt sehr langsam. Im nachfolgenden Diagramm ist die Abkühlungskurve einer untemperierten Schokolade dargestellt (durchgehende Linie). Vorkristallisierte Schokolade erstarrt bei höherer Temperatur und in wesentlich kürzerer Zeit. Die gesamteSchmelzenthalpiewird dabei freigesetzt. Dadurch ändert sich die Abkühlungskurve beträchtlich. Eine Selbsterwärmung der Schokolade während der Erstarrungsphase ist möglich. Nach der Erstarrung erfolgt ein erneuter Temperaturabfall. Eine typische Abkühlungskurve für vorkristallisierte (temperierte) Schokolade ist als unterbrochene Linie im Diagramm dargestellt. In einem weiteren Produktionsschritt wird die Masse in Formen abgefüllt oder als Überzugsmasse für Schokoriegel aufbereitet und danach abgekühlt. Diese Schokoladenmasse kann dann in entsprechende Formen wie Tafeln, Kugeln, Hohlformen oder Eier gegossen werden. Die mögliche Beigabe von Nüssen oder anderen harten Zutaten in die Masse erfolgt kurz nach dem Temperieren. Diese Stücke müssen in die Temperierberechnung einbezogen werden. Schließlich wird die flüssige Masse in vorgewärmte Formen gegossen. Luftblasen werden durchVibrationentfernt. Durch das Abkühlen ziehen sich die Tafeln zusammen, was das „Austafeln“ erleichtert. Üblicherweise wiegt eine Tafel Schokolade 100 Gramm und hatSollbruchstellenin Längs- und Querrichtung, sodass sie leicht in mundgerechte Schokoladenstücke zerbrochen werden kann. Zur Herstellung von gefüllter Schokolade kommen drei gängige Verfahren zur Anwendung. BeimOne-Shot-Verfahrenwird über eine außenliegende Ringdüse die Schokolade und über eine innerhalb der Ringdüse liegende Füllungsdüse die Füllung in einem geringen Zeitabstand dosiert. Vorteil dieses Verfahrens ist, dass nur noch ein Kühlvorgang nötig ist. Nachteile der One-Shot-Technik sind vor allem der erforderliche hohe technologische Aufwand zum Dosieren von Schokolade und Füllung im gleichen Arbeitsgang und das veränderte Kontraktionsverhalten des erzeugten Artikels beim Kühlen. Bestehende Rezepturen müssen in der Regel für das Verfahren angepasst werden. Bis zur Einführung des One-Shot-Verfahrens wurde bei mittleren und großen Produktionsanlagen mit einem Ausstoß von mehr als 500 kg/h überwiegend dasWendeverfahreneingesetzt. Zunächst wird die flüssige Schokolade in die Gussform dosiert. Dann wird die Form gewendet, wodurch die bereits abgekühlte Schokoladenmasse an den Wänden der Form zurückbleibt, während der Rest wieder abfließt. Anschließend wird die entstandene Schokoladenhülse mit der Füllung versehen und nach einem weiteren Kühlvorgang der sogenannte Deckel (der Boden der Praline oder Tafel) dosiert. Das Schokoladenprodukt muss jeweils nach dem Dosieren der Hülse, der Füllung und des Deckels abkühlen. Dieses Verfahren eignet sich auch zur Herstellung von Schokoladenhohlkörpern, die eine flüssig bleibende Füllung oder Spielzeug enthalten sollen. Hierbei werden zumeist zwei Hälften gegossen, dann zusammengefügt und durch kurzes Erhitzen verschweißt. Durch mehrfaches Ausgießen und Wenden mit verschiedenfarbigen Sorten kann der Hohlkörper außen und innen farblich gestaltet werden. Ist eine flüssige oder pastöse Füllung vorgesehen, weist eine der Hälften ein Loch auf, durch das nachträglich die Füllung und dann ein Verschlusstropfen aus Schokolade eingebracht werden. Zur Herstellung von Hohlkörpern wieWeihnachtsmännernund Osterhasen wird zunächst Schokolade in die eine Formhälfte gefüllt und gleich darauf die zweite Formhälfte aufgesetzt. Die geschlossene Form wird anschließend rotiert, bis die Schokolade an der inneren Wandung der Form erstarrt ist. Bei vollautomatischen Produktionsanlagen wird dieser Vorgang im Kühler durchgeführt, um die Erstarrung der Schokolade zu beschleunigen. Bei mittleren und kleinen Anlagen ist zur Hülsenbildung auch das Verfahren desKaltstempelnsverbreitet. Hierbei wird nach dem Dosieren der Schokolade ein gekühlter Stempel in die Form eingedrückt. Dadurch wird die eindosierte Schokolade zur Hülse ausgeformt und gleichzeitig verfestigt. Anschließend kann die Füllung in die Schokoladenhülse dosiert und – nach einem weiteren Kühlvorgang – die Praline oder Tafel gedeckelt werden. Der energetische Aufwand, um die warme Schokolade in adäquater Zeit genügend abzukühlen, sodass die Hülse stabil bleibt, ist recht hoch. Auch besteht die Möglichkeit, dass Luftfeuchtigkeit auf dem Kaltstempel kondensiert, wodurch die Schokoladenhülse beim Wiederanheben des Stempels aus der Form gezogen werden kann. Ein besonderes Verfahren zur Herstellung von Schokoladenhülsen ist, einen gekühlten Stempel einige Sekunden in ein Bad aus flüssiger Schokolade zu tauchen. Nach dem Herausheben haften am Stempel die fertigen Schokoladenhülsen, die dann durch Druckluft von diesem gelöst werden. Dieses Tauchverfahren wird auch zum Überziehen von Stieleis mit Schokolade eingesetzt. Weltweit wurde 2021 Schokolade im Gesamtwert von 31 Milliarden Euro gehandelt.[16]Deutschland war dabei noch vor Belgien und Italien das international wichtigste Exportland gemessen am Ausfuhrwert. Die Schokoladenhersteller beziehen ihre Rohstoffe zu schwankendenRohstoffpreisen. Die Weltmarktpreise für die beiden wichtigsten Rohstoffe Zucker und Kakao waren 2018 ungefähr so hoch wie ein Jahrzehnt zuvor (angegeben in Euro). In der Schweiz lagen bis zum 1. Januar 2019Exportsubventionenauf Schokolade vor, um den Milchpreis zu stabilisieren.[19]Wegen Anforderung durch dieWelthandelsorganisation(WTO) wurde Ende 2018 das „Schoggi-Gesetz“ entsprechend aufgehoben.[20] Es existiert eine Vielzahl von Sorten und Qualitäten, Formen und Geschmacksrichtungen. Unterschieden wird zwischenBitterschokolade(auch Edelbitter-, Zartbitter-, Herren- oder dunkle Schokolade genannt),Milchschokoladeundweißer Schokolade. Eine zumBackenundGlasierenmit einem höheren Fettanteil versehene Schokolade wird alsKuvertürebezeichnet und gehandelt. Neben den grundsätzlichen Einteilungen, meist wie beschrieben auf Grundlage der Mischung oder des Herstellungsprozesses, werden beim Verkauf edlerer Schokoladen auch Unterscheidungen nach dem Anbaugebiet der Kakaobohne gemacht. Hierbei werden auch Qualitätsbegriffe wie das vom Wein bekannte „Premier Cru“ verwendet. Neben der oben beschriebenenechtenSchokolade wird für Schokoladenkekse oder auch teilweise bei industriell hergestelltem Speiseeis eineFettglasureingesetzt, die nicht unter den Begriff der Schokolade fällt, da sie keine Kakaobutter enthält. Seit 2017 wird unter der Bezeichnung „Ruby“ rosafarbene Schokoladenware angeboten, deren abweichende Färbung durch Selektion aus bereits zuvor bekannten Kakaobohnensorten und – nach einer Vermutung der Verbraucherzentrale – weitgehenden Verzicht auf Fermentierung bedingt ist und durchCitronensäurestabilisiert wird.[21] Schokoladen (Tafeln zum direkten Verzehr wie auchKuvertüren) mit hohen Ansprüchen an Reinheit und Qualität des verwendeten Kakaos (oft nach Herkunft sortiert) werden von Herstellern in vielen Ländern produziert. Eine der letzten Variationen sind Schokoladen, denen in die Grundmasse Aromastoffe und Gewürze wie beispielsweiseChili,Zimt,Hanf,Schwarzer PfefferoderThymianbeigemengt wurden.Luftschokoladeist eine Sonderform, die zahlreiche kleine Luftblasen enthält. In Konditoreien wirdNussbruchfrisch hergestellt. Schokolade wird aber nicht nur pur (als Tafel) genossen, sondern ist Ausgangsbasis fürPralinenundKonfekt(Trüffel-,Nougat-,Marzipan- oder andere Pralinen).Chocolatiersschätzen die feinen Kuvertüren mit hohem Kakaobutteranteil und ausgewählten Edelkakaos (couvert: „überzogen/umhüllt“). Es gibt tausende Kombinationen wie beispielsweise Champagner-Sahne-Trüffel, die viele kleinere Spezialisten anbieten. Eine „knackige“ Schokolade, eine „cremige“ Füllung mit „zartem“ Schmelz und wenig Zucker, dafür Sahne und frische Butter, sind einige der qualitätsbestimmenden Merkmale. Hochwertige (und allgemein bekannte industrielle) Hersteller finden sich in derSchweiz, inBelgien, aber auch inDeutschlandund neuerdings auch inÖsterreichsowieosteuropäischen Ländern. Qualitativ besonders gut können naturgemäß nur tagesfrisch verkaufte Pralinen sein, die man in vielen Confiserien und guten Konditoreien erhält. Die Produkte sind begrenzt haltbar und aufgrund des hohen Fettanteils relativ wärmeempfindlich. Man unterscheidet Konsumprodukte von Premiumprodukten, daher ist der Preis sehr unterschiedlich. Die bekannteste Form sind die „geigelten“ Trüffel – runde Kugeln mit kleinen Stacheln, die an dieErdtrüffelerinnern. Echte Trinkschokolade wird – im Unterschied zu den durch Einrühren von Kakao oder leicht löslichem „kakaohaltigen Getränkepulver“ in Milch erzeugten Getränken – üblicherweise aus Milch oder Wasser und zerkleinerter Schokolade, eventuell unter Zusatz von Zucker undVerdickungsmittelnwieMaismehl,GuarkernmehloderJohannisbrotkernmehl, hergestellt; das Schmelzen und Emulgieren der Kakaobutter in der wässrigenPhasefunktioniert nach Erwärmen besser. Zu Anfang des 19. Jahrhunderts wurde Schokolade in Apotheken als Stärkungsmittel verkauft.[22]Es existieren keine Hinweise, dass Schokolade körperlichabhängigodersüchtigmachen könnte.[23][24]Auch die Hinweise aufstimmungsförderndeEffekte verschiedener Inhaltsstoffe der Schokolade[25]reichen nicht aus, um den Effekt zu erklären,[24]sodass auch psychische Ursachen berücksichtigt werden müssen.[26] Schokolade besteht zu beträchtlichen Teilen aus Fett und Zucker und enthält neben anderen InhaltsstoffenSaccharose(Haushaltszucker, einDisaccharid). Derglykämische Index(GI) von Schokolade ist mit dem von Roggenbrot vergleichbar und liegt mit etwa 65 im Mittelfeld. Die meisten Arten von Schokolade haben einenphysiologischen Brennwertzwischen 2100 und 2500kJpro 100 Gramm (= 500 bis 600kcal). Milchschokolade liegt mit 2300 kJ pro 100 Gramm (= 550 kcal) in der Mitte. Das entspricht ungefähr einem Viertel desTagesbedarfsan Energiezufuhr eines erwachsenen Menschen. In einer türkischen Studie wurden auf dem dortigen Markt verbreitete Milch- und Bitterschokoladensorten auf ihrenCholesteringehaltund ihrFettsäureprofiluntersucht. Der Fettanteil bestand hauptsächlich ausStearinsäure(39 %),Ölsäure(26 %) undPalmitinsäure(26 %). Die genannten Fettsäuren gelten ernährungsphysiologisch als unbedenklich. Das Verhältnis vongesättigtenzuungesättigten Fettsäurenbetrug 70/30. Der Cholesteringehalt betrug im Mittel 1,14 mg/kg, ein im Vergleich zu Nahrungsmitteln tierischer Herkunft sehr geringer Wert.[27] Nach einer 2007 veröffentlichten Untersuchung der ZeitschriftÖkotesthatten insbesondere Bitterschokoladen südamerikanischer Herkunft einen erhöhtenCadmiumgehalt, was bei dauerhaft hohem Konsum zu einerCadmiumvergiftungführen kann.[28]Weitere Untersuchungen zeigten dann in der überwiegenden Mehrheit der Proben keine erhöhten Cadmiumgehalte.[29]Bei Untersuchungen in einer Klinik derChristian-Albrechts-Universität zu Kielim Jahr 2009 fanden sich in allen untersuchten Proben dunkler Schokoladen Spuren des SchimmelgiftesOchratoxinA in ungefährlichen Konzentrationen.[30] Ungesüßtes Kakaopulver enthält 1 bis 3 ProzentTheobromin, das chemisch demKoffeinähnlich ist. Es wirkt auf den Organismus mild und dauerhaft anregend und leicht stimmungsaufhellend. Für Menschen ist dieser Anteil – im Gegensatz zuHunden,KatzenundPferden– ungefährlich. Weitere Inhaltsstoffe, die in Zusammenhang mit der stimmungsaufhellenden Wirkung von Schokolade gebracht werden, sind unter anderem dasmolekulare Grundskelett des AmphetaminsPhenylethylamin, dieSerotonin-VorstufeTryptophan, ein natürlichesAntidepressivum, und das CannabinoidAnandamid,[31]letzteres ein Derivat derArachidonsäure. Die enthaltene Menge Anandamid ist jedoch für einen merklichen Effekt viel zu gering,[32]obwohl in Schokolade zusätzlich Substanzen enthalten sind, die den Abbau hinauszögern.[33]Der stimmungsaufhellende Effekt von Schokolade ist durch die Inhaltsstoffe alleine nicht schlüssig erklärbar und schließt wohl auch psychische Einflüsse ein.[26] Schokolade, insbesondere Bitterschokolade mit hohem Kakaoanteil, kann den Spiegel an herzschützendenAntioxidantienim Blut für einige Stunden stark anheben. Durch gleichzeitigen Genuss vonMilchwird dieser Effekt wieder neutralisiert. Bei dem im Falle von Schokolade und Kakao wirksamen Antioxidans handelt es sich um einFlavonoidnamensEpicatechin.[34] Schokolade enthält über den Kakaoanteil auchN-Phenylpropenoyl-L-aminosäureamid, das sich wachstumsfördernd auf Hautzellen auswirkt und damit Wundheilung unterstützt, Hautschäden therapiert, Falten vorbeugt und das Risiko von Magengeschwüren verringert. Es existieren Hinweise, dass der in Schokolade enthaltene KakaokarieshemmendeWirkung hat,[35]während der enthaltene Zucker natürlich kariesfördernd wirkt. Das im Kakao enthalteneTheobrominkann nach einer imFASEB Journalveröffentlichten Studie aus dem Jahre 2004 möglicherweiseHustenanfälleabmildern.[36] Regelmäßiger Verzehr von Schokolade kann den Blutdruck und das Risiko für Herzinfarkt und Schlaganfall verringern. Die für den Effekt vermutetenPolyphenolesind aber auch in anderen Nahrungsmitteln wie Äpfeln zu finden. Für eine Ernährungsempfehlung zu Gunsten kakaohaltiger Schokolade reichen die Studienergebnisse noch nicht aus.[37]Die enthaltenen Kakaoflavanolescheinen sich günstig auf die Gefäßflexibilität (Vasodilatation) auszuwirken, was unterstützend auf die Regulation des Blutdrucks wirken kann.[38] Nach einer amerikanischen Studie, die von 1996 bis 2000 an 2291 Schwangeren durchgeführt wurde, kann der Genuss von Bitterschokolade das Risiko vonPräeklampsiebei Schwangeren senken. Die Anzahl der Frauen mit entsprechenden Symptomen war dabei direkt negativ abhängig vom Spiegel des in der Schokolade enthaltenenTheobrominsim Serum.[39][40] Umstritten ist, ob Schokolade Hautunreinheiten hervorrufen kann. Eine vielzitierte Studie von J. Fulton aus dem Jahr 1969, finanziert durch die „Chocolate Manufacturers' Association of the USA“, kam zu dem Ergebnis, dass zwischen Schokoladenkonsum und dem Auftreten vonAkne– entgegen weit verbreitetem Glauben – kein Zusammenhang existiere.[41]Eine spätere Analyse der Durchführung der Studie zeigte jedoch zahlreiche Mängel.[42]Eine neuere Studie von 2011 konnte Hinweise für einen Zusammenhang vonAcne vulgarisund dem Konsum von Schokolade finden.[43] Eine Legende vermutet, Schokolade oder Schokoladenprodukten wieNougatcremewürde Rinderblut beigemengt. Tatsächlich wurde bei einem altenneapolitanischenSchokoladensoßengericht aus Bitterschokolade und Sahne namens „Sanguinaccio“ (Sangue = Blut) ursprünglich Schweine- oder Rinderblut verwendet.[44]Diese Zubereitungsart widerspricht der EU-Richtlinie 2000/36/EG[12]und ist auch inItaliennicht mehr üblich. In derDDRwurde versucht, Schokoladenbestandteile durch einheimische Rohstoffe zu ersetzen, um Devisen zu sparen. Die genannte EU-Richtlinie 2000/36/EG[12]und deutsche Kakao- und Schokoladenverordnung[13]schreiben vor, welche Bestandteile in Schokolade enthalten sein dürfen. Mandeln, Haselnüsse und andere Nüsse dürfen (ganz, in Stücken oder gemahlen) bis zu 60 % dem Gesamtgewicht des Erzeugnisses zugesetzt werden, wodurch Kakaomasse eingespart werden kann. Der sich bei unsachgemäßer Lagerung auf Schokoladenerzeugnissen absetzende fleckige, weiche, weißliche bis hellgraue Belag wird häufig mitSchimmelverwechselt, hängt aber mit dem nicht zusammen; dieserZucker- und/oder Fettreifmindert nicht den Geschmack des Produkts und ist auch aus gesundheitlicher Sicht unbedenklich. DasTheobrominin der Schokolade besitzt fürKatzen,Hunde,Vögel,PferdeundRindereine höhere Toxizität als für den Menschen, da ihrStoffwechselTheobromin nur langsam abbauen kann. Die Resorption nach oraler Aufnahme erfolgt nahezu vollständig mit einerBioverfügbarkeitvon 77 ± 12 %. Die Halbwertszeit schwankt beim Hund zwischen 6,5 und 17,5 Stunden;[45][46][47]dieletale Dosis(LD50) liegt bei etwa 300 Milligramm pro Kilogramm Körpergewicht. Das entspricht in etwa 1,5 Kilogramm Vollmilchschokolade für einen 10 Kilogramm schweren Hund;[47][48]bei getrocknetem Kakaopulver (Gehalt Theobromin 28,5 mg/g) sind dies rund 100 Gramm und bei Bitterschokolade (enthält 16 mg/g) etwa 190 Gramm. Tödliche Vergiftungen sind meist auf eine Herzarrhythmie,HyperthermieoderAtemstillstandzurückzuführen.[49][50]Beim Hund können schon Dosen zwischen 16 und 100 Milligramm pro Kilogramm zu Vergiftungssymptomen wie Erhöhung desBlutdrucksoder derPulsfrequenz,VerengungderBlutgefäße, verringerterReizschwelledes Nervensystems und dadurch zu Unruhe, Zittern undHyperreflexiebis hin zuKrampfanfällen, häufig auchErbrechenundDurchfall, führen.[46][49]Ein Hund, der eine große Menge Schokolade zu sich genommen hat, sollte möglichst schnell zum Erbrechen geführt und zumTierarztgebracht werden. Aufgrund unsachgemäßer Verbrauchergewohnheiten und entsprechender Nachfrage sind unterdessen auch spezielle, schokoladenähnliche Futtermittel auf dem Markt (wieHundeschokolade). Bei Katzen ist die Gefahr der Theobrominvergiftung geringer, da diese keineGeschmacksknospenfür süße Stoffe besitzen und daher Schokolade verschmähen. Andere Säugetiere wieRattenundMäusekönnen – wie der Mensch – das Theobromin rasch abbauen. Allerdings kann der große Fettanteil in der Schokolade zu gesundheitlichen Problemen führen. Des Weiteren wurden bei einer Studie von männlichen Ratten pathologische Veränderungen an den Hoden festgestellt. Den Versuchstieren wurde über 30 Tage eine am Körpergewicht gemessen hohe Dosis Theobromin verabreicht. Die folgende Tabelle gibt den europäischen Schokoladenkonsum in Kilogramm pro Kopf im Jahr 2018 wieder (mit Ausnahme der Schweiz, hier ist der Wert vom Jahr 2020).[51] Der Anbau von Kakao hat einen hohen Flächenbedarf. Um 1 kg davon zu produzieren, benötigt man 20 m² landwirtschaftliche Fläche. Im Vergleich sind es bei Reis 2,5 m², Weizen 1,8 m² und bei Südfrüchten 0,6 m².[52]70 % der globalen Kakaoproduktion stammen aus Westafrika, dort vor allem aus den LändernElfenbeinküsteundGhana.[53]Für die Kleinbauern stellt der Anbau von Kakao eine wichtige Einkommensquelle dar. Deswegen und wegen desBevölkerungswachstumsin dieser Region werden immer mehr Flächen des dortigentropischen Regenwaldesgerodet. Die Elfenbeinküste hat seit 1960 bereits 80 % ihres Regenwalds verloren, bis 2024 wird er dort voraussichtlich komplett verschwunden sein.[54]Entwaldungist einer der wichtigsten Faktoren bei der Entstehung derKlimakatastrophe. Ein Drittel der menschengemachtenKohlenstoffdioxid(CO2)-Emissionen sind auf Entwaldung zurückzuführen.[55]Mit dem Regenwald verschwinden wichtige Lebensräume für wildlebende Tiere und Pflanzen. DieBiodiversitätgeht dramatisch zurück.[56] Ein soziales Problem des Kakaoanbaus in Westafrika ist dieKinderarbeit. Immer noch ist üblich, dass die Kinder der Kleinbauern auf den Feldern mitarbeiten. Häufig steht dies in Konkurrenz zum Schulbesuch. Obwohl die Quote der schulbesuchenden Kinder in den letzten Jahren gestiegen ist, arbeiten immer noch bei 45 % aller Familien im Kakaoanbau die Kinder auf den Feldern mit.[57] Laut derInternationalen Kakao-Organisationwaren 2021 folgende Unternehmen die am Nettoumsatz gemessenen weltweit größten Hersteller von Schokoladeartikeln:[58] Weitere Unternehmen finden sich unter derKategorie:Hersteller von Kakao- und Schokoladenwaren, Unternehmer/Personen unter derKategorie:Schokoladenhersteller (Person). Chocolatiersarbeiten handwerklich mit vorgefertigter Schokoladenmasse und sind daher von den industriellen Schokoladenherstellern zu unterscheiden. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie 2Geschichte 3Herstellung 3.1Herstellung von Schokolade 3.2Temperieren (Vorkristallisieren) von Schokolade 3.2.1Traditionelle Temperierung 3.2.2Impfkristallisierung 3.2.3Temperiergrad 3.3Eintafeln 3.4Herstellung von gefüllter Schokolade 3.4.1One-Shot-Verfahren 3.4.2Wendeverfahren 3.4.3Kaltstempeln 3.4.4Tauchverfahren 4Handel 5Marktsituation 5.1Rohstoffpreise 5.2Subventionen 6Schokoladensorten 7Produkte aus Schokolade und Kuvertüre 8Trinkschokolade 9Schokolade und Gesundheit 9.1Inhaltsstoffe 9.2Ernährungsphysiologie 9.3Schadstoffe 9.4Positive Wirkungen 9.5Einfluss auf Blutdruck und Blutgefäße 9.6Annahmen und Irrtümer 10Schokolade und Haustiere 11Schokoladenkonsum 11.1Ökologische und soziale Aspekte des Schokoladenkonsums"
  },
  {
    "label": 0,
    "text": "Sehenswürdigkeit – Wikipedia Sehenswürdigkeit Inhaltsverzeichnis Kritik Sehenswürdigkeiten in Deutschland (Auswahl) Sehenswürdigkeiten international (Auswahl) Siehe auch Einzelnachweise Literatur Weblinks EineSehenswürdigkeitist ein bedeutsamesNaturdenkmal,Kulturdenkmaloder etwas anderweitig Attraktives,SpektakuläresbeziehungsweiseProminentes, das häufig intouristischeProgramme einbezogen wird. Oft handelt es sich auch um Objekte, die der Reisende durch die Medien (Zeitschriften, Fernsehen, Internet, Reiseromane etc.) bereits kennt und die er eigenständig erleben will. Manche Sehenswürdigkeiten haben zugleichWahrzeichencharakterfür ein Land, eine Region oder eine Stadt. Auch dieZielgruppekann bestimmen, ob etwas für sie sehenswürdig ist, was zum Beispiel an Besucherzahlen abgelesen werden kann. Typische Sehenswürdigkeiten sind historische Gebäude und Bauwerke, manchmal ganze Stadtviertel wie die Altstädte vonVenedigoderRothenburg ob der Tauber, aber auch besondere Zeugnisse derZeitgeschichte(z. B. Reste derBerliner Mauer) oder einMuseum. Sehenswerte Naturdenkmäler können Seen, Vegetationszonen oder auch Berge sein. Hier spielt die Infrastruktur eine wichtige Rolle, wie etwa dieAlpenvereinshüttenim Gebirge. Veranstalter, insbesondere sogenannter alternativerStädtereisen, kritisieren den Begriff. Denn er legt nahe, dass es einige wenige spektakuläre, prominente „des Sehens würdige“ Stätten gibt. Demgegenüber gehören wichtige Stätten des Alltags in der Regel nicht dazu, obwohl sie für das Verstehen mindestens ebenso bedeutsam seien. Aus tourismuskritischer Perspektive wird hervorgehoben, dass Objekte nichtper sesehenswert sind, sondern dass Sehenswürdigkeiten durch Instanzen desKulturtourismuszu solchen ernannt werden.[1] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Kritik 2Sehenswürdigkeiten in Deutschland (Auswahl) 3Sehenswürdigkeiten international (Auswahl) 4Siehe auch 5Einzelnachweise 6Literatur 7Weblinks العربية Azərbaycanca Bikol Central Беларуская (тарашкевіца) भोजपुरी বাংলা Чӑвашла Dansk English Esperanto Español Eesti Euskara فارسی Suomi Français עברית हिन्दी Hrvatski Magyar Հայերեն Bahasa Indonesia Italiano 日本語"
  },
  {
    "label": 0,
    "text": "Sport – Wikipedia Sport Inhaltsverzeichnis Etymologie Definitionen Geschichte des Sports Sport als Bewegungskultur Sportwettbewerb Sportarten Sportdisziplinen Sportwissenschaft Sportpolitik Sportmedien Sportfans Abgrenzung zum Spiel Siehe auch Literatur Weblinks Einzelnachweise Moderner Sport 3000 v. Chr. – 600 n. Chr. 1700 – Gegenwart Gesundheitsaspekte Training Einige Ordnungsmöglichkeiten Olympische Sportarten Sportförderung Sportumfasst zielgerichtete körperliche oder geistige Aktivitäten, bei denen intensive körperliche Anstrengung oder den Körper fordernde Konzentration beziehungsweise mentale Anstrengung erforderlich ist. Zielgerichtet bedeutet hier sich mit anderen allein oder als Mannschaft zu messen oder die eigene Leistung zu verbessern. Das Wort selbst wurde im 19. Jahrhundert vomenglischensportentlehnt, welches durchProkopeausdisportentstand und aus demAltfranzösischenentlehnt ist.[1]In Frankreich hatte es die Bedeutung „sich entspannen, ergötzen“ (französischse desporter), aus dem „Sport“ wurden ähnlich klingende Variationen (englischsports,italienischdiporto,portugiesischeporto,spanischdeporte) entwickelt.[2]Diese neolatinischen Sprachvarianten gehen zurück auf „wegtragen, die Aufmerksamkeit woanders hinlenken“ (lateinischdisportare).[3] Ins Deutsche wurde das Wort „Sport“ dann durchFürst Pücklereingeführt.[4]Der BegriffSportwurde 1887 erstmals imDudenerwähnt. DasNomen Agentisist der einen Sport ausübendeSportler. „Seit Beginn des 20. Jahrhunderts hat sich Sport zu einem umgangssprachlichen, weltweit gebrauchten Begriff entwickelt. Eine präzise oder gar eindeutige begriffliche Abgrenzung lässt sich deshalb nicht vornehmen. Was im Allgemeinen unter Sport verstanden wird, ist weniger eine Frage wissenschaftlicher Dimensionsanalysen, sondern wird weit mehr vom alltagstheoretischen Gebrauch sowie von den historisch gewachsenen und tradierten Einbindungen in soziale, ökonomische, politische und rechtliche Gegebenheiten bestimmt. Darüber hinaus verändert, erweitert und differenziert das faktische Geschehen des Sporttreibens selbst das Begriffsverständnis von Sport.“ Dieses Zitat verdeutlicht, dass die hinter dem Begriff Sport liegenden Bedeutungszuweisungen ganz wesentlich durch den umgangssprachlichen Gebrauch und den Kontext geprägt sind, in dem der Begriff Sport verwendet wird.[5][6]Für denDeutschen Olympischen Sportbund(DOSB) steht beispielsweise die (körperliche)motorischeAktivität im Vordergrund. Denkspiele, dieDressurvon Tieren sowieMotorsportohne Einbeziehung solcher motorischer Aktivitäten entsprechen daher nicht dem Sportverständnis des DOSB.[7]Dennoch hat der DOSBSchachals Sportart anerkannt; dasInternationale Olympische Komitee(IOC) sogar Schach undBridge. Außerdem hat diese Bedeutungsfacette auch historische Gründe. Als sich Sport als neues Phänomen im deutschen Sprachraum seit Ende des 19. Jahrhunderts mehr und mehr verbreitete, stieß er auf das PhänomenTurnenund der vonFriedrich Ludwig Jahn[8]begründeten „Leibes- bzw. Körpererziehung“. Damit standen sich zwei sehr unterschiedliche Konzepte von Körper- und Bewegungskultur antithetisch gegenüber. Gerade der Begriff Sport stand für die stärkere Betonung des Wettbewerbsgedankens und für die Austragung standardisierter, messbarer und oft auch inszenierter Wettbewerbssituationen. Heute umfasst der Begriff Sport als Überbegriff auch Konzepte, die damals eher unter dem Begriff Turnen zusammengefasst wurden. Sport kann daher sowohl als „Körperkultur“, als auch als „Wettbewerbs- oder Wettkampfkultur“ verstanden werden. Je nach Auffassung umfasst der Begriff Sport in aller Regel sowohl Phänomene, die beide Aspekte erfüllen wie solche, die entweder überwiegend den motorischen Körper-Bewegungsaspekt betreffen (z. B.Krafttraining, privater Frühsport oderTrimm-dich-Bewegung) oder überwiegend den Wettkampf-Aspekt (Schach, Motorsport,Bodybuilding-Wettbewerbe). Für die Geschichte der „Körperkultur“ und des „Sports“ ist bedeutend, dass diese ursprünglich alsSpielangesehen wurden, losgelöst (lat.disportare) von derErwerbstätigkeitoder irgendwelchen feindseligen, kriegerischen Auseinandersetzungen. Außerdem musste sich der Sport von religiösen Vorbehalten befreien, wie sie gegenüber demGlücksspielbestanden und konnte sich ähnlich wie Tanz und Theater zunächst nur imHofstaatentfalten (Jagd,Turnier). Seit der Definition des modernen Sports vonAllen Guttmann(1978) ist dieser Begriff des „modernen Sports“ umstritten, da die von Guttmann angeführten sieben Elemente (Weltlichkeit, Chancengleichheit, Rollenspezialisierung, Rationalisierung, Bürokratisierung, Quantifizierung, Suche nach Rekorden) auch bereits in früherer Zeit einzelne Sportarten charakterisierten. So zeigten u. a. Krüger & McClelland (1984),[9]Carter & Krüger (1990),[10]Szymanski (2008),[11]Arnd Krüger(2008),[12]McClelland (2012)[13]und Behringer (2012),[14]dass das moderne Denken in Gesellschaften und Sport wesentlich früher einsetzte und damit auch zumindest einzelne Sportarten von diesem Denken durchdrungen waren. Nach diesen Autoren sind die Elemente Guttmanns Eigenschaften eines jeden Sports, der diesen von allgemeinen Bewegungsformen unterscheidet. Man unterscheidet im WesentlichenBreitensportundLeistungssport, außerdemfreizeitlichenAmateursportundarbeitsweltlichenProfisport. Zudem gibt es die in der jüngeren Vergangenheit entstandenen KategorienExtremsportundFunsport, die sich von traditionellen Sportarten teilweise deutlich unterscheiden. Sport kann alsMannschaftssport(zum BeispielBallsportarten) oder alsIndividualsportbetrieben werden. Sport wird in unterschiedlichen gesellschaftlichen Kontexten ausgeübt und bildet einen wesentlichen Teil der Freizeitgestaltung und performativenUnterhaltungskultur. Neben den traditionellenSportvereinenund demSchulsporttreten im Bereich des Breitensports seit den 1980er-Jahren vermehrt auch kommerzielleFitnessstudiosundSportcenterin Erscheinung. Darüber hinaus wird auch jenseits dieser Strukturen im privaten Umfeld Sport getrieben, entweder allein oder im Kreis der Familie bzw. von Freunden (Beispiele:Jogging,Lauftreff). Dieser wird als informeller Sport bezeichnet und umfasst beispielsweise das Fußball- und Basketballspielen auf öffentlichen Plätzen, in Stadtparks, auf Bolzplätzen oder Hinterhöfen. Kennzeichnend für den informellen Sport ist, dass dieser von den Akteuren selbst organisiert und reguliert wird. Im Bereich des Leistungssports sind die Strukturen wesentlich komplexer; deshalb sei an dieser Stelle nur auf die entsprechenden Artikel verwiesen. Mit zunehmender Integration der Menschen mit einer Behinderung entwickeln sich in jüngster Zeit immer mehrBehindertensportarten, die oft auch auf Leistungssportniveau betrieben werden. Als Beispiel hierfür dienen dieParalympischen Spiele, welche erstmals im Jahr 1976 in Schweden stattfanden. Konnte inMeyers Konversations-Lexikonvon 1888[15]noch gesagt werden: „Als ein wesentliches Merkmal des Sports ist endlich anzuführen, dass dessen Ausübung nicht um des Gelderwerbs wegen geschieht“ (Bd. 15, S. 176), so kann dies heute wegen des professionellen Sports nicht mehr als Merkmal angesehen werden. Darüber hinaus sind diekommerziellenInteressen mächtiger Dritter (z. B. Großkonzerne alsSponsoren, Förderer oderMäzene) nicht zu unterschätzen, die sich einenImagetransfervon Sportarten oder Sportlern auf ihr Unternehmen erhoffen. Das ist die interne Differenz des Sports alsSpektakelund Körperkult. Darüber hinaus ist auch der Breitensport etwa durch Sportausrüstung und Sportbekleidung erheblich mit wirtschaftlichen Interessen verbunden. DieWHOdefiniertGesundheitals „Zustand völligen körperlichen,seelischenund sozialen Wohlbefindens“. Diese ermöglichen es, drei elementaren Krankheitsfaktoren entgegenzuwirken: Bewegungsmangel, Stress und sozialer Isolation. Um gesund zu bleiben, gilt es, dem Körper, seinem Bewegungsapparat, seinen Organen, demImmunsystemund derPsycheReserven zu geben. Dabei kommt sportlicher Betätigung eine Schlüsselrolle zu. Die Erweiterung der Belastungsbereiche bezeichnet man als Fitness. Wichtig für die Fitness ist das differenziert zu betrachtendeSuperkompensationsprinzip. Auf einen körperlichen Belastungsreiz erfolgt eine Adaption desOrganismusund seiner Systeme. Bewegungstraining verbessert die organische Kapazität sowie Systeme der Psychoregulation und findet häufig in Gruppen statt, schafft also soziale Anknüpfungspunkte.[16] Obgleich der Nutzen körperlichen Trainings für Herz und Kreislauf wissenschaftlich unbestritten ist, treiben 45 Prozent der deutschen Erwachsenen gar keinen Sport und nur jeder Achte erreicht die derzeitigen Empfehlungen für ausreichende körperliche Aktivität.[17]Eine neue Studie zeigt den hohen gesundheitlichen Effekt des Sports bis ins Alter.[18] In synonym verwendeten Begriffen – wie „Körperertüchtigung“, „Leibesübung“ oder „Körperkultur“ – wird deutlich, dass sportliche Betätigung auch unter Aspekten derHygieneundGesundheitals essentiell anzusehen ist.[19][20][16] Im Sport ist der Wettkampf ein wesentliches Element. Sport ist in seiner heutigen Ausprägung überwiegend eine Wettkampfkultur. Teilweise wird der Begriff Sport sogar alsSynonymfür Wettbewerb gebraucht. Sport in seinem Verständnis als Streben nach demCitius, altius, fortius(„schneller, höher, stärker“) legte eine besondere Betonung auf das Leistungs- und Wettkampfprinzip. Für den Sport in seiner ursprünglichen Bedeutung war der spielerische und inszenierte Wettbewerb ein konstituierendes Element. Damit stand der Sport und damit der sportliche Wettkampf anfangs in Abgrenzung zu anderen Konzepten derKörper-und Bewegungskultur, wie demTurnenund derGymnastikbzw. der heutigenFitnessbewegung, bei denen es weniger stark um Wettbewerb ging (bzw. geht), bei denen stattdessen stärker das gemeinschaftliche Trainieren oder das Erleben des Körpers bzw. die Ertüchtigung der Leistungsfähigkeit oder derSpaßfaktorim Vordergrund stehen. Heute umfasst der Begriff Sport (in einer erweiterten Bedeutung) weithin alle Bereiche der Bewegungskultur, aber seit der Etablierung des Konzepts Sports im ausgehenden 19. Jahrhundert hat auch insgesamt das Wettbewerbsprinzip in der Bewegungskultur an Bedeutung gewonnen. Bei einem Sportwettkampf treten mehrere Teilnehmer (beiSportlernihreLeistungen) gegeneinander an und messen sich miteinander, wobei ein Ergebnis ermittelt wird. Dies kann im direkten Vergleich oder aber bei einer größeren Menge von Teilnehmern durch einen Ausscheidungsmodus in Vorrunden geschehen. Der Sieger geht dann imFinaleaus den Besten der Vorrunden hervor (sieheTurnierform). Training im Sport ist die regelmäßige körperliche Bewegung (Belastung) zum Zwecke der Leistungssteigerung bzw. zur Erhaltung der sportlichen Leistungsfähigkeit auf der Basis von Wachstumsprozessen.[21]Systematisches Training zielt darauf, möglichst langfristig stabile Anpassungserscheinungen, d. h. Trainingseffekte zu erzielen. Die Menschen in Deutschland treiben etwas mehr Sport als früher: 34 Minuten verbringen Menschen ab 10 Jahren durchschnittlich pro Tag mit Sport, wie das Statistische Bundesamt (Destatis) nach Ergebnissen der Zeitverwendungserhebung 2022 mitteilte. Das waren täglich 5 Minuten mehr als zehn Jahre zuvor.[22] Als Trainingslager werden Aufenthalte bezeichnet, in denen sich Sportler intensiv auf einen bevorstehenden Wettkampf vorbereiten.[23]DiePeriodisierung des sportlichen Trainingssorgt dafür, dass Training über einen längeren Zeitraum so organisiert ist, dass es im Sinne derSuperkompensationÜber- und Unterbelastungen vermeidet. Zu den Trainingsarten zählen u. a.Ausdauertraining,Intervalltraining, Training derintramuskulären Koordination,Hypertrophietraining,Krafttraining,Schnellkrafttraining,Beweglichkeitstraining,Konditionstraining, Koordinationstraining,Fitnesstrainingund Konzentrationstraining. Sportarten sind grobe Einteilungen inGattungsbegriffeund können nach bestimmten Gesichtspunkten in verschiedene Kategorien unterteilt werden. Es gibt aber keine offiziellen Klassifikationen. Eine Sportart kann auch in mehreren Kategorien auftauchen:Radpoloetwa ist sowohl eine Ball- als auch eine Radsportart. Die Anzahl der olympischen Sportarten befindet sich in einem stetigen Wandel. Bei den Sommerspielen 2016 waren 41 Sportarten aus 28 Verbänden, bei den Winterspielen 15 Sportarten aus 7 Verbänden zugelassen.(Stand: Februar 2016)[24] Sportdisziplinen sind die Feineinteilung jeder Sportart. Jede Sportdisziplin formt den Körper der Sportler durch permanentesTrainingauf spezifische, von der Disziplin geforderteLeistungsniveaus.[25]So sind die Leistungsanforderungen an einzelneKörperteilebei der DisziplinTennisvöllig andere als beimBoxen. Einzelne Sportarten können in folgende Sportdisziplinen unterteilt werden: Teilweise besitzen einzelne Disziplinen noch weitere Unterarten wie beim Schwimmsport (Brustschwimmen,Kraulschwimmen,Lagenschwimmen,Rückenschwimmen,Schmetterlingsschwimmen,Wasserball,Wasserspringen). ImLaufsportgibt es z. B. Wettbewerbe über 100, 200, 400, 800, 1500, 5000 und 10.000 Meter. Teilweise werden auch verschiedene Stilarten der Ausübung (z. B. dieEinzellagenimSchwimmsport), des Sportgeräts (z. B. verschiedeneSportwaffenimSchießsport), unterschiedliche Mannschaftsgrößen (z. B. Einer-, Zweier- und Viererteams beimBobfahrenoderKanusport) als einzelne Disziplinen ausgetragen. Sportarten und Sportdisziplinen erfordern lautDeutschem Olympischen Sportbund(DOSB) eigene sportartbestimmendemotorischeAktivitäten, bei denenBewegungzumSelbstzweckausgeführt wird und ethische Werte wieFair Play,ChancengleichheitoderUnverletzlichkeit der Personunter Beachtung vonWettkampfregelneingehalten werden müssen.[26] DieSportwissenschaftist eineinterdisziplinäre Wissenschaft(Querschnittswissenschaft), die Erscheinungsformen im Bereich von Sport und Bewegung zum Gegenstand hat. Die Sportwissenschaft greift auf eine Reihe anderer Wissenschaften zurück und hat entsprechend spezialisierte Einzeldisziplinen herausgebildet. Häufig wird auch vonSportwissenschaftengesprochen. Der Ursprung der auf denSport bezogenen Wissenschaften(sciences appliquées aux sports) reicht bis in die Renaissance zurück, aber erst gegen Ende des 19. Jahrhunderts hat sich eine eigenständigeSportwissenschaftherausgebildet. In den Sportwissenschaften sind unter vielen anderen die DisziplinenBiomechanik,Bewegungswissenschaft,Sport und Technik,Sportdidaktik,Sportethik,Sportgeschichte,Sportinformatik,Sportjournalistik,Sportmedizin,Sportökologie,Sportökonomie,Sportpädagogik,Sportphilosophie,Sportpsychologie,Sportrecht,SportsoziologieundTrainingswissenschaftvon Bedeutung. Im Zuge einer Debatte um die mögliche Aufnahme des Sports alsStaatszielin das deutsche Grundgesetz fasste der ehemalige BundesverfassungsrichterDieter Grimmdie positiven und negativen gesellschaftlichen Auswirkungen des Sports folgendermaßen zusammen: Aktivitäten und Anlagen mancher Sportarten können sich belastend auf Natur und Umwelt auswirken. Es gibt Möglichkeiten, solche Konflikte zu vermeiden oder auf ein tragbares Maß zu mindern.[28] Die Sportförderung des Spitzensport liegt in der Zuständigkeit des Bundes. Für denSchulsportsind die Bundesländer zuständig und die Errichtung und der Unterhalt von Sportstätten liegt in der Verantwortung der Kommunen (mit Zuschüssen der Länder).[29] Die konkrete Ausgestaltung der Förderung des Sports ist durch dieBundesländerüber Sportfördergesetze geregelt. Sportmedien haben seit dem Beginn des 20. Jahrhunderts erheblich zur Popularisierung des Sports als Kulturträger beigetragen. In Deutschland gibt es spezialisierteSportzeitschriften,FernsehsenderundInternet-Streaminganbieter. Die auflagenstärkste Sportzeitschrift ist dieSport Bild. Auch das MagazinKickererzielt eine große Reichweite. Im frei empfangbaren Fernsehen überträgtSport1Sportveranstaltungen. Sendungen wie dieSportschauoderDas aktuelle Sportstudioerzielen hohe Einschaltquoten. Im Bezahlfernsehen bzw. als Streamingdienst zeigenSportdigital,DynundMagentaTVLiveereignisse. Zahlreiche große Sportvereine betreiben eigene Informationsplattformen im Internet. Zugelassene internationale Anbieter von TV- und Streamingdiensten im Sportbereich sindEurosport,Sky,DAZNundAmazonPrime.Sky Sport News HDberichtet rund um die Uhr über aktuelle Nachrichten.[30][31][32][33][34][35][36][37][38][39][40][41] Neben der aktiven Sportausübung hat das Verfolgen von Sportwettkämpfen als Zuschauer am Austragungsort oder über dieMassenmedieneinen wichtigen gesellschaftlichen Stellenwert. Leistungssportler und Profimannschaften werden nicht selten als Idole verehrt. Die extreme Identifikation mit Sportlern führt zum Phänomen desSportfan. Insbesondere imFußballsporthat sich speziell im europäischen Raum ein Passiv-Sportkult gebildet, der vonFußballfans, „Ultras“ und Fußballrowdys („Hooligans“) gepflegt wird. Solch eine Fankultur konzentriert sich in verschiedenen Ländern oft auf unterschiedliche Sportarten. Sport lässt sich als eine bestimmte Weise zu spielen verstehen. Sportarten beruhen auf erkennbarem körperlichenHandelndurchBewegungdes Menschen.[42]Unter Sport werden die „verschiedenen, nachRegelnbetriebenenLeibesübungen[verstanden], die sowohl im kleinen privaten Rahmen ausgeübt als auch über große und zum Teil weltweiteOrganisationenundInstitutionenveranstaltet werden“.[43]Diese Definition ist für einige Sportarten zu eng, wie das Beispiel desSchachzeigt.Spielgrenzt sich vom Sport insbesondere dadurch ab, dass letzterer zur körperlichenFitnessoder alsBeruf(Berufssportler) betrieben wird. Spielen läuft ebenfalls nach Regeln (Spielregeln) ab, jedoch steht hier das spielerische Element im Vordergrund: so istKegelnein Spiel,Sportkegelndagegen ein Sport. DasBundesverwaltungsgericht(BVerwG) kam zu folgender Definition: „Die Abgrenzung wird im Allgemeinen danach erfolgen, dass Sport regelmäßig auf die Erhaltung und ggf. Steigerung derLeistungsfähigkeitzielt, während beim Spiel Zeitvertreib, Entspannung und Zerstreuung im Vordergrund stehen.“[44]Beide haben meist gemeinsam, dass im Kern der Betätigung derWettkampfmit dem Ziel desSiegesim Vordergrund steht. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie 2Definitionen 2.1Moderner Sport 3Geschichte des Sports 3.13000 v. Chr. – 600 n. Chr. 3.21700 – Gegenwart 4Sport als Bewegungskultur 4.1Gesundheitsaspekte 5Sportwettbewerb 5.1Training 6Sportarten 6.1Einige Ordnungsmöglichkeiten 6.2Olympische Sportarten 7Sportdisziplinen 8Sportwissenschaft 9Sportpolitik 9.1Sportförderung 10Sportmedien 11Sportfans 12Abgrenzung zum Spiel 13Siehe auch 14Literatur 15Weblinks 16Einzelnachweise Аԥсшәа Afrikaans Alemannisch አማርኛ Aragonés Ænglisc अंगिका"
  },
  {
    "label": 0,
    "text": "Straße – Wikipedia Straße Inhaltsverzeichnis Etymologie Ausbauzustand Einzelne Kriterien Bezeichnung Geschichte Straßen in Industriestaaten Verkehrstechnische Rekorde Verkehrsökonomie Trivia Siehe auch Literatur Weblinks Einzelnachweise Schreibweise Bestandteile des Straßenquerschnittes Baumaterialien Verkehrsarten und Nutzungsansprüche Verkehrskapazität Straßenkategorien Name Namenszusätze EineStraßeist imLandverkehreinVerkehrsbauwerk, dasFußgängernundFahrzeugenalsTransport-undVerkehrswegüberwiegend demPersonentransport, demGütertransportund demTiertransportzurOrtsveränderungdient. Der Begriff Straße bezieht sich insbesondere auf planmäßig mitStraßenbelagangelegte Verkehrsbauwerke.[1]Straßen dienen in erster Linie dem allgemeinen und freizügigen Straßenverkehr.[2]Sie bilden alsVerkehrsträgereinen bedeutsamen Teil derVerkehrsinfrastruktur, ein System von Straßen innerhalb einerRegionoder einesStaatesheißtStraßennetz. Die Straßen dienen als Verkehrsweg zurFortbewegungvon Fußgängern oder Fahrzeugen, so dass die Straßenverhältnisse den Verkehr mit Fußgängern oder Fahrzeugen auch tragen müssen. Straßen sind über die verkehrliche Nutzung hinaus weit mehr. Straßen sind Teil des öffentlichen Raums, in dem Menschen leben. Auf Spielstraßen spielen Kinder, bei einem Straßenfest feiert die Nachbarschaft. Im Alltag wird ein großer Teil der Fläche zum Parken von Autos verwendet. Auch Bürgersteige sind Teil der Straße und damit häufig auch der Zugang zu Wohnhäusern. Der Raum unter den Fahrbahnen wird häufig für die Kanalisation genutzt, Wasser-, Strom- und Gasleitungen sowie Kabelinfrastruktur dort verlegt. Es sind Rettungswege für die Rettungsdienste und die Möglichkeit für die Müllabfuhr ihren Dienst zu tun. Straßen können, je nach Bauweise und Beschaffenheit der Oberfläche, zur Regenwasserableitung oder direkt zur Versickerung genutzt werden; auch ein Straßengraben oder Restgrünflächen im Straßenraum gehören zur Straße. Straßen sind auch im übertragenen Sinn Lebensräume von Obdachlosen, die dann mehr oder weniger wortwörtlich „auf der Straße leben“. Das Wort „Straße“ geht zurück aufalthochdeutsch„straza“, das seinen Ursprung inlateinisch(via) strata(„gepflasterter Weg, derOrtschaftenverbindet“) hat.[3]Das Wort „Wasserstraße“ ist möglicherweise eher auf dieMeerenge(englischstraits) zurückzuführen. Je nachdem, auf oder in welcherMateriesich Straßen befinden, unterscheidet man: In diesem Artikel werden ausschließlich die auf Landflächen befindlichen Straßen beschrieben. Je nachAusbauzustand(mit oder ohneStraßenbelag) wird unterschieden zwischenNaturstraße(„Piste“),Weg,LandesstraßeoderAutobahn(Highway,Motorway). Naturstraßen sind typisch für die meistenEntwicklungs-undSchwellenländerund bestehen häufig aus demBodentypder die Straße umgebendenLandschaft. Als Bodentypen kommen insbesondereVertisol(englischblack cotton soil),Schotter,Schwarzerde,BraunerdeoderLateritbödenvor. Sie sind nicht ganzjährig befahrbar (nicht regenfest oder winterfest). Wege haben eine gewalzte (oder plattgefahrene) Oberfläche, die aber auch saisonal unterschiedliche Eigenschaften aufweisen kann. Typisch für solche unbefestigten Pisten ist ihrWellblechprofil(englischcorrugations). WichtigeÜberlandstraßensind weltweit häufiggepflastertoderasphaltiert, wodurch ihre Befahrbarkeit auch bei schwierigenWitterungsbedingungenverbessert wird. Straßen nähern sich überwiegend dem natürlichen Verlauf desGeländesan, sofern die entsprechend der vorgesehenen maximalen Fahrzeuggröße und -geschwindigkeit angemessenen Krümmungsradien der Straße einen oberflächennahen Verlauf zulassen. Die Straßen dasWasserverkehrsheißenWasserstraßen(Binnenschifffahrt:Flüsse,KanäleundSeen;Seeschifffahrt:Seewege), imLuftverkehrgibt esLuftstraßenundOrganized Track Systeme. UnterStraßenwerden im Allgemeinen die dem öffentlichen VerkehrgewidmetenStraßen, Wege und Plätze verstanden. Im Besonderen gehören zu einer Straße: Rechtlich gesehen bildet der gesamte öffentliche Bereich die Straße. Zum Beispiel ist der Begriff „auf der Straße Rad fahren“ insofern irreführend, da dies auch den Gehweg einschließt. Fußgänger, Radfahrer und Kraftfahrzeuge bewegen sich alsVerkehrsteilnehmeralle auf der Straße. Verkehrlich getrennt werden: Auch nach derRechtschreibreform von 2006ist inÖsterreichund inDeutschlandallein die Schreibung mit „ß“ korrekt: „Straße“. Vielerorts findet man die sowohl nach den aktuellen als auch den alten Regeln falsche Schreibweise „Strasse“. Dort ist bei Verwendung vonGroßbuchstaben„STRASSE“ korrekt. In derSchweiz und in Liechtensteinhingegen ist „Strasse“ regelgerecht und wird amtlich so bezeichnet. Ein Straßenquerschnitt soll die Verkehrssicherheit garantieren, ohne dabei die Leistungsfähigkeit zu beeinträchtigen, die Ziele von Umweltschutz und Städtebau müssen berücksichtigt werden, und die Wirtschaftlichkeit muss gewährleistet sein. Je nach der benötigten Funktion setzt sich der Straßenquerschnitt aus den folgenden Elementen zusammen: DieFahrbahndeckewird ausAsphaltbeton,Zementbeton,Pflasteroder aus unbefestigtem Material (beispielsweiseSchotter) hergestellt. Es bestehen verschiedene Nutzungsansprüche für die Verkehrsfläche, weshalb auf Straßen verschiedene Bereiche eingerichtet werden. Hierzu gehören: Mit der BezeichnungWegewerden die ausschließlich für den nicht motorisiertenVerkehrausgelegten Straßen bezeichnet. Eine Ausnahme bilden hier die so genanntenländlichen Wege. Diese sind auch für den motorisierten Verkehr der Land- und Forstwirtschaft ausgelegt. Im Bereich der Straßen und Wege unterscheidet man weiterhin diefreie Strecke, dieOrtsdurchfahrt, denKnotenpunktund dieNebenanlagen. Die Verkehrskapazität einer Straße ist maßgeblich von deren Ausbauzustand abhängig (Anzahl und Breite der Fahrstreifen, Linienführung). Ferner spielen auch Ortsdurchfahrten mit zahlreichen Einmündungen und Kreuzungen,Bahnübergängeund die Geländetopografie (Steigung/Gefälle) eine Rolle. Auch der Anteil des LKW-Verkehrs ist wichtig. Je höher dieser ausfällt, umso geringer ist die Zahl der Fahrzeuge, die die Straße insgesamt nutzen können, ohne dass der Verkehrsfluss stockt. In Deutschland definieren zwei Regelwerke die Baustandards für Straßen außerhalb von Ortschaften. Für Autobahnen sind dies dieRichtlinien für die Anlage von Autobahnenund für Bundes-, Landes- und Kommunalstraßen dieRichtlinien für die Anlage von Landstraßen. Sie definieren die Maximalkapazität einer zweistreifigen Straße mit rund 20.000 Fahrzeugen am Tag, wobei sich schon bei Verkehrsstärken über 10.000 Fahrzeugen am Tag die Verkehrsstockungen – etwa in Kreuzungsbereichen, bei langsamen LKW und fehlenden Überholmöglichkeiten oder an Bahnübergängen – häufen können und ein Ausbau, z. B. im2+1-Systemoder durch den Umbau von Kreuzungen zu höhenfreien Ein-/Ausfahrten sinnvoll sein kann. Zwei Fahrstreifen pro Richtung, insgesamt vier Streifen, sind der Standard für Autobahnen und hoch frequentierte Bundesstraßenabschnitte mit Verkehrsdichten zwischen 20.000 und 60.000 Fahrzeugen am Tag. Drei Fahrstreifen pro Richtung (sechs Streifen insgesamt) sollen für Verkehrsstärken zwischen 60.000 und 100.000 Fahrzeugen täglich angewandt werden, was auf höher frequentierte Autobahnabschnitte zutrifft. Verkehrsstärken über 100.000 Fahrzeugen am Tag treten nur auf den wichtigsten Autobahnen in Ballungsgebieten auf und machen die Anlage zusätzlicher Richtungsfahrstreifen erforderlich. Straßen werden nachStraßenkategorienunterteilt. Dazu zählen zum BeispielAutobahnen,Fernstraßen,Hauptstraßen,Ortsstraßen,Erschließungsstraßen,Spielstraßen,Land- und Forstwirtschaftliche Wege, eigenständig geführte Radwege,Gehwege,Kreisverkehrsplätze,TunneloderParkplätze. In Orten ist die Unterscheidung vonDurchgangsstraßen,Stadtstraßen, Siedlungsstraßen,Wohnstraßenwichtig. In Deutschland wird in den Regelwerken derForschungsgesellschaft für Straßen- und Verkehrsweseninnerorts zwischenHauptverkehrsstraßen,Sammelstraßenund Erschließungsstraßen unterschieden, wobei dies vorrangig die Bedeutung im Kfz-Netz benennt. Spezielle Kategorien, auch nach Verwendungszweck, sindAnliegerstraßen,Einbahnstraßen,Einkaufsstraßen,Fahrradstraßen,Ferienstraßen,Fußgängerzonen,Panoramastraßen,Passstraßen,Radschnellwege,Rennstrecken,Sackgassen,Zollstraßen(Zollfrei-Straßen) oder temporär geschlossene oder geöffnete Straßen. Innerörtliche Straßen haben in Deutschland meistNamen(eine Ausnahme ist dieMannheimer Innenstadt), die an Kreuzungen oder Einmündungen durchSchilderangezeigt werden. In der Regel kommt in jedem Ort jeder Straßenname nur einmal vor, so dass die Straßen durch Nennung von Ort und Name eindeutig zu identifizieren sind. DurchEingemeindungenkönnen Straßennamen mehrfach vorkommen. In der Regel wird dann die kleinere der betroffenen Straßen umbenannt. Wurde dies, wie inCottbus, unterlassen, existieren Straßennamen mehrfach. Eine genaue Identifikation ist dann normalerweise über den Stadtteilnamen oder die Postleitzahl möglich. Bei der Bildung vonGroß-Berlinim Jahre 1920 wurden eine große Anzahl von Landgemeinden und Dörfern im Umkreis einbezogen, so gab es mehrere gleichlautende Straßennamen, die sich allerdings durch die (deshalb übliche und notwendige) Angabe der Verwaltungsbezirke oder Ortsteile unterscheiden ließen. Im Jahre 1938 erfolgte mit der Zentralisierung von Verwaltungsaufgaben eine größere Aktion zur Umbenennung, die jedoch nicht alle Doppelungen abdeckte. Zudem existiert eine große Anzahl von Straßen, die nur durch Nummerierung – zumeist nach Bebauungsplan – unterschieden werden, wiederum teilweise auch doppelt vergeben (Beispiel: Straße 101 Nr. 3a). Durch die nachfolgenden Kriegs- und Nachkriegsjahre wurden erst später und bezirksweise wieder doppelt vorhandene Straßennamen umbenannt, verbliebene Beispiele findet sich in den Ortsteillisten derStraßen und Plätze in Berlin. So werden auch noch in den 2000er Jahren bei anderweitiger Veranlassung Straßen umbenannt. Gewidmete nummerierte Straßen werden bei passender Veranlassung umbenannt. Mittlerweile findet bei Neubenennungen (insbesondere nach derWiedervereinigung) eine Abstimmung zwischen den Berliner Bezirken und innerhalb der Bezirke statt. Als Beispiel kann die Auswahl von 42 neu zu benennenden Straßen inBerlin-Blankenburggenannt werden, wo die anlageneinheitliche Namensgebung nach Vogelarten zu Rücksichten auf Straßennamen in anderen Siedlungen und Ausweichnamen führte. Außerortsstraßen und Fernstraßen tragen Nummern. In Deutschland werden normalerweise nur die Nummern der Bundesstraßen und Autobahnen dem Kraftfahrer signalisiert. Landesstraßen und Kreisstraßen sind ebenfalls mit Nummern gekennzeichnet. Erkennbar ist diese Nummer amStationszeichen, das für den Verkehrsteilnehmer jedoch nicht weiter von Bedeutung ist. Bei Namen für Ortsstraßen ist es notwendig, den WortbestandsteilStraßezu setzen. Dabei ist allerdings je nach Eigenschaft die Nutzung anderer straßenbeschreibende Begriffe üblich. Für Siedlungsstraßen wird oft der Wortbestandsteil-weggenutzt, da hier vom Belag oder der Straßenbreite von Behörden oder Anliegern die Abgrenzung zur Stadtstraße gewünscht ist. Eine Zuordnung solcher Wortbestandsteile zu den Verkehrsbedingungen ist jedoch nicht üblich, beispielsweise kann ein „X-Pfad“ durchaus eine kraftfahrzeugfähige Straße sein. Ein Beispiel sind die in den 1960er- und 1970er-Jahren neu angelegtenStraßen in Berlin-Gropiusstadt. Hier wurden (mitunter aus historischen Gründen) verkehrsfähige Straßen der Klasse V (sonstige Straßen) desBerliner Straßenverzeichnissesmit Pfad, Zeile, Steig benannt, andererseits sind Allee oder Damm begrünte, aber schmale Straßen. Die nachfolgende Übersicht zeigt eine Liste der Straßen- und Platzbezeichnungen im deutschsprachigen Raum von häufig vorkommendennachgestellten Namenszusätze, die ohne oder mitBindestrich– getrennt sowie zusammengeschrieben werden: Die nachfolgende Übersicht zeigt eine Liste der im deutschsprachigen Raum häufig vorkommendenvorangestellten Namenszusätze bei Straßen- und Platzbezeichnungen: Es hat im Laufe der Geschichte viele Gründe gegeben, Straßen zu bauen: Sie boten Zugang zu Nahrung und Unterkunft, dienten als Routen für jahreszeitliche Wanderungen, alsProzessionsstraße, fürWallfahrtenoder für den Handel. Die Straßen im heutigen Verständnis entwickelten sich aus Straßen desAltertums, den so genanntenAltstraßen. Gesellschaftliche und wirtschaftliche Entwicklungen führten zur Einführung von Fahrzeugen, was das Verkehrsaufkommen noch verstärkte. Im Zuge der gesellschaftlichen Differenzierung brauchte man Straßen auch für den Zugang zu Arbeit, Bildung und Unterhaltung. Jedoch warenmilitärischeundstaatspolitischeÜberlegungen häufigstes Motiv für den Straßenbau. Die ersten Militärfahrzeuge (Streitwagen) wurden um 2500 v. Chr. entwickelt. Von da an waren Straßen ein wichtiges Hilfsmittel bei Angriff und Verteidigung, und viele Herrscher verwendeten beträchtliche Mittel für ihren Bau undUnterhalt(sieheMaut). Die bisher frühesten Zeugnisse eines geordneten, planvoll alsSchachbrettangelegten Straßenbaus finden sich in derBronzezeitzwischen 2600 und 1800 v. Chr. in derHarappaoderIndus-Kultur. In der ersten indischen Hochkultur, die über weite Handelsbeziehungen bis nachVorderasienund zum Mittelmeer verfügte, gab es in den Städten wie Harappa oderMohenjo-DarobereitsgepflasterteStraßen, die über eine Abwasser-Kanalisationverfügten.[4]Als älteste erhaltene befestigte Straße der Welt gilt dieSteinbruchstraße am QarunseeinÄgypten, die auf ca. 2600–2100 vuZ datiert wird.[5][6] ImAssyrischen Reich(dem Gebiet um dasZweistromland) wurde eine Königsstraße (harran šarri) gebaut, an der in regelmäßigen Abständen Karawansereien oder Straßenstationen (kalliu) lagen.[7]EineurartäischeStraße von bis zu 5,4 m Breite mit Straßenstationen im Abstand von ca. 30 km wurde zwischenElazığundBingölnachgewiesen. DieachämenidischeKönigsstraße, die vonDareios I.im 5. Jahrhundert v. Chr. angelegt wurde, führte vonSusaüberPersepolisundPasargadaenachSardes.[8]Der König ließ diese Straße zur schnellen Kommunikation innerhalb seines riesigen Reiches bauen. Ein Abschnitt der Königsstraße wurde beiNaqsch-e Rostamausgegraben, er war ca. 5 m breit und gepflastert.[9] Die prachtvolleProzessionsstraße(Aj-ibur-shapu) zumIschtar-TorinBabylonwurde unterNebukadnezar II.bis 562 v. Chr. erschaffen. In ihrer Anlage und Ausführung unterschied sie sich wesentlich gegenüber dem damals üblichen Straßenbau. DiePflasterungder Straße bestand ausreliefartig-glasiertenSteinen, die schon zu dieser Zeit auf einem Bett ausAsphaltverlegt worden sind. Das Ischtar-Tor war Teil derMauern von Babylon, die bis zu ihrer Zerstörung zu den siebenWeltwundernderAntikegehörten. Ein großes Straßennetz bildeten dieRömerstraßen(sieheListe der Römerstraßen), von denen dieVia Appia(312 v. Chr.) die älteste ist. Als Nachbarn derRömerbauten dieEtruskerbereits vor diesen bis zu 15 m breite gepflasterte Straßen – mit Fußgängerstreifen – in ihren Städten, wie in dem gut erforschtenMarzabottoin denApenninen. Unter den Straßen befand sich eine Wasserleitung. Dasschachbrettartigangelegte Straßennetz der Stadt diente später in derRenaissanceals Vorbild italienischer Architekten.[10] Wie in anderenHochkulturenzuvor, gab es in dengriechischenund römischen Städten Stadtstraßen, um die einzelnenInsulaezu erschließen. Die Römer bauten Straßen vorwiegend für militärische Zwecke, um Truppen möglichst schnell an die Grenzen desRömischen Reichesverlegen zu können (sieheRömerstraße). Der britische IngenieurJohn McAdamhatte sich lange mit dem Straßenbau beschäftigt. Im Jahr 1815 ließ er beiBristoldie erste geschotterte Landstraße bauen. Das Straßenbett lag höher als die umgebenden Felder, damit das Regenwasser abfließen konnte, es hatte einen Unterbau aus grobemSchotter, darüber eine Lage aus kleineren Steinen und war mitSchlackebefestigt. Diese Konstruktion bewährte sich dermaßen gut, dass sie sich schnell in anderen Ländern verbreitete. Von dem Namen McAdam leitete sich das noch lange gebräuchliche Wort „Makadam“ für diese Art Straßenbau ab. In Mitteleuropa wurden die Altstraßen erst ab etwa 1850 von denChausseenabgelöst, die dann, alsGuglielminetti1902 eine alte Schotterstraße nahe Monte Carlo mit einemTeerbelagüberzog,[11]zu den Straßen wurden, wie man sie kennt. Speziell in jüngerer Zeit wurden „neue“ Straßen erfunden, um sie touristisch besser vermarkten zu können. Es sind dieFerienstraßen, die teilweise Bezüge zu den Altstraßen haben. InIndustriestaatenwerden Straßen inStraßenkategorieneingeteilt, etwa inBundesfernstraßen,LandesstraßenoderStaatsstraßen,KreisstraßenundGemeindestraßen. Diese deutsche Kategorisierung findet sich auch in anderen Ländern (USA:Interstate Highways,FreewaysundExpressways, Principal Arterials, Minor Arterials, Collectors und Local Roads).[12] Der Zusammenhang zwischen Nutzung und vorhandener Verkehrsinfrastruktur zeigt sich bei Straßen wie folgt: DerStraßenbaumuss die Landschaft berücksichtigen, so dassStraßentunneldurchBergeoderStraßenbrückenüberFlüsseoderTälergeführt werden. Berühmte nationale oder transkontinentaleFernstraßensind weltweit: Diese transkontinentalen Straßen gehören zu den längsten Straßen der Welt. Nicht alle besitzen die angegebene Bezeichnung als einheitlichenStraßennamen, sondern lediglich der National Highway 1, Trans-Canada Highway, U.S. Highway 20, Interstate 90, Stuart Highway und Alaska Highway. Nicht in der Tabelle erwähnt ist derTranssibirien-HighwayinRussland(9.947 km) zwischenSt. PetersburgundWladiwostok, denn es handelt sich um die nicht-offizielle Bezeichnung mehrerer russischer Fernstraßen (M10,M5,R254,R255,R258,R297undA370). Diebreiteste Straßeist weltweit mit 250 Metern für sechs Fahrbahnen dieEixo MonumentalinBrasília, die schmalsten Straßen sind mit 31 cm dieSpreuerhofstraßeinReutlingenund mit 43 cm derVicolo della VirilitáinRipatransone, als bekannteste kurvenreichste gilt die zwischen zwei Querstraßen mit acht Kurven auf einer Länge von 145 Metern verseheneLombard StreetinSan Franciscomit einem Gefälle von 18 %.[15] Eine leere Straße ist in derVerkehrsökonomieundVolkswirtschaftslehreeinöffentliches Gut, doch wird sie zumAllmendegut, wenn es zurHauptverkehrszeitzuVerkehrsstaus(externer Effekt) kommt,[16]was sich in der gegenseitigenVerkehrsbehinderung(Rivalität) der betroffenenVerkehrsteilnehmerzeigt.[17]Die Zuordnung einer Straße zu einer Güterkategorie ist jedoch nicht dauerhaft, sondern kann Veränderungen unterworfen sein. Das öffentliche Gut Straße wird beim Verkehrsstau zu einemKlubgut, wenn sie verkehrsarm ist und der Verkehrsteilnehmer eineMautentrichten muss.[18]Schließlich wird sie bei Verkehrsstau und Mautgebühr sogar zu einemprivaten Gut.[19] Bei starkerNutzungdesStraßennetzesdurch die Bevölkerung entsteht Rivalität.[20]Rivalität bedeutet, dass jeder Verkehrsteilnehmer, der eine Straße benutzt, denNutzenfür weitere Verkehrsteilnehmer schmälert, weil sich Verkehrsstaus ergeben und zuVerspätungenundStaukostenführen können. Im Extremfall zeigt sich dies in derÜbernutzung(ÜberweidungundÜberdüngungvonAgrarflächen,ÜberjagungderTierweltÜberfischungderWeltmeere, aber auchVerkehrsinfarktim Straßennetz undUmweltbelastung) derAllmende. Hauptsächlich über Straßen handelt dasFilmgenredesRoadmovies, als dessen Archetypus der im Juli 1969 in die Kinos gekommeneKultfilmEasy RidermitDennis Hopper,Peter Fonda,Jack NicholsonundMusikproduzentPhil Spector(in einerCameo-Rolle) gilt. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie 2Ausbauzustand 3Einzelne Kriterien 3.1Schreibweise 3.2Bestandteile des Straßenquerschnittes 3.3Baumaterialien 3.4Verkehrsarten und Nutzungsansprüche 3.5Verkehrskapazität 3.6Straßenkategorien 4Bezeichnung 4.1Name 4.2Namenszusätze 5Geschichte 6Straßen in Industriestaaten 7Verkehrstechnische Rekorde 8Verkehrsökonomie 9Trivia 10Siehe auch 11Literatur 12Weblinks 13Einzelnachweise Afrikaans Pangcah Aragonés العربية ܐܪܡܝܐ Aymar aru Azərbaycanca تۆرکجه Башҡортса Žemaitėška"
  },
  {
    "label": 0,
    "text": "Tennis – Wikipedia Tennis Inhaltsverzeichnis Geschichte Etymologie Verbreitung Spielvarianten Regeln Ausrüstung Laufvarianten Schlagtechnik Strategische Schläge Gewinnschläge Fehler Tennisstatistik Spielstrategien Doppel Turniere Turnierbetrieb Externe Einflüsse Einkommen Verletzungen beim Tennis Kondition Ernährung Aufwärmphase Regeneration Dopingtests Korruption Abwandlungen Siehe auch Literatur Weblinks Einzelnachweise Internationale Wettbewerbe Nationale Tennis-Verbände Fast4 Tennis Ultimate Tennis Showdown Tie Break Tens Spielfeld Spielprinzip Zählweise Schiedsrichter Verhaltenskodex Zeitregeln Coaching Schläger Besaitung Bälle Bekleidung Vorhand Rückhand Aufschlag Ballrotation Aufschlag Return Schmetterball Angriffsball Passierschlag Lob Tweener Sabr Stoppball Volley Halbvolley Cross Longline Winner Ass und Service Winner Big Points Forced Error und Unforced Error Doppelfehler Angstgegner Serve and Volley Grundlinienspiel Chip and Charge Inside-Out-Schläge Rhythmuswechsel Mixed ATP Tour WTA Tour Grand-Slam-Turniere ATP Finals – WTA Finals Junior Accelerator Programme ITF Junior Masters Next Generation ATP Finals Mannschaftswettbewerbe Olympische Sommerspiele Special Olympics Showturniere Hawk-Eye Weitere Messgeräte Künstliche Intelligenz Matchanalyse International Tennis Integrity Agency Balljungen und Ballmädchen Security ATP Baseline Grand Slam Player Development Programme Preisgelder der größten Turniere Karriere-Preisgeldrangliste Ewige Preisgeldrangliste früherer Topspieler Preisgeldparität Muskelbeschwerden Schulterschmerzen Rückenschmerzen Tennisarm Handgelenk Verletzungen am Bein, Knöchel, Knie Schienbeinkantensyndrom Hautblase Schürfwunde Dehydrierung Energiebereitstellung Flüssigkeitszufuhr Ernährung am Matchtag Beachtennis Padel Soft Tennis Kleinfeldtennis Play & Stay Racketlon Pickleball Rollstuhltennis Blindentennis Gehörlosentennis Para-Standing-Tennis Tennis bei geistiger Beeinträchtigung Kombinationsturniere Touchtennis Fußballtennis Tennis Clash Deutschland Österreich Schweiz Weitere Länder Varianten der Zählweise Geschichte der Zählweise Frühere Coachingregeln Effet Drive Topspin Slice Tennisist einRückschlagspiel, bei dem der Spielball von den Spielern mit speziellen Schlägern wechselseitig über ein Netz in die gegnerische Spielfeldhälfte geschlagen wird. Dieser ursprünglich als eher elitär geltende Sport hat sich mit fortschreitender Zeit zum beliebten Breitensport entwickelt. Nach einer ersten Phase von 1896 bis 1924 ist Tennis seit 1988 wieder eineolympische Sportart. Der französische Vorläufer des heutigen Spiels,Jeu de Paume, wurde zunächst in Klosterhöfen, später inBallspielhäusernauf einem rechteckigen Feld gespielt. Die Spieler machten ihreAngabe, indem sie den Ball über das Netz gegen eine Wand schlugen, welche entlang des Feldes verlief. Die Zuschauer saßen an der Wand gegenüber. Linien teilten das Feld in vier fünfzehn Zoll (knapp vierzig Zentimeter) breite, parallel verlaufende Streifen zu beiden Netzseiten. In Paris wurden zwischen 1500 und 1600 zahlreiche kommerzielle Tennisanlagen erbaut, in denen die Besitzer die für das Spiel nötige Ausrüstung verliehen oder verkauften. Die Anlagen wurden durch die verschiedensten Bevölkerungsschichten als Freizeiteinrichtung genutzt.[1] Der englische MajorWalter Clopton Wingfieldließ sich 1874 seine Art von Tennis, das erSphairistikè(griechisches Wort für Ballspiele) nannte, patentieren. Dabei wurden erstmals verbindliche Regeln definiert. Da es auf Rasen gespielt wurde, nannte man das Spiel auchLawn Tennis(engl. für Rasentennis). Das bis heute übliche Tennis entstand mit neuen Regeln im Zuge der ersten Meisterschaften inWimbledon(London) im Juli 1877. Mit dem Aufkommen des Profitennis Mitte der 1920er Jahre[2]wurden zu vielen großen Turnieren nur noch Amateure zugelassen. 1925 wurde Tennis aus dem olympischen Programm gestrichen. 1968 wurde die Beschränkung aufgehoben (siehe auchOpen Era), wodurch große Turniere, wie zum Beispiel dieUS Openoder dieFrench Open, erhebliche finanzielle Bedeutung erlangten. Seit 1988 ist Tennis wieder eine olympische Disziplin. Der Ursprung des WortesTennisist ungewiss. Für die wahrscheinlichsteEtymologiehältAnatoly Libermandie auch vonWalter W. Skeatfavorisierte Theorie, dass es auf einen französischen oder vielmehranglonormannischenAusruf*tenez!zurückgehe, also den Imperativ Plural vontenir(„halten“) mit der Bedeutung „Nehmt, haltet (den Ball)!“, der jedoch in dieser Form und Bedeutung nicht bezeugt ist. Für diese Theorie spricht, dass das Spiel ein Zeitvertreib der vorwiegend anglonormannisch sprechenden Aristokratie war, sowie der Umstand, dass das Wort imMittelenglischennoch auf der zweiten Silbe betont wurde. Zudem ist der früheste Nachweis ein um das Jahr 1440 datiertes Manuskript eines Gedichtes vonJohn Gowermit der Schreibungtenetz.[3] Die Bezeichnung des Ballspiels ist eine Übernahme (Anfang 19. Jh.) von gleichbedeutend englisch: tennis, gekürzt aus englischlawn tennis‘Rasentennis’, älterfield tennis‘Feldtennis’.[4] Andere Theorien leiten das Wort von deutschTenneoderTanz, von lateinischtaenia(„Kopfbinde“) oder vom französischen OrtsnamenTennoisab. Erwähnung verdient außerdem die einfallsreiche Mutmaßung des Lexikographen Frank Chance, der enthauptete christliche MärtyrerDionysius von Paris, französischSt. Denis, sei einst der Schutzpatron und Namenspate des Spiels gewesen: In derIkonographieträgt er oftmals sein Haupt wie einen Ball in seinen Händen.[5] Laut einer Untersuchung derInternational Tennis Federation(ITF), dem Tennisweltverband, gibt es weltweit 87 Millionen Tennisspieler (Stand 2019), davon sind 41 % weiblich. China (etwa 22 %) und die Vereinigten Staaten (27,3 %) machen zusammen knapp 50 % der weltweiten Tennisbevölkerung aus. Auch in Europa ist Tennis sehr beliebt (25,9 %). Weltweit hat die ITF 115.000 Tennisclubs und 578.000 Tennisplätze ermittelt. 2137 Spieler haben einATP-Ranking, 1482 Spielerinnen einWTA-Ranking. Insgesamt waren 640 Profis im Jahre 2019 unter den Top 250. 173 waren zumindest einmal unter den Top 100. 3572 Junioren hatten ein Ranking in der ATP-Juniorenweltrangliste, 3703 Juniorinnen hatten ein solches in der WTA-Juniorinnenweltrangliste.[6] Während auf Freizeitanlagen eine breite Schicht der Bevölkerung Zugang zu Tennisanlagen hat und frei von Bekleidungsvorschriften ist, gibt es parallel dazu Clubs, in denen Mitglieder gezielt ausgewählt werden und hohe Mitgliedsbeiträge eine Hürde für Bevölkerungsschichten mit kleinerem Einkommen darstellen. Die Ausprägung von Tennis als „reine“ Oberschichtensportart ist in Ländern mit niedrigerem Bruttonationaleinkommen und einer kleineren Mittelschicht stärker ausgeprägt oder aber als Sportart kaum vorhanden. Abgesehen von den aktiven Tennisspielern zählt die Tennis-Fangemeinde eine Milliarde Menschen weltweit und belegt damit Platz vier der beliebtesten Sportarten, hinterFußball,CricketundFeldhockey.[7] 2024 nahmen insgesamt 10.979 Spieler an 1.200 Turnieren der ITF World Tennis Tour teil, gegenüber 1.135 Events im Jahr 2023. 598 Herren- und 602 Damenturniere wurden in 72 verschiedenen Gastgeberländern ausgetragen. Für Damen-Events wurde im Jahr 2024 ein Rekordpreisgeld von 17,9 Millionen US-Dollar ausgezahlt (gegenüber 17,5 Millionen US-Dollar im Jahr 2023), wobei den Herren ein Rekordpreisgeld von 11,3 Millionen US-Dollar geboten wurde (gegenüber 11,1 Millionen US-Dollar im Jahr 2022).[8] Ägypten|Angola|Australien|Argentinien|Bahamas|Belgien|Bosnien und Herzegowina|Brasilien|Bulgarien|Chile|China|Dänemark|Deutschland|Ecuador|Estland|Finnland|Frankreich|Griechenland|Großbritannien|Guinea-Bissau|Hongkong|Indien|Indonesien|Irland|Island|Israel|Italien|Jamaika|Japan|Kanada|Kap Verde|Kasachstan|Katar|Kolumbien|Kroatien|Lettland|Liechtenstein|Litauen|Luxemburg|Macau|Malta|Marokko|Mexiko|Mosambik|Namibia|Neuseeland|Niederlande|Norwegen|Österreich|Paraguay|Peru|Polen|Portugal|Puerto Rico|Rumänien|Russland|San Marino|Schweden|Schweiz|Senegal|Sierra Leone|Simbabwe|Singapur|Slowakei|Slowenien|Spanien|Südafrika|Taiwan|ThailandTschechien|Tunesien|Türkei|Ungarn|Uruguay|Usbekistan|Venezuela|Vereinigte Staaten DerDeutsche Tennis Bund(DTB) hat 2024 8685 Tennisvereine mit 1,94 Millionen Vereinsmitgliedern in Deutschland gezählt und ist damit der mitgliederstärkste Tennisverband weltweit. Hinzu kommen Freizeitspieler ohne Vereinszugehörigkeit. Sie spielen allesamt auf 44.897 Tennis-Courts in Deutschland.[9]Tennis ist mit über fünf Millionen aktiven Spielern eine der beliebtesten Sportarten in Deutschland. In Deutschland wird der Tennissport vom DTB organisiert, der sich in 17 Landesverbände aufteilt. Er organisiert den Ligaspielbetrieb, die Turniere, das Nationalteam (Davis-Cup- bzw.Billie-Jean-King-Cup-Team) und auch Tennistrainerausbildung, wobei er die Ausbildung zum A-Trainer selber vornimmt. Von den Landesverbänden bzw. von den ihnen untergeordneten Bezirken werden in allen Altersklassen Mannschaftswettkämpfe, die sogenanntenMedenspiele, organisiert. Dabei wird in Amateurligen von Kreisklassen für Freizeitsportler bis hinauf zu Verbands- bzw. Regionalligen um Auf- und Abstieg gerungen. Den Oberbau mit Ausrichtung zum Profitennis (bei den aktiven Herren und Damen) bilden die1. und 2. Bundesliga, die vom DTB organisiert werden. Daneben bilden die Landesverbände B- und C-Trainer im Auftrag des DTB aus.[10] In Österreich ist derÖTV(Österreichischer Tennis Verband) als Unterorganisation des ITF gemeldet. Seit 2009 gibt es in Österreich eineBundesliga. Ende 2023 hatten die Tennisvereine in Österreich rund 196.700 Mitglieder, die in 1700 Vereinen organisiert sind.[11]Mehr als 400.000 Menschen spielen in Österreich gelegentlich Tennis. In der Schweiz fungiertSwiss Tennisals nationaler Dachverband. Dort entsprechen derInterclub der Nationalliga A(NLA) und der NLB den deutschen Bundesligen. Die Herrenliga wurde 1911, die Damenliga 1925 gegründet.[12]Im Jahr 2023 verzeichnete der Verband Swiss Tennis 167.234 Mitglieder.[13]Im Jahr 2022 gab es in der Schweiz insgesamt 3601 Tennisplätze; davon 650 in einer Halle. 620.000 Schweizerinnen und Schweizer üben die Sportart aus. Im Jahr 2023 erreichte die Zahl der Tennisspieler in den Vereinigten Staaten mit rund 23,84 Millionen eine neue Höchstzahl. Sie sind in derUnited States Tennis Associationorganisiert. DieFédération française de tennisumfasst etwa 1,1 Millionen Mitglieder in 8300 Vereinen. In China spielen 19,92 Millionen Tennis. 50.000 Courts gibt es in China, aber nur 800 Clubs. In Italien wuchs die Anzahl der Tennisclubs 2021 auf 3433 mit insgesamt geschätzten zwei Millionen Spielern. Wettkampfmäßigwird Tennis alsEinzel, bei dem ein Spieler gegen einen anderen antritt, oder alsDoppel, mit je zwei Spielern, gespielt. In denVerbändenhaben sich dafür jeweils reine Herren- und Damen-Wettbewerbe sowie dasGemischte Doppel (Mixed)– mit je einem männlichen und weiblichen Spieler auf Seiten beider Gegner – etabliert. Daneben findenRollstuhlwettbewerbeund folgende Sonderformate statt: ImFast4 Tenniswerden Spiele im Einzel mit Best-of-Three (zwei Gewinnsätzen) oder Best-of-Five (drei Gewinnsätze), kurzen Sätzen (auf vier gewonnene Spiele) mit einem kurzen Tie-Break beim Stand von 3:3 gespielt. Es gelten die No-Let-Regel und die No-Ad-Regel. BeimUltimate Tennis Showdownwird ein modifiziertes Wertungsformat verwendet, bei dem das Match in vier Viertel unterteilt wird. Am Ende von acht Minuten reiner Spielzeit werden Entscheidungspunkte ausgespielt, wobei der Führende einen Vorteil hat: Er muss nur noch einen Punkt gewinnen, um das Viertel zu gewinnen. Die Zeitregeln sind verkürzt. Tie Break Tenswird nach den üblichen Tie-Break-Regeln gespielt. Spieler gewinnen, wenn sie 10 Punkte mit einem Vorsprung von zwei Punkten erreichen. Das Turnier erfolgt in einem Achterfeld nach demK.-o.-Systemmit einem Viertelfinale, Halbfinale und Finale. Das jeweilige Match dauert nur etwa zehn bis fünfzehn Minuten. Die Regeln für den Wettspielbetrieb werden international von der ITF vorgegeben. Diese sind damit auch bindend für die angeschlossenen internationalen und nationalen Verbände wie demDeutschen Tennisbund(DTB) sowie der ATP und WTA. Die Regeln enthalten jedoch Öffnungsklauseln, die den jeweiligen Veranstaltern Wahlmöglichkeiten einräumen. Diese betreffen insbesondere dieBälle(Anzahl, Wechsel, Ballmarken), Pausenzeiten, Einschlagdauer, Anzahl der Gewinnsätze,Zählweise,Coaching, Technik für Spieleranalysen und Anwendung einesVerhaltenskodexes.[14] Das Tennis-Spielfeld(Tenniscourtoder einfachCourt) ist rechteckig und wird durch das Netz in zwei Hälften geteilt. Die Maße des Spielfeldes wurden ursprünglich in englischenFuß(1 ft = 0,3048 m) definiert. Es ist 78 ft (23,77 m) lang und für das Einzelspiel 27 ft (8,23 m) breit, für das Doppelspiel 36 ft (10,97 m) breit. Das Spielfeld wird durch Linien begrenzt, die sogenanntenGrundlinien(Baselines) undSeitenlinien(Sidelines). Die Linien sind Bestandteil des Spielfeldes; d. h. fällt der Ball (auch nur teilweise) auf eine der Linien, so ist er weiterhin im Spiel. Die Grundlinien verlaufen parallel zum Netz, die Seitenlinien rechtwinklig dazu. Parallel zum Netz liegen auf beiden Seiten im Abstand von 21 ft (6,40 m) dieAufschlaglinien. In der Mitte des Feldes führt vom Netz zur Aufschlaglinie dieAufschlagmittellinie. Die Fläche zwischen Netz und Aufschlaglinien wird inoffiziell auch alsT-Feld(Aufschlagmittellinie und Aufschlaglinie bilden den Buchstaben „T“) oderHalbfeld(Aufschlaglinie teilt die Spielhälfte in der Mitte) bezeichnet. Dementsprechend wird die Aufschlaglinie auch alsT-Liniebezeichnet. Die zwei Flächen eines T-Feldes werdenAufschlagfeldergenannt. Innerhalb dieser Aufschlagfelder wird insbesondere im KinderbereichKleinfeldtennisgespielt. Das Regelwerk der ITF lässt verschiedene Breiten der Linien zu. Die Markierungslinien sollen bei der Aufschlagmittellinie fünf Zentimeter und alle anderen Linien zwischen 2,5 und 5 cm breit sein, ausgenommen die Grundlinien, deren Breite bis zu 10 cm betragen darf. Auf Sandplätzen sind Linien in einer Breite von vier oder fünf Zentimetern gängig. Das Netz ist in der Mitte 3 ft (0,914 m) und an den Seitenenden 3,5 ft (1,07 m) hoch. Die Mitte des Netzpfostens ragt 3 ft (0,914 m) über das Spielfeld hinaus bei Doppelspielen. Dies gilt auch bei Einzelspielen, sofern Einzelnetzpfosten vorhanden sind, ansonsten sind dort 3,5 ft (1,07 m) hohe Einzelstützen anzubringen. Die Grundlinie und Einzäunung sollen als Richtlinie einen Abstand von mindestens 18 ft (5,50 m) bei Freizeit- und Vereinsplätzen und bis 21 ft (6,40 m) bei internationalen Wettbewerben haben, zwischen Seitenlinie und Einzäunung 10 ft bis 12 ft (3,04 m bis 3,66 m). DerCenter Court Philippe Chatriervon Roland Garros hat elf Meter Auslauf hinter der Grundlinie. Die häufigsten Beläge von Tennisplätzen sind roterSand(meistZiegelmehl),Hartplatz,Teppichboden, Kunststoffgranulat,RasenoderKunstrasen. Vor allem in nordamerikanischen Ländern ist der Hartplatz sehr verbreitet. Im Freien sind in Deutschland Sandplätze vorherrschend, in der Halle wird meist auf einem Hart-, Granulat-, Teppichboden- oder Sandplatz gespielt. Die verschiedenen Beläge prägen das jeweilige Spielgeschehen vornehmlich durch die Art und Weise, wie sich der Ball nach dem Aufprallen verhält: Auf einem Rasenplatz springt der Ball flacher ab und bleibt schneller, auf einem Sandplatz dagegen höher und langsamer, was jeweils unterschiedliche Anforderungen an den Spielstil stellt. Über die Seitenwahl und das Recht des ersten Aufschlagsspiels entscheidet vor Beginn des Spiels das Los. Zu Beginn eines Ballwechsels steht derAufschlag. Beim Aufschlag muss der Ball in das diagonal gegenüberliegende kleinere Feld der gegnerischen Spielfeldhälfte, das Aufschlagfeld, gespielt werden. Sofern dies nicht beim ersten Versuch gelingt, hat der aufschlagende Spieler einen zweiten Versuch. Misslingt auch dieser, so erhält der Gegner einenPunkt; man spricht in diesem Fall von einemDoppelfehler. Falls der Ball beim Aufschlag das Netz berührt und danach im Aufschlagfeld aufkommt,Letgenannt, wird der Aufschlag wiederholt. Die ITF-Regeln lassen alternativ das Spiel mit der No-Let-Regel zu, die gelegentlich, etwa beimFast4 Tenniseingesetzt wird. Bei einem solchen Let (auch Netzaufschlag) wird dann das Spiel fortgesetzt und der Aufschlag nicht wiederholt. Der Veranstalter kann festlegen, dass im Doppel mit kurzen Sätzen in Kombination mit der No-Ad-Regelung der Ball, der das Netz beim Aufschlag berührt hat und im richtigen Feld landet, von beiden Rückspielern zurückgespielt werden darf. Die No-Ad-Regelung bedeutet, dass beim Spielstand „Einstand“ (40:40) der nachfolgende Ballwechsel über das Spiel entscheidet. Dabei darf der Rückschläger entscheiden, ob der Aufschläger von rechts oder von links aufschlagen muss.[15] Der Aufschläger darf das Tennisfeld beziehungsweise die Grundlinie erst berühren, nachdem der Ball Kontakt mit dem Schläger hatte, sonst begeht er einenFußfehler. Der Aufschlag muss aus dem Stand erfolgen. Ein Hochspringen aus dem Stand ist erlaubt, ein Anlauf, wie etwa beimVolleyball, ist jedoch unzulässig. Nach dem Aufschlag besteht das Ziel des Tennisspiels darin, den vom Gegner in die eigene Spielfeldhälfte gespieltenTennisballimmer wieder regelgerecht über das Netz in dessen Spielfeldhälfte zurückzuspielen. Regelgerecht bedeutet dabei, dass Derjenige Spieler, der den Ball zuletzt regelkonform spielt, erhält einenPunkt. Deshalb wird auch der Ballwechsel selbst als Punkt bezeichnet. Verliert ein Spieler versehentlich einen Ball während eines Ballwechsels (den er normalerweise in der Hosentasche deponiert hat), ist der Spielzug zu wiederholen. Wegen der Verletzungsgefahr darf kein Ball auf dem Platz liegen. Geschieht dies ein zweites Mal während eines Matches, verliert der aufschlagende Spieler den Punkt. Verliert ein Balljunge oder ein Ballmädchen den Ball während eines Ballwechsels, ist der Spielzug ebenfalls zu wiederholen. EinSpielbesteht aus mehreren Ballwechseln, bei denen die Spieler Punkte erzielen müssen, wobei der erste Gewinn eines Ballwechsels eines Spielers als15, der zweite als30und der dritte als40gezählt wird (siehe AbschnittGeschichte der Zählweise). Ein vierter Gewinn eines Ballwechsels durch einen Spieler entscheidet das Spiel für ihn, falls er dann einen Vorsprung von mindestens zwei gewonnenen Ballwechseln hat, also nach den Spielständen 40:0, 40:15 und 40:30 (bzw. umgekehrt). Bei einem Spielstand von 40:40 spricht man vomEinstand(englischdeuce,Aussprache[djuːs]). Der darauf folgende gewonnene Ballwechsel wird alsVorteilbezeichnet. Gewinnt der Spieler, der den Vorteil erzielt hat, auch den nächsten Ballwechsel, dann geht das Spiel an ihn. Gelingt es aber seinem Gegner, den Ballwechsel für sich zu entscheiden, so ist der Spielstand erneut „Einstand“. Es wird dann so lange gespielt, bis ein Spieler, der „Vorteil hat“, auch den darauf folgenden Ballwechsels gewinnt, d. h., nach einem Einstand muss ein Spieler zwei gewonnene Ballwechsel unmittelbar hintereinander erzielen. Um eine Tennisbegegnung (auchMatchoderPartiegenannt) zu gewinnen, ist es nötig, eine vorher festgelegte Anzahl vonSätzenfür sich zu entscheiden. Meist wird auf zwei Gewinnsätze gespielt, im Herrentennis bei großen Turnieren (Grand Slam,Davis Cupsowie im Finale der Olympischen Sommerspiele bis2016)[16][17]auch auf drei. Ein Satz unterteilt sich in einzelne Spiele. Ein Spieler gewinnt einen Satz, wenn er 6 Spiele gewonnen und einen Vorsprung von mindestens zwei gewonnenen Spielen hat, z. B. bei Spielständen 6:4 oder 7:5. Haben beide Spieler 6 Spiele gewonnen, wird meistens einTie-Breakgespielt, der den Satz entscheidet. Während im Fußball oder anderen zeitbegrenzten Sportarten ein Spielstand über die Zeit gerettet werden kann, ist dies im Tennis nicht möglich. Ein Match geht so lang, bis der letzte Punkt gespielt ist. Das Recht auf den Aufschlag steht innerhalb eines Spieles nur einem Spieler zu; es wechselt im folgenden Spiel. Die Spielfeldseite, von der der Aufschlag auszuführen ist, wechselt nach jedem Ballwechsel, wobei der erste Aufschlag von der aus Sicht des Aufschlägers rechten Seite ausgeführt werden muss. Man bezeichnet die rechte Seite der Spielfeldhälfte auch alsEinstandseite, die linke alsVorteilseite. Die Spieler wechseln die Seite des Spielfelds, wenn die Summe der im Satz gespielten Spiele ungerade ist. Im Tie-Break werden alle sechs Punkte die Seiten gewechselt. Das verlorene Spiel des Aufschlagenden wirdBreakgenannt; der Aufschlagvorteil wurde „durchbrochen“. Ein Break stellt wegen der Bedeutung des Aufschlags einen besonderen Vorteil dar. Gewinnt der Spieler, der das Break hinnehmen musste, das folgende Aufschlagspiel seines Gegners, so wird dies alsRebreakbezeichnet. Entscheidende Punkte werden alsBreakball(falls der Gewinn zu einem Break führt),SatzballoderMatchballbezeichnet. Ein zu Null gewonnener Satz wird englisch alsBagelbezeichnet, was auf die Form dieses Gebäcks verweist. Im Tennisjargon wird ein zu Null verlorener Satz auch als „Höchststrafe“ bezeichnet. Ein ohne Verlustpunkt gespielter Satz (6:0, jeweils nach Spielgewinnen zu 0, demnach 24 in Folge gewonnene Punkte) wird alsGolden Setbezeichnet. Wer ein Match 0:6 0:6 verliert, hat im Tennis-Jargon eineBrilleverpasst bekommen, oderenglischDouble Bagel, drei solche Sätze sind ein Triple Bagel. Ein Satz der 6:1 endet, wirdenglischBreadstickgenannt, wobei die „Eins“ an die gerade Form einesGrissinoerinnern soll (in der italienischen Küche als eine knapp fingerdicke, mürbe Brotstange aus Hefeteig bekannt). Breadstick (wörtlich: Brotstange) ist die englische Bezeichnung für ein Grissino. Im Englischen wird die „Eins“ ohne Aufwärtstrich, also „I“ statt „1“ geschrieben. Konform zu den ITF Regeln: Außerhalb der ITF Regeln: Zur Geschichte der Zählweise gibt es zwei Erklärungen. Meist wird vermutet, dass die Zählweise auf Geldeinsätze und Spielwetten im 14. Jahrhundert in Frankreich zurückgeht. So setzte man zum Beispiel einengrosdenier, der wiederum einen Wert von 15denierhatte. In einem Satz, der damals oft aus vier Spielen bestand, wurden also 4-mal 15 „deniers“ gesetzt: 15 – 30 – 45 – 60. Im 16. Jahrhundert wurde der kürzeren Aussprache wegen „45“ durch „40“ ersetzt. Eine andere Erklärung bezieht sich auf die Linien auf dem Spielfeld. Jedes Mal, wenn ein Spieler beimjeu de paumeeinen Punkt machte, bewegte er sich einen Streifen weiter und kam so allmählich der Mitte des Feldes näher. Das Spiel begann an der 0-Zoll-Linie. Gewann ein Spieler einen Punkt, rückte er zur 15-Zoll-Linie vor, dann zur 30-Zoll-Linie und schließlich zur 45-Zoll-Linie. Dann erst hatte er das Spiel gewonnen. Da man fand, dass diese Linie dem Netz zu nahe war, wurde die letzte Angabe auf eine 40-Zoll-Linie zurückversetzt. ImEnglischenwird der Spielstand „0“ mit dem Wortlove(Liebe,Aussprache[lʌv]) bezeichnet. Ein Spiel, bei dem der Gegner null Punkte erzielt, heißt daher auchlove game. Der Ausdrucklovefür „keine Punkte“ ist ab 1742 nachweisbar.[20]Er ergab sich aus der bereits in den 1670er Jahren gebräuchlichen Redewendungplaying for love(für Liebe spielen, d. h. nicht für Geld spielen).[20]Laut Duden wurde er aus der Redewendungto be love(umsonst sein) gekürzt.[21]Eine andere Erklärung basiert darauf, dass das Wortloveaus dem französischenl´œuf(das Ei,Aussprache[lœf]) abgeleitet wurde, weil das Ei der „0“ (Null) ähnelt.[22] Im professionellen und auch höherrangigen Tennis ist ein Stuhlschiedsrichter üblich, der auf einer Seite des Spielfelds auf einem Hochstuhl sitzt. Er wird von mehreren Linienrichtern unterstützt, die ein „Aus“ des Balls durch einen lauten Ausruf und einen seitlich ausgestreckten Arm anzeigen. Falls der Ball knapp innerhalb des Feldes aufkommt, so zeigt der Linienrichter dies an, indem er mit langgestreckten Armen den Buchstaben „V“ auf Kniehöhe bildet. Der Schiedsrichter hat allerdings die alleinige Entscheidungsgewalt und kann die Entscheidung eines Linienrichters überstimmen (englischOverrule). Früher wurden darüber hinausNetzrichtereingesetzt, die an beiden Enden des Netzes saßen und den Schiedsrichter auf eine Netzberührung des Balls beim Aufschlag hinwiesen. Sie hielten hierfür ein Ohr an die Netzkante. Durch die Einführung technischer Hilfsmittel werden Netzrichter heute nicht mehr benötigt. Es gibt darüber hinaus einenOberschiedsrichter, der sich nicht auf dem Platz befindet. Er kann von einem Spieler gerufen werden, wenn dieser der Meinung ist, dass eine Entscheidung des Schiedsrichters die Tennisregeln verletzt. Der Oberschiedsrichter darf nicht dieTatsachenentscheidungdes Schiedsrichters überstimmen (z. B. ob ein Ball „aus“ war oder nicht), sondern nur die sich daraus aus den Tennisregeln ergebende Konsequenz für den weiteren Spielverlauf. Darüber hinaus wird er bei Verstößen gegen den Verhaltenskodex (Code Violation, s. u.) vom Schiedsrichter zu Rate gezogen, insbesondere, wenn diese eine Disqualifikation eines Spielers zur Folge haben. Auf Vereinsebene wird meist ohne Stuhlschiedsrichter gespielt. Hier entscheidet jeder Spieler auf seiner Seite, was einFair Playvoraussetzt.[23] Ein Schiedsrichter kann einen Spieler bestrafen, falls dieser gegen den Verhaltenskodex[24](engl.:Code of Conduct) verstößt. Dieser verbietet unter anderem Beim ersten Verstoß (engl.:Code Violation) spricht der Schiedsrichter eine Verwarnung aus, beim nächsten erfolgt ein Punktverlust. Beim dritten Vergehen erfolgt ein Spielverlust. Ab dem vierten Verstoß kann der Schiedsrichter den Spieler disqualifizieren (Matchverlust) oder erneut einen Spielverlust verhängen. In besonders schweren Fällen kann der Schiedsrichter den Spieler auch bereits beim ersten Verstoß disqualifizieren. Darüber hinaus können Bußgelder verhängt werden. Die ATP und die WTA tragen die Verantwortung für die Verhängung von Bußgeldern gegen Spieler, die an ihren Veranstaltungen teilnehmen, aufgrund von Verstößen gegen die Vorschriften in ihren Regelbüchern, und die ITF ist für alle Bußgelder verantwortlich, die für von ihr durchgeführte Veranstaltungen relevant sind wie die ITF World Tennis Tour, den Davis Cup oder den Billie Jean King Cup. Der Grand-Slam-Vorstand ist für die Durchsetzung seines eigenen Verhaltenskodex bei den Grand Slams verantwortlich. Bußgelder werden abhängig vom Ranglistenstatus eines Spielers und damit vom Verdienst verhängt und können je nach Schwere des Vergehens zwischen 1.000 US-Dollar (für Spieler außerhalb der Top 150) und bis zu 20.000 US-Dollar für Spieler in den Top 10 der Welt liegen. Bei schwerwiegenden Verstößen könnte ein Spieler jedoch mit einer Geldstrafe von bis zu 250.000 US-Dollar oder in der Höhe des Preisgeldes bestraft werden, das er beim Turnier gewonnen hat.[25] Zeitregeln sollen einen kontinuierlichen Spielverlauf sicherstellen. Grundlage sind die Regeln derInternational Tennis Federation(ITF).[26]So sind zwischen den Punken höchstens 25 Sekunden, beim Seitenwechsel höchstens 90 Sekunden und zwischen den Sätzen höchstens 120 Sekunden erlaubt. Coaching, im Sinne der Beratung eines Spielers während eines Matches, war gemäß den ITF-Regeln generell untersagt. Im Oktober 2024 hat die ITF eine Änderung der Regel 30 beschlossen, wonach ab Januar 2025 das Coaching unter bestimmten Bedingungen erlaubt ist.[27]Dabei wird zwischen demOn-Court-Coaching(Beratung auf dem Tennisplatz, etwa der Spielerbank) undOff-Court-Coaching(Beratung von außerhalb des Tennisplatzes, etwa der Spielerbox) unterschieden. Die diesbezüglichen Änderungen lauten:[28] Bis 2024 war das Coaching während eines Matches gemäß ITF-Regeln untersagt, außer bei Mannschaftwettbewerben, während denen der auf der Spielerbank sitzende Mannschaftskapitän coachen durfte. Andernfalls wurde das Coaching mit einer Ermahnung, nachfolgend mit einem Punktverlust und dann bei weiteren Verstößen mit einer Disqualifikation bestraft. Seit 2017 fanden bereits Versuche mit Off-Court-Coaching statt, seit 2023 bei Veranstaltungen aller internationalen Veranstalter, darunter alle vier Grand Slams, die ATP- und WTA-Touren, die ITF World Tennis Tour (WTT) und die ITF UNIQLO Wheelchair Tennis Tour. Es wird erwartet, dass der weltweite Tennisausrüstungsmarkt von 4,16 Milliarden US-Dollar im Jahr 2023 auf 5,85 Milliarden US-Dollar im Jahr 2030 wachsen wird. 38 % davon entfallen auf den US-Markt. Inzwischen dominiert der online-Vertrieb mit 54 % das Marktgeschehen. 29 % des Umsatzes entfallen auf Tennisschuhe, 25 % auf Tennisbekleidung, 18 % auf Tennisschläger und 14 % auf Tennisbälle. Marktführer istWilson, gefolgt vonBabolat,HeadundYonex.[29] Ein Tennisschläger besteht aus einem mit einem Band umwickeltenGriff, demSchaftund demKopf, in den die aus Saiten bestehende Schlagfläche eingespannt ist. Früher wurden Tennisschläger ausschließlich aus Holz gefertigt. DerDunlop Maxplyerrang während 50 Jahren Kultstatus.[30]Nachdem in den 1960er Jahren kurzzeitig Metallrahmen ausAluminiumoderStahlaufkamen, bestehen heutige Rahmen hauptsächlich auskohlenstofffaserverstärktem Kunststoff, der ein geringes Gewicht in Verbindung mit einer hohenSteifigkeitgewährleistet. Ein Tennisschläger wiegt etwa 280 bis 350 Gramm. Nach den Regularien derITFdarf er maximal 29Zoll(73,7 cm) lang und 12,5 Zoll (31,7 cm) breit sein. Bei der Schlägerfläche sind verschiedene Größen (von etwa 625 cm² bis zu 750 cm²) üblich; eine größere Schlägerfläche bietet einen größeren optimalen Treffpunkt (sweet spot) für den Ball und erlaubt eine weniger kraftintensive Spielweise, während eine kleinere Fläche die Ballkontrolle verbessert. Es gibt eine große Vielfalt an Tennissaiten, die sich in ihren Spieleigenschaften unterscheiden. Tennissaiten bestehen ausNylonoderPolyester, die eine längere Lebensdauer als die im modernen Profitennis nur noch selten verwendeten reine Naturdarmsaiten aufweisen, sowie Multifilament-Saiten. Daneben gibt es die Hybridbespannung, eine Kombination aus zwei unterschiedlichen Saiten, von denen die Multifilamentsaite für die Längs- und die Naturdarmsaite für die Querbespannung verwendet wird, oft mit differierenden Bespannungshärten. Die Besaitungshärte lässt innerhalb von zwei Wochen um 2 kp nach. Wichtige Eigenschaften sind die Elastizität für die Ballbeschleunigung, die Vibrationsdämpfung für den Spielkomfort, die Haftung zur Kontrolle des Dralls (Effet, Topspin) und eine lange Haltbarkeit. Die Bespannungshärte spielt eine entscheidende Rolle für das Spielgefühl und die Kontrolle des Balls. Im Profitennis wollen manche Spieler, dass ihre Schläger am Abend vor dem Match besaitet werden. Andere wollen, dass dies am Tag des Matches erfolgt. Über Nacht kann die Besaitungshärte bereits um 0,75 kp nachlassen. Manche Spieler setzen je nach Empfinden Schläger mit unterschiedlichen Bespannungshärten ein. Manche wechseln den Schläger, sobald mit neuen Bällen gespielt wird. Tennisbälle bestehen in der Regel aus einer mit Überdruck gefüllten Gummiblase, über die ein Filzüberzug gespannt ist. Sobald der Überdruck nach einer gewissen Zeit aus dem Ball entwichen ist, lässt die Sprungeigenschaft stark nach und der Ball muss ausgetauscht werden. Daneben existieren drucklose Bälle, deren Sprungeigenschaft auf der Verwendung verschiedener Gummischichten basiert. Bei offiziellen Turnieren wird fast ausschließlich mit Druckbällen gespielt. InBallmaschinenwerden meist drucklose Bälle verwendet. Während früher Tennisbälle in schwarz oder weiß üblich waren, setzten sich mit dem Aufkommen der Fernsehübertragungen von Tennisspielen in den 1970er Jahren hellgelbe Bälle durch, die auf Farbfernsehern besser zu erkennen sind. Bei denWimbledon Championshipswurde bis 1986 ausschließlich mit weißen Bällen gespielt. Wimbledon hält auch an der Tradition fest, dass seit 1902 nur mit Bällen vonSlazengergespielt wird. Nach den offiziellen Regeln muss ein Tennisball zwischen 56,7 g und 58,5 g schwer sein und einen Durchmesser von 6,54 cm bis 6,86 cm besitzen. Jedes Jahr werden etwa 325 Millionen Tennisbälle produziert, wodurch etwa 20.000 Tonnen Abfall in Form von Gummi entstehen, der nicht leicht biologisch abbaubar ist. Im Profitennis werden in der Regel während eines Matches die Bälle nach sieben Spielen und nachfolgend alle neun Spiele gegen neue ausgetauscht. Vor und während einem Tie-Break werden jedoch die Bälle nicht gewechselt. Wenn der Ball keine Luft mehr hat und sich so sehr eindrücken lässt, dass sich die Innenseiten des Balles berühren, wird der Punkt wiederholt. Ist der Ball hingegen nur weich geworden, so bleibt der gespielte Punkt bestehen. Ausgetauscht wird der Ball in beiden Fällen. Auf der ATP-Tour muss bis zwei absolvierten Spielen ein neuer Ball als Ersatz verwendet werden. Andernfalls muss ein Ball mit gleichem Verschleiß bereitgestellt werden.[31] Bei der Bekleidung schreibt die ITF im § 24 für Wettkämpfe vor, wo und in welcher Größe Werbung, insbesondere des Herstellers, angebracht werden darf. Sie darf an zwei Stellen je nach Platzierung 13 cm², 26 cm² beziehungsweise 39 cm² groß sein. Bei regelwidriger Bekleidung kann der Spieler beziehungsweise Spielerin zum Kleiderwechsel aufgefordert werden. Wird dem nicht nachgekommen, folgt eine Disqualifikation.[26]Auf der Kleidung oder Ausrüstung der Spieler dürfen keine Kennzeichen angebracht sein, die für Wettbüros, Tabak- oder E-Zigarettenprodukte, Spirituosen, politische Aktivitäten oder andere Kategorien werben/diese zeigen, die als schädlich für den Tennissport, die ITF oder die ITF World Tennis Tour erachtet werden. BeimTennisschuhsollen Sohle, Ober- und Innenmaterial, Stabilität und Dämpfung aufeinander abgestimmt sein. Sie werden für die spezifischen Bodenbeläge angefertigt, ob auf Sandplatz, Hartplatz, Rasenplatz oder Teppichplatz gespielt wird, und berücksichtigen den individuellen Spielstil. In Wimbledon gelten zusätzlich besondere Regeln, wonach die Spielbekleidung zu mindestens 90 % weiß zu sein hat (selbst „gebrochenes weiß“ oder cremefarben ist nicht zulässig). Wimbledon hält damit an der Tradition des „weißen Sports“ fest. Ein bis zu zehn Millimeter breiter farbiger Rand ist gestattet. Sogar die Schuhsohlen der Tennisschuhe haben weiß zu sein. Erst seit 2023 ist es Tennisspielerinnen erlaubt, auch dunkle Unterwäsche zu tragen. Darauf haben Tennisspielerinnen wegen ihrer möglichenMenstruationbestanden. Sie darf jedoch nicht über den Tennisrock hinausragen.[32]Die Kleiderregeln erstrecken sich auch auf Stuhlschiedsrichter, die ein gestreiftes Hemd mit weißem Kragen, beige Hosen für Männer und einen langen beigen Rock für Frauen sowie ein dunkles Jacket tragen müssen. Tennis ist ein Laufsport. Jedoch werden verschiedene Lauftechniken benötigt, die sich von anderen Sportarten unterscheiden. Früher dominierten sogenannte Sidesteps, bei denen der Oberkörper meist parallel zum Netz steht und man seitlich läuft. Im modernen Tennis sind inzwischen kurze Sprints vorherrschend, sowohl nach links, als auch nach rechts, als auch nach vorne, um beispielsweise einen kurzen Ball oder Stoppball zu erlaufen. Sie bestehen aus sechs bis acht Schritten. Wichtig ist auch das Rückwärtslaufen, um etwa einen Lob durch einen Smash spielen zu können. Immer häufiger wird im Tennis eine beidhändige Rückhand gespielt. Sie erfordert jedoch jeweils einen Schritt mehr, um den Ball zu erreichen. In dem Augenblick, in dem der Spieler das Ziel des Schlages zu erkennen glaubt, wendet er denSplitstepan, damit er in eine Bereitschaftsstellung springt, aus der er „explosiv“\" in alle Richtungen starten kann. Er sorgt dafür, dass die für einen schnellen Antritt verantwortliche Muskulatur im entscheidenden Moment optimal vorgedehnt und damit gespannt ist. Grundsätzlich versucht man als Spieler nach dem Schlagen eines Balles möglichst schnell vom Scheitelpunkt gesehen eineWinkelhalbierendemöglicher Rückschläge zu erreichen, also nicht unbedingt zur Mitte des Spielfeldes zurückzulaufen. Vom Scheitelpunkt hat man die kürzeste Strecke, um den kommenden Ball des rückschlagenden Gegners zu erreichen. Ein Lauftraining wird diesen Anforderungen angepasst, indem es aus kurzen Sprints besteht und einen schnellen Antritt erfordert. Ebenso gehört eine entsprechende Technik zum Abbremsen des Laufs dazu, was insbesondere auf dem Sandplatz durch ein Rutschen bewirkt wird. Während eines Matches kann die Gesamtlaufstrecke sechs bis acht Kilometer betragen. Der Treffpunkt ist der wichtigste Teil des gesamten Schlages und entscheidet über Erfolg oder Misserfolg jedes Schlages. Der optimale Treffpunkt hängt von vielen vorangegangenen Aktionen, Bewegungsabläufen und vom Schlägergriff ab. Die Kontaktzeit zwischen Ball und Schläger beträgt durchschnittlich nur vier Millisekunden. Die erste Entscheidung betrifft die Schlägerhaltung, ob man einenKontinentalgriff,WesterngriffoderEasterngriff, oder Mischvarianten bevorzugt. Wichtig ist die eigene optimale Entfernung zum Treffpunkt. Es geht darum, diese bei den meisten Schlägen einzuhalten. Nur so kann man problemlos durchschwingen. Hierfür sind vor allem ein optimales Gleichgewicht und unter anderem die Oberkörperhaltung verantwortlich. Hierzu gehört die richtige Antizipation des Ballfluges, die Erahnung des Absprungpunktes und eine gute Beinarbeit. Eine erhöhte Schwierigkeit besteht darin, den Ball nach dem Aufsprung noch im Ansteigen zu schlagen. Hierfür muss das Timing der Schlagbewegung stimmen. Es gilt eine Stabilität während des Schlages zu erreichen. Bei der Schwungbewegung ist darauf zu achten, dass sich die Beschleunigung in Richtung Schlägerspitze stetig vergrößert. Wenn man den Schläger sauber schwingen kann, dann ist die Entfernung zum Treffpunkt optimal. Das Handgelenk muss fest sein, weil man gegenüber dem ankommenden Ball im Treffmoment einen Schlägerwiderstand leisten muss.[33]Das Handgelenk spielt beim Tennis eine besonders wichtige Rolle. Für kraftvolle und präzise Schläge ist eine gute Kombination aus Griffkraft und Geschmeidigkeit nötig. Bei der Vorhand (engl.forehand) wird der Ball auf der Seite der Schlaghand (bei einem Rechtshänder also rechts, bei einem Linkshänder links) gespielt. In der Regel wird der Vorhandschlag mit einer Ausholbewegung eingeleitet und der Ball idealerweise etwa hüfthoch seitlich vor dem Körper getroffen. Früher war auch der sogenannteWesterngriffverbreitet, bei dem der Ball weit vor dem Körper getroffen wird. In der Regel wird die Vorhand einhändig gespielt und auch so gelehrt. Zu den wenigen Spielern, welche die Vorhand beidhändig spielen, gehörtenMonica SelesundFabrice Santoro. VonCarlos Moyáheißt es, er spielte die beste Vorhand im Herrentennis. Später trainierte er den Spieler mit der „zweitbesten“ Vorhand:Rafael Nadal. Bei der Rückhand (engl.backhand) wird der Ball auf der der Schlaghand gegenüberliegenden Seite (bei einem Rechtshänder also links, Linkshänder rechts) geschlagen. Ein Rückhandschlag kann sowohl einhändig als auch beidhändig ausgeführt werden. Die beidhändige Rückhand ist erst in den 1970er Jahren bekannt geworden. Anfängern wird diese häufig empfohlen, da die Durchführung einfacher zu erlernen ist als die einhändige Rückhand. Mittlerweile ist die beidhändige Rückhand auch unter Profispielern vorherrschend. Ausnahme ist der Rückhand-Slice, der meist einhändig geschlagen wird. Die beste zweihändige Rückhand sollNovak Đokovićspielen, die beste einhändige RückhandStan Wawrinka.[34] Der Aufschlag (engl.Service) leitet den Ballwechsel ein. Aus einer Position hinter der Grundlinie muss der Ball in das gegenüberliegende Aufschlagfeld geschlagen werden. Die Linien gehören zum Aufschlagfeld. Ein Überschreiten oder die Berührung der Grundlinie vor dem Treffen des Balls ist regelwidrig. Der Aufschlag beginnt mit einer Konzentrationsphase. Normalerweise lässt man den Tennisball ein paar Mal aufspringen, um ihn dann für den Aufschlag einzusetzen. Jeder Aufschläger hat zwei Versuche; nach erfolglosem zweiten Versuch gehört der Punkt dem Gegner. Nach einem Spiel wechselt das Aufschlagrecht. Der Aufschlag kann, wie bei einem Grundlinienschlag, je nach taktischer Auslegung mit unterschiedlicher Rotation gespielt werden. Man unterscheidet dabei zwischen einem flachen und schnellen Aufschlag mit nur geringem Vorwärtsdrall, einem Topspin- und einem Sliceaufschlag. Den günstigsten Schlagwinkel und dadurch höchste Ballgeschwindigkeiten erreicht man, wenn der Ball über Kopfhöhe geworfen und am höchstmöglichen Punkt getroffen wird. Im modernen Herrentennis werden mit dem ersten Aufschlag oft Geschwindigkeiten über 200 km/h erreicht. Aufschlaggeschwindigkeiten beim Tennis werden mittelsRadargerätengemessen. Der Weltrekord wird vom AustralierSam Grothgehalten und beträgt 263 km/h.John Isnerschlug 14.470 Asse in insgesamt 772 Matches und hält damit den Rekord. Er profitierte dabei auch von seiner Körpergröße von 2,08 Meter. Während des Schlages kann der Spieler die Eigenrotation des Balles beeinflussen, die wiederum die Flugkurve und das Absprungverhalten des Balles bestimmt. Man unterscheidet hierbei Drive (kaum Eigenrotation), Topspin (Rotation in Flugrichtung) und Slice (Rotation entgegen der Flugrichtung). Im Tennis sind verschiedene Varianten desEffetsüblich. Dabei wird der Ball angeschnitten und damit in eine Rotationsbewegung versetzt. Hier bewirkt die Reibungskraft zwischen Luft und Balloberfläche einen Luftdruckunterschied, der zur Bahnablenkung führt (Magnus-Effekt). Neben diesem Einfluss auf die Flugbahn wird der Absprungwinkel des Balls nach dem Aufprall auf dem Boden geändert. Letztes ist besonders beim Stoppball relevant. Ist der Effet im Balltreffpunkt noch erhalten, beeinflusst er auch die nachfolgende Flugbahn des Balles beim Rückschlag. Ein Drive (dt.: Treibschlag, Treibball) ist ein Grundschlag mit nur geringer Eigenrotation des Balles. Der Schlägerkopf trifft dabei den Ball in einem Winkel von ungefähr 90 Grad. Dieser bei Anfängern und Hobbyspielern übliche Schlag ist der intuitivste und kraftsparendste. Er hat durch seine flache Flugkurve allerdings den Nachteil, dass kraftvollere Schläge von der Grundlinie aus oft nicht mehr im gegnerischen Feld aufkommen und aus gehen. Ein Treibball ist für den Gegner am wenigsten problematisch, weil hier das Prinzip Einfallswinkel gleich Ausfallswinkel gilt, und der Gegner den Treffpunkt besser berechnen kann. Ein Vorteil des Treibballs ist seine maximale Fluggeschwindigkeit, die den Gegner zu extrem schnellen Reaktionen und Laufbewegungen zwingt. Der Topspin ist inzwischen die häufigste Schlagvariante im modernen Tennis. Topspin beschreibt eine Vorwärtsrotation des Balles, die durch ein Überstreichen mit der Schlägerfläche auf der Oberseite („Top“) des Balles erzeugt wird. Die Schlägerfläche ist also im Treffpunkt leicht nach unten geneigt (weniger als 90 Grad). Der Schläger beschreibt eine Schleife. Dies bewirkt eine stärkere, vertikale Krümmung der Flugkurve, welche die Flugdauer des Balles verlängert, aber die Flugweite verkürzt. Zusätzlich haben Topspinbälle ein verändertes Absprungverhalten. Der Ball springt im Verhältnis zur Flugkurve höher und zusätzlich schneller ab, was ihn unberechenbarer als einen Drive-Schlag macht. Lang gespielte Topspinbälle zwingen den Gegner dazu, entweder weiter hinter die Grundlinie zurückzuweichen oder alternativ den Ball weiter vorne in größerer Höhe über dem Boden zu treffen, was oft schwierig ist. Eine Spielweise mit Topspin erfordert neben guter Technik auch deutlich mehr Kraft. Mit dem Handgelenk kann der Spin erhöht werden. Die Gefahr, den Ball durch die nach vorne geneigte Schlägerfläche mit dem Schlägerahmen zu treffen, ist deutlich höher als beim Drive. Auf Sand, wo der Ball beim Absprung langsamer wird, ist es noch wichtiger, den Topspin zu maximieren. Der zusätzliche Topspin kompensiert den Tempoverlust und sorgt dafür, dass die Bälle tief und hoch abprallen. Liegt der Tennisplatz in größerer Höhe über Meeresspiegel, ist die Luft weniger dicht. Dies reduziert den Magnus-Effekt. Um dies zu kompensieren, muss man noch mehr Spin erzeugen, um die gleiche Flugbahn zu erreichen, wie auf Meereshöhe. Beim Slice erfährt der Ball eine Rückwärtsrotation (engl.backspin). Dadurch kann der Ball sehr flach über das Netz fliegen und springt beim Auftreffen auf den Boden kaum mehr ab. Er kann einerseits als Vorbereitungsschlag für einen Netzangriff dienen, da er dem Spieler, bedingt durch den längeren Flug des Balles, mehr Zeit verschafft ans Netz vorzurücken. Andererseits kann der Slice dazu verwendet werden, sich aus einer Defensivsituation heraus Zeit zu verschaffen, etwa bei der Annahme eines harten Aufschlags. Zudem kann er für eine Variation des Spieltempos benutzt werden, um den Gegner aus dem Rhythmus zu bringen. Ein Slice kann meist nur langsam gespielt werden, da er durch die flache Flugkurve ansonsten ins Aus „segelt“ und wird bei einem Rückhand-Slice, genau wie bei einem Vorhand-Slice, in der Regel mit nur einer Hand ausgeführt. DerAufschlagkann auf die Rückhand oder die Vorhand des Gegners gespielt werden. Wird er weit nach außen gespielt, kann man dadurch das Spielfeld öffnen, um mit dem Folgeschlag ins freie Feld den Punkt zu erzielen. Ein Aufschlag auf den Körper des Gegners erschwert diesem den Return, denn er muss dem Ball unter Zeitnot ausweichen und kann ihn meist nur blocken. Im Regelfall wird der Aufschlag jeweils rechts oder links von der gleichen Position ausgeführt. Als seltene Variante kann der Aufschläger die Position zwischen der Mittellinie und Außenlinie beliebig einnehmen, wodurch der Winkel der Flugbahn des Balles verändert wird. Ebenfalls selten wird ein kurzer Aufschlag von unten durchgeführt, um den Gegner zu überraschen, wenn dieser weit hinter der Grundlinie steht. Return(deutsche BezeichnungRückschlag) ist die Bezeichnung des Schlags, mit dem der Ball nach dem gegnerischen Aufschlag zurückgespielt – retourniert – wird. Neben dem Aufschlag, dem härtesten Schlag im Tennis, ist der Return von größter Bedeutung. Beide Schlagarten sind „Eröffnungsschläge“, mit denen der Ballwechsel eingeleitet wird. Ein schwacher Return erlaubt es dem Gegner, bereits mit dem Aufschlag Druck aufzubauen und den Ballwechsel zu bestimmen. Meist ist der Returnierende beim ersten Aufschlag nur in der Lage, den Schläger passiv, mit einer kurzen Ausholbewegung, hinzuhalten (zu „blocken“). Er versucht hierbei den Druck des gegnerischen Aufschlags mitzunehmen und dem Rückschlag die gewünschte Richtung zu geben. Der – in der Regel mit weniger Härte geschlagene – zweite Aufschlag kann dann vom retournierenden Spieler angegriffen werden. Der Schmetterball (englischSmash) ähnelt von der Bewegungsausführung der Aufschlagbewegung. Es handelt sich um einen Überkopfschlag mit hoher Geschwindigkeit. Er wird meist als Reaktion auf einenLob-Versuch des Gegners gespielt und gilt als der kraftvollste Angriffsschlag. Besonders schwierig gestaltet sich ein Rückhand-Smash, bei dem der Spieler meist mit dem Rücken zum Gegner steht. Als Angriffsball bezeichnet man einen Schlag, der den Gegner in die Defensive zwingt, um anschließend den Punkt zu gewinnen. Überwiegend wird der Angriffsball aus dem Halbfeld als Antwort auf einen kürzeren Ball des Gegners gespielt, um anschließend an das Netz vorzurücken und mit einem Volley den Spielzug zu gewinnen. Im Profitennis werden Angriffsbälle auch von der Grundlinie aus gespielt, indem der Ball mit hoher Präzision entweder in die Ecken des gegnerischen Spielfelds nahe an die Linien oder als Topspin kurz cross gespielt werden, um das Spielfeld zu öffnen und mit dem nachfolgenden Schlag zur anderen Seite den Punkt für sich zu entscheiden. Als Passierschlag, auch Passierball genannt (Vorhand oder Rückhand), bezeichnet man einen Schlag, mit dem der Ball am in der Nähe des Netzes postierten Gegner für diesen unerreichbar seitlich vorbei gespielt wird. Beim Lob versucht der Spieler, den an das Netz vorgedrungenen Gegner mittels eines hoch geschlagenen Balls zu überwinden. Ist der Schlag zu flach oder zu kurz geschlagen, kann der Gegner mit einem Schmetterschlag antworten. Gelegentlich bleibt – wenn der Lob zu hoch und zu langsam gespielt wird – auch noch genug Zeit zurückzulaufen und den Ball mit Vor- oder Rückhand zu schlagen. Höherklassige Spieler spielen den Lob deshalb mit Topspin. Hierdurch erreicht auch ein hoch als Lob gespielter Ball eine große Fluggeschwindigkeit und ist deshalb bei technisch sauberer Ausführung praktisch nicht mehr zu erlaufen. Der Topspinlob gehört zu den technisch schwierigsten Schlägen und ist deshalb im Amateurtennis kaum zu beobachten. Neben dem Lob kann einMondballgespielt werden, bei dem ein Ball absichtlich sehr hoch über das Netz befördert wird, um Geschwindigkeit aus dem Ballwechsel zu nehmen und dem Spieler einen Neuaufbau zu erlauben. Als spektakuläre Variante wird derTweenerals Notlösung gespielt, falls der Spieler überlobt wird, er deshalb keinen Smash ausführen kann und er den Ball – meist mit dem Rücken zum Netz – zwischen seinen Beinen zurückspielt. BeimSabrwird der zweite Aufschlag des Kontrahenten – der zur Vermeidung eines möglichenDoppelfehlermeist vorsichtiger gespielt wird – mit frühem Laufen ins Feld beantwortet, während der Gegner noch in der Aufschlagbewegung steckt, und möglichst früh retourniert, um dem Gegenspieler damit weniger Zeit für dessen nächsten Schlag zu gewähren. Üblicherweise drängt der Rückschläger nach dem Sabr wie beim Chip and Charge ans Netz vor. Populär wurde dieser Schlag im Jahr 2015 durchRoger Federer, von dem auch die WortschöpfungSabr(Sneak Attack by Roger) stammt. Bei einem Stoppball (engl.drop shot) wird der Ball so gespielt, dass er kurz hinter dem Netz „herunterfällt“. Der Stoppball wird, ähnlich dem Slice, mit Rückwärtsdrall gespielt, wodurch er zum einen nach dem Auftreffen auf dem Boden kaum noch abspringt, und zum anderen nicht mehr vorwärts, sondern seitwärts oder möglichst sogar rückwärts springt. Ein Stoppball wird meistens dann benutzt, wenn sich der Gegner besonders weit hinter der Grundlinie befindet. Er kann aufgrund seiner überraschenden Wirkung sehr effektiv sein. Je härter der ankommende Ball geschlagen wurde, umso schwieriger ist es, die Ballgeschwindigkeit für einen wirkungsvollen Stoppball zu reduzieren. Volley (deutsche Bezeichnung:Flugball) bezeichnet einen Schlag, bei dem der Ball noch vor dessen Auftreffen auf dem Boden zurückgespielt wird. Üblicherweise wird dieser Schlag nahe dem Netz gespielt, sodass sich der Spieler schnell – meist durch einenSplitstep– in Position bringen muss. Entscheidend ist, dass der Volley ohne Ausholbewegung vor dem Körper getroffen wird. Ein Ball, der auf den Körper gespielt wird, wird grundsätzlich als Rückhandvolley gespielt. Es gibt verschiedene Sonderformen. DerDrivevolleyist ein Volleyschlag, bei dem man den Ball nicht ins Feld „drückt“, sondern – wie bei der Vorhand – mit hoher Geschwindigkeit und Spin durchzieht. Populär machten diesen Schlag vor allemAndre Agassiund die Williams-SchwesternVenusundSerena. EinVolleystopist eine Schlagtechnik, bei der aus einem gegnerischen Schlag ein Stoppball wird. Die Schwierigkeit des Schlages besteht darin, dem Ball die Geschwindigkeit zu nehmen und ihn kurz hinter dem Netz zu platzieren. DerHechtvolleyist eine spezielle Sonderform des Volley, bei der versucht wird, einen Passierschlag des Gegners noch mit Hilfe eines Sprungs zur Seite zu erreichen und so den Ball noch ins Feld des Gegners zu bringen. Dieser Schlag wurde durchBoris Beckerbekannt. Ursprünglich hatte ein Jugendtrainer Beckers diesen Schlag entwickelt, um die Reichweite von Nachwuchsspielern am Netz zu erhöhen. Becker behielt aber diesen Schlag in seinemRepertoireund setzte ihn zur Verblüffung seiner Gegner und der Zuschauer auch bei internationalen Turnieren (z. B. 1985 inWimbledon) erfolgreich ein. Naturgemäß landet man nach einem Hechtvolley unsanft auf dem Boden. Um Handverletzungen zu vermeiden, muss der Schläger vor dem Aufprall losgelassen werden. AlsHalbvolleyoderHalbflugballwird ein Schlag bezeichnet, bei dem der Ball kurz nach dem Aufspringen getroffen wird, unabhängig davon, ob er an der Grundlinie oder im Halbfeld geschlagen wird. Dies kommt häufiger bei einem versuchten Netzangriff vor, bei dem der Gegner einen solchen Verteidigungsschlag einsetzt und dem Angreifer den Ball gewissermaßen „vor die Füße“ spielt. Auch im Doppel kommt es häufig zu dieser Spielsituation, da die Spieler im Feld stehen. Oft wird mit dem Halbvolley ein Stoppball gespielt. Ein cross (deutschquer) geschlagener Ball ist ein Ball, der in die jeweils diagonal gegenüber liegende Ecke des Platzes gespielt wird. Ein Rechtshänder spielt demnach eine cross geschlagene Vorhand einem rechtshändigen Gegner ebenfalls auf die Vorhand (bzw. Rückhand auf Rückhand). Da die Diagonale des Platzes länger ist als die Seitenlinie, können etwa härtere und damit weiter fliegende Bälle cross geschlagen werden. Eine effektvolle Variante ist der kurz cross geschlagene Ball, wodurch der Gegner weit nach außen gezwungen wird und mit dem das gegnerische Spielfeld geöffnet wird, um den anschließenden Ball ins freie Feld zu spielen. Ein longline (dt.entlang der Linie) geschlagener Ball ist ein Ball, der in die jeweils gegenüber liegende Ecke des Platzes gespielt wird, d. h., der Ball fliegtparallelzur Seitenauslinie. Ein Rechtshänder spielt demnach eine longline geschlagene Vorhand einem ebenfalls rechtshändigen Gegner auf dieRückhand(bzw. Rückhand auf Vorhand). Befindet sich der ausführende Spieler seitlich außerhalb des Feldes, kann ein Longline-Schlag auch regelkonform seitlich am Netzpfosten vorbei gespielt werden. Der longline geschlagene Ball weist zusätzliche Schwierigkeiten auf und erfordert hohe Präzision im Schlag. Bei der Richtungsänderung des Ballwechsels von cross zu longline ist der Ausschlagwinkel im Schläger ein anderer, als der Einschlagwinkel des ankommenden Balls. Die Flugstrecke ist kürzer, als die eines cross geschlagenen Balls. Der longline geschlagene Ball hat wenig Spielraum zur Außenlinie und kann leichter ins „aus“ fliegen. Antwortet der Gegner mit einem kurz cross geschlagenen Ball, hat man eine weite Laufstrecke zu bewältigen. Ein Winner (dt. auchGewinnschlag) ist ein Ball, der so platziert gespielt wurde, dass der Gegner keine Möglichkeit hat, diesen zu retournieren. EinAssist einAufschlagins Feld, der vom Gegner nicht einmal mehr berührt werden kann. Er zählt demnach auch automatisch als ein Winner. Führt der Aufschlag zu einem direkten Punkt, kann aber noch erreicht werden, nennt man diesService Winner. Sowohl Talent als auch Erfahrung spielen eine gravierende Rolle, um ein Ass zu schlagen. Nicht nur die Aufschlagsgeschwindigkeit ist hier von großer Bedeutung, sondern auch das Erkennen, in welche Richtung der Rückschläger sich bewegen könnte. Außerdem spielen hier der Tennisplatz und sein Belag eine immense Rolle. Die optimale Grundlage für ein Ass ist ein Hartplatz. Die schlechteste Option wäre ein Sandplatz, da hier der Ball durch den Sand abgebremst wird und der Gegner dadurch mehr Zeit für seine Reaktion gewinnt. Die höchste Wahrscheinlichkeit ein Ass zu erzielen findet man beim Rasentennis. Eine besondere psychische Herausforderung sind die Entscheidungsbälle im Tennis, die auch Big Points genannt werden. Dazu gehört, alle vorherigen Spielstände ausblenden zu können. Die Frage, ob es Punkte in einer Tennispartie gibt, die in ihrer Wichtigkeit höher einzustufen sind als andere, wird in der Wissenschaft kontrovers diskutiert. Während derDeutsche Tennis Bund(DTB) sowie Richard Schönborn (war über 26 Jahre Cheftrainer des DTB), der Existenz von Big Points widersprechen, weisen verschiedene andere Experten und Wissenschaftler in ihren Arbeiten explizit auf dessen Existenz hin.[35][36] Man umschreibt mit dem Big Point eine Situation, bei der man entweder unter Druck geraten ist oder eine Chance für einen Erfolg hat. Ein Faktor dabei ist das sogenannte „Momentum“, womit der psychologische Vorteil gemeint ist, den ein Spieler für den Spielverlauf gegenüber seinem Gegner erlangt. Den Satz- oder Matchgewinn vor Augen, beeinflusst jedoch häufig der psychische Druck das Können des Spielers. DieMuskelspannungoder üblicherweise automatisiert ablaufendeBewegungsabläufekönnen sich geringfügig ändern, was zu einem Fehler führen kann. Ebenso kann die psychische Anspannung die Spielstrategie beeinflussen. So wechselt man zu einem defensiveren Spiel oder spielt plötzlich aggressiver oder man glaubt einen „besonderen“ Ball spielen zu müssen. Die umkämpften und bedeutenden Punkte können wesentlich für den Ausgang des Spiels sein. Somit ist der Spielball bei 1:0 Satzführung zum 4:1 ein viel wichtigerer Punkt, als beispielsweise der Punkt zum 15:0 im ersten Spiel des zweiten Satzes. Beide Punkte haben gewissermaßen denselben Zählwert, aber der Punkt zum 4:1 entscheidet maßgeblich darüber, wie der weitere Abschnitt des zweiten Satzes taktisch angegangen werden kann. So kommt es immer wieder vor, dass ein Match gedreht wird und der vermeintliche Sieg in einer Niederlage endet.[37][38] AlsUnforced Error(dt.unerzwungener,vermeidbareroderleichter FehleroderFehler ohne Not) wird ein Schlag bezeichnet, mit dem ein Spieler den Punkt auf Grund eines eigenen Fehlers verliert, ohne dass dies durch den vorangegangenen Schlag des Gegners erzwungen wurde. Kann er hingegen den Ball auf Grund eines vorhergegangenen harten, platzierten oder sonst schwer zu spielenden oder erreichenden Schlages des Gegners nicht regelgerecht retournieren, spricht man von einemForced Error(dt.erzwungener Fehler). Die Übergänge zwischen vermeidbarem und erzwungenem Fehler können im Einzelfall fließend sein. Die Einteilung unterliegt damit häufig einem Beurteilungsspielraum des jeweiligen Statistikführenden bzw. Kommentators. AlsDoppelfehler(engl.double fault) wird ein Punktverlust des Aufschlagenden durch zwei regelwidrige Aufschläge bezeichnet. Ein Doppelfehler zählt zugleich als Unforced Error. Im Tennissport gibt es eine ganze Reihe an Möglichkeiten, um eine Karriere, ein Match oder auch nur einen Satz statistisch auszuwerten. Damit kann abgelesen werden, wie effektiv ein Spieler oder eine Spielerin agiert hatte.[39] Erhoben werden folgende Werte bei einer Satz- oder Matchauswertung: Daneben werden globale Statistiken geführt, aus denen sich Rekordlisten ergeben. Im modernen Profitennis wird nach Möglichkeit die Spielweise des kommenden Gegners umfassend analysiert und eine Spielstrategie festgelegt. Dazu gehört etwa die Analyse des Aufschlags, wohin der Gegner bevorzugt aufschlägt, unterschieden nach erstem und zweitem Aufschlag und Spielstand, wie die jeweilige Stärke des Spielers, was Aufschlag, Rückschlag, Vorhand, Rückhand und Volleys angeht. Ist der Gegner sprintstark, laufstark und konditionell auf lange Rallyes vorbereitet. Wie ist seine mentale Stärke bei verschiedenen Spielständen. Wo sind seine (relativen) Stärken und Schwächen. In die Strategie fließen Erfahrungen aus vergangenen Begegnungen ein. Bei der hohen Leistungsdichte im Profitennis, insbesondere in der Weltspitze, kann die Spielstrategie matchentschiedend sein. Hierzu gehört auch die Fähigkeit, auf eine alternative Strategie auszuweichen, falls die geplante Strategie nicht erfolgreich zu sein scheint. Manchmal entscheidet die Tagesform über den Matchausgang. So kann es an mangelnder Konzentration, am Timing, Ermüdung oder Verkrampfungen liegen, dass der Spieler seine gewohnte Leistung nicht abrufen kann. Bekannt sind auch sogenannte „Trainingsweltmeister“, die im Training hervorragend Tennis spielen, jedoch nicht in der Lage sind, ihre Leistungen während eines Matches umzusetzen. Bei der hohen Ballgeschwindigkeit im modernen Tennis ist es wichtig, sich einen Zeitvorteil zu erarbeiten, indem früh erkannt wird, wie und wohin der Tennisball vom Gegner geschlagen wird. Hierzu muss die Körperhaltung, die Schlägerhaltung und die Schlagbewegung möglichst früh erfasst („gelesen“) werden. Manche Spieler haben einen „Angstgegner“, einen Spieler, gegen den sie nur schwer oder gar nicht gewinnen können. Die Leistungserwartung bei einem Angstgegner erwächst sowohl aus der Überschätzung seiner Leistungspotenzen als auch aus einer Unterschätzung der eigenen Erfolgsmöglichkeiten. Solche Fehlhaltungen können ihre Ursachen sowohl in mehreren Niederlagen gegen diese Sportler als auch in einem einmaligen schockierenden Ergebnis haben. In einzelnen Fällen kann sich das Angstgegnerdenken nicht vorrangig auf das komplexe Leistungsvermögen beziehen, sondern es wird sich vor allem auf bestimmte technisch-taktische Besonderheiten und Merkmale des kämpferischen Einsatzes konzentrieren.[40]So gilt etwaTaylor Fritzals Angstgegner vonAlexander Zverev,Roberto Bautista Agutals derjenige vonDominic Thiem,Adrian MannarinovonStan Wawrinka,Daniil MedwedewvonJannik SinneroderTommy PaulvonCarlos Alcaraz. Beim Serve-and-Volley-Spiel folgt der Spieler seinem Aufschlag (Service), versucht möglichst weit zum Netz vorzudringen und den Ballwechsel mit einem Volley abzuschließen. Diese Strategie ist besonders auf schnellen Belägen, insbesondere auf Rasen, erfolgversprechend. Die Position des Angreifers dicht am Netz verkürzt die Reaktionszeit des Gegenspielers; häufig kann der angreifende Spieler bereits mit dem ersten Volley punkten. Für ein erfolgreiches Serve-and-Volley-Spiel sind zwei Grundvarianten des Aufschlagspiels möglich. Zum einen kann der Aufschlag mit großer Härte gespielt werden. Dem Gegner bleibt dann nur geringe Reaktionszeit, er kann den Aufschlag oft nur noch „blocken“, das heißt, er kann den Schläger nur passiv ohne eigene Ausholbewegung hinhalten. Der Ball wird dann nur noch mäßig kontrolliert und ohne Drall zurückgespielt, was dem Aufschläger ermöglicht, den Volley mit hohem Tempo und platziert zurückzuspielen. Nachteil der hohen Aufschlaghärte ist, dass auch dem Aufschläger verhältnismäßig wenig Zeit bleibt, Richtung Netz vorzurücken. Er muss den ersten Volley in der Regel in Höhe der T-Linie im sogenannten Halbfeld schlagen. Ist der Aufschlag gut platziert, so ist dies wegen der oft hohen Flugbahn des Returns unproblematisch. Erahnt der Rückschläger die Richtung des Aufschlags oder ist dieser schlecht platziert, so wird der Aufschläger oft zum Halbvolley gezwungen, da der Returnierende ihm den Ball mit Topspin „auf die Füße“ spielen kann. Typische Vertreter dieser Spielweise sind großgewachsene Spieler, wie es beispielsweiseBoris BeckerundGoran Ivaniševićwaren. Als zweite Variante kann der Aufschlag mit weniger Tempo, aber hoher Genauigkeit und vielSchnittgespielt werden. Der Druck auf den Gegner entfaltet sich dann durch die stärkeren Winkel; der Rückschläger hat größere Probleme, den Ball noch zu returnieren und wird häufig bereits mit dem Aufschlag aus der Platzmitte gedrängt. Wegen der geringeren Aufschlaggeschwindigkeit hat der Aufschläger mehr Zeit für den Weg zum Netz. Hierdurch erreicht er eine bessere Position für den ersten Volley, kann den Ball auch fast immer von oben nach unten spielen und wird nur selten zum Halbvolley gezwungen. Meist wird der Aufschlag mit Vorwärtsdrall (Kick) gespielt. Wegen des hohen Absprungs ist es zusätzlich schwer für den Returnierenden, den Ball flach zu halten, da er diesen von oben nach unten spielen muss. Nachteil dieser Variante ist, dass nur selten bereits mit dem Aufschlag gepunktet werden kann. Der Angreifer benötigt ein herausragendes Volleyspiel, um mit dem ersten Volley den Druck aufrechterhalten zu können. Typische Vertreter dieser Variante warenStefan EdbergoderJohn McEnroe. Früher wurde die Serve-and-Volley-Taktik von vielen Weltklassespielern angewandt. Da die Spieler heutiger Zeit auch in der Lage sind, äußerst druckvoll Returns zu spielen, hat diese Strategie zumindest im Einzelwettbewerb an Bedeutung verloren. Beim Grundlinienspiel bleiben beide Spieler an der Grundlinie. Dabei versuchen sie, dadurch einen Vorteil zu erzielen, dass sie den Ball auf die vom Gegner weiter entfernte Seite oder gegen die Laufrichtung spielen. Mit dieser Taktik erzielt man meist erst dann einen Punktgewinn, wenn mehrere gut platzierte Schläge hintereinander gespielt werden. Dabei werden die Schläge überwiegend mit Topspin, seltener auch mit Slice gespielt. Beim Grundlinienspiel versucht man den Ball möglichst nahe an die gegnerische Grundlinie zu spielen. Die Schwierigkeit besteht darin, dass die Balllänge, die man dafür spielen will, von der eigenen Schlagposition und der Ballgeschwindigkeit und Rotation des ankommenden Balls abhängt. Steht man weit hinter der Grundlinie, muss der Ball eine weitere Strecke fliegen und deshalb entweder stärker geschlagen werden oder weniger Spin erhalten, als wenn man eher nahe an seiner eigenen Grundlinie oder gar im Feld steht. Ebenso kommt es darauf an, ob man den Ball mit oder gegen die Rotation des ankommenden Balls spielt und ob man den Ball bereits im Aufsteigen schlägt. Diese Feinjustierung des eigenen Schlages stellt im Hochgeschwindigkeitstennis der Weltklassespieler eine besondere Herausforderung dar. Diese erhöht sich noch weiter, wenn der Schlag aus vollem Lauf erfolgen muss. Hierbei antwortet der Rückschläger auf den Aufschlag direkt mit einem Angriffsball (chip) und rückt anschließend sofort ans Netz auf (charge). Dort versucht er, per Volley den Ballwechsel zu entscheiden. Dieser Spielzug wird fast ausschließlich gegen den zweiten Aufschlag angewandt. Wird diese Schlagfolge taktisch klug eingesetzt, setzt sie den Gegner unter Druck, da dieser dazu veranlasst werden könnte, beim zweiten Aufschlag mehr zu riskieren. Diese Angriffsvariante wird immer seltener gespielt, weil die Grundlinienspieler inzwischen dynamisch und schnell geworden sind, dass man sie nicht wirklich unter Druck setzen kann. Sie haben im Gegenteil sogar viel mehr Zeit, um den Ball zu spielen und können einen Passierschlag anbringen oder einen Lob spielen. Bei Inside-Out-Bällen (engl. fürvon innen nach außen) vermeidet der Spieler jeweils Vor- oder häufiger die Rückhand, indem er den Schlag „umläuft“. Der Ball wird, wenn die Rückhand umlaufen wird, von der Rückhandseite mit der Vorhand diagonal gespielt. Ziel ist es, mit der häufig druckvolleren Vorhand das Tempo hochzuhalten oder den unsichereren Schlag zu vermeiden. Seltener wird der Ball statt diagonal auch entlang der Seitenlinie gespielt. In diesem Fall spricht man von einemInside-In-Schlag. Als taktische Maßnahme der Spielgestaltung gilt der Rhythmuswechsel, bei der die Ballgeschwindigkeit beschleunigt oder verzögert wird. Der Rhythmuswechsel verfolgt je nach Spielverlauf und Spielsituation unterschiedliche Ziele, so z. B. Anwendung einer kraftsparenden Spielweise, Wiedererlangung eines ruhigen und besonnenen Spielaufbaus durch Verlangsamen des Spieltempos, Provokation gegnerischer Fehler durch plötzliche Temposteigerung, Durchsetzen des Powerplay oder des Pressing Game. Hierzu kann der Wechsel von Topspinschlägen zu Sliceschlägen und umgekehrt angewendet werden, wodurch der Gegner – insbesondere bei langen Ballwechseln – aus seinem Rhythmus gebracht werden soll. Das Doppel verlangt etwas andere Stärken als das Einzel. Wichtig ist der Aufschlag und der Return sowie das Volleyspiel, gefolgt vom Lob und Smash. Die beiden Spieler eines Teams müssen versuchen, das gesamte Spielfeld abzudecken, ggf. muss schnell von einer Seite auf die andere gewechselt und das Spielfeld nach vorne und hinten abgedeckt werden. Ein auf einander eingespieltes Team ist hier von Vorteil. Vor dem Aufschlag sprechen sich die beiden Teammitglieder ab, wohin der Aufschlag platziert werden soll. Um ein Lippenlesen zu verhindern, bedecken sie dabei meist den Mund. Eine andere Variante der Kommunikation ist die Zeichensprache. Dabei signalisiert der am Netz stehende Spieler seinem aufschlagenden Partner hinter dem Rücken mittels abgesprochenen Fingerzeichen, wohin er aufschlagen soll oder wohin der Netzspieler sich nach dem Aufschlag bewegen wird. Die ATP unternimmt Versuche, das Doppel attraktiver zu machen. Hierfür werden testweise geänderte Regeln ausprobiert. Die Pausen zwischen den Ballwechseln sollen auf 15 Sekunden verkürzt werden, ebenso die Pausen beim Seitenwechsel auf 60 Sekunden. Die Spieler sollen mit Mikrophonen ausgestattet werden, damit das Publikum Konversationen zwischen den Spielern mitverfolgen kann. Ebenso ist das Publikum gehalten, die Spieler auch während eines Ballwechsels anzufeuern.[41] Das Mixed wird teilweise in einer neuen Variante gespielt. Geändert wurde die Dauer der Matches, ein Satz wird lediglich bis vier gespielt – bei einem 4:4 kommt es zumTie-Break, bei Satzgleichstand zumMatch-Tie-Break. Es gilt dieNo-Ad-Regel, die besagt, dass bei Einstand (40:40) ein Entscheidungsball gespielt wird, dabei serviert immer Mann auf Mann und Frau auf Frau. Ausnahme ist das Finale, wo auf zwei herkömmliche Gewinnsätze gespielt wird, jedoch gegebenenfalls ein Match-Tiebreak als Entscheidungssatz bei Satzgleichstand. Das professionelle Tennis wird von internationalen Turnieren bestimmt, die das ganze Jahr über stattfinden und zumeist imK.-o.-Systemausgespielt werden. Die Dachorganisation für diese Turniere ist bei den Damen dieWomen’s Tennis Association(WTA) und bei den Herren dieAssociation of Tennis Professionals(ATP). Über ATP und WTA steht noch dieInternational Tennis Federation(ITF), die die Spielregeln bestimmt. Bei den Turnieren werden jeweils Punkte für die Tennisweltrangliste vergeben. Die Zahl hinter der jeweiligen Turnierbezeichnung bedeutet die Punktezahl des Siegers für die Weltrangliste. Dadurch wird die Wertigkeit und die Gewichtung des jeweiligen Turniers ausgedrückt. Die Weltranglistenposition der Spieler wiederum entscheidet über die Teilnahmeberechtigung bzw. Setzposition bei den einzelnen Turnieren. Auf der ATP Tour zählen gewonnene Punkte im Ranking maximal 52 Wochen, bis sie aus der Gesamtpunktzahl des Spielers entfallen. Es zählen maximal 19 Turniere für das Ranking: Ist die Weltranglistenposition zu niedrig, um an Grand Slams oder Masters-Turnieren teilzunehmen, kann ein anderes Turnier in die Wertung des Spielers eingebracht werden. Sollte der Spieler als Qualifikant,Lucky Loseroder perWildcardim Hauptfeld in einem dieser Pflichtturniere stehen, wird das Ergebnis gewertet. Spieler, die zu Beginn einer Saison mehr als 600 Spiele in ihrer Karriere gespielt haben, mindestens 12 Saisons absolvierten oder mindestens 31 Jahre alt sind, dürfen auf die Teilnahme an drei B-Turnieren (Masters 1000) verzichten und können dafür ein anderes Turnier in die Wertung mit einbeziehen. Männliche Spieler unter vierzehn Jahren sind nicht zur Teilnahme an Turnieren der ATP Tour oder der ATP Challenger Tour berechtigt. Ab dem Alter von vierzehn Jahren dürfen sie an maximal acht Turnieren, ab dem Alter von fünfzehn Jahren an maximal zwölf Turnieren der ATP Tour oder der ATP Challenger Tour teilnehmen.[42] Die Turniere sind danach benannt, wie viele Weltranglistenpunkte der jeweilige Sieger erhält. Sie lassen sich nach ihrer Wertigkeit in drei Kategorien unterteilen: Es folgen dieATP Challenger Tourund dann dieITF Future Tour. Die Challenger Tour dient vornehmlich Spielern außerhalb der Top 100 der Weltrangliste genügend Ranglistenpunkte für die Teilnahme an höherwertigen Turnieren der ATP Tour zu sammeln. Sie ist damit das Bindeglied zwischen der Future Tour und der ATP Tour. Es gibt mehr als 150 Challenger-Turniere pro Jahr in weltweit über 30 Ländern. In der WTA Tour zählen gewonnene Punkte im Ranking ebenfalls maximal 52 Wochen, bis sie aus der Gesamtpunktzahl der Spielerin entfallen. Die Anzahl gewerteter Turniere setzt sich aus maximal 17 Turnieren zusammen: Ist die Weltranglistenposition zu niedrig, um an Grand Slams oder WTA-1000-Turnieren teilzunehmen, kann ein anderes Turnier in die Wertung des Spielers eingebracht werden. Um in der Weltrangliste der Damen zu erscheinen, müssen in mindestens 3 Turnieren Weltranglistenpunkte erreicht worden sein oder 10 Punkte gewonnen werden. Spielerinnen unter vierzehn Jahren sind nicht zur Teilnahme an Profiturnieren der WTA berechtigt. Ab vierzehn Jahren dürfen sie an maximal acht Turnieren, zwischen vierzehn und fünfzehn Jahren an maximal zehn Turnieren, zwischen fünfzehn und sechzehn Jahren an maximal zwölf Turnieren, zwischen sechzehn und siebzehn Jahren an maximal 16 Turnieren teilnehmen.[43] Laut einer besonderen Regel der WTA darf bei einem Event der Kategorie 250 nur eine Spielerin der Top Ten teilnehmen. So will die WTA besonders bei kleineren Turnieren einen ausgewogenen Wettbewerb gewährleisten. (Die WTA lässt bei dieser Regel nur eine Ausnahme zu: Wenn die Titelverteidigerin als Top-Ten-Spielerin zurückkehrt, ist ein zweiter Startplatz möglich).[44] In der WTA Tour werden die Turnierserien wie folgt unterteilt: Es folgt als Pendant zur ATP Challenger Tour dieWTA 125. Sie dient vor allem Spielerinnen außerhalb der Top 100 der Weltrangliste dazu, Ranglistenpunkte zu sammeln, um an den höher dotierten WTA-Turnieren teilnehmen zu können. Die prestigeträchtigsten Turniere im Tennis sind die vier Grand-Slam-Turniere, die vom TennisweltverbandITFausgetragen werden. Im Jahr 2024 wurden bei den Grand Slams Preisgelder in Höhe von 254 Millionen US-Dollar ausgezahlt. Die Popularität des Grand-Slam-Tennis erreichte 2024 neue Höhen mit einer Gesamtzuschauerzahl von fast 2 Milliarden Menschen in mehr als 200 Ländern. Mehr als 3.360.000 Fans besuchten die Turniere, an denen 800 Profispieler teilnahmen.[45] DieATP Finals(aktuell:ATP Finals 2024) und dieWTA Finals(aktuell:WTA Finals 2024) gelten nach den vier Grand-Slam-Turnieren als die wichtigsten Tennisturniere und sind die Saisonabschlussturniere. Das Teilnehmerfeld besteht üblicherweise aus den jeweiligen Top 8 der jeweiligen Weltrangliste. Dabei zählen nur die Weltranglistenpunkte des laufenden Kalenderjahres, die im sogenanntenATP Racebeziehungsweise von der WTA alsRace to the Championshipsermittelt werden. 2024 bekamJannik Sinnerals Turniersieger der ATP Finals ohne Niederlage 1500 Punkte und 4.881.100 US-Dollar im Einzel beziehungsweise das Siegerteam im Doppel,Kevin KrawietzmitTim Pütz, zusammen 862.700 US-Dollar und jeweils 1000 Punkte. Die 2023 von der ITF eingeführte Initiative desJunior Accelerator Programme(englischBeschleunigungsprogramm für Junioren und Juniorinnen) soll den Fortschritt der weltbesten Jungen und Mädchen fördern.[46] DasITF Junior Mastersist das Saisonabschlussturnier der jeweils acht bestplatzierten Junioren und Juniorinnen der Kategorie der unter 18-jährigen im Tennis. Es ist nach den vier Junioren-Grand-Slam-Turnieren das bedeutendste auf derITF Junior Tour. DieNext Generation ATP Finalssind ein seit 2017 jährlich ausgetragenes Tennisturnier für die besten unter 21-jährigen Tennisspieler der Saison. Ab 2024 wurde die Altersgrenze auf unter 20-jährige Tennisspieler abgesenkt und eine Vielzahl an besonderen Regeln für das Turnier eingeführt. Die olympischen Spiele wollen Teilnehmer der ganzen Welt teilnehmen lassen. Deshalb gelten dort modifizierte Qualifikationskriterien. So waren 2024 nur Spieler teilnahmeberechtigt, die seit den letzten Olympischen Spielen an zweiBillie Jean King Cups(Frauen) oderDavis Cups(Männer) teilgenommen hatten. JedesNationale Olympische Komitee(NOK) durfte bis zu sechs männliche und sechs weibliche Athleten melden. Im Einzel durften maximal vier Athleten pro Land und Geschlecht einen Quotenplatz beanspruchen, in den Doppeln maximal zwei Paare pro Land und Geschlecht. Bei den Einzelwettbewerben qualifizierten sich die 56 besten Spieler beziehungsweise Spielerinnen der Weltranglisten. Maximal durften vier Athleten pro Nation sich auf diesem Wege qualifizieren. Für die Herren- und Damendoppel konnten sich jeweils 32 Paare qualifizieren. Für die Teilnehmer gibt es keine Weltranglistenpunkte, aber olympische Medaillen haben einen hohen Prestigewert für die Spieler.[48]Teilweise erhalten die Medaillengewinner unterschiedliche Geldprämien von ihren Heimatregierungen oder nationalen Sportverbänden. So erhielt 2024 der Serbe Novak Đoković 201.000 € für seine Goldmedaille, die er aber einem humanitären Zweck zugeführt hat.[49] Tennis bei den Special Olympicsist eine Sportart, die auf den Regeln von Tennis beruht und in Wettbewerben und Trainingseinheiten der OrganisationSpecial Olympicsweltweit für geistig und mehrfach behinderte Menschen angeboten wird. Die Sportart ist seit 1987 beiSpecial Olympics World Gamesvertreten. Das Regelwerk der International Tennis Federation wurde für Special Olympics leicht verändert, um die Länge der Spiele durch kürzere Sets verändern zu können. Außerdem besteht die Möglichkeit, auf kleineren Feldern und mit kompressionsarmen Bällen zu spielen. Vor den eigentlichen Wettbewerben finden Klassifizierungsrunden statt, damit die Athleten in möglichst leistungshomogene Gruppen eingeteilt werden können. So wie es imFußballFreundschaftsspielegibt, so gibt es im Tennis Showturniere (englischExhibition Tournaments). Jedes Jahr finden sie weltweit außerhalb der ATP- und WTA-Tour statt. Sie sind eine willkommene zusätzliche Einnahmequelle für Tennisprofis. Zuletzt hatSaudi-Arabienein neues Showturnier ins Leben gerufen, mit der Bezeichnung „Six Kings Slam“, zu dem sechs führende Tennisspieler eingeladen werden. Das Turnier ist kein Teil der ATP-Tour, es gibt demnach auch keine Weltranglistenpunkte. Dafür schlägt das Preisgeld alle Rekorde: Allein die Antrittsprämie für jeden einzelnen Teilnehmer beträgt 1,5 Millionen US-Dollar. Der Sieger erhält eine Siegprämie in Höhe von sechs Millionen US-Dollar. Erstmals wurde es vom 16. bis 19. Oktober 2024 inRiadausgetragen. Sieger wurdeJannik Sinner.[50]Kritiker werfen dem Land vor, mithilfe von „Sportswashing“ Verfehlungen wieMenschenrechtsverletzungenübertünchen zu wollen.[51] Bei großen Turnieren wird, um Benachteiligungen der Spieler durch Fehlentscheidungen der Schieds- oder Linienrichter zu reduzieren, einElectronic Line Calling(ELC) verwendet, ein computergestütztes System zur Ballverfolgung. Durchgesetzt hat sich das Hawk-Eye-System. Dabei erhält jeder Spieler pro Satz eine feste Anzahl von sog.Challenges(von engl.to challenge sth.‚etwas infrage stellen‘), die es ihm ermöglichen, eine Entscheidung durch das Hawk-Eye-System überprüfen zu lassen. Stellt sich dabei heraus, dass der Spieler recht hatte, so vergibt der Schiedsrichter je nach Eindeutigkeit der Spielsituation entweder direkt einen Punkt oder lässt den Ball wiederholen; die Anzahl der Challenges des Spielers bleibt in diesem Fall gleich. Wenn der Spieler in seiner Einschätzung falsch liegt, wird ihm eine Challenge abgezogen. Je nach Turnier erhält ein Spieler pro Satz zwei bis drei Challenges; falls der Satz durch ein Tie-Break entschieden werden muss, erhält jeder Spieler eine Challenge zusätzlich. In Sätzen, in denen kein Tie-Break gespielt wird (sog.Advantage- bzw. Vorteilssätze), erhält jeder Spieler nach jeweils zwölf gespielten Spielen, somit beim Stand von 6:6, 12:12 usw., wieder die ursprüngliche Zahl von Challenges. Eine Weiterentwicklung ist das Hawk-Eye-live, das seit 2023 verwendet wird. DieHochgeschwindigkeitskamerasnehmen dabei 340 Bilder pro Sekunde auf. Inzwischen wurde die Anzahl der Kameras auf 12 erhöht. Hinzukommen sechs Kameras zur Erkennung von Fußfehlern. Laut Hawk-Eye kostet die Ausrüstung für einen Platz fast 100.000 US-Dollar. Bei denUS Opensummieren sich die Kosten bei 17 Tennisplätzen auf 1,7 Millionen US-Dollar. Vor Beginn eines Turniers wird das System drei Tage lang justiert und getestet. Hierzu werden etwa 250 Tennisbälle systematisch auf dem Tennisplatz verteilt.[52] DieAssociation of Tennis Professionals(ATP) hat entschieden, das Hawk-Eye-live ab 2025 auf allen Touren verbindlich einzusetzen. DasOverrule, die Berechtigung des Stuhlschiedsrichters, eine Entscheidung zu überstimmen, ob ein Ball „aus“ ist oder nicht, entfällt bei Verwendung des Hawk-Eye. Die Entscheidung auf Grund der Hawk-Eye-Feststellung ist endgültig. Zur Veranschaulichung wird die Flugbahn des Balls und dessen Auftreffpunkt bei einem „close call“ (sinngemäß: „knappe Entscheidung“) nicht live projiziert, sondern in einer 3D-Animation dargestellt, die aus den gemessenen Daten durch eine spezielle Software generiert wird. Neben dem Hawk-Eye wird auch ein Sensor verwendet, der eineNetzberührungbeim Aufschlag durch einen Piepton anzeigt, was zu einer Wiederholung des Aufschlags führt. Zudem können Videoaufzeichnungen erfolgen, um etwa eine unzulässige Netzberührung des Spielers, einen Körperkontakt des Tennisballs oder ein zweimaliges Auftreffen des Balls am Boden zu dokumentieren. Der Netzsensor, die Aufschlag- und Fußfehlergeräte werden während des Aufschlagvorgangs eines Spielers zu Beginn jedes Punktes gleichzeitig eingeschaltet und dann automatisch ausgeschaltet. Die Aufschlaggeschwindigkeit wird mit demDoppler-Radargemessen. Dabei wird ein Radar-Signal geschickt und eines kommt wieder retour. Dazu stehen bei großen Turnieren an der Grundlinie Radarmessgeräte. Sie messen die Geschwindigkeit genau zu dem Zeitpunkt, wenn der Ball den Schläger verlässt. Der Ball befindet sich nur sechsmsauf dem Schläger. Bei einem harten Schlag wirkt dabei eine Kraft von 550Nauf ihn, was dem Gewicht von etwa 56 kg entspricht. DerLuftwiderstandbremst den Ball, am meisten aber der Aufprall auf dem Boden. Trifft ein Ball mit etwa 100 km/h schräg auf Rasen, springt er nur mit rund 70 km/h wieder ab. Auf Sand wird er auf unter 60 km/h gebremst. Gegenwind verstärkt den Luftwiderstand. Die Integration vonKünstlicher Intelligenz(KI) in den Tennissport stellt einen Paradigmenwechsel in der Herangehensweise und Wertschätzung des Sports dar. Durch die Kombination modernster Technologie mit traditioneller Spielweise hat KI die Leistung der Spieler und das Erlebnis für die Fans optimiert. Die vielfältigen Einsatzmöglichkeiten von KI gehen von der Verbesserung von Trainingsplänen und der Vorbeugung von Verletzungen bis hin zur Konstruktion von Tennisschlägern, Bereicherung des Zuschauererlebnisses, Unterstützung der Kommentatoren und der Verbesserung der Talentsuche. So bieten KI-gestützte Anwendungen und Plattformen auch psychologisches Training und helfen Spielern, ihre mentale Stärke zu verbessern.Predictive Analyticsim Tennis geht über einfache Gewinn-Verlust-Wahrscheinlichkeiten hinaus. So wurde IBMs „Watson“ in Wimbledon eingesetzt, um Spieldaten zu analysieren und Vorhersagen über Spielergebnisse zu liefern. Dieses System verwendet historische Daten und aktuelle Turnierdaten, um entsprechende Prognosen zu erstellen.[53] Im Tennis hat sich die Leistungsanalyse vor allem alsNotationsanalyseentwickelt. Die Analysetechniken haben sich insbesondere seit dem Jahr 2000 deutlich weiterentwickelt, insbesondere in der Matchanalyse. In einem Review wurde die Datenerfassung und -analyse untersucht, inwieweit die Analysen für den Ausgang von Matches hilfreich sein können, denn zunehmend werden diese Analysen von führenden Tennisspielern verwendet, um einerseits ihre Leistung zu verbessern und andererseits Strategien für anstehende Matches zu entwickeln.[54]Die Bedeutung der Tennisdatenanalyse liegt in ihrer Fähigkeit, Muster und Trends aufzudecken, die mit bloßem Auge nicht erkennbar sind. Durch eine umfassende Analyse wichtiger Kennzahlen können Trainer ein tieferes Verständnis für das Spiel ihrer Spieler gewinnen und sie dazu bringen, während eines Spiels fundiertere Entscheidungen zu treffen. Dieser datengesteuerte Ansatz ermöglicht gezieltes Training, strategische Anpassungen und eine präzise Spielplanung, die darauf zugeschnitten ist, die Schwächen des Gegners auszunutzen.[55]Die Analysegeräte (Player Analysis Technology (PAT)) müssen von der ITF zugelassen sein und werden in einer Liste veröffentlicht.[56] DieInternational Tennis Integrity Agency(ITIA) ist die Organisation, die für die Wahrung der Integrität des professionellen Tennis weltweit verantwortlich ist. Es handelt sich um eine Nachfolgeorganisation derTennis Integrity Unit(2008–2020). Neben Präventions-, Aufklärungs- und Drogentestaktivitäten sammelt sie Informationen und untersucht Wettbewerbsmanipulationen, insbesondere Spielmanipulationen im Tennis. Sie hat die Möglichkeit, Bußgelder und Sanktionen zu verhängen und Spielern, Schiedsrichtern und anderen Tennisfunktionären die Teilnahme an Turnieren zu verbieten. Die ITIA ist eine gemeinsame Initiative der ITF, der ATP, der WTA und der vier Grand-Slam-Turniere.[57] Im Profitennis kommen Teams, die in der Regel aus sechsBalljungenund -mädchen bestehen, zum Einsatz, um einen zügigen Spielverlauf zu ermöglichen. Diese sind in den Courtecken und am Netz positioniert. Teilweise unterstützen sie auch die Spieler während der Seitenwechsel, um ihnen Handtücher oder Getränke zu reichen, Tennisschläger zum Besaiten zu bringen oder ihnen mit einem Sonnenschirm Schatten zu spenden. Üblicherweise werden Jugendliche ab 14 Jahre eingesetzt, die nach bestimmten Einsatzzeiten rotieren. DieUS Openlassen auch Ältere zu. Die US Open sind auch das einzige Grand Slam Turnier, das die Balljungen und -Mädchen mit 17 $ pro Stunde entlohnt. Bis 2018 bestanden die US Open auf einer Tradition, wonach die Bälle zwischen den Balljungen und -mädchen über den Platz geworfen wurden. Erst seit 2018 haben sie sich den anderen Grand-Slam-Turnieren angeschlossen, wonach die Bälle nur noch gerollt werden.[58][59]Ihre Einsätze unterliegen festen Regeln. Bei großen Turnieren werden sie nach einem Auswahlverfahren mehrere Wochen auf ihre Einsätze vorbereitet. InWimbledonkamen erstmals Balljungen in den 1920er Jahren zum Einsatz. Seit 1977 werden dort auch Mädchen in dieser Funktion eingesetzt.[60] Bei großen Turnieren, insbesondere den vier Grand-Slam-Turnieren, sindSicherheitsbeamteim Einsatz, um die Spieler und den Turnierverlauf zu schützen. Beispielsweise findet jeden Morgen bei denAustralian Openein Ritual in einem hell erleuchteten Korridor am Rande der Rod Laver Arena statt. Etwa 20 Sicherheitsleute bilden ein Spalier. Kurz vor 11 Uhr folgen dutzende Spieler ihren Begleitern in den 63 Hektar großen Melbourne Park. Die Begleitpersonen achten auf potentielle Bedrohungen. Teilweise können Spieler mittels unterirdischen Gängen die Courts erreichen. Im Jahr 1993 stach ein Mann mit einem Messer aufMonica Selesaus Jugoslawien ein, als sie bei einem Turnier in Hamburg auf der Spielerbank saß. Seles war damals die Nummer eins der Welt und brauchte zwei Jahre, um sich zu erholen. Seitdem wurden die Sicherheitsvorschriften stark verschärft. So stehen beim Seitenwechsel Sicherheitsleute im Rücken der Spieler.[61]Die ITF hat spezielle Richtlinien zur Gewährleistung der Sicherheit bei Wettbewerben erlassen. Die Kontrollen beginnen bereits am Eingang zu den Tennisstadien.[62] Externe Einflüsse beeinflussen zusätzlich den Matchverlauf, insbesondere im Profitennis: Bei hohen Temperaturen steht den Spielern eine tragbare, batteriebetriebene Klimaanlage (IcyBreeze) zur Verfügung, aus der sie sich über einen Schlauch von kalter Luft anblasen lassen können. Geeiste Handtücher liegen in einem Kühlschrank bereit. Man geht allgemein davon aus, dass ein Spieler, der auf derATP TouroderWTA Toureine auskömmliche Summe verdienen will, in den Top 150 der Rangliste platziert sein muss. Für Spieler und Spielerinnen unterhalb der Top 250 gibt es nur wenigeSponsorenoder Werbeverträge. Auch müssen sie vom Preisgeld ihre Flüge, Hotelaufenthalte, Verpflegung, auch für mitreisendeTrainer, Schlagpartner, Betreuer, Agenten,MasseureundPhysiotherapeuteneinschließlich deren Honorare, bezahlen, entrichten natürlich auch Steuern und Versicherungen und müssen ihre Altersversorgung absichern. So erhalten beispielsweise die Ersatzspieler bei denATP Finals– ohne gespielt haben zu müssen – eine Art Anwesenheitsgeld von 152.000 $, um deren Kosten zu decken. Auf Grund eines besonderenBesteuerungssystems in Englandkann es sogar sein, dass Athleten mit hohen Werbeeinnahmen mehr Steuern zahlen müssen, als sie durch ein Preisgeld einnehmen. Insgesamt bedeutet dies, dass die meisten Profispieler und Profispielerinnen jenseits der Weltranglistenposition 250 kaum genug Geld verdienen, um es als Vollzeitjob zu betreiben. Viele von ihnen arbeiten zusätzlich alsTennislehreroder spielen gegen Honorar in Vereinsmannschaften, wo sie je nach Ranglistenplatz zwischen 1000 € und 20.000 € pro Match erhalten. Hinzu kommt, dass meistens die Tenniskarriere von Profis um das 35. Lebensjahr endet. Die unter anderem vonNovak ĐokovićbegründeteProfessional Tennis Players Associationsetzt sich für bessere Bedingungen für Profispieler ein. Beispielsweise erhielt ein Spieler beimATP-Challenger-Tampere-Turnier, dessen Hauptfeld aus 32 Spielern bestand, in der 1. Runde ein Preisgeld von 440 €, der Sieger 6150 €.[63]Teilweise haben Spieler aus Geldnot in den Umkleideräumen der Tennisanlage übernachtet. Seit 2019 müssen alleChallenger-Turniereihren Spielern „Hospitality“ gewähren, das heißt, sie übernehmen die Kosten für Unterkunft und Verpflegung, jedoch nicht die Reisekosten. Die Challenger Tour dient vornehmlich Spielern außerhalb der Top 100 der Weltrangliste dazu, genügend Ranglistenpunkte für die Teilnahme an höherwertigen Turnieren der ATP Tour zu sammeln. Um das Einkommen der Spieler auf der Challenger Tour aufzubessern beschloss die ATP ein Rekordpreisgeld von 28,5 Millionen US-Dollar für die Saison 2025 auszuschütten, 6,2 Millionen US-Dollar mehr als 2024, was eine Steigerung von 135 % seit 2022 bedeutet.[64] Die Herrentour ATP legte 2024 das Programm „Baseline“ auf. In dem wird den Spielern zwischen Platz 1 und 250 ein Grundeinkommen zugesichert. Wer 2024 als Top-100-Spieler nicht mindestens 300.000 Dollar Preisgeld in einem Jahr erwirtschaftet hat, für den stockt die ATP sein Einkommen bis zu dieser Grenze auf. Spieler auf den Rängen 101 bis 175 bekommen eine Einkommensgarantie von 150.000 Dollar und alle restlichen Spieler bis Platz 250 noch eine in Höhe von 75.000 Dollar.[65]Insgesamt wurden 2024 1,3 Millionen Dollar an 26 Spieler ausgeschüttet. 17 Spieler fielen unter die Einkommensgarantie. Im Jahre 2025 stockt die ATP die Einkommensgarantien auf. Spieler auf den Rängen 101 bis 175 erhalten eine Einkommensgarantie von 200.000 Dollar und alle restlichen Spieler bis Platz 250 noch eine in Höhe von 100.000 Dollar.[66]Auch die Aufstockung muss versteuert werden. Eine zweite Säule des Baselineprogramms sichert Spielern, die wegen einer Verletzung an weniger als neun ATP-Tour- und Challenger-Tour-Events in einem Jahr teilnehmen konnten, ein Mindestpreisgeld zu, das Top-100-Spielern 200.000 $, 100.000 $ zwischen Weltranglistenplatz 101 und 175 und 50.000 $ Spielern zwischen Weltranglistenplatz 176 und 250 garantiert. Davon profitierten für 2024 drei Spieler.[66] Die Förderung erhalten jedoch nur Spieler, die bislang weniger als 15 Millionen $ Preisgeld in ihrer Karriere verdient haben. Damit sind aktive Spieler förderberechtigt, die in derewigen Preisgeldranglisteder ATP etwa hinter Rang 50 stehen. Das bedeutet, dass etwa 25 aktive Spieler derewigen Preisgeldranglistevon der Förderung ausgeschlossen sind. Diese hohe Preisgeldhürde relativiert die Preisgelder im Profitennis. Eine dritte Säule fördert Nachwuchsprofis, die es erstmals in die Top 125 schaffen mit einer Preisgeldvorauszahlung von bis zu 200.000 $. Diese Förderung erhielten 2024 sieben Spieler.[66]Das Baseline-Programm ist zunächst für drei Jahre angelegt und soll anschließend evaluiert und ausgebaut werden.[67] DieInternational Tennis Federationunterhält einen Fonds, der mit jährlichen Beiträgen der vier Grand-Slam-Turniere Spieler direkt durch Tourteams oder Reisestipendien unterstützt, die alsGrand Slam Player Grantsbekannt sind. Sie wurden erstmals 2017 eingeführt, damit Nachwuchsspielerinnen und -spieler internationale Wettkampferfahrung sammeln können. Seinerzeit erhielten Spieler wie die spätere Wimbledon-Siegerin von 2022,Jelena Rybakina, und die Grand-Slam-FinalistenOns JabeurundCasper Ruuddiese Grand-Slam-Player-Grants sowieGustavo Kuerten,Li Na,Jeļena Ostapenko,Wiktoryja AsarankaundSimona Halep. 2024 erhielten insgesamt 51 Nachwuchs- und Profispieler Zuschüsse von 12.500 US-Dollar beziehungsweise 25.000 US-Dollar, zwei Spieler erhielten 50.000 US-Dollar. Zu den Empfängern zählten die Sieger derITF Junior TourVictoria Jiménez Kasintseva,Alexandra Eala,Petra MarčinkoundDino Prižmić.[68]2025 werden 56 Spieler gefördert. Zwei Spieler, der BrasilianerJoão FonsecaundXinran Sunaus China. erhalten 50.000 $, 45 Spieler erhalten 25.000 $ und neun Spieler erhalten 12.500 $.[69] Preisgelder der Sieger der vier Grand-Slam-Turniere: Obwohl die eine Woche dauernden ATP-Finals und WTA-Finals am Ende des Jahres deutlich weniger öffentliche Aufmerksamkeit auf sich ziehen als die vier Grand-Slam-Turniere, sind sie für den exklusiven Kreis der jeweils üblichen acht Teilnehmer und Teilnehmerinnen äußerst lukrativ. Dort errechnet sich das Gesamtpreisgeld aus der Summe der Preisgelder für die gewonnenen Matches. So erhielt der SiegerJannik SinnerderATP-Finals 2024insgesamt 4,88 Millionen US-Dollar, die SiegerinCoco GauffderWTA-Finals 2024insgesamt 4,80 Millionen US Dollar. Die Siegerin der WTA Finals 2024 verlor bei dem Event ein Rundenturnier. Wäre sie ungeschlagen geblieben, hätte ihr Preisgeld 5,15 Millionen Dollar betragen. Den Rekord im Preisgeld hält das Showturnier „Six Kings Slam“. Der Sieger des Turniers 2024 (Jannik Sinner) erhielt eine Siegprämie in Höhe von sechs Millionen US-Dollar. Anders sieht es bei den Topspielern aus, die hohe Millionenbeträge an Preisgeldern während ihrer Karriere verdient haben. Es zählen nur Preisgelder der ATP- und WTA-Tour ohne den Preisgeldern aus Showturnieren und ohne Antrittsgelder. Nachfolgend die Top Ten derewigen Preisgeldrangliste(Stand: 14. Juli 2025): Selbst der Hundertste auf dieserewigen Preisgeldrangliste,Richard Krajicek, kommt auf eine Preisgeldsumme von 10.077.425 $ (Stand: 3. Februar 2025).[73][74] Die hundertste Spielerin dieserewigen Preisgeldrangliste,Ai Sugiyama, kommt auf eine Preisgeldsumme von 8.128.125 $ (Stand: 3. Februar 2025). Diese Spieler kassieren bei kleineren Turnieren (ATP Tour 250,ATP Tour 500undWTA Tour) zusätzlich Antrittsgelder. So sollRafael Nadaleine Million Euro Antrittsgeld verlangt haben. Damit die kleineren Turniere, welche meist gleichzeitig stattfinden, die Zuschauerzahlen erhöhen können, sichern sie sich dadurch die Gunst der besten Spieler. Hinzu kommen auch noch weit höhere Beträge aus Werbeverträgen, die die Preisgelder beträchtlich übersteigen.[75]So war 2020 der Anteil der Preisgelder an den Gesamteinnahmen vonRoger Federernur 0,6 Millionen gegenüber 82,4 Millionen aus Werbeeinnahmen.Naomi Ōsakanahm 50,4 Millionen ein, davon nur 4,7 Millionen an Preisgeldern,Serena Williams36,6 Millionen zu 1,7 Millionen,Novak Đoković27,5 zu 7,3 und Rafael Nadal 21,1 zu 3,7.[76] 2024 führtCarlos Alcarazbei den erzielten Werbeeinnahmen (ohne Preisgelder) in Höhe von 32 Millionen (Nike, Rolex, BMW), gefolgt vonNovak Đokovićmit 25 Millionen (Peugeot, Seiko, Lacoste, Hublot),Coco Gauffmit 20 Millionen (Carol's Daughter, Naked Brand),Jannik Sinnermit 15,1 Millionen (La Roche-Posay, De Cecco),Iga Świątekmit 15 Millionen (Lego, Lancôme, On) undAlexander Zverev(Richard Mille, Adidas, Head, Peugeot).[77] In den Doppelwettbewerben wird nur ein Bruchteil dessen verdient. So kommen gerade mal neun Tennisprofis (Marcel Granollers,Horacio Zeballos,Jordan Thompson,Mate Pavić,Nikola Mektić,Wesley Koolhof,Kevin Krawietz,Marcelo ArévaloundTim Pütz) auf eine Preisgeldsumme während ihrer gesamten Karriere von etwas über einer Million Dollar, können aber durch zusätzlich in Einzelwettbewerben gewonnenes Preisgeld oder Sponsoren davon leben (Zum Vergleich: Etwa 800 männliche Tennisprofis erzielten im Einzel und Doppel eine Karriere-Preisgeldsumme von mindestens einer Million Dollar).[78] Die ATP und WTA führen neben den Punkteranglisten auch Preisgeldranglisten, deren Reihung nach der Summe der gewonnenen Preisgelder erfolgt. Neben der Preisgeldrangliste des laufenden Kalenderjahres werden sogenannte „ewige Preisgeldranglisten“ geführt, die die Summe der Preisgelder des gesamten Profiturnierlebens enthalten. Nachdem die Preisgelder jedes Jahr beträchtlich steigen, rutschen die Topspieler früherer Zeiten in derewigen Preisgeldranglistekontinuierlich nach hinten, wobei die kumulierte Summe davon abhängt, in welchem Zeitraum die Preisgelder gewonnen wurden. Dieewige Preisgeldranglisteder Damen umfasst rund 12.000 Spielerinnen, die der Herren doppelt so viel. Nachfolgende Auswahl beinhaltet nur ehemalige Sieger mindestens eines Grand-Slam-Turniers (Stand: 17. Februar 2025). 1973 gelang es Billie Jean King unter Androhung eines Boykotts, dass die Verantwortlichen beim Grand-Slam-Turnier derUS Opender Siegerin erstmals das gleiche Preisgeld wie das der Herren in Höhe von damals 10.000 US-Dollar zahlten (50 Jahre später ist das Preisgeld der Sieger 360 mal so hoch). DieAustralian Openzogen 28 Jahre später nach, bei denFrench Openund inWimbledonerhalten seit 2007 Damen und Herren Preisgelder in gleicher Höhe. Die WTA möchte bei gemeinsamen Turnieren mit der ATP auf den Ebenen 1000 und 500 bis zum Jahr 2027 das gleiche Preisgeld auszahlen. Bei Events an verschiedenen Schauplätzen soll bis 2033 Parität einkehren.[79]Ein Anfang wurde mit denWTA Finals 2024gemacht, indem das Gesamtpreisgeld auf 15.250.000 US-Dollar und damit auf das Niveau derATP Finals 2024angehoben wurde, was einer Steigerung von 69,44 % im Vergleich zum Vorjahr entspricht. Durch plötzliche Sprints, schnelle Richtungswechsel oder die Beschaffenheit der Tennisplätze kann der Körper übermäßig belastet werden. Auch eine falsche Schlagtechnik oder ein unpassender Schläger können Auslöser für Schmerzen beim Tennisspielen sein.[80][81]Durch ein gezieltes Muskeltraining und entsprechende Dehnübungen – außerhalb des Tennisplatzes – kann das Verletzungsrisiko minimiert werden. Ein Profitennisspieler, der durch ärztliches Attest eine Verletzung nachweist, ist von der Teilnahme an den Pflichtturnieren befreit (sieheBerechnung des ATP-Rankings seit 2009undWTA Tour Ranking). Einem Spieler, der sich während eines Matches eine behandelbare Verletzung zuzieht, kann einemedizinische Auszeitvon drei Minuten (medical time-out) für die Behandlung gewährt werden. Sollte eine Wunde bluten, kann durch den Oberschiedsrichter die Auszeit auf fünf Minuten verlängert werden. Durch besondere Überbeanspruchung verschiedener Muskelgruppen kann es zumMuskelkater,Muskelzerrungen,Muskelfaserrissoder zu Mikrotraumata imSarkomerkommen. Häufig ist ein „PosterosuperioresImpingement“ für die Schmerzen verantwortlich. Durch die wiederholte Außendrehung und Abspreizung der Schulter kann es zu Druck- bzw. Scherbelastungen im Bereich der hinteren Gelenklippe, der dortigen Kapselstruktur und derRotatorenmanschettemit der Folge von Schädigungen kommen. Das ständige Rotieren des Oberkörpers, die vielen Dreh-, Sprint- und Stopp­-Bewegungen, ebenso wie die Stauchung der Wirbelsäule beim Aufschlag können zu Rückenschmerzen führen. Hierzu gehörenMuskelverspannungen,LumbaleÜberlastung,Bandscheibenprobleme, Entzündung desSakroiliakalgelenks,Facettengelenksyndrombis hin zuStressfrakturen. Ebenso kann die Entwicklung einerSkolioseoder eines Hohlkreuzes durch Tennis begünstigt werden. Beim Tennisarm handelt sich um eine entzündungsähnliche Verletzung an den Sehnenansätzen (Epicondylitis), die durch Überbelastung entsteht. Der Entzündungsherd liegt meist an der Außenseite des Ellenbogens. Verletzungen an der Hand oder demHandgelenkbeim Tennis werden durch eine falsche Rückhand- oder Vorhandtechnik begünstigt. Dauerhaft kann es hier zurSehnenscheidenentzündungkommen. Bei der beidhändigen Rückhand wird diese sehr einseitig belastet. Die nicht-dominante Hand leidet unter der Monotonie der Bewegung im Vergleich mit der einhändigen Vorhand und schadet dadurch dem Handgelenk mehr. Es entstehen muskuläre Dysbalancen. Im Handgelenk werden der sogenannte „Diskus“ und die dazugehörigen Sehnen überbelastet. Die Folgen sind Einrisse und Entzündungen. Verletzungen am Handgelenk und den Fingern können ferner durch Stürze entstehen, insbesondere wenn man den Schläger noch in der Hand hält. An den Beinen kann es zuMuskelfaserrissen,Sehnenrissen,ZerrungenundPrellungenkommen. Diese sind meist an den hinteren Oberschenkeln oder der Wadenmuskulatur und demKniegelenk(Läuferknie) zu finden, etwa dasPatellaspitzensyndrom. Ebenso kann derMeniskusbetroffen sein oder dieKnöchel. Ballsportarten mit charakteristischen schnellen Richtungswechseln können zumSchienbeinkantensyndrom(Shin Splints) führen. Dazu gehören die Überbeanspruchung derMuskulaturoderKnochenhautentzündungenan stark belasteten Stellen amSchienbein. Durch Reibung an der Handinnenfläche der Schlaghand kann es zur Bildung einerHautblasekommen, ebenso an den Füßen. Durch Stürze könne sich SpielerAbschürfungenzuziehen. Es entsteht eine oberflächliche Wunde, wenn die Haut durch Reibungskräfte verletzt wird. Bei einer blutenden Wunde kann bei Wettkämpfen ein verlängertesmedical time-out(medizinische Auszeit) von fünf statt drei Minuten zur Versorgung der Wunde gewährt werden. Insbesondere bei hohen Außentemperaturen und hoher körperlicher Belastung kann es zu einerDehydrierungkommen, die leichte bis schwere Folgen nach sich ziehen kann, insbesondere einer Reduzierung des Allgemeinzustandes. Die körperliche Leistungsfähigkeit besteht aus Kraft, Schnelligkeit, Ausdauer, Beweglichkeit, koordinativen Fähigkeiten, Bewegungsfertigkeiten und psychischen Fähigkeiten. Je nach Spiellevel steigen die entsprechenden Anforderungen an den Sportler.Anthropometrie, Laufschnelligkeit (Antritts- und Beschleunigungsfähigkeit mit und ohne Richtungswechsel), Sprungfähigkeit (Schnellkraft und Reaktivkraft), Wurfkraft und Koordination gehören dazu. Im Profitennis darf die körperliche Leistungsfähigkeit auch während mehrere Stunden dauernden Matches möglichst nicht nachlassen. Ermüdungserscheinungen verändern nicht nur die Laufgeschwindigkeit, sondern auch den Muskeltonusund damit den Bewegungsablauf und reduzieren dieKonzentration, wodurch die Fehlerquote steigt. Beim Training ist die Interaktion zwischen Athleten und Trainern sehr wichtig. Nicht wenige Tennisspieler beschäftigen spezielle Konditionstrainer. Sie führen eineLeistungsdiagnostikdurch und steuern anschließend das Training gezielt, dass sich die Athleten maximal verbessern. Diese Trainingssteuerung beinhaltet eine Trainingsplanung, -durchführung, -kontrolle und -auswertung.[82] Ermüdung beeinflusst die Fairness beim derzeitigen Turniersystem. Spieler, die sich durch die Qualifikation erst für den Hauptbewerb qualifizieren, gehen im Hauptbewerb mit einer deutlich höheren Vorbelastung in ihre Begegnungen. Dies kann in der ersten Runde ein Vorteil sein, da die Bedingungen auf den Plätzen bereits bekannt sind und positive Erfahrungen das Selbstbewusstsein bestärken. In den letzten Runden eines Turnieres ist dies allerdings ein nicht zu unterschätzender Nachteil. Eine ausgewogeneErnährungist entscheidend für die Leistungsfähigkeit im Tennis. Höchstleistungen können nur mit einer optimalen Trainings- und Ernährungsbasis realisiert werden. Mit der richtigen Ernährung werden fünf Energiebilanzen im Gleichgewicht gehalten: DerDeutsche Tennis Bundhat hierzu ein detailliertes Ernährungskonzept entwickelt,[83]wobei seit geraumer Zeit Profitennisspieler ein individuell abgestimmtes Ernährungsprogramm einhalten. Bei 91 % der Ballwechsel liegt die Belastungsdauer bei unter zehn Sekunden, somit überwiegt im Tennis dieanaerob-alaktazideEnergiebereitstellung durchAdenosintriphosphat(ATP) undKreatinphosphat(KP) während der Belastungsphasen. In den Pausen (zwischen den Ballwechseln) werden diese energiereichen Phosphate (ATP und KP) in der Arbeitsmuskulatur aerob über den Abbau vonKohlenhydratenundFettenwiederhergestellt. Kumuliert ergibt sich daraus eine Energiebereitstellung zu ca. 60–85 % durch den Abbau von Kohlenhydraten und zu ca. 15–40 % über dieFettsäureoxidation, wobei es aufgrund der Natur des Tennissports (Wettkampfdauer zwischen <60 min und >5h, Bodenbelag, Länge der Ballwechsel etc.) zu Abweichungen kommen kann. Grundsätzlich ist anzunehmen, dass mit zunehmender Wettkampf- bzw. Trainingsdauer der Fettstoffwechsel stärker in den Vordergrund tritt, wobei im Damentennis die Fettoxidation etwas höher liegt als beim Herrentennis. Der (Brutto-)Kalorienumsatzim Tenniswettkampf beträgt bei männlichen Turnierspielern (ca. 80 kg) durchschnittlich ca. 600–800 kcal/Stunde. Leistungssportlern wird empfohlen, täglich ca. drei Liter Flüssigkeit zuzüglich der belastungsbedingtenSchweißverlustezusätzlich zur Nahrung aufzunehmen. Der durchschnittliche Schweißverlust im Leistungstennis beträgt bei Männern etwa 1–1,5 l/h und bei Frauen 30 % weniger, ist allerdings auch individuell unterschiedlich und hängt von vielen Faktoren wie Spielertyp, Spielweise, Temperatur, Netto-Spielzeit etc. ab. In Extremsituationen kann die Schweißproduktion auf 2–3 l/h ansteigen. Grundsätzlich sollten mindestens 50–80 % davon, maximal jedoch 1,2–1,5 l/h ersetzt werden. Neben dem allgemeinen Ernährungsprogramm werden am Matchtag selbst drei Phasen unterschieden. Etwa zwei bis drei Stunden vor dem Match werden kohlenhydratreiche, leicht verdauliche Speisen eingenommen. Während des Matches wird auf eine ausreichende Flüssigkeitszufuhr geachtet, wobei meistisotonische Elektrolytgetränkeeingenommen werden. Zudem sind kleine Snacks hilfreich für die Energieaufnahme während der Pausen. Der wohl bekannteste unter Tennisspielern ist dieBananeoderDatteln. So wurden bei denAustralian Open 2025von den Spielern 5000 Bananen ausQueenslandkonsumiert. Beliebte Snacks sind aber auchEnergieriegeloder sogenannte Energie-Gele. Kohlenhydratkonzentrate enthalten verschiedeneZuckerarten, die vom Körper unterschiedlich aufgenommen werden. Dies sind meistGlucose,Fructose,TrehaloseoderMaltodextrin. Nach dem Match nimmt man für die Regeneration der Muskeln nach etwa 30 Minuten einen proteinreichen Snack ein. Ungefähr zwei Stunden nach Matchende stärkt man sich mit einer Mahlzeit aus 80 % Kohlenhydraten und 20 %Proteinen.[84]Ein Abendessen mit vielen Kohlenhydraten eignet sich zwar gut zum Auffüllen derGlykogenspeicher, bewirkt jedoch einen kürzerenSchlaf, wohingegen ein Abendessen mit viel Proteinen (Verzweigtkettige Aminosäuren) nicht nur gut gegenMuskelkaterist, sondern die Schlafqualität verbessert. Insbesondere bei großen Turnieren steht den Spielern einFitnessraumzur Verfügung, in dem sie sich vor einem Match aufwärmen können und verschiedene Trainingsmethoden an zahlreichen Geräten anwenden können. Er kann mit Gymnastikmatten, Klimmzugstangen,Lang- und Kurzhanteln,Laufbändern,TRX-Trainer,FahrradergometerundCrosstrainer/Stepperausgestattet sein. Der Raum wird auch zumCool Down(englischAbwärmen oder Auslaufen) nach einem Match aufgesucht. Neben der Ernährungsweise nimmt insbesondere im Profitennis, bei dem oft während eines Turniers in kurzen Abständen lange Matches bis zu fünf Sätzen gespielt werden müssen, die physische Regeneration einen hohen Stellenwert ein, zu derPhysiotherapie,MassagenundKaltwasserimmersion(Eisbad) gehören. Zur psychischen Regeneration gehören einepsychologische Betreuung, Atemübungen,VisualisierungstechnikenundEntspannungsverfahrenwieMeditation,Yoga,EMDRoderStretching. Wesentlich sind dabei auch ausreichend langeSchlafpausen.[85]Es gibt zahlreiche Studien, welche Nahrungsmittel die Schlafqualität und -länge positiv beeinflussen.[86] DieWelt-Anti-Doping-Agentur(WADA) untersucht mehrmals im Jahr die Werte der Tennisspieler bezüglich verbotener Substanzen. Die ITF legte die Zahlen offen, wie oft solcheDopingteststatsächlich stattfinden, etwa zehn bis vierzehn Mal pro Jahr. Die Sportler werden nicht nur bei Wettbewerben und Turnieren kontrolliert, sondern müssen sich auch abseits der Stadien auf Untersuchungen einstellen.[87]Im Laufe der Jahre wurden nicht wenige prominente Profitennisspieler überführt, unerlaubte Substanzen eingenommen zu haben, darunter auchKokain[88](siehe auchDopingfälle im Tennis). Eine ausgewählte Gruppe von Tennisspielern wird im Rahmen desAthlete Biological Passport(ABP)-Programms regelmäßig getestet. ABPs werden im Tennis seit 2013 eingesetzt.[89]Der Deutsche Tennisbund (DTB) hat daran angelehnt eine Anti-Doping-Ordnung erlassen. Dort heißt es zu Beginn: „Es ist die persönliche Pflicht der Athleten*innen, dafür zu sorgen, dass keine Verbotenen Substanzen in ihren Körper gelangen. Athleten*innen sind für jede Verbotene Substanz oder ihre Metaboliten oder Marker verantwortlich, die in ihrer Probe gefunden werden. Demzufolge ist es nicht erforderlich, dass Vorsatz, Verschulden, Fahrlässigkeit oder bewusster Gebrauch aufseiten der Athleten*innen nachgewiesen wird, um einen Verstoß gegen Anti-Doping-Bestimmungen gemäß Artikel 2.1 zu begründen.“ DieInternational Tennis Integrity Agency(ITIA,deutschInternationale Agentur für Rechtschaffenheit im Tennis) ist eine Organisation, die für die Wahrung der Integrität des professionellen Tennis weltweit verantwortlich ist. Sie untersucht Wettbewerbsmanipulationen, insbesondereSpielmanipulationen im Tennis. Sie hat die Möglichkeit, Bußgelder und Sanktionen zu verhängen und Spielern, Schiedsrichtern und anderen Tennisfunktionären die Teilnahme, selbst die Anwesenheit an Turnieren, zu verbieten.[91] Beachtennisist ein Rückschlagspiel, das Tennis,BeachvolleyballundBadmintonmiteinander verbindet. Die häufigste Wettkampfform ist die des Doppels. Es kann jedoch auch ein Einzel gespielt werden, wobei sich dann die Spielfeldgröße verkleinert. Seit dem Sommer 1998 ist Beachtennis auch im Regelwerk desDeutschen Tennis Bundes(DTB) festgeschrieben. BeimPadelhandelt sich um dem Tennis ähnliche, beziehungsweise davon abgeleitete Spiele, die auf kleineren Feldern mit kurzen Schlägern ohne Bespannung gespielt werden. Der Sport wird international in derInternational Padel Federation(IPF) durch denDeutschen Padel Verband(DPV), in Österreich durch dieAustrian Padel Union(APU)[92], in der Schweiz durch denSchweizer Padel Verband[93]vertreten und imDeutschen Olympischen Sportbund(DOSB) durch den DTB, der seinerseits Turniere durchführt. Soft Tennis(auch Softball-Tennis) ähnelt stark dem Tennis, wird jedoch mit weichen und innen ausgehöhlt Bällen gespielt. Die Schläger sind ebenfalls leichter als normale Tennisschläger. Es ist der ideale Einstieg in die Welt der Rückschlagspiele. Ob Tennis, Badminton oder Speedminton – dank der leichten Softtennis-Schläger und dem weichen Softball erlernen Kinder beim Soft-Tennis Ballgefühl, Kraftdosierung und verbessern zusätzlich die Auge-Hand-Koordination. Die Kinder der Altersklassen U8–U12 spielenKleinfeldtennisim Mannschaftswettspielbetrieb im Kleinfeld oder Midcourt. Es ist eine altersgerechte Vorbereitung auf das reguläre Tennis. Play & Stayist eine weltweite Initiative derInternational Tennis Federation{ITF} und bildet das Fundament des Kinder- und JugendkonzeptsTalentinos. Die Kinder lernen Tennis auf Plätzen und mit Schlägern unterschiedlicher Größe sowie mitStage-Tennisbällen, unterschiedlich farbigen Bällen, die altersgerecht springen und ein technisch sauberes Spiel ermöglichen.[94] Racketlonist ein Kombinationsturnier, das aus den vier Disziplinen Tennis,Tischtennis,BadmintonundSquashbesteht. Es treten jeweils dieselben Spieler (Einzel oder Doppel) in allen vier Disziplinen gegeneinander an. Aspekte desPickleballsähneln Tennis, aber Pickleball hat spezifische Regeln, Schläger und Spielfeldgrößen. Der Platz ist kleiner als ein Tennisplatz und die kombinierte Länge und Breite des Padels darf 61,0 cm nicht überschreiten. Die Bälle müssen aus einem haltbaren Formmaterial mit glatter Oberfläche bestehen und zwischen 26 und 40 gleichmäßig verteilte kreisförmige Löcher aufweisen. Rollstuhltenniswird im Wesentlichen nach den bekannten Regeln ausgetragen, mit der Ausnahme, dass die Spieler den Ball vor dem Schlag zweimal aufspringen lassen dürfen. Die Sportart ist seit 1992 reguläre Disziplin derParalympischen Spiele. Auch bei den vierGrand-Slam-Turnieren existieren inzwischen eigene Wettbewerbe für Damen und Herren im Einzel und Doppel. Spieler, die ihre Arme nur eingeschränkt bewegen können, treten in einem eigenen Wettbewerb (sogenannte „Quad Singles“ und „Quad Doubles“) gegeneinander an. Quad-Spieler befestigen die Schläger häufig mit einemKlebebandan ihrer Hand, um den Funktionsverlust auszugleichen, bestimmte Spieler dürfenelektrisch betriebene Rollstühlebenutzen. BeimBlindentenniskönnen blinde, sehbehinderte und sehende Spieler gegeneinander antreten. Der Platz ist verkürzt, die Markierungen sindhaptischspürbar abgeklebt. Gespielt wird mit verkürzten Schlägern und geräuschentwickelnden Softbällen. Je nach Sehbehinderung darf der Ball ein- bis dreimal aufspringen. Die Regeln beimGehörlosentennisentsprechen den allgemeinen ITF-Regeln. Allerdings ist bei Wettkämpfen im Gehörlosensport das Tragen von Hörhilfen verboten. Es muss eine klare Kommunikation zwischen Spielern, Trainern und Offiziellen sichergestellt werden. Um sich für spezifische Wettbewerbe zu qualifizieren, müssen sie auf ihrem besseren Ohr einen Hörverlust von mindestens 55 dB haben. Alle vier Jahre werden die ‚Deaflympics‘ (englischdeaf‚taub‘) ausgetragen – das vomInternational Olympic Committee(IOC) anerkannte olympische Äquivalent für Sport von Gehörlosen. Para-Standing-Tennis(PST) ist eine Abwandlung für behinderte Tennisspieler, die ohne Rollstuhl spielen. Die Einteilung erfolgt in vier Klassen: PST 1 – Einseitig Arm amputiert oder ähnliche Einschränkung; PST 2 – Verlust eines Beines unterhalb des Kniegelenks oder halbseitige Lähmung; PST3 – Verlust des kompletten Beines, Einschränkung an beiden Armen oder Beinen, starke Lähmung; PST 4 – Kleinwüchsige.[95] Zielgruppen sind geistig beeinträchtigte Tennisspieler und -spielerinnen mit einemIQim Bereich von 50 bis 75. Es gelten die üblichen Regeln, Materialien und Feldgrößen, wobei diese im Training und Wettkampf durch die Play&Stay-Kampagne angepasst werden können.[96]Vom 26. bis 28. Januar 2024 fanden die PWIIAustralian OpenChampionships statt. Ein Tennisturnier, bei dem Menschen mit geistigen Behinderungen antreten. Als erste Deutsche überhaupt errangSophia Schmidtim Doppel mit der BrittinAnna McBridedie Goldmedaille.[97] Tennis wird mit anderen Sportarten kombiniert, etwa Tennis und Golf. Hierfür gibt es keine festen Regeln. Die Kombinationswertungen werden jeweils vom Ausrichter festgelegt.[98] Touchtennis ist eine Variante, die Tennis auf selbstgestalteten Plätzen auf allen Bodenbelägen (Asphalt, Kies, Parkett, Rasen) im Hinterhof oder auf dem Parkhausdach ermöglichen soll. Hierfür werden Linien mit Kreide auf den Boden gemalt und ein zusammenklappbares Netz aufgespannt, das eine Höhe von 80 cm hat. Die Spielfeldgröße entspricht meist einem Feld für Pickleball. Die Touchtennisbälle bestehen aus Schaumstoff und sind mit einem Durchmesser von 8 cm.größer und schwerer als normale Tennisbälle. Es gelten die No-let-Regel, die No-ad-Regel und Sätze bis vier Spielen.[99]Wenn der Schläger beim Schlag die Hand verlässt, zählt dieser Schlag, sofern der Schläger nicht über das Netz zur Seite des Gegners fliegt und das Netz nicht berührt, solange der Ball noch im Spiel ist. Fußballtennisist eine von vielen Abwandlungen des klassischenFußballsund hat nur mit sehr wenigen Eigenheiten des Tennis zu tun. DieInternational Tennis Federation(ITF) hat eine neueE-Sport-Initiative ins Leben gerufen, die World Tennis eChampionship, die mittels des mobilen SpielsTennis Clashgestartet ist. Die seit 2021 laufende Partnerschaft zwischen der ITF und denWildlife Studioshat denBillie Jean King Cupauf Mobilgeräte transferiert. Im Oktober 2023 hatten bereits 97.000 Spieler an über 500 Turnieren in verschiedenen Ländern, darunter den Vereinigten Staaten, Frankreich und Italien teilgenommen.[100][101]Daneben gibt es Computerspiele wieTopSpin 2K25,Top Spin 4oder vonNintendoMario TennisoderTennis. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Geschichte 2Etymologie 3Verbreitung 3.1Internationale Wettbewerbe 3.2Nationale Tennis-Verbände 3.2.1Deutschland 3.2.2Österreich 3.2.3Schweiz 3.2.4Weitere Länder 4Spielvarianten 4.1Fast4 Tennis 4.2Ultimate Tennis Showdown 4.3Tie Break Tens 5Regeln 5.1Spielfeld 5.2Spielprinzip 5.3Zählweise 5.3.1Varianten der Zählweise 5.3.2Geschichte der Zählweise 5.4Schiedsrichter 5.5Verhaltenskodex 5.6Zeitregeln 5.7Coaching 5.7.1Frühere Coachingregeln 6Ausrüstung 6.1Schläger 6.2Besaitung 6.3Bälle 6.4Bekleidung 7Laufvarianten 8Schlagtechnik"
  },
  {
    "label": 0,
    "text": "Theater – Wikipedia Theater Inhaltsverzeichnis Traditionelle Sparten des Theaters Kunstform Theater Geschichte des Theaters im Westen Moderner Theaterbau im Westen Aufbau eines heutigen Theaters Theater und Ökonomie Theaterlandschaft Siehe auch Literatur Weblinks Einzelnachweise Organisatorischer Bereich Künstlerischer Bereich Technischer Bereich Öffentliche Trägerschaft Gastspieltheater Privattheater Theater im deutschen Sprachraum Theater in anderen Ländern Theater(vonaltgriechischτὸ θέατρονthéatron‚Schaustätte‘, ‚Theater‘; von θεᾶσθαιtheasthai‚anschauen‘) ist die Bezeichnung für eine szenischeDarstellungzwischenSchauspielern, die ihre Rollen auf der Bühne in einem fiktiven Raum, in einer fiktiven Zeit, über einen fiktiven Inhalt hinweg vor demPublikumverhandeln. Mit dem WortTheaterkann dasGebäudegemeint sein, in dem Theater gespielt wird (sieheTheaterarchitektur), oder der Prozess desTheater-Spielensoder allgemein eine Gruppe von Menschen, die Theater machen, also eineTheatergruppe.[1] Es gibt vier klassische Sparten des Theaters: Überschneidungen der hier genannten Sparten sind in der Geschichte des Theaters der Regelfall. Eine Trennung vollzog sich erst spät, etwa im 19. Jahrhundert. In den jeweiligen Sparten sind unterschiedlich ausgebildete und qualifizierte Künstler tätig. Als Minimalformel von Theater kann gelten: A spielt (B) und C schaut zu (und beide haben ein Bewusstsein von ihrenRollenals Spieler und Zuschauer). Zum Theater gehört ein Publikum. Aufgrund der kollektiven Rezeption und des Live-Charakters von Aufführungen (wegen des transitorischen Elements also) steht Theater in besonderer Nähe zur (realen) Gesellschaft: Es erzählt von Menschen und vom Leben. Sprachliche Formulierungen, die Figuren und die Gesten der Schauspieler unterliegen hierbei einer ständigen Anpassung an den gesellschaftlichen Kontext.[2] Frühe Formen des Theaters entwickelten sich schon in der Frühzeit der Zivilisation in Form vonTänzeninSteinzeitkulturen. Mit dem „Theatron“, demZuschauerraum, wurde einerseits die Möglichkeit zu Diskussionen der griechischenDemokratieermöglicht, aber auch die religiösen Feste, vor allem dieDionysien, abgehalten. Die Abhandlungen desAristotelesbegründeten auch dieTheaterwissenschaft, vor allem verlangte er die Einheit von Handlung, Ort und Zeit im Drama. Das athenischeDionysostheaterwurde zumPrototypdes Theaters und in die griechischen Kolonien im ganzenMittelmeerraumexportiert. Es besaß neben dem Zuschauerraum eine Bühne, dieskené, auf der die danach benannte Szene dargestellt wurde (auf in die skené gehängten Bildern). In Tragödien, der ersten Form desDramasseit spätestens 534 v. Chr., und Komödien seit etwa 480 v. Chr., wurde in den „Großen Dionysien“ der GottDionysosverehrt. Das heitere Nachspiel der Tragödien bildete einSatyrspiel[3]. Moderne Stadttheater, Landesbühnen und Staatstheater sind meistens eigens errichtete Bauten und werden als architektonische Glanzbauten verstanden. Besonders herausgefordert werden die Architekten durch den Zwang, raffinierte ästhetische Vorstellungen und praktische Erfordernisse miteinander vereinen zu müssen, bis hin zu dem profanen Umstand, dass ein Theater vor allem auch von der Akustik im Inneren her hohen Ansprüchen genügen und zudem gegen Außenlärm so gut wie schalldicht sein soll. Zum modernen Theaterbetrieb gehören aufwändige Licht- und Tonanlagen (beides in der Regel computergesteuert), wofür nur geschultes Fachpersonal einzusetzen ist. Die großen Häuser haben ausreichend große Seitenbühnen, auf denen die Kulissen undRequisitenfür die verschiedenen Szenen des laufenden Stückes „versteckt“ werden können. DieHinterbühnewird heutzutage in modernen Inszenierungen als willkommene Möglichkeit gesehen, große Tiefe der Spielfläche zu erreichen. DieOberbühne, derSchnürboden(„Bühnen-Himmel“), ist allein schon wegen des Eisernen Vorhangs mindestens ebenso hoch wie die sichtbare Bühne selbst. Dort oben hängt, was beim Umbau der Bühne von einer Szene zur anderen an Vorhängen, Bühnenbildern u. a. mit Hilfe von Seilzügen herabgelassen werden kann. In Bühnennähe findet man die Künstler-Garderoben sowie Handmagazine für den Tagesbedarf an Requisiten und Dekorationen. InOpernhäusernund Mehrspartenhäusern sitzt zwischen der ersten Sitzreihe und der Bühnenrampe das Orchester imOrchestergraben, der bei Musicals, Opern und Operetten meist abgesenkt ist. Große Häuser habenDrehbühnenund auch Teile des Bühnenbodens, die hydraulisch versenkt werden können. Weil der Zuschauerraum während des Spiels dunkel, die Bühne aber aus Richtung der Zuschauer mit Scheinwerfern ausgeleuchtet ist, sind die Akteure auf der Bühne „geblendet“. Sie sehen das Publikum nicht. Sie spielen gegen die „vierte Wand“, die durch die Helligkeit der En-face-Beleuchtung errichtet wird. Viele Neubauten wenden sich ab von der traditionellenGuckkastenbühnehin zur Raumbühne,Arenabühneund Rundumbühne, um eine andere Zuschauer-Darsteller-Zuordnung zu erreichen (die so neu allerdings nicht ist, denkt man etwa an dieShakespeare-Bühne, das „Globe-Theater“). Damit das Geschehen eine direkte Verbindung zum Publikum hat, gehen Bühne und Zuschauerraum architektonisch ineinander über. Es gibt einen (immer mal wieder aufflammenden) Disput unter Theatermachern darüber, ob es nicht dem Wesen des Theaters widerspricht, die Zuschauer rund um eine Bühne zu setzen. Kritiker dieser aus ihrer Sicht nur scheinbar modernen Praxis meinen, dabei entstünde kein (Bühnen-)„Raum“ – das Geheimnis des „Dahinter“, der imaginären anderen, nicht sichtbaren Räume ginge verloren. Es fehle der Zuschauerfokus auf das Geschehen, die Phantasie über das „Dahinter“ werde beschnitten. Man werde zum Beobachter der jeweiligen anderen Besucher, was die Beobachteten an der völligen Konzentration hindere, sie negativ beeinflusse und die Rezeption des Bühnengeschehens mit allen Sinnen unmöglich mache. Beobachtet man das aktuelle Theatergeschehen, stellt man fest: Auch ansonsten frei und unkonventionell arbeitende Theatermacher greifen, was die Zuordnung Bühne / Zuschauerraum angeht, eher wieder zur „alten“ Praxis zurück. In einem Theater arbeiten Menschen zusammen, die sehr unterschiedliche Berufe haben. Vor allem in den Theatern in öffentlicher Trägerschaft, aber auch in den meisten mit diesen vergleichbaren größeren Privattheatern wird arbeitsteilig „produziert“. Dort dürfen Schauspieler beispielsweise keine technischen Arbeiten verrichten. Von der guten Kooperation der künstlerisch und nichtkünstlerisch Beschäftigten hängt der Erfolg der Theater-Produktionen ab. Theater in öffentlicher Trägerschaft werden in der Regel künstlerisch vomIntendanten(von der Intendantin) geleitet. Intendanten von Stadttheatern werden zum Beispiel (für eine bestimmte Zeit) vom Rat der Kommune gewählt. Meistens ist ein (beamteter) Verwaltungsdirektor zur Seite gestellt. Die Intendanten-Verträge legen den Aufgabenbereich fest. Dazu gehören Einzelheiten, zum Beispiel ob und wie oft Intendanten selbst im eigenen Hauseinszenieren, ob und wie viele auswärtige Regie-Arbeiten diese übernehmen dürfen. Es werden auch Rahmenbedingungen festgeschrieben, so die Zahl der (Neu-)Inszenierungen (in den Sparten und Genres) pro Spielzeit und vieles mehr. In enger Zusammenarbeit mit demDramaturgenwird für ein bis zwei Jahre im Voraus der komplette Spielplan erstellt. Er ist Grundlage für alle Dispositionen bis hin zum Lösen bisheriger Verträge und zu (Neu-)Verpflichtungen im Bereich künstlerisches Personal. DiePresse-undÖffentlichkeitsarbeitist für die Medien und andere Ansprechpartner verantwortlich. Sie gibt Pressemitteilungen heraus, steuert die Werbung (Plakate,Flyer,PostkartenundMonatsleporello). In vielen Häusern gibt es Spezialisten für die Zusammenarbeit mit Besucherringen, Schulen, mit dem jungen Publikum und anderen Zielgruppen. Die Arbeit der Marketingabteilungen der Musicalhäuser und anderer Privattheater wird als zentral für den Erfolg des Hauses angesehen. Viele Leiter von Theatern in öffentlicher Trägerschaft setzen eher auf die vermeintliche Attraktivität ihres künstlerischen Angebotes, stehen dem „Verkauf“ ihrer Produkte reserviert gegenüber und verlassen sich auf immer weniger greifende herkömmliche Mittel und Wege beim Bemühen, ihr – oder ein neues – Publikum zu erreichen. Die Verwaltung, zu der auch einePersonalabteilunggehört, plant, kontrolliert und bilanziert alle finanz- und verwaltungstechnischen Vorgänge. Der Etat der öffentlich getragenen Theater wird von den Trägern vorgegeben, inklusive der zu erzielenden Eigeneinnahmen. Dabei wird an einigen Häusern immer noch nach demkameralistischen Systemverfahren, viele Theater haben aber bereits auf dieDoppik, die aus der Industrie bekannte doppelte Buchführung umgestellt. In der Regel erhalten die Theater Budgets, die einen gewissen Spielraum beim Verwenden der Gelder zulassen, wobei etwa 85 % des Budgets für Personalausgaben gebunden sind. Für die Verwaltung fallen in der Regel etwa 9 % des Budgets an.[4] Im Theater arbeiten vieleKünstlerund Personen auf und hinter der Bühne: Die meisten Theater haben eigenetechnischeAbteilungen, unterteilt in: Von den Technischen Abteilungen wird im Theater großeKunstfertigkeit, Erfindungsreichtum,Flexibilitätund Verständnis für künstlerische Prozesse verlangt. In vielen kleineren und sogenannten Freien undOff-Theaternbeschränkt sich der Technische Bereich oft auf ein Minimum. Es gibt inDeutschlandrund 140 Theater in öffentlicher Trägerschaft. Diese Häuser werden mit Mitteln aus Landes- und Kommunalhaushalten unterstützt. Die Einnahmen durch Kartenverkauf (Eigenanteil) belaufen sich in diesen Theatern durchschnittlich auf rund 20 Prozent des Gesamtetats. EineTheaterkartein Deutschland wird im Durchschnitt mit 95,74 Euro gestützt. Trotz knapper öffentlicher Haushalte halten die Subventionsgeber weitgehend an der öffentlichen Finanzierung der Theater fest; zudem sind sie vor allem beim nichtkünstlerischen Personal als Arbeitgeber an Tarifverträge gebunden. Da aber dennoch dieSubventionenin den letzten Jahren eingefroren oder auch gekürzt wurden, suchen die Theater nach anderen Quellen:Mäzene, Sponsoren, Fördervereine und Stiftungen (Kulturstiftung des Bundes). Weiterhin sind in den neuen Ländern viele Theater in Haustarifverträgen, in denen die Gehälter gekürzt sind. Somit finanzieren die Mitarbeiter der Theater ihr Theater selbst mit. Nicht jede Stadt kann ein eigenes Theater, womöglich mit festem Ensemble, unterhalten. Eine Alternative für solche Kommunen, die aber Wert auf ein breites kulturelles und sogar künstlerisches Angebot legen, sind Gastspiele: In manchen Städten gibt es feste Theater, in anderen Kulturhäuser, Saalbauten, Kongresszentren oder andere Spielstätten mit den entsprechenden technischen Einrichtungen, die für gute Aufführungen notwendig sind. Es gibt auch Kommunen mit eigenem Theater-Ensemble, die dennoch in anderen Spielstätten Auftritte von Tournee-Veranstaltern undTheaterproduzentenermöglichen, wobei in diesen Fällen ein breiteres Spektrum von Programmen präsentiert werden kann. Deutsche Städte mit Spielstätten, aber ohne eigenes Ensemble haben sich zurInteressengemeinschaft der Städte mit Theatergastspielenzusammengeschlossen. Der Vorteil des Tourneetheater-Modells aus Sicht der gastgebenden Kommune: Es ist eine Vielfalt an unterschiedlichen Aufführungen möglich, da zahlreiche Tournee-Theater und Theaterproduzenten Produktionen anbieten, ohne dass die Dauerkosten eines Theater-Betriebes anfallen. Abgesehen davon machen Theater mit eigenem festen Ensemble auch zahlreiche Gastspiele, vor allem dieLandestheater, aber auch andere Bühnen. Dabei haben sich in der Regel viele feste Partnerschaften ergeben. Deutschlands größte Gastspieltheater sind dasLandestheater Detmoldund dieLandesbühnen Sachsen. Neben den rund 150 öffentlich getragenen Theatern (Stadttheater, Staatstheater und Landesbühnen) gibt es in Deutschland etwa 220Privattheater. Das sind Theater unterschiedlicher Größe, künstlerischer Ausrichtung, Provenienz und Tradition. Rund 80 dieser Privattheater sind im Deutschen Bühnenverein organisiert (Beispiele:Altes Schauspielhausin Stuttgart;Ohnsorg-Theaterin Hamburg;Komödie am Kurfürstendammin Berlin;Millowitsch-Theaterin Köln;Komödie im Bayerischen Hofin München, dasGrenzlandtheater Aachen). Es gibt kaum etwas, was für alle Privattheater gleichermaßen gilt, sei es in künstlerischer oder in organisatorisch-verwaltungstechnischer oder in finanzieller Hinsicht. Die Privattheater, die größere Prozentsätze ihrer Finanzmittel aus Eigeneinnahmen erwirtschaften müssen als die öffentlich getragenen Häuser, sind nicht an die Tarife des öffentlichen Dienstes gebunden. Das Bild der Theaterlandschaft in Deutschland wird wesentlich durch die rund 140 öffentlich getragenen Theater bestimmt, also durch Stadttheater, Staatstheater und Landesbühnen. Hinzu kommen rund 220 Privattheater und ca. 70 Festspiele, rund 150 Theater- und Spielstätten ohne festes Ensemble und um die 100 Tournee- und Gastspielbühnen ohne festes Haus. Darüber hinaus gibt es noch eine unübersehbare Anzahl freier Gruppen.[6]Die meisten der heutigen Stadttheater entstanden auf private Initiative und wurden auch zunächst als Privattheater geführt. Noch zu Beginn des 20. Jahrhunderts gab es nur 16 Stadttheater in kommunaler Verantwortung, aber es gab 360 Privattheater. InÖsterreichkonzentriert sich das Geschehen auf die Bundestheater(Staatsoper und Volksoper, Burg- und Akademietheater, inklusive Nebenbühnen), die großen Wiener Privattheater, die Vereinigten Bühnen Wien, das Theater der Jugend sowie die Länderbühnen und Stadttheater. Die dortigen Aufführungen wurden 2012/13 von 3,59 Mio. Zuschauern und Zuschauerinnen besucht.[7]Zudem gibt es noch einige private Amateurtheaterbühnen. InLiechtensteinzählen das Theater am Kirchplatz(TaK)inSchaanmit 295 Plätzen, die Nebenspielstätte im TaKino mit 100 Plätzen[8]und die Kleinkunstbühne inVaduzzu den meistbesuchten Theaterbühnen.[9] Für die Schweiz gibt es keine genauen Zahlen, in Bern, Basel, Zürich und Genf gibt es jedoch eine reiche Theatertradition.[10] Am Broadway gibt es rund 40 Privattheater, wobei diese sich überwiegend auf Musicals spezialisiert haben. In Paris gibt es 208 Theater und Cabarets.[11] Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Traditionelle Sparten des Theaters 2Kunstform Theater 3Geschichte des Theaters im Westen 4Moderner Theaterbau im Westen 5Aufbau eines heutigen Theaters 5.1Organisatorischer Bereich 5.2Künstlerischer Bereich 5.3Technischer Bereich 6Theater und Ökonomie 6.1Öffentliche Trägerschaft 6.2Gastspieltheater 6.3Privattheater 7Theaterlandschaft 7.1Theater im deutschen Sprachraum 7.2Theater in anderen Ländern 8Siehe auch 9Literatur 10Weblinks 11Einzelnachweise Afrikaans Alemannisch አማርኛ Aragonés अंगिका العربية مصرى অসমীয়া Asturianu Azərbaycanca تۆرکجه Башҡортса"
  },
  {
    "label": 0,
    "text": "Tourismus – Wikipedia Tourismus Inhaltsverzeichnis Etymologie und Abgrenzungen Merkmale Allgemeines Segmente des Tourismus Geschichte Zukunft Wirtschaftliche Bedeutung Tourismusvermarktung Strukturen und Organisation Tourismusberufe und Ausbildung Tourismusforschung Das Wortfeld „Fremdenverkehr“ in der Kritik Rechtliche Stellung der Touristen Tourist Siehe auch Literatur Weblinks Einzelnachweise Entwicklung des Reisens und der Urlaubsgestaltung Gesellschaftliche Bedeutung Kulturelle Auswirkungen Auswirkungen auf Umwelt und Natur Fairer Handel im Tourismus Trends Aussichten Probleme Weltweit Tourismusländer Deutschland Österreich Schweiz Südtirol Weitere europäische Länder Tourismusstatistiken International Europa National Regional und lokal Tourismusmessen Deutsche Touristen Tourismus in Deutschland DerTourismus(auchTouristikoderFremdenverkehr) ist die temporäreOrtsveränderungdurchReisenvonPersoneninDestinationen, die sich außerhalb ihres üblichenWohn-oderArbeitsortsbefinden. Die reisenden Personen werdenTouristengenannt. Das Lehnwort Tourismus stammt aus „kreisförmige Bewegung, Spaziergang, Ausflug, Reise“ (französischle tour), zum Verb für „drehen, umdrehen, wenden“ (französischtourner), das wiederum aus „runden“ (lateinischtornare) entlehnt ist.[1]Mit „runden, wenden“ ist die einer Reise immanente Rückkehr gemeint. Zunächst tauchte das Wort „Tourist“ auf, erstmals um 1800 im Englischen, 1816 im Französischen und um 1830 im Deutschen.[2]Der Begriff „Tourismus“ erschien in Deutschland erstmals häufiger nach dem Zweiten Weltkrieg und ersetzte zunehmend den Begriff „Fremdenverkehr“.[3]Die französischen Wörtertourismeundtouristewurden als offizielle Bezeichnungen erstmals vomVölkerbundverwendet, um Reisende zu beschreiben, die mehr als 24 Stunden imAuslandverbringen. Der Völkerbund hatte Französisch als Verkehrssprache. Der Tourismus umfasst heute nicht nur grenzüberschreitende Reisen, sondern auch denBinnentourismus. Touristische Reisen dienen sowohl derErholungundEntspannung(Erholungsurlaub) als auch derBildung(Bildungsurlaub,Kulturtourismus) undWellness. Darüber hinaus werden auchDienst-undGeschäftsreisenvon derReisebrancheals Tourismus angesehen, deren Zweck weitgehend zurArbeitszeitzu zählen ist. Deshalb findet Tourismus nicht nur in derFreizeit(Urlaub) der Touristen statt. Auch die „United Nations Conference on International Travel and Tourism“ fasste 1963 für statistische Zwecke „Geschäft“ und „Konferenz“ zum Tourismus.[4]Bedingung war ein mindestens 24 Stunden dauernder Aufenthalt in der Destination, unterhalb von 24 Stunden hießen die Reisen „Ausflug“. Beiden gemeinsam ist, dass die temporäre Reise mit einer Rückfahrt in das Herkunftsland enden muss.[5]Die Vorgängerin derEurostat(SAEG) setzte 1991 voraus, dass Tourismus eine vorübergehende Ortsveränderung außerhalb des gewöhnlichen Aufenthaltsortes zur Folge hat und dieser Aufenthalt nicht entlohnt wird. Den BegriffFremdenverkehrdefinierten 1942 die SchweizerWalter Hunzikerund Kurt Krapf als die „Beziehungen und Erscheinungen, die sich aus der Reise und dem Aufenthalt Ortsfremder ergeben, sofern daraus keine dauernde Niederlassung entsteht und damit keineErwerbstätigkeitverbunden ist“.[6]Das Wort selbst dürfte auf Louise Otto-Peters zurückgehen, die inRecht der Frauen auf Erwerb(1866) den starken Fremdenverkehr in Dresden erwähnte.[7]Der Wortbestandteil „fremd“ ist heute eher negativ konnotiert, weswegen überwiegend von Tourismus gesprochen wird, auch wenn er noch inFremdenverkehrsamt,Fremdenverkehrsbeitrag,Fremdenverkehrsgemeindeund anderen Zusammensetzungen vorkommt. Zum Tourismus zählen mehrereWirtschaftszweige, wie z. B.Personentransportunternehmen,Reisebüros,HotellerieundGastgewerbeoder Freizeitwirtschaft. Tourismus wird in verschiedene Kategorien untergeordnet, z. B. mit welchemTransportmittelman reist oder um welche Art von Reisen es sich handelt (Safari,Erholungsurlaubetc.). Als wirtschaftliche Grundlage des Tourismus gelten im Wesentlichen dieKulturgüterund dieNaturdes Reiseortes.[8]Aber selbst die gegenwärtige oder ehemalige Staatsform eines Landes können für den Tourismus entscheidend sein. So bringt zum Beispiel die Faszination derbritischen Königsfamiliejedes Jahr Millionen von Touristen nach Großbritannien und damit der Volkswirtschaft jährlich rund 600 Millionen Euro. In Zentraleuropa ist die Familie Habsburg zu nennen. Nach Einschätzung dürfte die Marke Habsburg allein für Wien für Tourismus-Umsätze von 60 Mio. Euro im Jahr sorgen.[9] Die Branche zählt weltweit zu den größten Wirtschaftszweigen. 2004 wurden nach Angaben derWelttourismusorganisationin diesem Bereich Erlöse von etwa 623 MilliardenUS-Dollarerzielt. Mit weltweit rund 100 Millionen Beschäftigten gilt der Tourismus als einer der bedeutendstenArbeitgeber. Grenzüberschreitende Reisen machen 25 bis 30 Prozent des Welthandels in diesemDienstleistungs­bereich aus. Auswertungen und Trends zum Thema liefert dieTourismusstatistik. Etwa 8 Prozent derglobalen Treibhausgasemissionenentfallen auf den globalen Tourismus.[10] Tourismuskann folgendermaßen definiert werden: Die in einem bestimmten Ort oder Gebiet durch den Zustrom von Zugereisten oder wenigstens nicht dort Ansässigen (Freizeitreisenden, Geschäftsreisenden, Verwandten- und Bekanntenbesuchern, Eigentümern bzw. Mietern von Wochenendhäusern und Zweitwohnungen)[11][12]entstehende wirtschaftliche und gesellschaftliche Veränderung und die daraus dort und anderswo resultierende Industrie oder Tätigkeit. Aus beruflichen Gründen täglich in einen anderen Ort fahrende Unternehmer oder Arbeitskräfte (Pendler) werden hier nicht erfasst. „Touristen sind Personen, die zu Orten außerhalb ihres gewöhnlichen Umfeldes reisen und sich dort für nicht mehr als ein Jahr aufhalten aus Freizeit- oder geschäftlichen Motiven, die nicht mit der Ausübung einer bezahlten Aktivität am besuchten Ort verbunden sind.“ Das Bildungswesen bleibt bei diesen Definitionen weitgehend ausgeklammert. Befindet sich einStudent, der aus seinem Wohnort für zehn Monate in ein Studentenheim einer Universitätsstadt zieht, dort aus „geschäftlichen Motiven“? Wird diese Frage bejaht, so lassen seine 300 Nächtigungen in diesem Heim ohne Weiteres in die Tourismusergebnisse der Universitätsstadt aufnehmen. In der praktischen Anwendung der Definitionen bestehen in Europa unterschiedliche Vorgangsweisen, soweit eine amtlicheTourismusstatistiküberhaupt geführt wird. Für einen erweiterten Begriff vonTourismus- und Freizeitwirtschaftwird der nicht-touristische Freizeitkonsum der Ortsansässigen am Wohnort hinzugerechnet.[11]DiesesvolkswirtschaftlicheKonzept erfordert nicht mehr, gleiches Verhalten (etwa Kinobesuch, Baden, Schifahren) in der Skalierung der jeweiligen Ortsansässigkeit (einer Stadt, einer Region, eines Staates) getrennt zu betrachten und mehrfach zu erheben. Damit zerfällt Tourismus- und Freizeitwirtschaftliche Rechnung in drei Bilanzen,Incoming(in eine Region Einreisende, von außen eingebrachte Dienstleistungen),OutgoingundBinnentourismus(Freizeit und Tourismuswirtschaft der Bewohner der Region). So lassen sich soziologisch-geographisch etwa typische Tourismusregionen (hohe Wertschöpfung, hoher Incoming Tourismus) oder „lebendige“ Regionen (hoher Binnenanteil) feststellen. Unter den Begriff Tourismus fallen unterschiedliche Reisearten und -formen. Diese lassen sich nach unterschiedlichen Kriterien klassifizieren, wobei sich demographische von verhaltensorientierten Kriterien unterscheiden lassen. Üblich sind Klassifikationen z. B. nach Motivation (z. B.Kultur- oder Bildungstourismus,Filmtourismus,naturnaher Tourismus, Sporttourismus etc.), Dauer, Organisationsform (Individual-/Veranstalterreisen), Teilnehmerzahl (Massen-/Exklusivtourismus), Zielort, Entfernung, Transportmittel, Ökobilanz (Sanfter Tourismus), Herkunft der Touristen (Ausländer-/Binnentourismus), Unterkunftsart, Alter, Familienstand und Reisezeit.[13]Als „schwarzer Tourismus“ oder Thanatourismus werden Besichtigungen von Gedenkstätten oder historisch relevanten Tatorten, wieAuschwitz,Ground ZeroundVerdun, bezeichnet. Das Phänomen findet besonders in derTiefenpsychologie,Konfliktstudienund denKulturwissenschaftenBeachtung.[14][15] Darüber hinaus gibt es noch Bezeichnungen für Tourismus-Zwecke, die in der Presse und in der Umgangssprache zu finden sind, die aber in der Tourismus-Branche selbst und der Werbung meist keine Verwendung finden, so die BezeichnungenSextourismus, Sauftourismus (englisch:alcotourism)[16],Ballermann-Tourismus, Party-Tourismus etc. Je nach der Anzahl der zu einer bestimmten Destination reisenden Touristen unterscheidet man zwischenIndividual-undMassentourismus. Je nach Reisedauer gibt esTagesausflüge(1 Tag),Städtereisen(Kurzurlaubsreisen; 2 bis 4 Tage) oder Erholungsreisen (5 Tage und mehr). Eine Reise muss statistisch fünf Tage dauern, um in der Reisestatistik erfasst zu werden. Die durchschnittliche Reisedauer der Deutschen betrug 2019 12,3 Tage, wobei der Trend zu einer Verkürzung der Reisedauer führt.[17] Waren es anfangs praktische Gründe wie die Suche nach Nahrungsplätzen oder Wasser oder die Flucht vor Naturkatastrophen, die Menschen zu Reisenden werden ließen, so änderten sich die Gründe nach ihremSesshaftwerden. Schon imalten Ägyptenund in anderenHochkulturenauf allen Kontinenten gab (und gibt) es Fahrten bzw. Reisen aus religiösen Gründen:Wallfahrtenzu den Tempeln der Gottheiten, so zum Beispiel dieHadschgenanntenPilger-Reisen frommerMuslimenachMekkaoder die Treffen vonHinduszum rituellen Bad imGanges. Weitere Reiseanlässe waren derFernhandel, Erkundungsfahrten über den „eigenen Horizont“ sowie die eigene Umgebung hinaus und die wirtschaftlichen und machtpolitischen Beziehungen zuKolonienund anderen abhängigen Gebieten. ReicheRömerbesaßen Güter inProvinzendesrömischen Reiches, die sie von Zeit zu Zeit besuchten. Die „Nordmänner“ bereistenGrönlandundAmerika, dieAraberden gesamtenIndischen Ozean. Nicht selten wurden damals Reisen von den „Bereisten“ als Aggression oder Krieg verstanden. Europa erholte sich nach derVölkerwanderung(Reisemotiv: bessere Lebensbedingungen) nur langsam von seinem wirtschaftlichen und politischen Niedergang (während zum Beispiel inChinaundJapanstabile Verhältnisse herrschten). Bald entwickelte sich in Europa reger Wallfahrtstourismus. Entlang solcher Pilgerwege und an verkehrsgeografisch begünstigten Orten (Häfen, Kreuzungen von Handelsrouten) entstanden in allen Kontinenten Handelszentren, die wiederum Handelsreisende hervorbrachten. Seewege entwickelten sich zu Reisewegen, hier seien, was Europa betrifft, die SeerepublikVenedigsowiePortugalundSpanienals frühe Kolonialmächte erwähnt. Die „Serenissima“ hatte regelmäßige Schiffsverbindung mitKonstantinopel,Marco Poloreiste, soweit seinen Angaben glaubhaft sind, auf dem Landweg nach China. Der moderne Tourismus kann auf dieGrand Tourzurückgeführt werden, die eine traditionelle Reise durchEuropawar. Im Jahre 1624 begann der junge Prinz von Polen,Ladislaus Sigismund Wasa, der älteste Sohn vonSigismund III., eine Reise durch ganz Europa.[18]Er reiste durch Territorien Deutschlands, Belgiens, der Niederlande, wo er dieBelagerung von Bredadurch spanische Truppen bewunderte, Frankreich, Schweiz nach Italien, Österreich und Tschechien.[18]Es war eine pädagogische Reise[19]und eines der Ergebnisse war die Einführung der italienischen Oper in der RepublikPolen-Litauen.[20] Christliche Pilgerwaren bis ins 19. Jahrhundert auf Kost und Logis in kirchlichen Herbergen angewiesen, da die meisten von ihnen arm waren. Selbstbestimmt zu reisen war in Europa bis in die 1950er Jahre dem kleinen Teil der Bevölkerung vorbehalten, der die teuren Reisen bezahlen konnte. Insbesondere Reisen zu Bildungszwecken waren lange Zeit Privileg des Adels, der seine Söhne aufKavaliersreisenschickte, sowie später des gehobenen Bürgertums. Erholungsreisen waren unbekannt. Diese kamen in Europa erst im 19. Jahrhundert auf. Dierasche Ausbreitung des Schienennetzesund dieIndustrielle Revolutionerleichterten das Reisen. Während Reisen vor der Industriellen Revolution meist einen bestimmten Zweck erfüllen sollten, wurde nun zunehmend das Reisen selbst zum Zweck. Die Geschichte des Tourismus ist mit der Geschichte des Reisens größtenteils identisch. Allerdings gab und gibt es in der Entwicklung starke regionale Unterschiede. DerAlpinismus, der Ende des 18. Jahrhunderts auf dem europäischenKontinentstärker einsetzte, brachte bescheidenen „Fremdenverkehr“ zunächst in derSchweiz, im 19. Jahrhundert inÖsterreich(am 28. Juli 1800: Erstbesteigung desGroßglockners, dann 1856: Besuch von KaiserFranz Joseph I.mit seiner GattinElisabethder Franz-Josefs-Höhe), um die Wende zum 20. Jahrhundert in Küstenorten wieBinz,Heiligendamm,Heringsdorf,Nizza,GradoundOpatija. Es waren zumeist europäische Bergsteiger, die lohnende Ziele in anderen Kontinenten fanden: Berge, zu deren Besteigung die Einheimischen, wie zuvor in Europa, keinen Anlass sahen.Bädertourismusschied, von rituellen Waschungen abgesehen, in vielen anderen Kulturen aus religiösen Gründen aus. Begründer des internationalen „Erlebnistourismus“ in Europa waren die Briten:Thomas Cook(1808–1892) gilt als der Erfinder derPauschalreise. In den letzten Jahrzehnten des 19. Jahrhunderts waren die oberen Gesellschaftsschichten desUnited Kingdomso wohlhabend, dass sie sich Reisen in weit entfernte, für den Tourismus noch kaum erschlossene Gebiete leisten konnten. Die militärische Macht desBritish Empire(mit Stützpunkten in allen Kontinenten) und diebritische Flotteboten dazu die erwünschte Sicherheit. Das britische Beispiel wurde in Kontinentaleuropa bald nachgeahmt. 1891 startete der deutsche GeschäftsmannAlbert BallinvonHamburgaus ins Mittelmeer mit dem SchiffAugusta Victoria. Das weltweit ersteKreuzfahrtschiffwar die 1901 gebautePrinzessin Victoria Luise. Dies war der Beginn derKreuzfahrtreisen.[21] DasRecht auf Urlaub(Urlaubsanspruch) ist in Europa und Nordamerika etwa seit 1880 bekannt, konnte aber, soweit es sich nicht um unbezahlten Urlaub handelte, sondern um freie Tage, in denen der Gehaltsanspruch weiter läuft, auf breiter Basis erst im 20. Jahrhundert durchgesetzt werden. Nach § 24 derMenschenrechtskonventiongibt es das Recht auf Erholung. Sogar dieUdSSRhatte in einer ihrer letzten Verfassungen in Artikel 41 die Förderung des Tourismus ausdrücklich erwähnt. Im deutschsprachigen Raum war im 20. Jahrhundert das organisierte Reisen desKraft-durch-Freude-Programms desNS-Staatesder erste Ansatz zumMassentourismus. Nach Kriegsbeginn wurden die KdF-Schiffe allerdings zuLazarett-Schiffen umgenutzt. Nach demZweiten Weltkriegwar es in Deutschland und Österreich zunächst schwierig, überhaupt zu reisen. Die Zonengrenzen deralliierten Besatzungszonenwaren für die Mehrheit der Bevölkerung unpassierbar. Anfang der 1950er Jahre setzte in Westdeutschland und Österreich ein Anstieg der Reisetätigkeit aller Bevölkerungsschichten ein, auch weil infolge der technischen und sozialen Entwicklung die Freizeit deutlich zunahm. In sehr großen Staaten wie den USA tritt vor allem Inlandstourismus auf, da Tausende Kilometer gereist werden kann, ohne das Land verlassen zu müssen. Deshalb besitzt die Mehrheit der US-Bürger keinen Reisepass, obwohl die Menschen teilweise überaus mobil sind. In den 1970er Jahren bremste die Ölkrise vorübergehend den Aufschwung. Dann aber führte der allgemeine wirtschaftliche Aufschwung in Europa zum neuen Phänomen desMassentourismus. In den anderen Kontinenten ist Tourismus meist nach wie vor nur für die höheren Gesellschaftsschichten finanzierbar. In vielen Ländern besitzt der Durchschnittsbürger kein Geld für touristische Reisen. Der Anstieg des Tourismus kann in den bereisten Ländern und Regionen gravierende Folgen für die einheimische Bevölkerung, für Natur und Kultur haben.[22]Für die Touristen wird dabei oft eine entsprechende Infrastruktur (Hotelanlagen, Straßen, Transportmöglichkeiten bis hin zu eigens gebauten Flughäfen) errichtet. Naturerhaltung, Kultur und traditionelle Strukturen können dabei zu kurz kommen. Andererseits kann die neugeschaffene Infrastruktur auch der einheimischen Bevölkerung zugutekommen. Tourismus entsteht oft in abgelegenen bislang landwirtschaftlich genutzten Regionen. Der Kontakt zwischen Einheimischen und Touristen kann auf Seiten der einheimischen Bevölkerung zu geänderten Konsummustern und Werthaltungen führen. Je stärker die Anpassung an die Erfordernisse der Tourismuswirtschaft erfolgt, desto eher werden lokale kulturelleTraditionennur noch als Show und Inszenierung für die Touristen weitergeführt. Der Tourismus wird so zurMonokultur, dem sich ganze Landstriche aus Profitgründen unterordnen. Der Tourismus ist damit, wie die Unterhaltungsindustrie, Teil der ökonomischenGlobalisierung, die in vielen Teilen der Welt bisher zu einer „Verwestlichung“ führt. Touristen reisen in als solche beworbene und wahrgenommene „exotische (Urlaubs-)Paradiese“ und tragen gerade dadurch mit dazu bei, dass die kulturellen Eigenheiten dieser Länder zurückgedrängt werden. Das „Fremde“ wird den Wünschen der Gäste und den Vorgaben der Reiseveranstalter angepasst und damit letztlich zur Kulisse. Dies kann durchaus alsTeufelskreisbezeichnet werden. Denn die Touristen wiederum spüren, dass die traditionelle Gastfreundschaft der Einheimischen vielerorts pragmatischem Geschäftssinn gewichen ist. Sie beklagen sich über „Touristenfallen“ und den Verlust der Ursprünglichkeit des Reiseziels. Beträchtlich sind die Schäden an Umwelt und Natur: Zu nennen ist zunächst die durch die Reisetätigkeit hervorgerufeneLuftverschmutzung. DieSchwefeldioxid- undKohlenmonoxid-Belastung kann in kleineren Tourismusorten wieDavosoderGrindelwalddas sonst nur in Großstädten übliche Niveau erreichen und überschreitet bisweilen die in den USA zulässigen Grenzwerte. Kritisiert werden vor allem die Auswirkungen von Verkehrsmittel wieAutoundFlugzeug. Nach Untersuchungen des Tourismusexperten Martin Lohmann benutzen zur Anreise insgesamt 47 % das Auto, 37 % das Flugzeug, 9 % den Bus, 5 % die Bahn und 2 % das Fahrrad oder das Schiff. Zudem verursachte der Tourismus weltweit im Jahr 2013 ca. 4,5 Mrd. TonnenklimaschädlicherKohlenstoffdioxidemissionen(CO2), was ca. 8 % der weltweiten CO2-Emissionen entspricht. Die Emissionen wachsen dabei im Vergleich zur Weltwirtschaft überproportional stark.[23] WasserundBodensind lokalenVerschmutzungenetwa durch dasÖlvon Sportbooten oder Sonnenschutzmittel Badereisender ausgesetzt. Ein zunehmendes Problem stellt der von Touristen zurückgelasseneAbfalldar. In Gebirgsgegenden etwa oder an Stränden kann dieser häufig nur mit ungleich höherem Aufwand entsorgt werden. Die allein in den österreichischenAlpenzurückgelassene Abfallmenge wird auf jährlich ca.4.500 Tonnengeschätzt, dieAbwassermengeauf90.000 Kubikmeter. AmMount Everesthaben sich Schätzungen zufolge aufgrund der jährlich bis zu 40.000 Trekker 600 Tonnen Müll in freier Natur angesammelt.[24] Weiterhin führt Tourismus zum verstärkten Verbrauch natürlicher Ressourcen: So bringt etwa die Lebensweise westlicher Touristen in vielen Reiseländern mit den notwendigenKlimaanlagen, Swimmingpools und Golfanlagen einen problematischen Anstieg desEnergie- undWasserverbrauchsmit sich. Letzterer verursacht häufig ein Absinken desGrundwasserspiegelsmit all seinen Konsequenzen für die örtliche Trinkwasserversorgung, dieBewässerungin der Landwirtschaft und dieVerödungvon Landstrichen. Schließlich beeinträchtigt Tourismus vielfach bestehende Naturräume,BiotopeundÖkosysteme, und damit die Lebensgrundlage für Tiere und Pflanzen. Teilweise ist dies auf die mit der Errichtung von Unterkünften und Ferienanlagen typischerweise verbundeneBodenversiegelungzurückzuführen. Zu nennen sind die durchRodungenfürSkipistenbedingteErosion, dieAusbeutungder Wasserreserven fürSchneekanonen,[25]die Schädigung von Wasserbiotopen durch Segler, Surfer und Taucher, sowie die Störung der einheimischen Tierwelt etwa durchMountainbiker, Langläufer undTiefschneefahrer. Rodungen für Holz-Lodges in Nepal und deren Beheizung mit Brennholz haben in Nepal unabhängig vom Skitourismus zu Erosionsproblemen geführt.[24]Anzumerken ist in diesem Zusammenhang schließlich die ästhetische Verunstaltung gewachsener Natur- und Kulturlandschaften durch touristische Infrastrukturen. Nicht vergessen werden darf allerdings, dass die ökonomischen Interessen der mächtigen und finanzstarken Tourismuswirtschaft vielfach zum Schutz und Erhalt gefährdeter Naturräume beigetragen haben. Eine intakte und ästhetisch reizvolle Umwelt ist ein werbewirksames Angebot im Tourismus. So wurden etwa Feuchtgebiete aufJamaikaund kanadische Wälder ebenso aus touristischen Erwägungen erhalten und geschützt wie afrikanische Großwildbestände oder Bauernhäuser in derToskana. In vielen Ländern hat die Natur erst durch den Tourismus einenmateriellen Wertbekommen und konnte so geschützt werden. Der Tourismus hat vielerorts vom Niedergang bedrohte Wirtschaftszweige erhalten und – wie etwa in den Westalpen – der Entvölkerung ganzer Landstriche entgegengewirkt. Auch in Zusammenhang mit derCorona-Pandemiewird ein positives Bild von Tourismus gezeichnet.[26] Mit seinen klaren Grundsätzen zur Förderung von benachteiligten Produzenten und Arbeitnehmern eröffnet derFaire Handelauch im Tourismus einen konkreten Weg für eine sozial gerechte und nachhaltige Entwicklung. Dazu hat der Arbeitskreis Tourismus und Entwicklung zusammen mit Partnern aus Süd und Nord Konzeptarbeit geleistet und anhand von Praxisbeispielen Grundlagen entwickelt, die den Aufbau des zukunftsweisenden Fairen Handels im Tourismus ermöglichen. Analog zum Fairen Handel bei Produkten hat der Faire Handel im Tourismus zum Ziel, die Lebensbedingungen von Tourismusangestellten und Kleinunternehmern zu verbessern, ihre Lebensgrundlagen zu sichern und ihnen eine würdige Existenz zu ermöglichen. Zentral für den Fairen Handel im Tourismus ist ein gerechter Austausch zwischen allen beteiligten Akteuren. Dazu sind alle Akteure gefordert, auf ihrer jeweiligen Ebene fair zu handeln, Transparenz über ihre Aktivitäten zu schaffen und im vollen Respekt von Demokratie und Partizipation gleichberechtigte, partnerschaftliche Beziehungen aufzubauen und zu pflegen. Produkte des Fairen Handels sind in der Regel durch einLabelgekennzeichnet, das Konsumenten gegenüber die Einhaltung der Fairtrade-Kriterien deklariert. Unter der Vielzahl an Labels im Tourismus zertifiziert erst ein einziges, nämlich das Gütesiegel vonFair Trade in Tourism South Africa(FTTSA), Angebote wie Hotels und Ausflüge nach den Grundsätzen des Fairen Handels. Derzeit laufen auf internationaler Ebene erste Abklärungen, ob und wie im Rahmen des für die Fairtrade-Zertifizierung weltweit maßgeblichen DachverbandesFairtrade Labelling Organizations International(FLO) der Tourismus bewertet werden kann. Ziel der Entwicklung des Fairen Handels im Tourismus ist nicht, einfach eine neue Nische zu schaffen, sondern konkret den Weg zu weisen, wie die gesamte Tourismusbranche sozial gerechter wirtschaften kann. Die Herausforderung ist dabei, einen Tourismus zu realisieren, der umfassend – ökonomisch, ökologisch und sozial – nachhaltig ist oder zur nachhaltigen Entwicklung beiträgt und den Erwartungen der Reisenden nach einem attraktiven erholsamen Urlaub ebenso nachkommt wie denjenigen der Einheimischen am Reisezielort nach neuen Einkommen, dem Respekt ihrer Lebensgrundlagen und kulturellen Vielfalt sowie ihrer Würde.[27] Im Buchungsverhalten der Gäste spielt dasInterneteine große Rolle. Viele Gäste informieren sich auf Webseiten über Kommentare von ihresgleichen über den Urlaubsort und in Frage kommende Hotels, bevor sie buchen. Die Buchungen erfolgen oft sehr viel kurzfristiger als früher. Während gedruckteReiseführerals Nachschlagewerke noch stark verbreitet sind (z. B.Baedeker,Marco Polo,MichelinundVarta), erfahren Webportale wie der freie ReiseführerWikivoyageundVirtualtouristoder Austausch- und Bewertungsplattformen wietrivago,Opodo,Expedia,TripAdvisorundHolidaycheckzunehmende Verbreitung. Buchungsportale wieHRS,Booking.com,Hotel-ami,KAYAK,Unister,Travel24.comundhotel.dewerden häufiger für Hotelbuchungen genutzt als klassische Reisebüros, welche jedoch für Gesamtpakete und personalisierte Angebote weiterhin Bedeutung haben. Auch Urlaubsaktivitäten und der Besuch von Sehenswürdigkeiten werden häufig über das Internet bestellt, beispielsweise überGetYourGuide. Das Interesse an Destinationen im zeitlichen Verlauf kann anhand der Suchbegriffe bei Google grafisch dargestellt werden. Dabei wird deutlich, dass die Suche nach passenden Urlaubsregionen ab April ansteigt und im Sommer ihren Höhepunkt erreicht. Prognosenüber die Entwicklung des Tourismus begegnen erheblich größeren Schwierigkeiten als in anderen Wirtschaftszweigen. Zum Teil hängt dies damit zusammen, dass zentrale ökonomische Begriffe im Tourismusbereich oft weniger eindeutig definiert sind. Schwieriger zu fassen ist bereits das touristischeProdukt: Nachgefragt werden von den Reisenden nämlich nicht nur materielle Leistungen wie Unterkünfte oder Transfers, sondern auch immaterielle „Attraktionen“ wie Sehenswürdigkeiten, reizvolle Landschaften, bestimmte Wetterverhältnisse, Urlaubsglück und Erholung, Stimmungen und Träume aller Art, die schwer herzustellen und zu erneuern sind und sich ökonomischer Bewertung zu entziehen scheinen. Auch der touristischeKonsumlässt sich nur schwer quantifizieren, werden doch viele von Touristen nachgefragte Waren und Dienstleistungen wie etwa Leistungen der Gastronomie und des Verkehrsbereichs auch von Einheimischen genutzt, ohne dass eine Abgrenzung möglich wäre. Auch fehlt es an zuverlässigen Methoden, denKapitaleinsatzzu berechnen. DieTourismuswissenschaftist jedoch dabei, solche Methoden zu entwickeln. Die Unschärfe der Begriffe erschwert auch die Erhebung einer verlässlichen Datenbasis. Als weitere Unwägbarkeit kommt hinzu, dass dasNachfrageverhaltender Touristen in weitaus stärkerem Maße von irrationalen, subjektiven Determinanten bestimmt wird als das anderer Marktteilnehmer. In die Entscheidung fließen oft diffuse, von Zeitströmungen, Modetrends und kulturellen Prägungen abhängige Erwartungen, Bedürfnisse und Motive ein, die schwer analysierbar sind und auch durch gezielte Produktwerbung nur in sehr begrenztem Maße manipuliert werden können. Schließlich ist das touristische Produkt weder transportier- noch lagerbar: Es muss am Ort seiner Entstehung zu einem bestimmten festgelegten Zeitpunkt konsumiert werden, der Reisende muss sich also termingerecht zum Produkt hinbegeben. Unvorhersehbare Störungen wie etwaNaturkatastrophen,Terrorismus,BürgerkriegeundStreikswirken sich daher auf den Tourismus erheblich fataler aus als auf andere Branchen. So führten z. B. dieProteste in Chile 2019zu einem Rückgang bei den Hotelbuchungen.[29] Erstaunlicherweise „erholen“ sich Zielgebiete, die von Terroranschlägen und daraufhin von Gästerückgängen betroffen waren (wie Ägypten), mitunter relativ schnell. Gefahren für das gewünschte Urlaubserlebnis werden emotional offenbar sehr rasch ausgeblendet, auch wenn sie rational noch nicht vergessen sind. All diese Probleme führen dazu, dassökonomische Theoriennur sehr allgemeine und pauschale Aussagen zur Entwicklung der Tourismusbranche treffen können, wie etwa dass die Kosten der Raumüberwindung weiter abnehmen werden, eine stärkere Diversifizierung bei Angebot und Nachfrage zu beobachten sein werde und dieUrlaubsreisen„in vielen Fällen durch mehr als ein Motiv bestimmt“ sind. Vereinzelte Versuche, durch mathematische Formeln und Modelle ein tatsächlich nicht vorhandenes Maß an Objektivität und Rationalität zu suggerieren, vermögen daran nichts zu ändern. Die Tourismuswissenschaft ist trotz aller Schwierigkeiten jedoch dabei, aussagekräftige Methoden der Marktforschung zu entwickeln. Die Schwierigkeit bei der Erstellung verlässlicher Prognosen hat immer wieder zu Fehlinvestitionen geführt. Bekanntestes Beispiel ist der 1992 eröffneteFreizeitparkDisneyland Paris, der seinen Betreibern allein in den ersten beiden Jahren fast eine Milliarde Euro Verlust eingebracht hat. Aber auch großangelegte Ferienanlagen an der Costa del Sol und in den Westalpen sowie der verstärkte Ausbau der Hotelkapazitäten in westdeutschen Großstädten in den 1970er Jahren haben sich als Fehlinvestitionen erwiesen. Künstliche Inseln in Arabien, die riesige Hotelanlagen umfassen, werden sich ebenfalls nicht kurzfristig rentieren. Zu den wichtigsten bestimmenden Zukunftsfaktoren des Ferntourismus zählen, wie die Entwicklung seit Herbst 2008 zeigt, zweifellos die weltwirtschaftliche Konjunkturentwicklung und der Erdölpreis. Die 2009 voll realisierte Wirtschaftskrise hat die Tourismusnachfrage beträchtlich gedämpft. Das erwartete Steigen desKerosinpreiseswird die Erhöhung der Flugpreise unvermeidlich machen und die Nachfrage ebenfalls dämpfen. Experten (etwa bei der Ludwig-Bölkow-Systemtechnik GmbH, einer Gründung des einstigen FlugpioniersLudwig Bölkow, oder bei der Deutschen Bank) rechnen damit, dass sich schon auf mittlere Sicht der Tourismus stärker auf den nationalen und regionalen Nahbereich konzentrieren wird. Im Flugverkehr ist mit einer Konsolidierungsphase zu rechnen, in der die Anzahl der Fluganbieter sinken wird. Galt speziell Europa traditionell als sicherste Urlaubsregion, wachsen vor allem angesichts vonislamistischen Terroranschlägendie Bedenken, insbesondere bei Gästen aus dem ostasiatischen Raum, die zu den am stärksten nachfragenden Touristengruppen gehören.[30] Der amerikanischeFuturologeHerman Kahnerstellte 1979 inThe Futuristeine Prognose für den Tourismus bis 2029. Darin rechnete er weiterhin mit überdurchschnittlichen Wachstumsraten und stabilen gesellschaftspolitischen undnationalökonomischenVoraussetzungen. Was damals fehlte, waren nicht quantifizierbare und vor allem unberechenbare menschliche Faktoren. Erste Kritik kam Anfang der 1970er Jahre angesichts derMassentourismusin Ländern wieSpanienauf: 1973 hatte das Land ebenso viele Urlauber wie Einwohner.[31] Schon in den 1980er Jahren erkannteMohamed A. TangivomUnited Nations Environment Program, was für ein verträgliches Neben- und Miteinander von Gästen und Einheimischen notwendig sein wird:[32] Bislang wurde im Grunde kein einziger Punkt realisiert, wenn von Ansätzen zur Schaffung von Naturreservaten abgesehen wird. Das bedeutet, dass die Zukunftsprobleme für den Tourismus in diesen Ansätzen zu finden sind. Einer Reihe von Empfehlungen, die eine nachhaltige Entwicklung im Tourismus und die Beziehung zwischen Tourismus und Raumordnung verbessern sollten, wurden bei einem Seminar (CEMAT) des Europarats inPalma(Spanien) im Mai 1999 erarbeitet.[33]In anderen Kontinenten werden das Geschäft einschränkende Bedenken, wie sie in Europa diskutiert werden, von den lokalenOligarchienzumeist kaum beachtet. Der Tourismus zählt weltweit zu den größtenWirtschaftszweigen. 2011 erzielte er nach Angaben derWelttourismusorganisationeinen Gesamtumsatz von etwa 1030 Milliarden US-Dollar. Er absorbierte 2004 11 % der Konsumausgaben der westlichen Industriestaaten. Mit weltweit rund 100 Millionen Beschäftigten ist er eine große Branche. Grenzüberschreitende Reisen machen 25 bis 30 % des Welthandels im Dienstleistungsbereich aus.[34]Höhere Umsätze werden allenfalls noch in der Auto- und der Mineralölindustrie erzielt. Für viele Regionen ist der Tourismus zur wichtigsten Beschäftigungsgrundlage geworden. Gleichwohl sind die Einnahmen höchst ungleich verteilt, werden doch 50 % davon in nur sieben Ländern (Vereinigte Staaten,Vereinigtes Königreich, Frankreich, Italien, Spanien, Deutschland, Österreich) erzielt. Insbesondere die USA konnten von 2008 bis 2017 ihre Einnahmen auf 203,7 Milliarden CHF verdoppeln und nahmen fast 15 % aller weltweiten touristischen Exporteinnahmen ein. Dagegen gaben die Chinesen (inklusive Hongkong und Macao) im selben Zeitraum ungefähr neunmal so viel Geld durch Reisen ins Ausland aus. Nachfolgend die weltweit wichtigsten Staaten im grenzüberschreitenden Tourismus im Jahr 2017:[35] Die wirtschaftlichen Wirkungen des Tourismus können in direkte, indirekte und induzierte Wirkungen unterteilt werden. Die direkten Wirkungen entstehen dort, wo touristische Ausgaben getätigt werden (also zum Beispiel in derHotellerieoderGastronomie). Die indirekten Wirkungen entstehen durch Vorleistungen (also zum Beispiel Bau von touristischerInfrastruktur,Lebensmittelfür die Gastronomie). Die induzierten Wirkungen entstehen durch das Ausgeben der Einnahmen, die durch die direkten und indirekten Effekte geschaffen wurden. Für nationaleVolkswirtschaftenist auch von Bedeutung, inwiefern die Einnahmen aus dem Tourismus in dem jeweiligen Land verbleiben. Durch den Import von Gütern für den touristischen Konsum (zum Beispiel Lebensmittel) oder durch Tätigkeiten von ausländischen Unternehmen entstehen Gewinnabflüsse ins Ausland (sog.Sickerrateoder Leakages). Beschäftigung. DieInternationale Arbeitsorganisation(ILO) geht davon aus, dass eine Stelle im touristischen Kerngeschäft, anderthalb weitere Stellen schafft. Damit schafft die Tourismusindustrie (direkt und indirekt) über 230 Millionen Stellen. Dies stellt etwa 8 % der weltweiten Arbeitskraft dar. Zwischen 60 % und 70 % der Arbeitskräfte sind Frauen und mehr als die Hälfte sind unter 25 Jahre alt. Nach einer Studie des Instituts der deutschen Wirtschaft (IW) sichern 15 deutsche Touristen je einen Arbeitsplatz in ihren Reiseländern.[36] Auch wenn es Rationalisierungsbestrebungen gibt, bleibt der Tourismus ein arbeitsintensiver Sektor, der vor allem in Entwicklungsländern wertvolle Beschäftigungsmöglichkeiten für Niedrigqualifizierte mit sich bringt. Die Entwicklung des Tourismus erfolgt über die wirtschaftliche Entwicklung, den Umweltschutz und die Wahrung der Identität der lokalen Bevölkerung. Eine enge Verbindung lässt sich auch zwischen der Entwicklung des Tourismus und der Entwicklung des kulturellen Erbes herstellen: Der Tourismus schafft nicht nur Einkommen und Beschäftigung, sondern trägt auch zur Entwicklung einer lokalen und regionalen Identität bei. Der Tourismus bietet Beschäftigung und Einkommen für Personen, die in entwicklungsschwachen Regionen leben.[37] Die im grenzüberschreitenden Reiseverkehr meistbesuchten Länder sind laut einer Studie derWelttourismusorganisation(Ankünftevon Übernachtungsgästen pro Jahr): Der Tagestourismus (Reisen ohne Übernachtung am Zielort) ist in diesen Zahlen ebenso wenig berücksichtigt wie der Binnenreiseverkehr innerhalb des jeweiligen Landes. Dieser ist für viele Länder (darunter Deutschland) bedeutender als der internationale Reiseverkehr.[43] 2010 unternahmen Deutsche 63,3 Millionen Urlaubsreisen (Zweck: Erholung; Mindestdauer: fünf Übernachtungen), von denen 33 % insInlandführten. Dabei buchten sie 1,4 Milliarden Übernachtungen und gaben 120 Milliarden Euro aus. Im Ausland beliefen sich die Ausgaben deutscher Touristen im Jahr 2010 auf 59 Milliarden Euro.[44] 48,7 Millionen Deutsche über 14 Jahre haben an mindestens einer Urlaubsreise teilgenommen, was einer Reiseintensität von 75,1 % entspricht. Das beliebteste Reiseziel hierbei war Deutschland selbst mit 33,0 %, wovon anteilmäßig 6 Prozent allein aufMecklenburg-Vorpommernund 5,9 Prozentpunkte aufBayernentfielen.[45]Es folgten Spanien mit 13,0 %, Italien mit 7,7 %, dieTürkeimit 7,0 % und Österreich mit 5,2 %.[46] Die Tourismusanalyse der Stiftung für Zukunftsfragen – eine Initiative von British American Tobacco zeigt, dass auch 2021 mit 55,5 % noch immer Deutschland selbst das beliebteste Reiseziel ist. Auch die Urlaubsregionen Bayern und Mecklenburg-Vorpommern sind mit jeweils 9,2 % und 7,8 % beliebt wie eh und je. Innerhalb Europas haben Spanien (7,5 %) und Italien (7,2 %) die Gunst der Deutschen. Die Türkei (3,3 %) hat dagegen aufgrund von Terroranschlägen und politischen Unruhen an Zuspruch verloren und rangiert mit Österreich (3,3 %) jetzt gleichauf. Auch Kroatien (2,5 %) konnte in diesem Jahr im Zeitvergleich deutlich weniger Touristen anlocken. Skandinavien (3,7 %) wird dafür für deutsche Touristen immer attraktiver und auch Griechenland (3,8 %) befindet sich unter den Top-10-Reisezielen.[47] Reiseweltmeister. Lange wurden die Deutschen alsReiseweltmeisterbezeichnet, weil sie mehr Geld für Reisen ausgaben als alle andere Nationen.[48]Dieser Titel wird ihnen in letzter Zeit allerdings von den Chinesen streitig gemacht.[49] Reisekriterien. Ein gutes und faires Preis-Leistungs-Verhältnis ist dabei für mehr als drei Viertel aller Deutschen eine Grundvoraussetzung. Sonne, gesundes Klima und schöne Natur sind zudem deutlich wichtiger als materielle Qualitätsmerkmale wie etwa abwechslungsreiche Abendunterhaltung, gute Einkaufsmöglichkeiten, Wellness-, Aktiv- oder Kulturangebote.[50]Deutsche Urlauber schätzen bei ihren Reisen gemütliche Atmosphäre, Gastfreundschaft und Harmonie mit den Reisepartnern. Altersgruppen. Ruheständler und Jungsenioren werden dabei für die Tourismusbranche immer wichtiger. Verreisten 2004 nur 44 Prozent aller Ruheständler, sind es 2014 bereits fast 50 Prozent.[51]Die ältere Generation ist damit genauso reiselustig wie junge Erwachsene im Alter zwischen 18 und 24 Jahren und sogar öfter als Singles im mittleren Alter. Darüber hinaus steigt auch die Anzahl der aktiven Jungsenioren zwischen 50 und 64. In dieser Zielgruppe gibt es mittlerweile mehr Reisende als bei den Familien. Reiseart. Bei 48 % der Reisen erfolgte die An- und Rückreise mit demPKW, bei 36 % mit demFlugzeug, bei 8 % mit demBusund bei 5 % mit derEisenbahn. Hauptreiseländer mit demPKWsind Dänemark, Italien, Kroatien, Österreich, die Schweiz und Ungarn. Die Reisedauer betrug durchschnittlich 12,3 Tage, die Kosten 861 Euro pro Person.[52] In Deutschland erzielte der Fremdenverkehr 2012 mit 2,8 Millionen direkt Beschäftigten einen Umsatz von 140 Milliarden Euro. 125,3 Millionen Gäste (101,5 Mio. aus dem Inland, 23,5 Mio. aus dem Ausland) tätigten 351,4 Mio. Übernachtungen (davon 298,5 Mio. durch Inländer und 52,9 Mio. durch Ausländer) in 54.166 Unterkünften mit etwa 2,6 Mio. Betten.[53] Das wichtigste Herkunftsland ist Deutschland (113.139.484 Ankünfte 2010). Aus dem Ausland ergibt sich folgende Reihenfolge: 4.000 der 12.431GemeindenDeutschlands sind in Tourismusverbänden organisiert, 310 davon sind alsHeilbäderoderKurorteanerkannt. 2007 wurden in Österreich 31,1 Millionen Gäste und 121,4 Millionen Nächtigungen (im Vergleich zu 2006: gesamt: +1,6 %, davon: Ausländer +1,3 %, Inländer +2,7 %) registriert. Acht der zwölf nächtigungsstärksten Quellmärkte wiesen 2007 ein Plus auf (in Klammern der Anteil an den Gesamtnächtigungen in Österreich): 2007 entfielen die Übernachtungen vor allem auf die BundesländerTirol(41,8 Millionen),Salzburg(23,4 Millionen),Kärnten(12,8 Millionen),Steiermark(10,0 Millionen) undWien(9,7 Millionen). Die Österreicher bevorzugten als Reiseziele im Inland die Bundesländer Steiermark (6,4 Millionen), Salzburg (5,5 Millionen) und Kärnten (4,7 Millionen). Die durchschnittliche Aufenthaltsdauer lag für ausländische Gäste bei 4,3, für Inländer bei 3,2 Nächtigungen pro Ankunft. Besonders in den wirtschaftlich schwachen Bergregionen der Schweiz ist der Tourismus ein bedeutender Wirtschaftsfaktor. InGraubündenund imWallisbeträgt der Anteil am Bruttoinlandprodukt (BIP) bis zu 30 Prozent, schweizweit waren es im Jahr 2015 2,6 Prozent und etwa 200.000 Beschäftigte. 2016 schrieben 65,4 % aller Gastbetriebe Verlust; und auch die Tourismusbilanz war erstmals seit langem negativ mit 300 Mio. Schweizerfranken, nachdem sie 2011 noch einen Gewinn von über 3 Mia ausgewiesen hatte.[55][56] Die beliebtesten Ferienregionen sind Graubünden, das Wallis, dieBerner Alpenund dasTessin. Daneben verzeichnen auch Städte wieLuzern,Zürich,GenfundLausanneviele Besucher, wozu oft auch Geschäftsreisende und Kulturliebhaber gehören. 2011 zählte die Schweiz zählte 35.486.256 Logiernächte in der Hotellerie, dies ist im Vergleich zu 2010 ein Rückgang von 2,0 %.[57]1990 wurde mit 37,5 Millionen Logiernächten ein Rekordwert erreicht. Von 2007 bis 2016 haben die Destinationen Basel, Zürich, Waadt und Berner Oberland zugelegt, alle andern Regionen haben Gäste verloren. Die Logiernächte verteilten sich in den Jahren 2014 und 2017 wie folgt:[58][59] Das wichtigste Herkunftsland war die Schweiz selbst mit 16.920.000 Logiernächten im Jahr 2017, das entspricht einer Zunahme von 4,2 % gegenüber 2016 und einem Plus von 8,7 % gegenüber 2007. Bei den ausländischen Gästen haben in den letzten zehn Jahren alle westeuropäischen Länder und Japan deutlich abgenommen. Mehr Gäste kamen dagegen aus China, den Golfstaaten, Korea, Indien und auch aus den USA.[59]Die nachfragestärksten Nationen waren folgende (Ankünfte 2013 – Logiernächte 2017):[57][59] Die durchschnittliche Aufenthaltsdauer in Hotels lag 2011 für ausländische Gäste bei 2,3 Nächten, für Inländer bei 2,0.[57][60] Der Tourismus in der Schweiz war seit Jahren rückläufig – insbesondere aus dem westeuropäischen Raum –, obwohl er mit staatlichen Geldern unterstützt wurde. So kritisierte der Hotelunternehmer und ehemalige PolitikerPeter Bodenmannaus Brig verfehlte Werbestrategien und mangelndes Unternehmertum der Tourismusverantwortlichen. Die Aufhebung des Euro-Mindestkurses in der Schweiz im Januar 2015 hatte die negative Entwicklung noch verschärft. Politische Probleme waren auch der nicht realisierte Freihandel und der damit verbundene fehlende Wettbewerb.[61] 2010 wurden in Südtirol ca. 5,7 Millionen Gäste und 28 Millionen Nächtigungen (bei ca. 500.000 Einwohnern) gezählt. Über den Tourismus in anderen europäischen Ländern geben entsprechende Länderartikel nähere Auskunft: Albanien|Belarus|Belgien|Bosnien und Herzegowina|Bulgarien|Dänemark|Deutschland|Estland|Finnland|Frankreich|Griechenland|Irland|Island|Italien|Kasachstan|Kosovo|Kroatien|Lettland|Litauen|Luxemburg|Malta|Moldau|Montenegro|Niederlande|Nordmazedonien|Norwegen|Österreich|Polen|Portugal|Rumänien|Russland|Schweden|Schweiz|Serbien|Slowakei|Slowenien|Spanien|Tschechien|Türkei|Ukraine|Ungarn|Vereinigtes Königreich Tourismus nach Staat in:Afrika|Asien|Australien und Ozeanien|Europa|Nord- und Südamerika Die amtlichen Tourismusstatistiken dienen dazu, die Entwicklung des Tourismus zu beobachten und darüber alle Interessierten aktuell und objektiv zu informieren. Diese Statistiken werden nach den Vorschriften und Usancen des jeweiligen Staates erstellt und können daher von sehr unterschiedlicher Qualität sein. So werden zum Beispiel in Deutschland, Österreich und der Schweiz auf Grund gesetzlicher oder verordnungsmäßiger Verpflichtung der Beherbergungsbetriebe von diesen monatlich die Summen derAnkünfteund Nächtigungen von Gästen (nach Herkunftsländern der Gäste gegliedert) gemeldet. Außerdem wird der Bestand an Beherbergungsbetrieben sowie deren Zimmer- und Bettenanzahl erhoben. In Großbritannien und Irland, wo (auch für Einheimische) keine der mitteleuropäischen Rechtslage entsprechende Verpflichtung, seinen Wohnsitz oder vorübergehenden Aufenthalt zu melden, besteht, entstehen die Tourismusresultate aus Zählungen ankommender Gäste auf Flughäfen und in Häfen und aus Stichprobenerhebungen in der Hotellerie. In den Zahlen können daher hier auch Besuche bei Freunden und Verwandten (VFRs – Visits of Friends and Relatives) inkludiert sein, die in Deutschland nicht erhoben werden. Bei internationalen Vergleichen für Regionen und Städte[62]ist außerdem das Gebiet zu berücksichtigen, für das die Zahlen publiziert werden. Bei internationalen Vergleichen der Beherbergungskapazität ist zu berücksichtigen, was im jeweiligen Staat unter einem Beherbergungsbetrieb oder unter gewerblicher Beherbergung (im Unterschied zu Privatzimmern) verstanden wird. In Deutschland werden zwei zentrale Statistiken erstellt: Klassischerweise begann das Tourismusmarketing mit dem örtlichen „Fremdenverkehrsbüro“ (später „Tourist Info“), die sich aus „Ortsverschönerungsvereinen“ oder Interessengemeinschaften (Hoteliers, Bergführerverbände) schon um die Jahrhundertwende und besonders in den 1920ern entwickelten, in der Wiederaufbauzeit Europas und Internationalisierung des Reisens als Besucherlenkung, sowie seit den 1960ern als Werbung in den klassischen Medien. In jüngeren Jahren wird die Tourismusvermarktung auch zunehmend Anliegen der staatlichenWirtschaftsförderungund ist eng mitRaumordnungundStandortvermarktungverbunden. Die meisten Staaten haben eigene Dienststellen für Angelegenheiten des Tourismus, und vermarkten ihren Landesnamen alsMarkeundDestinationselbst. Das wichtigste Kommunikationsmedium mit potentiellen oder tatsächlichen Gästen sind die entsprechendenWebportaleder Institutionen, der Tourismusbetriebe und der Dachverbände. Neben Information über den Ort und die Region und über aktuelle Umstände (etwa Wetter, Schneelage, Badeseetemperatur, Veranstaltungen und ähnliches) wird auf diesen Websites oft auch die Möglichkeit geboten, Angeboteonlinezubuchen. Vermarkter besitzen bei Bedarf, um bei Buchungen alle Gästewünsche erfüllen zu können, einen gewerblichenReisebürobetrieb. Informationen über Vermarktungsstrukturen und -aktionen finden sich oft auf denB2B-Webseiten der Organisationen, die die Tourismusvermarktung betreiben.[66] Offizielle Tourismuswerbeorganisationen werden neuerdings in der Branche alsDestinationsmarketingorganisationenoderDestinationsmanagementorganisationen (DMO)bezeichnet. Jost Krippendorfdefiniert: „Marketingim Fremdenverkehr ist die systematische und koordinierte Ausrichtung der Unternehmenspolitik von Fremdenverkehrsbetrieben sowie der privaten und staatlichen Fremdenverkehrspolitik der lokalen, regionalen, nationalen und internationalen Ebene zur bestmöglichen Befriedigung der Bedürfnisse bestimmterKonsumentengruppenunter Erzielung eines angemessenen Gewinns“ Paul Berneckererkannte, dass der Tourismus durch seine starke wirtschaftliche Verflechtung mit großteils kapitalintensiver Struktur angesichts der immer kürzer werdenden Amortisationsfristen zur Steuerung seiner wirtschaftlichen Umwelt gezwungen wird. Das geschieht am ehesten mit Vermarktungsmethoden und -instrumenten. Destinationsmarketingorganisationen auf nationaler Ebene sind In der Schweiz gibt es neben Schweiz Tourismus alsInteressenvertretungdes Tourismus auf politischer und wirtschaftlicher Ebene denSchweizer Tourismus-Verband(STV), derLobbyingbetreibt. Zudem bringt die Basler FachorganisationArbeitskreis Tourismus und Entwicklungauf dem Reiseportal „Fair unterwegs“ die aktuellen Zahlen und Fakten aus entwicklungspolitischer Sicht zur wirtschaftlichen Bedeutung des Tourismus.[79] Daneben gibt es nationaleInteressenvertretungenwie als Dachverband der Deutschen Tourismuswirtschaft den Bundesverband der deutschen Tourismuswirtschaft (Bundesverband der Deutschen Tourismuswirtschaft– BTW) die Organisationen der Reiseveranstalter (zum BeispielUnited States Tour Operators Association– USTOA), derReisebüros(zum BeispielDeutscher Reisebüroverband– DRV), derReisevermittler, der Hoteliers (zum BeispielÖsterreichische Hoteliervereinigung– ÖHV), Autobusunternehmer (zum BeispielRing deutscher Autobusunternehmer– RDA), der Guides, der Hotelportiere (Les Clefs d’Or), der Seilbahnunternehmer, der Restaurants und der Unterhaltungsbetriebe. National koordiniert sind auch die gesetzlichen Regelungen (spezielleTourismusgesetzeund anderesWirtschafts-,Handels- sowie einschlägigesGewerberecht,VerkehrsrechtundArbeitsrecht), dieTourismuspolitik, die Belange der öffentlichen Verwaltung (etwa in Ministerien mit Kompetenzen für Tourismus, in manchen Staaten auch expliziteTourismusministerien) sowie dieTourismusförderung. Auf regionaler und lokaler Ebene sind oft Tourismusverbände (TV, früher „Fremdenverkehrsverbände“, FFV) der Gemeinden und Regionen Interessensvertretungen und Vermarktungsorganisationen; sie betreiben im Allgemeinen die meistTourist-Informationgenannten Informations- und Auskunftsstellen für Gäste am Reiseziel. Die jeweilige Institution koordiniert Angebot, Nachfrage, Zeit- und Werbepläne sowie Vermarktung des Angebots und fasst oft alle beteiligten Interessensgruppen zusammen. Vielerorts fungieren Abteilungen von Stadt- und Regionsverwaltungen als Tourismusbüro (DMO,Destination Marketing Organization), zunehmend werden diese alsöffentliches Unternehmengeführt. In den USA fungierenConvention and Visitor Bureaus(CVB) in diesem Sinn. In Europa heißen solche Organisationen meist auf EnglischTourist Board. Vereine und Verbände beruhen oft auf rechtlicher Basis (Tourismusgesetzgebung,Raumordnung,amtliche Statistik), die teils auch die Rechtsform (öffentlich-rechtlich, privatrechtlich nach dem Vereinsgesetz, privatrechtlich nach dem GesmbH-Gesetz) festlegen können. Die jeweilige Institution vertritt eineTourismusgemeindeoder eineTourismusregion. Diese beruht auf freiwilliger oder von staatlicher Seite durch die Steuerung von Finanzierungsquellen erwirkter Zusammenarbeit mehrerer benachbarter Gemeinden. Daneben gibt es auch andere touristisch orientierte private Gesellschaften und Vereine (zum BeispielDachverbändeder Tourismusakteure,Berufsverbändeder Gästeführer,Verschönerungsvereine, regionale Hoteliervereine, Direktvermarktungskooperativen). Bedeutende internationaleFachmessenfür Tourismus. Je intensiver eine DMO die von ihr definierten Quellmärkte ihres Gästeaufkommens direkt bearbeitet, umso weniger ist sie auf Messekontakte angewiesen. Für viele DMOs sind touristische Fachmessen allerdings trotz Internet eine kostengünstige Methode zur Kontaktaufnahme und -pflege mit (potentiellen) Geschäftspartnern im Ausland. Tourismus kann inDeutschlandmeist als Schwerpunkt innerhalb derBWL, im Rahmen derGeographieoder als eigenerStudiengangTourismus/Touristikbzw.Tourismusmanagementoder Tourismus-BWL studiert werden. Die Themenbereiche werden in einigen Hochschulen und Ausbildungseinrichtungen mit Lehrmodulen aus dem Freizeit- und Veranstaltungsmanagement angeboten. In mehrerenBundesländernin Deutschland gibt es auch eine umfassende und praxisorientierte Tourismusausbildung an Berufsfachschulen. Sie dauert zumeist zwei Jahre und endet mit der staatlichen Abschlussprüfung zumTouristikassistenten.[80] Durch bundeseinheitliche Ausbildungsordnungen sind die AusbildungsberufeReiseverkehrskaufmann/-fraubzw.Tourismuskauffrau/-mann(seit 2011) undKaufmann/-frau für Tourismus und Freizeitfestgelegt. Ein deutscher Abschluss auf Meisterebene ist derGeprüfte Tourismusfachwirt, der durchIHK-Prüfung erlangt wird. Relativ jung ist die wissenschaftliche Auseinandersetzung mit dem Phänomen des Tourismus. Gleichwohl hat sie in kurzer Zeit eine ganze Reihe spezialisierte Fachdisziplinen hervorgebracht, zu deren wichtigste etwaTourismusgeographie, -soziologie, -psychologie, -ökonomie und -geschichte gehören. Die Tourismuswissenschaft selbst ist ihrerseits bereits Gegenstand historischer Betrachtung und nahe verwandt mitFreizeitsoziologie. 1941 wurden gleichzeitig an der UniversitätBerndasForschungsinstitut für Fremdenverkehr(FIF) und an der HochschuleSt. GallendasSeminar für Fremdenverkehrgegründet. Die erste gemeinsame Studie vonWalter HunzikerundKurt Krapf1942 –Allgemeine Fremdenverkehrslehre– war bereits interdisziplinär aufgebaut und gilt noch immer als Standardwerk. Jost Krippendorf, der ehemalige FIF-Direktor und erste Leiter der IKAÖ, löste mit seinem BuchDie Landschaftsfresser(1975) eine ökologische Diskussion im Tourismus aus. 2007 wurde im Auftrag des österreichischenBundesministeriums für Wirtschaft und Arbeitdas Internetportaltourism-knowhow.at[81]zum Export von österreichischem Tourismus-Know-how entwickelt und im April des Jahres gestartet. Das Englische und das Französische kennen nur die Bezeichnung „tourism“ bzw. «tourisme», das Deutsche hingegen noch die ältere Bezeichnung Fremdenverkehr. Im deutschen Sprachgebrauch tauchte die Bezeichnung „Tourismus“ in den 1960er Jahren auf. Seit den 1980er Jahren wurden die Bezeichnungen vieler offizieller Fremdenverkehrsinstitutionen im deutschen Sprachraum aufTourismusumgestellt, da Gäste nicht länger als „Fremde“ bezeichnet werden sollten, weil bei der Verwendung des Begriffs „Fremder“ leicht dieKonnotation„Fremdenfeindlichkeit“ aufkommt undder Volkswirtschaft nützliche Menschennicht abgeschreckt werden sollen. Dass Reisende tatsächlich keineswegs immer bei Einheimischen willkommen sind, zeigt die um 2000 aufgetauchte WortprägungKriminaltourismus. Ebenso unwillkommen sind vielen Einheimischen Reisende, die alsNicht-EU-Inländermit einemTouristenvisumin ein Land der EU einreisen, um sich dort dauerhaftillegalaufzuhalten (und zu arbeiten). Die meisten Bestimmungen, die die rechtliche Stellung des Touristen beeinflussen, zählen zum Privatrecht (in Österreich: Zivilrecht), d. h. zu den zwischen Reisendem und Leistungserbringer vertraglich zu vereinbarenden Regeln. Zum Schutz des schwächeren Vertragspartners, des Reisenden, kommt zumeist zwingend das Konsumentenschutzrecht seines Wohnsitzlandes zur Anwendung. Konsumentenschutzorganisationen kontrollieren das „Kleingedruckte“ der Buchungen und bringen gegen unfaire Klauseln gelegentlich Verbandsklagen ein. In Europa hat dieEuropäische Unionmit folgenden Regelungen den Konsumentenschutz im Tourismus verbessert: In Deutschland und Österreich verpflichten gesetzliche Bestimmungen Reiseveranstalter, einen Fonds zu finanzieren, aus dem im Fall ihrer Zahlungsunfähigkeit der Heimtransport der Gäste beglichen werden kann. Gegen Personen, die sich auf der Durchreise befinden, kann imschweizerischenSchuldbetreibungs- und KonkursrechteinArrestbewilligt werden „für Forderungen, die ihrer Natur nach sofort zu erfüllen sind“ (Art. 271 Abs. 1 Z. 3 SchKG).[82] Touristen, die sich in unsichere Gebiete begeben, können damit rechnen, dass ihr Wohnsitzstaat ggf. an ihrer Befreiung aus Geiselnahme, ihrer medizinischen Versorgung und ihrem Heimtransport mitwirkt. Sie müssen allerdings damit rechnen, dass ihnen der Staat zumindest einen Teil der ihm dabei entstandenen Kosten zur Begleichung vorschreibt – vor allem, wenn die Gefahr, in die sie sich begeben haben, schon vorher allgemein bekannt war. Das WortTouristist nicht nur dasNomen Agentisvon Tourismus, sondern auch die unterstePreisklasseeiner europaweit geltenden Kategorie vonHotelzimmern, die von derDEHOGAübernommen wurde: Es handelt sich um eine europaweite Klassifizierung (Hotelstars Union). Sie ist drei Jahre gültig, danach erfolgt eine erneute Überprüfung nach den dann aktuellen Kriterien. Albanien|Belarus|Belgien|Bosnien und Herzegowina|Bulgarien|Dänemark|Deutschland|Estland|Finnland|Frankreich|Griechenland|Irland|Island|Italien|Kasachstan|Kosovo|Kroatien|Lettland|Litauen|Luxemburg|Malta|Moldau|Montenegro|Niederlande|Nordmazedonien|Norwegen|Österreich|Polen|Portugal|Rumänien|Russland|Schweden|Schweiz|Serbien|Slowakei|Slowenien|Spanien|Tschechien|Türkei|Ukraine|Ungarn|Vereinigtes Königreich Tourismus nach Staat in:Afrika|Asien|Australien und Ozeanien|Europa|Nord- und Südamerika Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Etymologie und Abgrenzungen 2Merkmale 3Allgemeines 4Segmente des Tourismus 5Geschichte 5.1Entwicklung des Reisens und der Urlaubsgestaltung 5.2Gesellschaftliche Bedeutung 5.3Kulturelle Auswirkungen 5.4Auswirkungen auf Umwelt und Natur 5.5Fairer Handel im Tourismus 6Zukunft 6.1Trends 6.2Aussichten 6.3Probleme 7Wirtschaftliche Bedeutung 7.1Weltweit 7.2Tourismusländer 7.3Deutschland 7.3.1Deutsche Touristen 7.3.2Tourismus in Deutschland 7.4Österreich 7.5Schweiz 7.6Südtirol 7.7Weitere europäische Länder 7.8Tourismusstatistiken 8Tourismusvermarktung 9Strukturen und Organisation 9.1International 9.2Europa 9.3National 9.4Regional und lokal"
  },
  {
    "label": 0,
    "text": "Vogel – Wikipedia Vogel Vogelist der Name folgender geographischer und astronomischer Objekte: Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden Alemannisch Беларуская Català Cebuano Čeština English Español Suomi Français עברית Italiano 日本語 한국어 Lëtzebuergesch Lietuvių Plattdüütsch Nederlands Polski Русский Slovenščina Українська 閩南語 / Bân-lâm-gí Links bearbeiten Artikel Diskussion Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Lesen Bearbeiten Quelltext bearbeiten"
  },
  {
    "label": 0,
    "text": "Wein – Wikipedia Wein Inhaltsverzeichnis Definitionen Etymologie Geschichte Weinkultur Weinbau Inhaltsstoffe Herstellung Qualitätsstufen Lagerung Genuss Wirtschaft Ausbildung Gesundheit Alkoholfreier Wein Siehe auch Dokumentarfilme Literatur Weblinks Einzelnachweise Antike Mythologie Antike und mittelalterliche Medizin Jüdische und christliche Religion Kunst- und Kulturgeschichte Literatur und Dichtung Weinfeste Weinbruderschaften und Weinkonvente Wahl der Rebflächen Rebsorten Pflegearbeiten im Weinberg Weinlese Gärung Weißwein Rotwein Schaumwein Likörwein Weinbehandlung und Stabilisierung Non-vegan Verfälschungen Temperatur Temperaturschwankungen Luftfeuchtigkeit Deutschland Schweiz Österreich Italien Weinberufe Geologische Faktoren (Bodentyp) Klima, Hydrologie (Bodenfeuchte), Geländeform und weitere Faktoren Schwefelung Wein(übermittelhochdeutschundalthochdeutschwīnauslateinischvinum) ist einalkoholisches Getränkaus demvergorenenSaft derBeerenderEdlen Weinrebe(Vitis vinifera). Wein ist einGenuss-undRauschmittel. Durch spezifischeönologischeAusbaumethodenkommt es bei der Lagerung zu zahlreichen biochemischenReifeprozessen, die sehr vielfältig sein können und auch dazu führen, dass manche Weine jahrzehntelang reifen und haltbar sind. Die häufigsten Weine sindRot-undWeißweinesowieRoséweine.Schaumwein(Sekt, Cava, Champagner) entsteht aus Wein während einer zweiten Gärung. Gering schäumende Weine werden alsPerlweinebezeichnet (Proseccofrizzante, Secco). Dabei wird in der Regel demWeindas Kohlendioxid technisch zugesetzt. Die für dieWeinherstellungbenötigten Beerenfrüchte wachsen intraubenartigen, länglichenRispenan derWeinrebe(Vitis vinifera). Sie stammen überwiegend von ihrer Unterart ab, der europäischenEdlen WeinrebeVitis viniferasubsp.vinifera. Da diese zu den nichtreblausresistentenRebenarten gehört, wird sie zum Schutz vor der Reblaus auf teilresistenteUnterlagen(Wurzeln) der wilden RebartenVitis riparia,Vitis rupestris,Vitis berlandieribzw. dereninterspezifischenKreuzungen (Hybridreben)gepfropft. Fachausdrücke zum Thema Wein werden im ArtikelWeinspracheerläutert. „Wein“ ist ein klassischesWanderwort, das im ganzenmediterranenRaum verbreitet war. Dasarabischewayn, daslateinischevinum, dasgriechischeοἶνος[oínos] bzw. *ϝοῖνος[woínos] – vonmyk.wo-no– sind miteinander verwandt, ohne dass man folgern könnte, aus welcher Sprache es ursprünglich stammt. Vermutlich kann eine ähnliche Verbindung zumgeorgischenWortღვინო[ghwino] gezogen werden. Zu beachten ist, dass nur das georgische WortGhwinoeine andere Bedeutung hat und „siedende“ bedeutet. Das hochdeutsche WortWein(ausalthochdeutschwînoderwinam) sowieengl.wine,walisischgwinundirischfíonsind alle gemeingermanisch aus lat.vinumbzw. provinzlateinischvino[1]entlehnt, währendfranz.vin(vgl. altfranzösischli vins) direkt auf das lateinische Wort zurückgeht. Dies erklärt sich durch den Umstand, dass sowohlGermanenals auchKeltenerstmals über die Römer in größerem Umfang mit Wein in Berührung kamen und somit das lateinische Wort übernahmen.[2] Über spätere Handelsbeziehungen gelangte der Begriff des Weins von den Germanen bis zu denSlawen(siehe dasrussischeWort vinó) und bis zu den Balten, wobei beispielsweise inLitauendas Wortvynasund inLettlanddas Wortvinsbekannt ist.[3] Das lateinischevinumist wahrscheinlich aus einer mediterranen oder pontischen Sprache entlehnt.[4] ImAltertumerfuhr der Weinanbau eine erhebliche Beachtung und Ausbreitung.Weinbauwurde schon seit dem 6. Jahrtausend v. Chr. inVorderasienbetrieben.Georgiensowie das heutigeArmeniengelten als die Ursprungsländer des Weines.[5][6][7][8][9][10][11][12][13] Der Wein spielte seit demAltertumals landwirtschaftliches Erzeugnis eine bedeutende Rolle, sowohl in der Wirtschaft als auch in der Medizin[14]sowie im sozialen und rituellen Leben. Insbesondere aber war und ist er einSymbolzahlreicherMythologienund Religionen. Wein war und ist ein wesentlicher Bestandteil ritueller Praktiken in verschiedenen Kulturen. Die im Weingenuss gesuchteEkstasewurde als etwas betrachtet, das Nähe zu einerGottheitschaffen kann. In der antiken Mythologie waren esOsiris(Ägypten),Dionysos(Griechenland),Bacchus(römische Mythologie) oderGilgamesch(Babylonien), die den Wein und den Weingenuss repräsentierten. In der griechischen Antike war der Wein ein Gegenstand religiöser Verehrung und Sinnbild der Kultur. Er stand im Mittelpunkt der Kulte und Mysterien des griechischen GottesDionysos. Die Bedeutung des Weines im antiken Kulturraum spiegelt sich auch in den Festen, die zu seinen Ehren abgehalten wurden: Im Dezember feierte man die Lenäen, das Fest der Weinpresse. Dabei wurde Dionysos der neue Wein geopfert. Im Februar folgten dieAnthesterien, wo der Wein der letzten Ernte gekostet wurde. Wein war zudem wichtiger Teil des griechischen und römischen Libationsopfers. Dabei wurde Wein direkt auf die darzubringenden Opfer, auf die Erde oder ins Feuer verspritzt. Die Römer verehrtenBacchusals Gott des Weines. Die Herstellung des Weines war von religiösen Normen bestimmt: Priester setzen die Tage des Erntebeginns fest. Selbst das Stutzen der Rebstöcke war eine religiöse Pflicht. Der Wein war auch ein wichtiger Bestandteil religiöser Feste im Alten Rom, so zum Beispiel beim Frauenfest derBona Dea, Göttin der weiblichen Fruchtbarkeit. NebenÖl,WasserundEssigwurde Wein aufgrund seines Alkoholgehalts seit Anbeginn seiner Entdeckung auch in derMedizinvielfach eingesetzt. Sehr früh und gerne nutze man Wein auf diesem Gebiet als Desinfektionsmittel für Wunden, einen Rauschzustand auslösend als eines der ersten Schmerzmittel und bediente sich seiner besonders im Mittelalter als Konservierungsmittel sowie zur Herstellung von einfachenArzneien,TinkturenundExtrakten. Im Allgemeinen schrieb man Wein, wie zahlreiche mittelalterliche Enzyklopädien belegen, ausgehend von der Antike, eine sogenannte „feurige Natur“ zu und sagte ihm nach verdauungsfördernd, harntreibend und reinigend auf den Körper einzuwirken.[15]Seine medizinische Anwendung im Zusammenhang von Arzneien ist sehr ausführlich unter anderem im12. Jahrhundertin den Schriften der ÄbtissinHildegard von Bingenbelegt.[16]Abgesehen von Konservierungszwecken wurde Wein in Kombination vonKräuternundMineralienim Mittelalter durch Einlegen, Bedampfen und anderen zum Teil komplexen Extraktionsverfahren zur Herstellung von einfachen Arzneimitteln eingesetzt.[16]Mit der Entdeckung verbesserterDestillationsverfahrenzur Gewinnung von reinem Alkohol wurde Wein spätestens seit derNeuzeitin der Medizin aber weitestgehend abgelöst. EinemessianischeBedeutung kommt dem Wein in der jüdischen und christlichen Religion zu. Das traditionelleKreuzderGeorgischen Orthodoxen Kirchegeht auf Ursprünge im 4. Jahrhundert zurück und stellt sich in Form einer Weinrebe dar (Weinrebenkreuz). DieBibel– woNoachals der ersteWinzergilt – macht vom Wein reichen symbolischen Gebrauch. ImBuch der Psalmendient der Wein der Lebensfreude, beiSalomoist er Arznei für Leidende, aber auch mit Vorsicht zu genießendes Rauschmittel. Das VolkIsraelwird mit einemWeinbergverglichen;Jesusbeschreibt die Verbindung zu seinen Nachfolgern wie die zwischen Rebstock undReben. Das Wirken desHeiligen Geisteswird mit gärendem neuen Wein verglichen. Wein kann verführen und auch – alsTaumelbecher– den göttlichen Zorn ausdrücken. Der Wein steht für das Fest. Er lässt den Menschen die Herrlichkeit der Schöpfung spüren. ImChristentumbildet der Wein imSakramentderEucharistiedas Element für dasBlut Christi. Neben dem Gebrauch vonMessweinwurde der Wein in derkatholischen KircheimMittelalterauch alsgeweihter Weinvielfältig alsSakramentaleverwendet.[17][18] In derjüdischen ReligiongehörtkoschererWein zu den Ritualen desKidduschamSchabbat, desPessachund derHochzeit. In der europäischenKunst-undKulturgeschichtestellt der Wein einen zentralen Motiv- und Themenkomplex mit verschiedenen Bedeutungsebenen dar. So verbindet die europäische Kultur der festlichen Tafel den Wein als Teil eines gesellschaftlichen repräsentativen Rituals mit demfestlichenEreignis.[19][20] Seit 1981 besteht in Vicenza, Italien, dieInternationale Bibliothek La Vignafür Weinkultur. Der Wein wurde zu allen Zeiten, von der Antike bis zur Gegenwart, in einem eigenen Literaturgenre, den Trinkliedern, besungen. Dem griechischen Mythos nach spendete der GottDionysosden Menschen den Wein. Er brachte einenSchlauch Weinin das Haus des Pflanzenzüchters Ikarios mit, den er in den Rebbau einweihte. DieAnakreontikist eine literarische Strömung, in der der Wein, Dionysos und das Feiern in lyrischer Form verehrt werden. Wein taucht außerdem in zahlreichen Erzählungen auf, wie zum Beispiel im griechischen Heldenepos derOdyssee.Odysseusgerät auf seiner hindernisreichen Heimfahrt vonTrojain die Höhle des einäugigen ZyklopenPolyphem. Die Lage scheint aussichtslos, doch Odysseus bietet dem Riesen Wein an, den er bei denKikonenerbeutet hatte.[21]Polyphem versinkt im Weinrausch, wird geblendet und Odysseus und seine Gefährten können sich retten.[22] Im alten Testament finden sich zahlreiche Belege für Rebbau und Weinkonsum. Gott selbst stiftete den Menschen nach der Sintflut den Weinstock undNoachbetätigte sich als Winzer. Selbst in den sorgsam bereinigten Märchen derGebrüder Grimmbringt beispielsweiseRotkäppchenseiner Großmutter „Kuchen und Wein“, wenn auch nicht als Arznei, so zumindest als Stärkung. In der Neuzeit preisen Dichter wieFriedrich Hölderlinin seiner Elegie „Brot und Wein“ den Wein als Gabe der Himmlischen. Ein musikalisches Denkmal gesetzt hat dem WeinErnst Marischkamit dem LiedDie Reblausvon 1940, das insbesondere in der Interpretation vonHans Moserbekannt geworden ist. Wein zählt zu den ältestenKulturgüternder Menschheit. Sowohl die Kunst der Weinbereitung als auch die Kultur des Weingenusses ist über Jahrtausende hinweg bis heute immer fortentwickelt worden. Die Weinkultur wird sowohl auf öffentlichen Festveranstaltungen als auch in privatenWeinprobengepflegt und ist auch der Zweck von Zusammenschlüssen unter Weingenießern. In Deutschland wird die kreative Beschäftigung von Künstlern mit dem Kulturgut Wein in der Vergabe desDeutschen Weinkulturpreisesgewürdigt. Weinfeste besitzen oft Volksfestcharakter. Sie werden in allenWeinbaugegendenEuropas (und zunehmend auch außerhalb derselben) gefeiert und dauern oft mehrere Tage. Nicht selten sind sie aus lokalen oder regionalen Festen, zum BeispielKirchweihfesten, entstanden. Hauptsächlich werden sie im Spätsommer oder Herbst gefeiert. In der Schweiz haben sie oft überregionale Bedeutung. Das größte Weinfest der Welt ist mit über 600.000 Besuchern derDürkheimer WurstmarktinBad Dürkheim. In vielen Ländern haben sich Weinliebhaber und -kenner zu Vereinen zusammengeschlossen, um den Weingenuss gemeinsam zu kultivieren. Im deutschen Sprachgebiet nennen sich diese Klubs meistWeinbruderschaftenoder Weinkonvente. Sie blicken teilweise auf eine jahrhundertelange Tradition zurück. Die Ursprünge liegen inheidnischenFruchtbarkeitsriten, die später von den christlichen Bruderschaften assimiliert und verändert wurden. Meist war dies auch mit der Verehrung vonSchutzpatronenverbunden. Waren früher ausschließlichMännerzugelassen, stehen die Zusammenschlüsse heute zunehmend auchFrauenoffen. Während sich in früherer Zeit die Mitglieder bei einem Glas Wein der Geselligkeit in der Tradition dergriechischenSymposienhingaben, organisieren sie heute auch öffentliche, kulturelle und fachliche Weinveranstaltungen. Weinbruderschaften pflegen und bewahren heute sowohl die Kultur und Geschichte des Weins als auch das Wissen um den Wein. Dazu gehört oftmals auch das Erinnern an historische Kleinodien derWeinbewertungwie demTastevin.[23] Struktur undTexturdes Bodens bestimmen maßgeblich den Stil des Weins. Kalkhaltige Böden ergeben Weine mit Finesse und einem guten Alterungspotenzial.LehmhaltigeBöden stehen für wuchtige Weine und sandige sowiekieshaltigeBöden begünstigen eine frühere Reife der Beeren. Ausschlaggebend ist dabei die Mächtigkeit der jeweiligen Bodenschicht und ein ausgeglichener Feuchtigkeitshaushalt – in niederschlagsarmen Gegenden entscheidend ist die Fähigkeit zum Speichern vorhandener Feuchtigkeit, bei hohen Niederschlagsmengen kommt einer gutenDrainagefähigkeitgroße Bedeutung zu. Im Laufe der Weinbauvergangenheit haben sich innerhalb der einzelnen Weinbaugegenden ideale Paarungen zwischen Bodentyp und Rebsorte herauskristallisiert. DerRieslinggedeiht vorzüglich auf denSchieferbödenderMosel, der roteMerlotzeigt seine Größe auf den lehmigen und kalkreichen Böden vonSaint-Émilionund derCabernet Sauvignonbenötigt zur vollen Reife die kieshaltigen Böden desMédoc. Die Rebe erbringt nur dann gute Qualität, wenn die Böden karg beziehungsweise nicht zu fruchtbar sind. Es ist die Aufgabe des Winzers, dem Boden nur so vielDüngerzuzuführen, wie von der Pflanze entnommen wird. Andernfalls steigen die Erträge auf Kosten der Qualität an. Zu den Standortfaktoren zählt neben dem Bodentyp auch die vorhandene Mikroflora, die durchTemperatur,Luftfeuchtigkeitund Licht (→Mikroklima) beeinflusst wird. In Weinbaugegenden mit kühlem Weinbauklima (WeinbauzoneA und B) kommt der Ausrichtung der Weinlage zur Sonne sowie die Nähe zu wärmespeicherndem Wasser (Flussläufe oder Seen) eine überragende Rolle zu. Dies kann insbesondere in den deutschen Weinbaugebieten vonAhr,Mosel,NaheundRheingaubeobachtet werden und erklärt die große Rolle derEinzellageim deutschenWeingesetz. Über die Jahrtausende haben sich tausende Rebsorten durch natürliche Kreuzung und folgender Auslese, oder aber durch gezielte Kreuzung einzelner oder mehrerer Sorten durch den Menschen, entwickelt. Die verschiedenen Sorten ermöglichen, besonders wenn sie für einen Standort gut geeignet sind, die Erzeugung differenzierter Weinqualitäten. Von insgesamt weltweit über 20.000 bekannten Rebsorten sind nur etwa 1000 Sorten im Rahmen der offiziellen Listen für den Weinbau zugelassen.[24] In jedem Weinbaugebiet der EU gibt eine Liste die für den Weinbau gesetzlich zugelassenenRebsortenvor. Die Liste autorisierter Sorten zur Erzeugung vonLandweinenoderTafelweinen(in der EU gesetzlich nurmehr als „Wein“ bezeichnet) ist umfangreich und enthält auchMassenträger. Die Liste der Sorten für die Erzeugung von Qualitätswein ist kleiner. Bei der Definition geschützterHerkunftsbezeichnungenwurde die Auswahl der Rebsorten innerhalb der EU stark eingeschränkt. In zahlreichen Weinbaugebieten werden Weine sortenrein ausgebaut. In einzelnen Weingebieten wieBordeaux,Châteauneuf-du-PapeoderChiantihat sich hingegen eine Tradition desVerschnittsverschiedener Sorten historisch entwickelt. Ein Weingarten bedarf einer Reihe von Pflegemaßnahmen wie Rebschnitt, Erziehung, Laubarbeiten, Traubenausdünnung, Bodenpflege und einer der Rebe (und dem Boden) angepassten organischen Humusdüngung und mineralischen Düngung (mineralische Handelsdünger). Zum Schutz gegen Krankheits- und Schädlingsbefall sind gezielte Pflanzenschutzmaßnahmen notwendig. Bei derTraubenlesekann sich der Winzer meist zwischen der manuellen und der maschinellen Durchführung entscheiden. Keine Wahl haben Winzer mit sehr kleinen Parzellen oder Steillagen. Hier bleibt dem Winzer nur die manuelle Lese. Die manuelle Lese ist erste Wahl, wenn man die Trauben möglichst unbeschädigt einbringen will: Bei gesundem, unbeschädigtem Lesegut kann die erforderliche Schwefelung des Weins stark reduziert werden. Die manuelle Lese ist im Weiteren erforderlich, wenn während des Lesevorgangs bereits eine Auslese stattfinden soll. BeiedelfaulenBeeren können bereits befallene Beeren einzeln und in mehreren Durchgängen in ausreichender Menge und in bester Güte eingeholt werden. Ein anderer Beweggrund für die Handarbeit ist, Beeren mit Stielen und Stielgerüst zu ernten. Bei einem gewissen Anteil von Stielen wird schonender gepresst, da die Stiele den entstehenden Beerenbrei auflockern, zudem kann der in den Stielen vorhandene Gerbstoff dem Wein förderlich sein. Der Vorteil der schonenden Handlese wird indes aufgehoben, wenn die gelesenen Trauben in den Erntebehältern mechanischem Druck ausgesetzt sind. In diesem Fall werden Beeren zerdrückt, und der austretende Saft kann gären. Der Einsatz desObstvollerntersist meist eine wirtschaftliche Entscheidung. In den europäischenHochlohnländernkann der Kostenanteil der Lese halbiert bis gedrittelt werden oder sich aufdrängen, wenn nicht genügend Erntehelfer zur Verfügung stehen. Ein qualitativer Vorteil der mechanischen Ernte ist, dass das Lesegut innerhalb kürzester Zeit und zeitnah zum optimalen Reifezeitpunkt eingebracht werden kann. Nicht zu unterschätzen ist der Vorteil des Vollernters, die Trauben nachts oder in den frühen Morgenstunden bei sehr kühlen Temperaturen zu ernten: Dadurch wird dem Verlust von Aromastoffen vorgebeugt und es erfolgt ein langsamerer Start der Gärung durch eine kühlere Mosttemperatur. Nachteilig ist, dass nicht jede Rebsorte gleich gut zur Lese mit der Maschine geeignet ist. Während Sorten wieChardonnayundCabernet Sauvignonsehr gut geeignet sind, kann derSpätburgundernur unter Qualitätseinbußen mit dem Vollernter eingebracht werden. Eine maschinelle Lese erfordert auch besondere Vorkehrungen im Weinkeller. Durch die hohe Ernteleistung der Maschinen werden in sehr kurzen Zeitabschnitten große Mengen an Lesegut angeliefert. Zur Erzeugung von Qualitätswein ist es jedoch wichtig, dass zwischen Lese undKelterung(im Fall von Weißwein) oderMaischung(im Fall von Rotwein) nur wenig Zeit vergehen sollte. Die Infrastruktur im Keller muss demnach die hohe Ernteleistung abbilden. In einigen Weinbaugebieten wie demBeaujolais, derChampagneund bei den Mitgliedern vonVinea Wachau Nobilis Districtusin derWachauist die maschinelle Ernte verboten. In Deutschland ist für die Mitgliedsbetriebe desVerbands Deutscher Prädikatsweingüter(VDP) ab demPrädikat Auslesedie Handlese obligatorisch. Nach der Ernte werden die Trauben auf Sortiertischen gesichtet. Dabei können Blätter sowie unreife oder faule Beeren entfernt werden. Insbesondere bei Beeren zur Erzeugung von Rotweinen müssenfauleBeeren rigoros ausgemustert werden, da derSchimmelpilzBotrytis cinereanegative Auswirkungen auf Geschmack und Farbe hat. Bei einer maschinellen Lese ist der Aufwand am Sortiertisch geringer, da beim Vollernter ein Großteil der Blätter durch Ventilatoren entfernt wird. Faule Beeren fallen durch das Rütteln derRebstöckemeist schon zu früh ab; noch unreife beziehungsweise getrocknete Beeren fallen bei einer gut eingestellten Maschine nicht vom Stock. Jeder Wein verfügt über folgende Grundbestandteile:[25] Der größte Anteil von Weiß- und Rotweintrauben wird zu Wein (und weiteren Produkten daraus) verarbeitet. Bei der Weinherstellung werden physiologisch reife Trauben verwendet. Allgemeiner Überblick zur Weinherstellung: Bei derWeißweinherstellungwerden die Trauben gepresst, anschließend wird der Saft (Traubenmost) zuWeißweinvergoren: Bei derRotweinherstellungwird dieMaischezuRotweinvergoren: Die Netto-Reaktionsgleichung lautet wie folgt: C6H12O6+2ADP+2Pi→2C2H5OH+2CO2+2ATP{\\displaystyle \\mathrm {C_{6}H_{12}O_{6}\\,+\\,2\\;ADP\\,+\\,2\\;P_{i}\\;\\rightarrow \\;2\\;C_{2}H_{5}OH\\,+\\,2\\;CO_{2}\\,+\\,2\\;ATP} } In Worten:Glucose+ 2Adenosindiphosphat+ 2Phosphatergibt 2Ethanol+ 2Kohlenstoffdioxid+ 2Adenosintriphosphat Auf der Schale der reifen Beeren befindet sich eine Vielzahl vonHefen(sogenannte „Wilde Hefen“): welche Hefe sich im Wein-Ansatz durchsetzen würde, wäre ungewiss. Die Qualität und das Endergebnis wären somit ebenfalls zu einem gewissen Grad ungewiss. Beeinflusst wird der Anteil erwünschter Hefen durch die vorhandenen Kulturen in einem gut unterhaltenen Weinkeller des Winzers. Über dieWeinpresse(im Fall von Weißwein) oder über das Umpumpen des noch gärenden Weins über den Tresterhut (im Fall des Rotweins) kann die Kellerflora in den Verlauf der Gärung eingreifen. Um dem Zufall keine Chance zu geben, entwickelte man dieReinzuchthefe. Schon in alten Weinfachbüchern ist von Hefeansätzen der Rassen „Zeltingen“, „Scharlachberg“, „Geisenheim“ oder „Burgund“ zu lesen.[28][29][30] Die verschiedenen heute erhältlichen Hefestämme wurden zum einen auf ein Einsatzgebiet hin gezüchtet und sind zum anderen frei von Verschmutzungen wie Bakterien oder Schimmelpilzen. Seit der Mitte der 1980er-Jahre gibt es Reinzuchthefen inGranulatform, diewirbelschichtgetrocknetsind. Die Qualitätswende im deutschen und internationalen Weinbau ist zum Teil auch der Verwendung dieser Hefen geschuldet. Durch die Bildung sekundärer Nebenprodukte wieEsterbeeinflusst die Hefe die Aromatik des Weins in seinem ganz jungen Stadium. Diese Aromen sind jedoch nur bedingt lagerfähig und zerfallen schnell. Die eigentlichenBukett- und Aromastoffe des Weins bilden sich später. Dabei wird das Gärbukett zum Jungweinbukett, das schon weitgehend dem jeweiligen Sortenbukett entspricht.[31] In Weingütern, die sich auf komplex strukturierte Weine spezialisiert haben, kann eine sogenannteSpontangärungdurch wilde Hefestämme erwünscht sein.[32]Spontangärung bedeutet in der Praxis, dass die Gärung ohne Zugabe von Reinzuchthefen verläuft. Ziel ist, die Vielfalt eines nicht standardisierten Weingeschmacks zu erreichen, die durch Reinzuchthefen eingeschränkt werden kann. Mit nicht beziehungsweise weniger gerichteten Hefen kann eine größere geschmackliche Bandbreite und Eigenart erreicht werden, da mehr Hefestämme an der Gärung beteiligt sind. Allerdings ist hier das Risiko oft für den Hersteller höher, da die Gärung in eine vom Menschen unerwünschte Richtung verlaufen kann (zum Beispiel hoher Gehalt an Restzucker oderflüchtiger Säure).[32] Bei der alkoholischen Gärung entstehtWärme. Hefen arbeiten in einem schmalen Temperaturband zwischen 12 und 37 °C. Früher verließ man sich darauf, dass die Klimabedingungen des Herbstes noch warm genug waren, um den Gärprozess in Gang zu bringen, gleichzeitig aber so kühl, dass die Temperatur im Gärbottich nicht über die Werte des Temperaturbandes stieg. Durch starke Belüftung des Kellers oder durch Bespritzen der Außenwand des Bottichs mit Wasser versuchte man eine Temperaturregulierung. Eine bessere Temperatursteuerung wurde erst durch den Einsatz vonEdelstahlbehälternoder durch den Einsatz vonWärmetauschernmöglich. Das Pumpen durch einen Wärmetauscher ermöglicht den Einsatz von Gärbehältern aus Holz oder Beton. Der Edelstahlbehälter kann durch eine Kühlschlange innerhalb der Außenwandung temperaturgeregelt werden, so dass der vergärende Most durch geringere Pumpaktivität schonender behandelt wird. Die Menge des erzeugten Alkohols im Wein hängt vom Zuckergehalt des Mosts (sieheMostgewicht) und somit vom Reifezustand der Beeren ab. In Gegenden mit kühlem Weinbauklima kann es in schlechten Jahrgängen zur Lese von nicht ausgereiften Trauben kommen. Um dennoch einen Wein mit ausreichend hohem Alkoholgehalt zu erzeugen, kann dem Most Zucker zugefügt werden. Diese weitverbreitete Praxis wird nach einem ihrer wichtigsten FürsprecherChaptalisationgenannt. Man gibt dem Most entweder Trockenzucker, unvergorenen Traubensaft mit hohem Zuckeranteil, die sogenannteSüßreserve, oder neuerdingsrektifiziertes Traubenmost-Konzentratbei. Der Einsatz dieser Praxis sowie die maximal zulässige Alkoholerhöhung werden durch die jeweilige nationale Weingesetzgebung geregelt. Die Anreicherung mit Zucker war ursprünglich eine Methode zur Rettung schwacher Jahrgänge, die sich jedoch zu einer gängigen Methode entwickelte. Angereicherte Weine schmecken angenehmer und reichhaltiger, da Alkohol ein ausgezeichneter Aromaträger ist. Der zu beobachtende Trend hin zu alkoholreichen Weinen lässt sich nicht nur mit derglobalen Erwärmungerklären. Eindrucksvoll lässt sich dieser Trend am Beispiel der großen Weine vonBordeauxbelegen. Die Weine, die in derKlassifikation des Jahres 1855in die Riege der führenden Güter eingestuft wurden, wären aus heutiger Sicht leichte Weine mit einem Alkoholgehalt von 11 bis 11,5 Prozent. Heute liegen die Werte um mindestens zwei Prozentpunkte höher. Die frühere Faustregel, dass ein Wein mit 12 Prozent Alkohol schon zu den mittelschweren Weinen gehört, gilt in dieser Form nicht mehr. Beeren zur Erzeugung vonWeißweinsollen von der Lese bis zumEntrappenmöglichst unbeschädigt bleiben. Bei einer Beschädigung der Beerenhaut beginnt praktisch in kleinem Umfang eine ungewollteMaischegärung. Der Most nimmt Farbe und Aroma der Beerenschale an und auch der Wein neigt zur Oxidation, weshalb Weißweine in der Regel möglichst inreduktivemZustand verarbeitet, vergoren undausgebautwerden. Die Beeren sollen nach der Lese möglichst schnell verarbeitet werden. Zur Vermeidung von Beschädigungen werden die Trauben in möglichst kleinen Kisten transportiert. Durch zu große Mengen an Trauben würden ansonsten die unten liegenden Beeren frühzeitig zerquetscht. In warmen Gebieten ist auch eine Lese während der Nacht oder in den frühen Morgenstunden qualitätsfördernd. Bei Rebsorten, die schnell zur Oxidation neigen, kann der Transport der Kisten vom Weinberg zum Weinkeller sowie das Pressen in einerinerten Gasatmosphärebewerkstelligt werden. Zuweilen werden die Trauben komplett mit den Stielen gepresst, meist werden die Trauben jedoch von den Stielen befreit (Abbeeren), da ein Großteil der im Weißwein wenig erwünschtenGerbstoffedort enthalten ist. Bei der Pressung sollte das Fruchtfleisch kühl bleiben, damit die Gärung nicht zu früh einsetzt. Moderne Kellereien verfügen daher im Kelterbereich über Kühlkammern. Die Pressen sollen beim Weißwein einerseits eine möglichst hohe Ausbeute erlauben, aber die bitteren Kerne nicht zerquetschen. In manchen Jahren kann ein kurzer Schalenkontakt bei der Gärung hilfreich sein, um dem Wein etwas mehr Extrakt zu geben. In diesem Fall limitiert sich der Kontakt jedoch auf einige wenige Stunden. Da sich der rote Farbstoff nur in der Beerenhaut befindet, werden für denRotweindie Trauben nicht gepresst, sondern lediglich ganz oder nur partiell entrappt und zerdrückt. Während der Gärung verbleiben die Schalen, Kerne sowie die beibehaltenen Stiele im Most. Dabei lösen sich diePhenoleundTannineaus den Schalen und färben den Most zunehmend ein. Damit die Farb- und Tanninausbeute ausreichend hoch ist, muss der Tresterhut regelmäßig mit dem Most vermengt werden. Der Tresterhut entsteht dadurch, dass die festen Bestandteile derMaischedurch die während der Gärung entstehende Kohlensäure an die Oberfläche gedrückt werden. Das Vermengen kann durch Umpumpen von am Boden des Gärbehälters befindlichem Most über den Hut geschehen. Wahlweise kann der Tresterhut auch über Stangen oder lange Löffel untergetaucht werden. Das manuelle Untertauchen wird auf Französischpigeagegenannt. Einfache Rotweine entstehen durch eine kurze Maischestandzeit von 2 bis 3 Tagen. Diese Zeit kann bei erstklassigen Weinen bis zu 4 Wochen betragen. Begrenzt wird die Standzeit durch die Dauer der Gärung. Eine Maischestandzeit über die Dauer der Gärung hinaus wirkt sich meist negativ aus. Über eine Temperaturregelung kann die Gärdauer und damit die Maischedauer beeinflusst werden. Bei einer kühlen Gärung können das Fruchtaroma und die Feinheit eines Weines besser herausgearbeitet werden. Eine Gärung bei höherer Temperatur begünstigt die Tiefe der Farbe und die Geschmacksintensität. Zur Konzentration von Geschmack und Aroma wurden diverse Methoden entwickelt. Bei derSaignée-Methode wird nach einigen Stunden oder wenigen Tagen ein geringer Anteil von 10 bis 20 Prozent des Mosts abgezogen und weiter zuRoséweinverarbeitet. Der verbleibende Mostanteil profitiert von einem vergleichsweise hohen Anteil von Schalen. Mit einer anderen Methode, derUmkehrosmose, kann dem Most Wasser entzogen werden. Nach erfolgter Gärung wird der Most abgelassen, geschwefelt und zum weiteren Ausbau in diversen Behältern, Holzfässern oder auchBarriquesausgebaut. DerTresterwird durch Pressen entsaftet. In gewissen Weinbaugebieten ist die durch Pressung erzielte Menge, die sogenannte Schüttung, limitiert. Grundlage für die Herstellung vonSchaumweinist einGrundweinmit einem gewissen Restzuckergehalt, der einer zweiten alkoholischen Gärung unterzogen wird. Dafür werden dem Grundwein höhervergärende Hefestämme (Saccharomyces bayanus) zugesetzt, die auch als Nachgär-, Sekt- oder Champagnerhefe bezeichnet werden.[33]Während der zweiten Gärung wird der Alkoholgehalt des Grundweins erhöht. Zudem bildet sichKohlenstoffdioxid, das in der Flüssigkeit bleibt.[34] Schaumwein kann mittels verschiedener Methoden hergestellt werden. Aus historischer und qualitativer Sicht stellt dieFlaschengärungdas klassische Verfahren dar. Dieses Verfahren ist für die Herstellung vonChampagner,Crémant,ProseccoundCavazwingend vorgeschrieben. Auch in Deutschland werden hochwertige Schaumweine wie derWinzersektzunehmend nach der sogenanntenChampagnermethodeerzeugt. Bei der Flaschengärung wird bereits vergorenem Grundwein etwa 24 g/l Zucker und Weinhefe (Fülldosage) zugefügt. Die in der Flasche stattfindende zweite alkoholische Gärung erzeugt Alkohol und Kohlenstoffdioxid. Durch die verschlossene Flasche bleibt das Kohlenstoffdioxid (etwa 12 g/l) in Lösung, woraus bei vollendeter Gärung 6 bis 8 bar CO2-Druck bei 20 °C im Wein resultieren. Durch die zweite Gärung gewinnt der Wein auch etwa 1,3 Volumenprozent Alkohol, weshalb leichte Grundweine bevorzugt werden.[34] Neben Kohlenstoffdioxid entsteht während der Flaschengärung ein Depot aus abgestorbenen Hefen. Im Kontakt mit diesem Hefelager gewinnt der Schaumwein an Qualität und Finesse. Die Verweildauer auf der Hefe während der Reifung ist ein Qualitätsfaktor. Um das Depot zu entfernen, werden die Flaschen einem mechanischen Klärprozess unterzogen, derRemuage(deutsch: Rütteln).[35]Für den Vorgang des Rüttelns werden die Flaschen in Rüttelgestelle beziehungsweise Rüttelpulte (französisch:pupitres) umgelagert. In diesen Gestellen werden die Flaschen täglich gerüttelt und leicht gedreht. Außerdem verändert man langsam die Neigung der Flasche, bis sie im Laufe mehrerer Wochen nahezu senkrecht auf dem Flaschenkopf stehen. Bei dieser manuell durchgeführten Tätigkeit sinkt das Hefedepot in den Flaschenhals. Das maschinelle Abrütteln geschieht mittels Gyropalette. Hierbei wird die komplette Palette programmgesteuert gerüttelt, geneigt und gedreht. Das Rütteln dient allein der optischen Klarheit des Schaumweins, seine Haltbarkeit oder geschmackliche Qualität werden davon nicht beeinflusst.[36] Beim Entfernen des Depots, dem Degorgieren (französisch:dégorgement), wird der Flaschenhals in eine Kühlflüssigkeit getaucht. Dadurch gefriert das Depot zu einem Pfropfen, der beim nachfolgenden Öffnen der Flasche durch das aus der Kohlensäure entstehende Kohlenstoffdioxid aus der Flasche gedrückt wird. Der bei diesem Arbeitsgang verloren gegangene Schaumwein wird durch eine Versanddosage aufgefüllt. Die Dosage besteht aus einer Mischung aus Wein und Zucker. Die Zusammenstellung und Menge der Dosage bestimmt das spätere Geschmacksbild des Schaumweins zwischen herb (französisch:brut) bis süß (französisch:doux). Nach der Dosage und der Einstellung der gesetzlich vorgesehenen Füllmenge werden die Flaschen verkorkt, agraffiert, verkapselt, etikettiert und in Versandkartons verpackt. Ab diesem Zeitpunkt gewinnt der Schaumwein nicht mehr an Qualität. Die Technik desTransvasierverfahrensähnelt in einer ersten Phase der klassischen Flaschengärung. Nach einer kurzen zweiten Gärung in der Flasche wird der vergorene Schaumwein in einen Druckbehälter überführt. Die Einstellung des Geschmacksbilds erfolgt über die direkte Dosage in den Tank. Über eine Filteranlage gelangt der Schaumwein aus dem Drucktank in die Flasche. Dadurch entfällt das aufwändige Rütteln sowie die manuelle Entfernung des Depots. Bereits im 19. Jahrhundert experimentierte man mit dem Umfüllen (französisch:transvaser) des entheften (degorgierten) Sekts in kleinere Gefäße. Das Problem des dabei auftretenden Druckverlustes konnte erst mit Drucktanks, druckstabilen Filteranlagen und Gegendruckfüllern gelöst werden. Die technischen Voraussetzungen hierfür standen allerdings erst Mitte des 20. Jahrhunderts zur Verfügung.[37] Bei der Großraumgärung (auchCharmat-Verfahrenodercuve closegenannt) erfolgt bereits die zweite Gärung in einem Drucktank. Obwohl der Gärprozess dem einer Flaschengärung ähnelt, kommen die fertigen Schaumweine mit dem Charmat-Verfahren qualitativ nicht ganz an die der Weine mit klassischer Flaschengärung heran. Die Familie der Likörweine ist groß.Portwein,Sherry,Marsala,Madeira,Commandaria,Mavrodaphne,Málaga,Moscatel de SetúbaloderVin Doux Naturelist eines gemeinsam: Durch Beimengung von hochprozentigem Alkohol wird die meist noch nicht beendete alkoholische Gärung gestoppt. Früher wurden die Weine in hauptsächlich warmen Weinbauregionen mit Alkohol stabilisiert, da die Weine häufig während des Transports durch ein erneutes Einsetzen einer unerwünschten Gärung verdarben. Aus technischer Sicht ist ein Aufspriten von Weinen nicht mehr nötig, sondern gehört vielmehr zum Stil des Likörweins. Während die Zugabe von Alkohol meist noch während der Gärung erfolgt, dient das Aufspriten beim Sherry der Stabilisierung eines Zustands nachAlterungund Verschnitt. Weine sind – wie auch Lebensmittel – (thermodynamisch) instabil. Je nach betrachteter Komponente des Weines äußert sich die Instabilität auf ihre Weise. Beispielsweise kann der im Wein befindliche Alkohol zuEssigsäureabgebaut (fermentiert) werden. Dieser Prozess benötigtEssigsäurebakterien, die sich in der Luft befinden. Man bezeichnet solche Weine auch alstote Weine. Sie schmecken dann säuerlich, dumpf, wie alte Rosinen. Man kann diesen Vorgang unterdrücken, indem man den Wein vor Luft schützt. Deshalb wird beimreduktivenAusbau des Weins darauf geachtet, dass jedesWeinfassund jeder Weintank ganz gefüllt und sicher verschlossen ist, sodass möglichst wenig Luft auf den Wein einwirken kann. Nach der Abfüllung desFassweinsübernimmt derKorkendiese Schutzfunktion. Einmal mit Luft in Kontakt gekommen, sollte der Wein bald verbraucht werden. Neben dem Verderb des Weins gibt es noch zahlreiche andere Mikroorganismen, die die Haltbarkeit des Weines negativ beeinflussen können. Die Stabilität eines Weines hängt davon ab, ob seine Inhaltsstoffe eine fördernde oder hemmende Wirkung auf Mikroorganismen haben: Je höher der (natürliche) Gehalt an Alkohol, Gerbsäure (Tanninen) und anderen Säuren (Weinsäure,Zitronensäure,Äpfelsäureund so weiter, nicht aber Essigsäure) ist, desto schlechter für die Mikroorganismen und besser für den Wein. Aber dieser natürliche Schutz reicht zumindest bei Weinen mit einer Alkoholkonzentrationen von weniger als etwa 18 Prozent nicht aus, so dass sie zusätzlich konserviert werden müssen. Schon seit dem Altertum wird Wein zur Konservierung „geschwefelt“, indemSchwefeldioxidhinzugegeben wird.[38]Diese Schwefelgabe wirkt starkantimikrobiell. Zum Schwefeln wurde elementarer Schwefel (aus Schwefelblüte) oberhalb der Flüssigkeit eines Weinfasses verbrannt. Es bildete sich dabei Schwefeldioxid. Dieses Schwefeldioxid löst sich teilweise im Wein alsSchweflige Säure, steht aber immer in einem Gleichgewicht mit freiem Schwefeldioxid (sogenannter Freier Schwefel). DieSchwefelungvon Maische, Most oder Wein soll Je höher der Gehalt an freiem Schwefel ist, desto stabiler ist der Wein. Das obige Gleichgewicht wird dabei durch einen höheren Säuregehalt nach links verschoben. Das bedeutet, dass ein säurereicher Wein mit insgesamt weniger Schwefeldioxid auskommt als ein säurearmer Wein. Wie stark ein Wein geschwefelt sein muss, hängt auch davon ab, ob er gelagert werden soll und auch von der mikrobiologischen Belastung selbst. Ein Wein, der aus Trauben gekeltert wird, die zum Zeitpunkt der Ernte schon stark von Fäulnis befallen waren, ist deutlich stärker belastet als ein Wein, der aus gesunden Trauben gekeltert wird. Werden diese Weine dann noch über weite Strecken transportiert und sind dabei auch noch größeren Temperaturschwankungen ausgesetzt (beispielsweise bei Container-Transporten), so muss der Schwefelgehalt auch dieses widerspiegeln. Heute werden Weine normalerweise durch die Zugabe von gasförmigem Schwefeldioxid ausGasflaschenoder durch Zugabe vonSalzender Schwefligen Säure geschwefelt, da sich diese Zugabe wesentlich genauer dosieren lässt. Der Ausbau von Wein im Eichenfass (französisch: „Barrique“) trägt zur Verbesserung der Haltbarkeit bei. Eine weitere Methode zur Steigerung der Haltbarkeit ist die Filtrierung vor dem Abfüllen. Hefen und Bakterien werden weitgehend ausgefiltert, ohne die weiteren Inhaltsstoffe des Weines zu beeinflussen. Dabei wird nicht nur der Gärvorgang unterbrochen, sondern auch die Haltbarkeit verbessert. Auch chemische Verfahren werden zur Haltbarmachung von Weinen eingesetzt. Insbesondere Überseeweine werden vor der Flaschenfüllung mit Kaltentkeimungsmittel (zum BeispielDimethyldicarbonat) versetzt. Diese töten in der verschlossenen Flasche alle Mikroorganismen ab und das Dimethyldicarbonat zerfällt dann zu natürlichen Bestandteilen des Weines. Um vor allem Rotwein von Schwebstoffen und ähnlichem zu klären oder filtrieren, wird traditionell etwa Gelatine aus Schweineschwarten oder getrockneteSchwimmblasen von Fischenverwendet, sowie auch Hühnereiweiß. Diese Stoffe werden „Schönungsmittel“ genannt. Sie binden die Schweb- und Gerbstoffe und werden anschließend mit diesen ausgefiltert, sind also kaum noch im Endprodukt enthalten. Dennoch gilt damit behandelter Wein als nicht mehr vegan. Einige dieser Stoffe können auch einen Fehl-Geschmack teilweise korrigieren. Diese Stoffe müssen auf dem Etikett nicht angegeben werden. Es gibt alternative Filtermittel und -methoden, die als vegan gelten.[39][40][41] Wein wird manchmalverfälscht. Weil dies früher kaum und heutzutage nur mit hohem technischem Aufwandidentifizierbarist (und auch nur wenn man danach sucht), wurden beispielsweise gerne Blüten desHolunderszur Aromatisierung von Weißwein und dessen Beeren zur Farbvertiefung von Rotwein verwendet. 1985 erschütterte derGlykolwein-Skandalden Weinabsatzmarkt. Österreichische Winzer hattenSpätlese-,Trockenbeerenauslese- undEisweinemit normalerweise hohenRestzuckergehalten, aber geringen Mengenerträgen und hohen Preisen aus billigen Massenweinen durch Zusatz vonDiethylenglykol„gepanscht“, deutsche Abfüllerverschnittenund veredelten ihre Weine illegal mit diesen preisgünstigen Süßweinen.[42]Große Nachfrage nach süßen Weinen bei niedrigsten Preisen[43]führten zur Ausweitung dieser unerlaubten Praxis, bis zufällig einem Finanzbeamten der (in der Buchhaltung ersichtliche) überhöhteFrostschutzmittel­bedarf eines Weinbauern auffiel. 1986 wurden in Italien „riesige Mengen an Billigwein“[42]mit giftigemMethanolaufgespritet (der Alkoholgehalt erhöht). Im Jahr 2000 wurden sechs Millionen FlaschenChianti-Wein(kontrollierte und garantierte HerkunftsbezeichnungDOCG, die teurer verkauft werden kann) entdeckt, die unter Zusatz von geringerwertigem Wein aus Süditalien produziert worden waren.[42]2001 wurde bekannt, dass jede zweite Flasche rumänischen Weins für den Inlandsmarkt gepanscht war.[42]2002 wurden französische für den Export nach Belgien vorgesehene Weine mit billigerem Wein gestreckt.[42] Dieweinrechtlichdefinierte Qualität eines Weins bemisst sich danach, wie er bei derAmtlichen Qualitätsweinprüfungeingeschätzt wird. Bei der sensorischen Prüfung von Wein werden auch die lateinischen BegriffeColor(Farbe),Odor(Geruch) undSapor(Geschmack) benutzt. Einfluss auf diesensorischenEigenschaften haben die Rebsorte, die Rebfläche und das Mengenverhältnis der Inhaltsstoffe, namentlich dasMostgewicht. Weitere qualitätsbestimmende Faktoren sind die Erntemenge, Behandlung des Leseguts bei der Ernte, die Gewinnung desMostsbeimKeltern, dieGärungund derAusbaudes Weins. Niederschlag findet die weinrechtlich definierte Qualität in den nach nationalem Recht festgelegtenQualitätsstufen.QualitätsweineundPrädikatsweinemüssen in Deutschland die sensorische und analytische Prüfung der Amtlichen Qualitätsweinprüfung erfolgreich bestehen, um als solche bezeichnet werden zu können. Das erfolgreiche Passieren der Amtlichen Qualitätsweinprüfung dokumentiert dieAmtliche Prüfungsnummer(AP-Nummer), die jedem Qualitäts- und Prädikatswein nach demDeutschen Weingesetzzugeteilt wird. Sie muss als obligatorische Angabe auf demEtikettdeklariert werden. Gehobene Qualitätsweine können als Prädikatswein, abhängig hauptsächlich vom Mostgewicht, eines der folgenden Prädikate erhalten: Die Qualitätsstufe bestimmt einerseits den Kostenaufwand des Erzeugers und andererseits den im Markt durchsetzbaren Preis des Weins. Trotz der Tendenz von Weinherstellern, möglichst frühzeitig trinkreife Weine zu erzeugen, erhalten zahlreiche traditionell hergestellte Qualitätsweine durch eine Nachreife während der Flaschenlagerung eine bessere Geschmacksnote. Massenware und kleinpreisige Markenweine verbessern sich durch Lagerung nicht, da sie trinkfertig abgefüllt werden. Auch viele Bordeauxweine der Klasse einesCru Bourgeoisgewinnen höchstens während einer Flaschenlagerung von fünf bis acht Jahren Charakter hinzu. Nur ausgesprochene Spitzengewächse erreichen erst nach 15 bis 20 Jahren ihren optimalen Entwicklungszeitpunkt. Der ideale Aufbewahrungsort für Wein ist ein lichtgeschützter, kühler Raum ohne größere Temperaturschwankungen und frei von Erschütterungen. Flaschen mit natürlichenKorkensollten liegend gelagert werden, damit der Korken feucht gehalten wird. Einzige Ausnahme hiervon ist derMadeirawein, der stehend gelagert werden soll. Die optimale Lagerungstemperatur von 10 bis 13 °C wird sehr selten eingehalten und meist überschritten. Der in Bezug zur optimalen Temperatur höhere Wert (typischerweise 13 bis 15 °C) bewirkt eine etwas schnellere Reifung der Weine und kann durchaus gewollt sein, wenn man sehr junge Weine mit großem Lagerpotenzial wie zum BeispielGrand-Cru-Weine aus dem Bordeaux oderJahrgangsportweinebinnen 12 bis 15 Jahren mit Genuss trinken möchte. Die Gastronomie nutzt diesen Effekt, um die Weine nicht zu lange lagern zu müssen. Die optimalen Bedingungen sind lediglich bei alten und sehr alten Gewächsen unabdingbar. Problematischer als die absolute Lagertemperatur sind sich auf die Qualität des Weines negativ auswirkende Temperaturschwankungen: Der Lagerraum sollte eine möglichst konstante Temperatur aufweisen. Durch Temperaturschwankungen entstehen Volumenänderungen des Weins in der Flasche, so dass es über den Korken zu gesteigertem Gasaustausch kommt. Je häufiger Flaschen solchen Schwankungen ausgesetzt sind, umso mehr Sauerstoff steht zurOxidationdes Weins zur Verfügung und führt zu einer beschleunigten Alterung. Die langjährigen Erfahrungen der Weinerzeuger belegen hingegen, dass jahreszeitliche Schwankungen von 5 Grad durchaus akzeptabel sind und kaum negative Auswirkungen auf den Wein zeigen. Werden Weinflaschen mit Naturkorken verschlossen, sollte dieLuftfeuchtigkeitam Aufbewahrungsort bei mindestens 60 Prozent liegen, damit der Korken nicht austrocknet. Zu hohe Luftfeuchtigkeit konnte in der Vergangenheit dazu führen, dass das Etikett schimmelte oder sich ablöste. Dies ist der Grund, warum Jahrgangsportweine und entsprechende Madeiraweine nicht mit einemEtikettversehen werden. Die relevanten Informationen werden direkt auf das Glas der Flasche aufgedruckt. Darüber hinaus versehen Spitzenweingüter den Korken mit demJahrgangund dem Namen des Weinguts. Wein wird ausWeingläserngenossen, von denen es je nach Art des Weines spezielle Formen und Größen gibt. Die unterschiedlichen Glasformen dienen zwei Zwecken: Zum Ersten soll die Gesamtform eines Glases die Entfaltung der mit der Nase aufgenommenen Aromen unterstützen. Zum Zweiten soll durch Art und Anordnung der Mündung die Kopfhaltung beim Trinken beeinflusst und die Wahrnehmung durch die Zunge gesteuert werden (Gläser für Süßweine bedingen zum Beispiel durch ihre Form eine Haltung, die beim Trinken dafür sorgt, dass die Geschmackswahrnehmung „süß“ in den Hintergrund tritt, um dem Trinkenden die Erfassung der übrigen, vorhandenen Aromen zu ermöglichen). Während Weißweine gekühlt (8 bis 12 °C) serviert werden, werdenRotweinebei 14 bis 18 °C getrunken. Zur Kühlung bei Tisch werden meist Weinkühler verwendet, vasenähnliche Behälter, die entweder durch Isolation, Eiseinlage oder durch Verdunstungskälte (poröse, genässteTerrakotta-Behälter oder ein um die Flasche gelegtes feuchtes Tuch) wirken. In der gehobenen Gastronomie ist es üblich, Weinflaschen erst am Tisch zu öffnen und den Gast zunächstdegustierenzu lassen. Einen besonders alten Rotwein, bei dem Inhaltsstoffe wieWeinsteinauskristallisiert sind, wird ein Kenner zunächst aus der Flasche in eineKaraffeumfüllen, wobei er ihndekantiert. Beim langsamen Abgießen über die Kante des Flaschenhalses bleibt eventueller Satz, dasDepot, in der Flasche zurück. Anschließend lässt man den Rotwein längere Zeit „atmen“, das heißt, man gibt den Inhaltsbestandteilen Gelegenheit, mit demLuftsauerstoffVerbindungen einzugehen. Bei sehr alten Weinen ist Vorsicht angeraten, da eine zu lange Oxidation durch Luftsauerstoff zum Verderb führen kann. Im Jahre 2006 veröffentlichte dasInstitut national de la recherche agronomiquein Paris eine Studie, nach der eine übermäßige Oxidation durch Zugabe einer Prise gewöhnlichen Speisesalzes verhindert werden kann. Weltweit wurden 2016 nach ersten Schätzungen der „Internationalen Organisation für Rebe und Wein(OIV)“ auf 7.528.000 Hektar (2012) Anbaufläche 259,5 Millionen Hektoliter Wein produziert. Die drei größten Produzenten waren Italien (48,8 Millionen Hektoliter), Frankreich (41,9 Millionen Hektoliter) und Spanien (37,8 Millionen Hektoliter).[45] Weinanbaufläche und -produktionsmenge der größten Weinerzeugerländer und deren weltweiter Anteil in % von der Gesamtfläche im Jahr 2012[46] Deutschland ist Netto-Importeur von Wein. In Deutschland wird mehr als doppelt so viel getrunken wie die Winzer hierzulande ernten. Etwas mehr als die Hälfte der weltweiten Weinerzeugung entfällt auf Europa. Deutschland rangiert etwa auf Platz 20 weit hinter China, Russland oder den Vereinigten Staaten. In den frühen 1980er-Jahren wurde rund ein Viertel mehr als heute produziert. Während der Finanzkrise 2008 beziehungsweise der Wirtschaftskrise 2009 gab es einen konjunkturbedingten Einbruch. Etwa ein Fünftel des deutschen Weins wird exportiert.[47] Die weltweite Weinproduktion entwickelte sich zwischen 2000 und 2017 wie folgt (in MillionenHektoliter): 2000: 279, 2001: 266, 2002: 257, 2003: 254, 2004: 298, 2005: 278, 2006: 283, 2007: 268, 2008: 269, 2009: k. A., 2010: 264, 2011: 268, 2012: 258, 2013: 290, 2014: 270, 2015: 277, 2016: 273, 2017: 251 (vorläufig).[48] Die Ausbildung in den Berufsfeldern Weinbau und Kellerwirtschaft kann in den angeführten deutschsprachigen Ländern in Landwirtschaftlichen Fachschulen (Weinbauschulen), Fachmittelschulen sowie Fachhochschulen und an Universitäten erfolgen. Übermäßiger Konsum von Wein kann zu körperlicher und psychischerAbhängigkeitführen sowie Erkrankungen wieLeberzirrhose, Entzündungen derBauchspeicheldrüse,Magenkrebs,Speiseröhrenkrebsund Nervenerkrankungen hervorrufen; regelmäßiger Konsum auch kleiner Mengen von Alkohol kann dasBrustkrebsrisikoerhöhen.[64][65][66] Laut einiger Studien soll Wein hingegen, wie andere alkoholische Getränke mit geringem Alkoholgehalt, in geringer Menge genossen, das Herz-Kreislauf-System positiv beeinflussen; dies ist jedoch umstritten.[66]Die dem Wein zugesprochenen positiven Wirkungen treffen einigen Studien zufolge auch auf Traubensaft zu.[67] Neuere Studien zeigen, dass Alkohol auch in kleinen Mengen schädlich ist. Die in roten Trauben enthaltenen Polyphenole können sich zwar positiv auf die Gesundheit auswirken. Aber Rotwein enthält Alkohol. Und die negativen Auswirkungen des Alkohols überwiegen. Laut WHO spielt der Alkoholkonsum bei der Entstehung von mehr als 200 Krankheiten, Verletzungen und anderen Gesundheitsproblemen eine Rolle. Alkoholfreier Wein ist eine Alternative zu herkömmlichem Wein mit Alkohol. Er ist trotz der Bezeichnungalkoholfreiungeeignet für Menschen, die aufgrund gesundheitlicher Aspekte auf Alkohol verzichten möchten oder müssen, da er immer noch bis zu 0,5 Vol.-% Alkohol enthalten kann. Alkoholfreier Wein wird hauptsächlich mitVakuumdestillationhergestellt. Bei geringen Temperaturen von ca. 28 Grad Celsius wird dem Wein unter erniedrigtem Druck der Alkohol entzogen. Die Herstellung von alkoholfreiem Wein ist in Deutschland gesetzlich geregelt. § 47 der Weinverordnung regelt explizit die Bestimmungen für alkoholfreien Wein. Danach müssen alkoholfreie Weine aus Wein durch den Entzug von Alkohol gewonnen werden. Voraussetzung für die Bezeichnungalkoholfreiist ein Alkoholgehalt von weniger als 0,5 Volumenprozent. Traubensaft oder eine Traubensaft-Mischung dürfen nicht als alkoholfreier Wein ausgegeben werden.[68] Kellerwirtschaft / Önologie Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Definitionen 2Etymologie 3Geschichte 3.1Antike Mythologie 3.2Antike und mittelalterliche Medizin 3.3Jüdische und christliche Religion 3.4Kunst- und Kulturgeschichte 3.5Literatur und Dichtung 4Weinkultur 4.1Weinfeste 4.2Weinbruderschaften und Weinkonvente 5Weinbau 5.1Wahl der Rebflächen 5.1.1Geologische Faktoren (Bodentyp) 5.1.2Klima, Hydrologie (Bodenfeuchte), Geländeform und weitere Faktoren 5.2Rebsorten 5.3Pflegearbeiten im Weinberg 5.4Weinlese 6Inhaltsstoffe 7Herstellung 7.1Gärung 7.2Weißwein 7.3Rotwein 7.4Schaumwein 7.5Likörwein 7.6Weinbehandlung und Stabilisierung 7.6.1Schwefelung 7.7Non-vegan 7.8Verfälschungen 8Qualitätsstufen 9Lagerung"
  },
  {
    "label": 0,
    "text": "Wetter – Wikipedia Wetter Inhaltsverzeichnis Begriffliche Abgrenzung Wetter in Meteorologie und Umgangssprache Elemente des Wetters und ihre Messung Faktoren des Wetters und ihre Dynamik Vorhersage des Wetters Wetter als wirtschaftlicher Faktor Einfluss des Wetters auf den Verlauf von Kriegen Physische Bedeutung Siehe auch Einzelnachweise Literatur Weblinks AlsWetter(vonalthochdeutschwetar„Wind, Wehen“) bezeichnet man den spürbaren, kurzfristigen Zustand derAtmosphäre(auch: messbarer Zustand derTroposphäre) an einem bestimmten Ort derErdoberfläche, der unter anderem alsSonnenschein,Bewölkung,Regen,Wind,HitzeoderKältein Erscheinung tritt. DieMeteorologieklassifiziert dasörtliche Wettereiner bestimmten Zeit anhand der verschiedenenPhänomenein derTroposphäre, dem unteren Teil der Atmosphäre. Den Verlauf des Wetters bestimmt die vonSonnenstrahlungund regionalerEnergiebilanzgeprägteatmosphärische Zirkulation. Physikalisch lässt sich ein Wetter durch thermodynamischeZustandsgrößenwie etwaDruck,Temperatur,Dichtebeschreiben. Ein „Wetter“ in diesem Sinne kann auch in einem Labor erzeugt werden. Darüber hinaus gibt es solche Zustände und Wetterphänomene (zum Beispiel Winde) auch auf anderenPlaneten, die eineAtmosphärehaben. Das Wetter charakterisiert den Zustand der Atmosphäre an einem bestimmten Ort und zu einem bestimmten Zeitpunkt. Kennzeichnend sind die meteorologischen Elemente Strahlung, Luftdruck, Lufttemperatur, Luftfeuchtigkeit und Wind, sowie die daraus ableitbaren Elemente Bewölkung, Niederschlag, Sichtweite etc. Das Wetter ist das augenblickliche Bild eines Vorganges (Wettergeschehen), das sich hauptsächlich in derTroposphäreabspielt. Es kann sich – im Gegensatz zur Wetterlage und Witterung – mehrmals täglich ändern. Das Wetter kann man als einSystembetrachten, das vor allem von den Elementen Temperatur, Niederschlag, Bewölkung, Wind und Luftdruck geprägt wird. Zwischen einigen der Elemente bestehen Zusammenhänge (KorrelationoderKausalität), zwischen anderen nicht. DieMeteorologenerfassen die einzelnen Elemente des Wetters mitMessgerätenund dieWetterlagemit Begriffen wie stabil oder wechselhaft,heiteroderwolkenfrei, 3/8 bewölkt, bedeckt oder trüb,Nebeltendenz, regnerisch,Regenschaueroder stürmisch. Umgangssprachlich sind sehr unscharfe Begriffe üblich: DieMeteorologieuntersucht das Wetter, quantifiziert seine einzelnen Elemente und charakterisiert sie durch eine Reihe fundamentaler sowie spezieller Größen (Wetterelemente): Diese Grundgrößen werden inWetterstationen, aufWetterschiffenundLeuchttürmen, mitWetterballonsoderRadiosonden, mitFlugzeugenundBojengemessen.Wettersatelliten, andereErdbeobachtungssatellitenundSpionagesatelliten(letztere liefern Wetterinformationen als 'Nebenprodukt') beobachten dieTroposphäreaus dem Weltall und sammeln besonders viele Informationen zurBewölkung(auch zu großflächigen Wolkensystemen), zuWellenhöhenund Wasseroberflächentemperaturen auf Meeren und zu Luftströmungen. Messinstrumentedie der Messung von Wetterelemente dienen nennt man Wettermessgerät (siehe auchWetterstation,Wetterhäuschen) bzw. danach was sie messen (z. B.Windmesser,Regenmesser,Hygrometer,Thermometer). Das Wetter findet fast ausschließlich in den unteren 10 Kilometern der irdischenLufthüllestatt, derTroposphäre. Nur hier gibt es merklicheBewölkung, weil derWasserdampfals entscheidender Faktor nicht über dieTropopause(je nach Ort und Jahreszeit etwa 8 bis 15 km hoch) hinaus gelangen kann. Überwiegend prägen die unteren 2 km derPeplosphäredas Wetter. Hier findet sich oft Dunst durch Anreicherung vonAerosolen, und die nächtlicheAbkühlungdurch Wärmestrahlung. DieBodenreibungbremst dengeostrophischen Wind, weshalb er mehr in Richtung zum tieferen Druck weht als in größerer Höhe. Der primäre Motor des Wetters ist die Energieeinstrahlung derSonneund die Abstrahlung (LichtundInfrarot) zu den Wolken bzw. in denWeltraum. Das erfassen heute nebenterrestrischenMessungen auch großräumig Satelliten und Wetterschiffe,Radiosondenund andere moderne Methoden gut. Für denVerlaufdes Wetters sind jedoch dieStrömungs-Verhältnisse in der Atmosphäre entscheidend, die von ihrer wechselndenFeuchtigkeitund den globalenWindsystemenabhängen, ferner von der regional unterschiedlichen Wärmereflexion der Erdoberfläche (Albedo), vom Gelände (insbesondere denGebirgen, Küsten und Wüsten) und von starken lokalen Einflüssen (zyklischeWinde, Neigung undBewuchsvonBerghängen…), und vom Widerstand gegen Winde, über den die Rauheit der Oberfläche (Wälder, Windschneisen, großeGebäudeusw.) entscheidet. Daher sind in Mitteleuropa nur dannlokal exakte Wetterprognosenmöglich, wenn alle diese Einzelheiten einerModellierungoder verlässlichenErfahrungzugänglich sind. Letztere wissen auch Laien zu nutzen – siehe die vielfach bewährtenBauernregelnmit „wetterzeigenden“ Bergen (Wetterstein, Wolkenstein usw.) oder typischenWolken-Formationen wieSchönwetter- und Schäfchenwolken,Nebel,Regen-und Fetzenwolken,Cirren,Föhnmauernusw. Hauptartikel:Wettervorhersage Ausgehend vom durch großflächige Messungen erfassten Wetter und damit dem Zustand der Atmosphäre werden in der MeteorologieWettermodellegenutzt, um die weitere Entwicklung des Wetters zu prognostizieren. Davon abgesehen ist es jedoch auch möglich, auf lokaler Ebene und mit vergleichsweise wenig Hilfsmitteln gute Vorhersagen zu geben, wozu jedoch auch mehr oder weniger umfangreiche Kenntnisse notwendig sind. Für eine Reihe von Unternehmen hat das Wetter Auswirkungen auf die betrieblichen Erfolgsgrößen. Klassische Beispiele dafür sind dieLandwirtschaftund dieGetränkeindustrie, bei denen Wetter sich stark auf den Umsatz auswirken kann. Während bei der Landwirtschaft überwiegend die Erntemengen betroffen sind, schwankt bei den Abfüllern vonMineralwasserundErfrischungsgetränkender Absatz in Abhängigkeit zur Temperatur. Zu den weiteren Branchen, bei denen sich das Wetter stark auswirken kann, gehören dieBaubranchesowie dieTourismus- undFreizeitindustrie. Für einige Unternehmen kann dasWetterrisikoso signifikant sein, dass es gezielt imRisikomanagementdes Unternehmens beobachtet und beispielsweise über so genannteWetterderivateabgesichert wird. Das Landgericht Cottbus beurteilte 2012 Wetter alshöhere Gewalt. Demnach geht schlechtes Wetter nicht zu Lasten des Auftraggebers; es gehört nicht zur Risikosphäre eines Bestellers von Bauleistungen.[1] Die Wetterlage spielt bei vielen kriegerischen Auseinandersetzungen eine wichtige Rolle. Beispiele: Seit Anfang der 1950er Jahre forscht auch das Militär über Möglichkeiten, das Wetter lokal zu beeinflussen. Eine Anwendung solcher Techniken wäre jedoch ein Verstoß gegen dieENMOD-Konvention. Wettergeschehen haben in verschiedenster Weise Auswirkungen auf das körperliche Befinden von Lebewesen. So schriebHerman BoerhaavesSchüler Thomas Schwenke, Verfasser der SchriftHaematologia, dem Wetter einen besonderen Einfluss auf dieBlutgerinnungzu.[2]Beim Menschen spricht man u. a. von „Wetterfühligkeit“, womit sich die Disziplin derMeteorotropiegenauer befasst. Für allgemein meteorologische Literatur sieheMeteorologie. Der Text ist unter der Lizenz„Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit denNutzungsbedingungenund derDatenschutzrichtlinieeinverstanden. Zum Inhalt springen Hauptseite Themenportale Zufälliger Artikel Spezialseiten Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Suche Jetzt spenden Benutzerkonto erstellen Anmelden Jetzt spenden Benutzerkonto erstellen Anmelden (Anfang) 1Begriffliche Abgrenzung 2Wetter in Meteorologie und Umgangssprache 3Elemente des Wetters und ihre Messung 4Faktoren des Wetters und ihre Dynamik 5Vorhersage des Wetters 6Wetter als wirtschaftlicher Faktor 7Einfluss des Wetters auf den Verlauf von Kriegen 8Physische Bedeutung 9Siehe auch 10Einzelnachweise 11Literatur 12Weblinks Afrikaans Alemannisch Aragonés Ænglisc العربية অসমীয়া Asturianu Авар अवधी Башҡортса Boarisch Žemaitėška Bikol Central Беларуская Беларуская (тарашкевіца) Betawi Български भोजपुरी বাংলা"
  }
]