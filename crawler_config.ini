# ============================================
# ALLGEMEINE CRAWLER-EINSTELLUNGEN
# ============================================

[CRAWLER]
# Seed URLs (komma-getrennt) - Start von Wikipedia Hauptseite
SEED_URLS = https://de.wikipedia.org/wiki/Wikipedia:Hauptseite

# Erlaubte Domains (komma-getrennt) - nur deutsche Wikipedia
ALLOWED_DOMAINS = de.wikipedia.org

# Ignorierte Wikipedia-Namespaces (komma-getrennt)
IGNORED_NAMESPACES = Benutzer:, Datei:, Spezial:, Kategorie:, Hilfe:, Diskussion:, Vorlage:, Portal:, MediaWiki:

# Batch-Größe für Best-N-First
BATCH_SIZE = 16

# Maximale Anzahl zu crawlender Seiten
MAX_PAGES = 50000

# Maximale Anzahl relevanter Seiten (Beendigungskriterium)
MAX_RELEVANT_PAGES = 30

# Relevanz-Schwellenwert (0.0 - 1.0)
RELEVANCE_THRESHOLD = 0.5

# Maximale Laufzeit in Minuten
MAX_RUNTIME_MINUTES = 30

# Report-Intervall in Sekunden (veraltet, siehe REPORTING)
REPORT_INTERVAL_SECONDS = 30

# Maximale Frontier-Größe (Speicher-Optimierung)
FRONTIER_MAX_SIZE = 3000

[REPORTING]
# Reporting nach Anzahl gecrawlter Seiten
REPORT_PAGES_INTERVAL = 25

# Reporting nach Zeit in Sekunden
REPORT_TIME_INTERVAL_SECONDS = 60

# Evaluierungs-Intervall (wird jetzt pro Batch gemacht, Parameter wird ignoriert)
EVALUATION_INTERVAL = 25

[WEIGHTS]
# Gewichtung für Link-Relevanz vs. Elterndokument-Relevanz
LINK_WEIGHT = 0.5
PARENT_WEIGHT = 0.5

# ============================================
# GEMEINSAME VECTORIZER EINSTELLUNGEN
# ============================================

[VECTORIZER_SETTINGS]
# TF-IDF Vectorizer Parameter (für VectorSpace und NaiveBayes bei MODE=tfidf)
# Optimiert für fokussiertere Feature-Extraktion
MAX_FEATURES = 500
NGRAM_MIN = 1
NGRAM_MAX = 2
MIN_DF = 3
MAX_DF = 0.7

# ============================================
# KEYWORD-SPIDER SPEZIFISCHE EINSTELLUNGEN
# ============================================

[KEYWORD]
# Keywords für Keyword-basierte Strategie (komma-getrennt)
KEYWORDS = künstliche intelligenz, machine learning, deep learning, neuronale netze, algorithmus, datenanalyse, ki, artificial intelligence, neural network, datenwissenschaft, maschinelles lernen, algorithmen, daten, modell, training, klassifikation, regression, clustering, supervised, unsupervised, reinforcement, tensorflow, pytorch, scikit, python, nlp, computer vision, bilderkennung, spracherkennung, robotik, automation, neuronales netz, deep, learning, intelligenz

# Multiplikatoren für Textteile
TITLE_MULTIPLIER = 4
HEADING_MULTIPLIER = 2
PARAGRAPH_MULTIPLIER = 1

# Normalisierungsfaktoren
# TEXT_NORM_FACTOR: Treffer pro 100 Wörter für Score 1.0 (je höher, desto strenger die Bewertung)
TEXT_NORM_FACTOR = 3.0

# PARENT_NORM_FACTOR: Gewichtete Treffer/Wörter für Score 1.0 (je höher, desto großzügiger die Bewertung)
PARENT_NORM_FACTOR = 100.0

# ============================================
# VECTORSPACE-SPIDER SPEZIFISCHE EINSTELLUNGEN
# ============================================

[VECTORSPACE]
# Keywords für Vokabular (bei tf_norm Modus)
KEYWORDS = künstliche intelligenz, machine learning, deep learning, neuronale netze, algorithmus, datenanalyse, ki, artificial intelligence, neural network, datenwissenschaft, maschinelles lernen, algorithmen, daten, modell, training, klassifikation, regression, clustering, supervised, unsupervised, reinforcement, tensorflow, pytorch, scikit, python, nlp, computer vision, bilderkennung, spracherkennung, robotik, automation, neuronales netz, deep, learning, intelligenz

# Pfad zu Trainingsdaten für IDF-Berechnung (bei tfidf Modus)
TRAINING_DATA_PATH = training_data/training_samples.json

# Vektorisierungs-Modus: tfidf oder tf_norm
MODE = tf_norm

# IDF Relevant-Ratio für TF-IDF Modus
# Steuert das Verhältnis von relevanten zu irrelevanten Dokumenten für IDF-Berechnung
# 1.0 = nur relevante Dokumente (100% relevant)
# 0.75 = 75% relevant, 25% irrelevant
# 0.5 = 50% relevant, 50% irrelevant (wie originaler Trainingssatz)
IDF_RELEVANT_RATIO = 0.75

# Multiplikatoren für Textteile - OPTIMIERT für Cosinus-Ähnlichkeit
# Reduzierte Werte für ausgewogenere Scores
TITLE_MULTIPLIER = 4
HEADING_MULTIPLIER = 2
PARAGRAPH_MULTIPLIER = 1

# ============================================
# NAIVEBAYES-SPIDER SPEZIFISCHE EINSTELLUNGEN
# ============================================

[NAIVEBAYES]
# Pfade für Naive Bayes Modell
MODEL_PATH = models/naive_bayes_model.pkl
VECTORIZER_PATH = models/vectorizer.pkl
TRAINING_DATA_PATH = training_data/training_samples.json

# Vektorisierungs-Modus: tfidf oder tf_norm (wie VectorSpace)
MODE = tfidf

# Bayes-spezifische Gewichtungen für Textteile (je höher, desto wichtiger)
BAYES_TITLE_WEIGHT = 0.4
BAYES_HEADING_WEIGHT = 0.3
BAYES_PARAGRAPH_WEIGHT = 0.3

# ============================================
# PLOTTING EINSTELLUNGEN
# ============================================

[PLOTTING]
# Automatische Grafik-Erstellung nach Crawl
CREATE_PLOTS = True

# Ausgabe-Verzeichnis für Grafiken
OUTPUT_DIRECTORY = plots

# ============================================
# TECHNISCHE EINSTELLUNGEN (SCRAPY)
# ============================================

[SCRAPY]
# Scrapy-spezifische Einstellungen
USER_AGENT = TopicalCrawler/1.0 (+http://example.com/bot)
ROBOTSTXT_OBEY = True
CONCURRENT_REQUESTS = 2
DOWNLOAD_DELAY = 0.5
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 0.5
AUTOTHROTTLE_MAX_DELAY = 3.0
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0