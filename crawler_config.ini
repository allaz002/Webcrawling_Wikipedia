[CRAWLER]
# Seed URLs (komma-getrennt)
SEED_URLS = https://de.wikipedia.org/wiki/Informatik

# Erlaubte Domains (komma-getrennt) - nur deutsche Wikipedia
ALLOWED_DOMAINS = de.wikipedia.org

# Ignorierte Wikipedia-Namespaces (komma-getrennt)
IGNORED_NAMESPACES = Benutzer:, Datei:, Spezial:, Kategorie:, Hilfe:, Diskussion:, Vorlage:, Portal:, MediaWiki:

# Batch-Größe für Best-N-First
BATCH_SIZE = 4

# Maximale Anzahl zu crawlender Seiten
MAX_PAGES = 1000

# Maximale Anzahl relevanter Seiten (Beendigungskriterium)
MAX_RELEVANT_PAGES = 100

# Relevanz-Schwellenwert (0.0 - 1.0)
RELEVANCE_THRESHOLD = 0.05

# Maximale Laufzeit in Minuten
MAX_RUNTIME_MINUTES = 30

# Report-Intervall in Sekunden (veraltet, siehe REPORTING)
REPORT_INTERVAL_SECONDS = 30

# Maximale Frontier-Größe (Speicher-Optimierung)
FRONTIER_MAX_SIZE = 10000

[REPORTING]
# Reporting nach Anzahl gecrawlter Seiten
REPORT_PAGES_INTERVAL = 50

# Reporting nach Zeit in Sekunden
REPORT_TIME_INTERVAL_SECONDS = 60

[WEIGHTS]
# Gewichtung für Link-Relevanz vs. Elterndokument-Relevanz
LINK_WEIGHT = 0.4
PARENT_WEIGHT = 0.6

# Gewichtungen für Elterndokument-Teile
TITLE_WEIGHT = 0.4
HEADING_WEIGHT = 0.3
PARAGRAPH_WEIGHT = 0.3

[MULTIPLIERS]
# Multiplikatoren für Textteile (ganzzahlig, >= 1)
# Verwendet von Keyword und VectorSpace Spiders
TITLE_MULTIPLIER = 8
HEADING_MULTIPLIER = 4
PARAGRAPH_MULTIPLIER = 2

[KEYWORDS]
# Keywords für Keyword-basierte Strategie (komma-getrennt)
KEYWORDS = ["Algorithmen", "Datenstrukturen", "Betriebssysteme", "Compilerbau", "Assembler", "Netzwerktechnik", "TCP/IP", "HTTP", "REST-API", "Datenbanken", "SQL", "NoSQL", "Graphdatenbanken", "Big Data", "Data Mining", "Data Science", "Künstliche Intelligenz", "Maschinelles Lernen", "Deep Learning", "Neuronale Netze", "Reinforcement Learning", "Computer Vision", "Natural Language Processing", "Bildverarbeitung", "Robotik", "Quantencomputing", "Cybersecurity", "Kryptographie", "Blockchain", "Cloud Computing", "Edge Computing", "Virtualisierung", "Docker", "Kubernetes", "Microservices", "DevOps", "CI/CD", "Software-Engineering", "Agiles Projektmanagement", "Testautomatisierung", "Unit Testing", "TDD", "UI/UX-Design", "Mensch-Computer-Interaktion", "Graphentheorie", "Parallelverarbeitung", "Verteilte Systeme", "Echtzeitsysteme", "FPGA", "IoT"]

[VECTORSPACE]
# Themenprofil-Text für Vektorraum-Modell
TOPIC_PROFILE = Künstliche Intelligenz (KI) ist ein Teilgebiet der Informatik, das sich mit der Automatisierung intelligenten Verhaltens und dem maschinellen Lernen befasst. Machine Learning und Deep Learning sind wichtige Untergebiete der KI. Neuronale Netze, Algorithmen und Datenanalyse sind zentrale Konzepte. Die Anwendung von KI reicht von der Bilderkennung über Sprachverarbeitung bis zur Robotik. Maschinelles Lernen ermöglicht es Computern, aus Daten zu lernen ohne explizit programmiert zu werden.

# Vektorisierungs-Modus: tfidf oder tf_norm (default: tf_norm)
MODE = tf_norm

[NAIVEBAYES]
# Pfade für Naive Bayes Modell
MODEL_PATH = models/naive_bayes_model.pkl
VECTORIZER_PATH = models/vectorizer.pkl
TRAINING_DATA_PATH = training_data/training_samples.txt

[SCRAPY]
# Scrapy-spezifische Einstellungen
USER_AGENT = TopicalCrawler/1.0 (+http://example.com/bot)
ROBOTSTXT_OBEY = False
CONCURRENT_REQUESTS = 4
DOWNLOAD_DELAY = 0
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 0
AUTOTHROTTLE_MAX_DELAY = 0
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0